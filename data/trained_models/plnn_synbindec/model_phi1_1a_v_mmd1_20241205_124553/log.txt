Args:
Namespace(name='model_phi1_1a_v_mmd1', outdir='out/model_training/model_phi1_1a_v_mmd1', training_data='data/training_data/data_phi1_1a/training', validation_data='data/training_data/data_phi1_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 261475185

Training model...

Saving initial model state to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 4.597989724645309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.597989724645309 | validation: 4.328872727911915]
	TIME [epoch: 405 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.9890984375628022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9890984375628022 | validation: 3.892106831419711]
	TIME [epoch: 6.2 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.777383066040835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.777383066040835 | validation: 3.8368544527491046]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.5561789755443463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5561789755443463 | validation: 3.6087951546261605]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.451709236049412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.451709236049412 | validation: 3.68279074084921]
	TIME [epoch: 6.14 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.363763473005462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.363763473005462 | validation: 3.5115418894871175]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1569025432539957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1569025432539957 | validation: 3.5243562388252294]
	TIME [epoch: 6.13 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0209843012451025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0209843012451025 | validation: 3.3878637393244757]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.8045588976491773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8045588976491773 | validation: 3.007324320090378]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.6287471038801415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6287471038801415 | validation: 2.5545899801611918]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.2826342662481878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2826342662481878 | validation: 2.380572862461996]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.03050581138712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.03050581138712 | validation: 2.274483390015896]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.6939410510983797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6939410510983797 | validation: 2.0013507994395523]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4039651260383168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4039651260383168 | validation: 2.1700838525703]
	TIME [epoch: 6.13 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.391599693925337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.391599693925337 | validation: 2.3285828250055847]
	TIME [epoch: 6.13 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.5925593685459076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5925593685459076 | validation: 1.955250265212229]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3114669511625026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3114669511625026 | validation: 2.0144408998942507]
	TIME [epoch: 6.14 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3472284914599477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3472284914599477 | validation: 1.8874712325064817]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.239690137317448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.239690137317448 | validation: 1.872097704301717]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4863003240425776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4863003240425776 | validation: 2.1076333641419875]
	TIME [epoch: 6.14 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2670721547506356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2670721547506356 | validation: 1.863903623992162]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1407126345251855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1407126345251855 | validation: 1.8807293658298727]
	TIME [epoch: 6.13 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.4443731871207148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4443731871207148 | validation: 1.8617683720738705]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.3698208942205379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3698208942205379 | validation: 1.632529720400464]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2672065583018592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2672065583018592 | validation: 2.667049552173081]
	TIME [epoch: 6.13 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.683457333219955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.683457333219955 | validation: 1.5253981666397252]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1167974675075167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1167974675075167 | validation: 1.116425967889157]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7579214007888538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7579214007888538 | validation: 1.2363715325826055]
	TIME [epoch: 6.14 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.1477120426127507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1477120426127507 | validation: 0.9744158442802073]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7124329684624127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7124329684624127 | validation: 0.8354549835375185]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.7179717201727199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179717201727199 | validation: 1.174450897218184]
	TIME [epoch: 6.14 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.0652991317068226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0652991317068226 | validation: 1.0681564618063932]
	TIME [epoch: 6.13 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6544726318372663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6544726318372663 | validation: 0.8380787969839858]
	TIME [epoch: 6.13 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.9411212824189044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9411212824189044 | validation: 1.1700076774403456]
	TIME [epoch: 6.13 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6774412698258763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774412698258763 | validation: 0.6007186900759662]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6107869031115893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6107869031115893 | validation: 1.039508950715152]
	TIME [epoch: 6.14 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8296158772877837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8296158772877837 | validation: 0.5745396454632802]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5433416070678162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5433416070678162 | validation: 1.0368847641710464]
	TIME [epoch: 6.14 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6690645269302755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690645269302755 | validation: 0.5475133656130157]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4959659550196102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4959659550196102 | validation: 0.6756235569875191]
	TIME [epoch: 6.14 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6483421207323092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6483421207323092 | validation: 0.5047217496802925]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.563786211389365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.563786211389365 | validation: 0.6249300143376467]
	TIME [epoch: 6.14 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4708901848419766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4708901848419766 | validation: 0.9868772371038212]
	TIME [epoch: 6.13 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.8380062665867081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8380062665867081 | validation: 0.5730219750126856]
	TIME [epoch: 6.13 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.49111819325207895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49111819325207895 | validation: 0.8107001523875941]
	TIME [epoch: 6.14 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5550403713878914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5550403713878914 | validation: 0.48794520055297896]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5598462627295012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5598462627295012 | validation: 0.4888544903869725]
	TIME [epoch: 6.14 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5031812538120819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5031812538120819 | validation: 0.8588928692446652]
	TIME [epoch: 6.14 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5553375384360068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5553375384360068 | validation: 0.4869459829993943]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5571188432330563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5571188432330563 | validation: 0.664741431879004]
	TIME [epoch: 6.13 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5259791517953148		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.5259791517953148 | validation: 0.5567176272357837]
	TIME [epoch: 6.13 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5186259511326277		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.5186259511326277 | validation: 0.5175528007576583]
	TIME [epoch: 6.11 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44033108473201255		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.44033108473201255 | validation: 0.5636600356293819]
	TIME [epoch: 6.12 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.398173431266154		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.398173431266154 | validation: 0.5081111923041765]
	TIME [epoch: 6.13 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5711276844817588		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.5711276844817588 | validation: 0.5407334507599726]
	TIME [epoch: 6.13 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40851236258772383		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.40851236258772383 | validation: 0.3857558969263003]
	TIME [epoch: 6.61 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5188402668416003		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.5188402668416003 | validation: 0.4114736102407302]
	TIME [epoch: 6.13 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5190336582889158		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.5190336582889158 | validation: 0.6323161272577125]
	TIME [epoch: 6.13 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4271388117948027		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.4271388117948027 | validation: 0.38554458283078535]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36733013267542003		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.36733013267542003 | validation: 0.5284377629267033]
	TIME [epoch: 6.13 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5128823651605419		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.5128823651605419 | validation: 0.42350159432309664]
	TIME [epoch: 6.12 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40159271681459274		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.40159271681459274 | validation: 0.4366410234955328]
	TIME [epoch: 6.12 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.078166837195435		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 1.078166837195435 | validation: 0.9433702563199353]
	TIME [epoch: 6.13 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5711011792055782		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.5711011792055782 | validation: 0.4787783640971838]
	TIME [epoch: 6.12 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3622720364714441		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.3622720364714441 | validation: 0.43960314867849837]
	TIME [epoch: 6.12 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33148246733201114		[learning rate: 0.0094573]
	Learning Rate: 0.00945735
	LOSS [training: 0.33148246733201114 | validation: 0.36267421736079364]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3097003062646362		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.3097003062646362 | validation: 0.6630018409710633]
	TIME [epoch: 6.13 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4833792907001714		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.4833792907001714 | validation: 0.39571899249661313]
	TIME [epoch: 6.12 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3473228413671486		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.3473228413671486 | validation: 0.2900927583991364]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3828359916747811		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.3828359916747811 | validation: 0.3877735430016311]
	TIME [epoch: 6.13 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46165306549949703		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.46165306549949703 | validation: 0.3428647953231787]
	TIME [epoch: 6.12 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31676968427207797		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.31676968427207797 | validation: 0.37185732199452304]
	TIME [epoch: 6.12 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3618174692426935		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.3618174692426935 | validation: 0.5279900499720667]
	TIME [epoch: 6.12 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3445412463792386		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.3445412463792386 | validation: 0.31144579955799523]
	TIME [epoch: 6.13 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3552445024961131		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.3552445024961131 | validation: 0.6904422931641587]
	TIME [epoch: 6.12 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3974267718093042		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.3974267718093042 | validation: 0.26890193662674583]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2808177110234433		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.2808177110234433 | validation: 0.31709933910667454]
	TIME [epoch: 6.13 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.44811985433538226		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.44811985433538226 | validation: 0.5776472638069707]
	TIME [epoch: 6.12 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40819562447742425		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.40819562447742425 | validation: 0.26724868181867734]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28174756343156876		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.28174756343156876 | validation: 0.3654914078681434]
	TIME [epoch: 6.13 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3527433346085501		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.3527433346085501 | validation: 0.3242760249160688]
	TIME [epoch: 6.12 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3296861501715803		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.3296861501715803 | validation: 0.6040387798419244]
	TIME [epoch: 6.12 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42492695413608805		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.42492695413608805 | validation: 0.45381927129794897]
	TIME [epoch: 6.12 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33172021495605386		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.33172021495605386 | validation: 0.28696889316896046]
	TIME [epoch: 6.12 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.346005054786685		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.346005054786685 | validation: 0.43580264742877284]
	TIME [epoch: 6.12 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31740666481878327		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.31740666481878327 | validation: 0.31259852170559876]
	TIME [epoch: 6.11 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31871668301154166		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.31871668301154166 | validation: 0.3262473536497016]
	TIME [epoch: 6.12 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.287025255582972		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.287025255582972 | validation: 0.27397797112047034]
	TIME [epoch: 6.12 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30383410450662174		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.30383410450662174 | validation: 0.28153031549739205]
	TIME [epoch: 6.12 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26829277282434083		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.26829277282434083 | validation: 0.44398828728333606]
	TIME [epoch: 6.12 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3982472270072763		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.3982472270072763 | validation: 0.29608363280453315]
	TIME [epoch: 6.12 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.34783232188401003		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.34783232188401003 | validation: 0.3990837908831839]
	TIME [epoch: 6.12 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2754946261786883		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.2754946261786883 | validation: 0.21826689152049317]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2964913529943115		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.2964913529943115 | validation: 0.3754996908728939]
	TIME [epoch: 6.13 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29681831009826576		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.29681831009826576 | validation: 0.5687060669512108]
	TIME [epoch: 6.12 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3515047149108661		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.3515047149108661 | validation: 0.327622062828708]
	TIME [epoch: 6.12 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2949843052934524		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.2949843052934524 | validation: 0.21893360380955768]
	TIME [epoch: 6.13 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2608354641359637		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.2608354641359637 | validation: 0.3200054215188659]
	TIME [epoch: 6.13 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2826244497420118		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.2826244497420118 | validation: 0.23176741083376418]
	TIME [epoch: 6.12 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2564237177817401		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.2564237177817401 | validation: 0.3563782797399116]
	TIME [epoch: 6.12 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2969853503503056		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.2969853503503056 | validation: 0.4153436779662725]
	TIME [epoch: 6.13 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3097863891056503		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.3097863891056503 | validation: 0.31303487267095687]
	TIME [epoch: 6.12 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2825689005433547		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.2825689005433547 | validation: 0.2850054270306693]
	TIME [epoch: 6.13 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.364798483765917		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.364798483765917 | validation: 0.3067928389466994]
	TIME [epoch: 6.13 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2851765108583935		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.2851765108583935 | validation: 0.21743917685233477]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25008795945290774		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.25008795945290774 | validation: 0.357680218851494]
	TIME [epoch: 6.15 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4073142375884457		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.4073142375884457 | validation: 0.3625598603461708]
	TIME [epoch: 6.14 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2878002700852029		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.2878002700852029 | validation: 0.21385725066105016]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23486006226899922		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.23486006226899922 | validation: 0.20706656750145666]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.293836203671667		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.293836203671667 | validation: 0.23655678963836446]
	TIME [epoch: 6.13 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2408413954721271		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.2408413954721271 | validation: 0.3369159496070963]
	TIME [epoch: 6.12 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27143599139615054		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.27143599139615054 | validation: 0.25836178919954234]
	TIME [epoch: 6.13 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28938613483229486		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.28938613483229486 | validation: 0.27822069922466164]
	TIME [epoch: 6.61 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2515690523148408		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.2515690523148408 | validation: 0.25905009681909796]
	TIME [epoch: 6.12 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2720272878200092		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.2720272878200092 | validation: 0.3124423831846689]
	TIME [epoch: 6.12 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3137429745644529		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.3137429745644529 | validation: 0.4062765229153472]
	TIME [epoch: 6.13 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3299149269769459		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.3299149269769459 | validation: 0.2392700226245985]
	TIME [epoch: 6.13 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22815348082234138		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.22815348082234138 | validation: 0.27688849017975653]
	TIME [epoch: 6.12 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24136431392373722		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.24136431392373722 | validation: 0.30491154366523765]
	TIME [epoch: 6.11 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2471286972129175		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.2471286972129175 | validation: 0.20476454296106222]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23446561281431655		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.23446561281431655 | validation: 0.23103407801511147]
	TIME [epoch: 6.12 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28894542299559045		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.28894542299559045 | validation: 0.2298931621867108]
	TIME [epoch: 6.12 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2042951784001178		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.2042951784001178 | validation: 0.21854392593448912]
	TIME [epoch: 6.12 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21378824653238676		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.21378824653238676 | validation: 0.37427256671331643]
	TIME [epoch: 6.12 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3261622851257452		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.3261622851257452 | validation: 0.22547355155931992]
	TIME [epoch: 6.12 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2683385785337627		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2683385785337627 | validation: 0.2343267178839179]
	TIME [epoch: 6.12 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20861678861417926		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.20861678861417926 | validation: 0.3489201081695683]
	TIME [epoch: 6.12 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3086722014102557		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.3086722014102557 | validation: 0.3193577522859697]
	TIME [epoch: 6.12 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25479568359848087		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.25479568359848087 | validation: 0.17338363518706737]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23460142435974118		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.23460142435974118 | validation: 0.21432103730932306]
	TIME [epoch: 6.13 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2307971153564712		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.2307971153564712 | validation: 0.1772555128094781]
	TIME [epoch: 6.12 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27882451845123546		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.27882451845123546 | validation: 0.3749004246997388]
	TIME [epoch: 6.12 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27180657320898766		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.27180657320898766 | validation: 0.2571479581352995]
	TIME [epoch: 6.12 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21123036516395405		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.21123036516395405 | validation: 0.15868498284728388]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23552050123360735		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.23552050123360735 | validation: 0.1918125535360351]
	TIME [epoch: 6.14 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2263969488569461		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.2263969488569461 | validation: 0.22048971764765468]
	TIME [epoch: 6.12 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18421092257199922		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.18421092257199922 | validation: 0.2291582371420875]
	TIME [epoch: 6.12 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26359164079212766		[learning rate: 0.0073282]
	Learning Rate: 0.00732824
	LOSS [training: 0.26359164079212766 | validation: 0.22186460836106478]
	TIME [epoch: 6.13 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2520020979467018		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.2520020979467018 | validation: 0.2913478431457007]
	TIME [epoch: 6.12 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23654356197355528		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.23654356197355528 | validation: 0.2044756293187697]
	TIME [epoch: 6.12 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17902221704890953		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.17902221704890953 | validation: 0.2609353759925146]
	TIME [epoch: 6.12 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24386471126759662		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.24386471126759662 | validation: 0.3123782038730798]
	TIME [epoch: 6.12 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2335845544397473		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.2335845544397473 | validation: 0.1842220082639969]
	TIME [epoch: 6.12 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2277021955147628		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.2277021955147628 | validation: 0.39463328206921366]
	TIME [epoch: 6.12 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2695767567995669		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.2695767567995669 | validation: 0.15891692896977477]
	TIME [epoch: 6.12 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2407362518280956		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.2407362518280956 | validation: 0.2510049703929637]
	TIME [epoch: 6.12 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2449119755943777		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.2449119755943777 | validation: 0.24039107539447782]
	TIME [epoch: 6.12 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2162534498825141		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.2162534498825141 | validation: 0.15267503939129456]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1767971894924951		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.1767971894924951 | validation: 0.18318406286832328]
	TIME [epoch: 6.14 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25058581666527646		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.25058581666527646 | validation: 0.2410969508656579]
	TIME [epoch: 6.13 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22148837261022025		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.22148837261022025 | validation: 0.17732653025190478]
	TIME [epoch: 6.13 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17991058446380723		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.17991058446380723 | validation: 0.19438017264277296]
	TIME [epoch: 6.13 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17750303149434882		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.17750303149434882 | validation: 0.17402737745396274]
	TIME [epoch: 6.13 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22582227892470694		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.22582227892470694 | validation: 0.18411039030764292]
	TIME [epoch: 6.13 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20339549968608758		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.20339549968608758 | validation: 0.20675311141301522]
	TIME [epoch: 6.13 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1984301135616698		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.1984301135616698 | validation: 0.2055927573199227]
	TIME [epoch: 6.14 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21854430910914452		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.21854430910914452 | validation: 0.18998992989598118]
	TIME [epoch: 6.14 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20372278588850065		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.20372278588850065 | validation: 0.15899750393281784]
	TIME [epoch: 6.13 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2191454586133308		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2191454586133308 | validation: 0.234120376451762]
	TIME [epoch: 6.14 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5157764121527655		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.5157764121527655 | validation: 1.0123815344346674]
	TIME [epoch: 6.13 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6060248094334562		[learning rate: 0.0067548]
	Learning Rate: 0.00675484
	LOSS [training: 0.6060248094334562 | validation: 0.23208189992463002]
	TIME [epoch: 6.13 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23174513665049615		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.23174513665049615 | validation: 0.15181923725091773]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16506682337396378		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.16506682337396378 | validation: 0.1966417356172085]
	TIME [epoch: 6.13 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15621241626618929		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.15621241626618929 | validation: 0.14041260352069196]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14526692531508953		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.14526692531508953 | validation: 0.1648735720423718]
	TIME [epoch: 6.14 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17943849146044333		[learning rate: 0.0066363]
	Learning Rate: 0.00663626
	LOSS [training: 0.17943849146044333 | validation: 0.1241934619480555]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15091799338061446		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.15091799338061446 | validation: 0.16960064714175543]
	TIME [epoch: 6.14 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.198333112598156		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.198333112598156 | validation: 0.15541357881845363]
	TIME [epoch: 6.13 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1963258166833795		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.1963258166833795 | validation: 0.1474248477908069]
	TIME [epoch: 6.13 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15435263577291816		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.15435263577291816 | validation: 0.10444094301157464]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16028290873500065		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.16028290873500065 | validation: 0.25575478061816925]
	TIME [epoch: 6.13 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25431331803465573		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.25431331803465573 | validation: 0.18964119067928129]
	TIME [epoch: 6.12 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19467900356153095		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.19467900356153095 | validation: 0.1357508190662431]
	TIME [epoch: 6.12 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17490158614783352		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.17490158614783352 | validation: 0.15854704543850134]
	TIME [epoch: 6.12 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1490751256045643		[learning rate: 0.006428]
	Learning Rate: 0.00642802
	LOSS [training: 0.1490751256045643 | validation: 0.29220021459729406]
	TIME [epoch: 6.12 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17551900200367082		[learning rate: 0.0064053]
	Learning Rate: 0.00640528
	LOSS [training: 0.17551900200367082 | validation: 0.1136479214608779]
	TIME [epoch: 6.13 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24771842290631688		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.24771842290631688 | validation: 0.2574265086835217]
	TIME [epoch: 6.12 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18250407339055463		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.18250407339055463 | validation: 0.13436772241882833]
	TIME [epoch: 6.11 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14102456654048237		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.14102456654048237 | validation: 0.11728828941538746]
	TIME [epoch: 6.12 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1476127058033852		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.1476127058033852 | validation: 0.3035943987447668]
	TIME [epoch: 6.12 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21164961117990627		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.21164961117990627 | validation: 0.22041727890835966]
	TIME [epoch: 6.12 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15492118352549591		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.15492118352549591 | validation: 0.10328363982349657]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24408380147618303		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.24408380147618303 | validation: 0.1653021674211822]
	TIME [epoch: 6.13 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15290080810984408		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.15290080810984408 | validation: 0.12275108106234778]
	TIME [epoch: 6.13 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13426410688744156		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.13426410688744156 | validation: 0.24569034903815323]
	TIME [epoch: 6.13 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17753320054497343		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.17753320054497343 | validation: 0.19946288411002736]
	TIME [epoch: 6.13 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13408420930162224		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.13408420930162224 | validation: 0.14293772982506958]
	TIME [epoch: 6.13 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20536176367052153		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.20536176367052153 | validation: 0.1875585414139238]
	TIME [epoch: 6.13 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1905352448256652		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.1905352448256652 | validation: 0.12783888440638977]
	TIME [epoch: 6.13 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1520438019467662		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.1520438019467662 | validation: 0.08321981705922163]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16071893719803138		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.16071893719803138 | validation: 0.13454148895639223]
	TIME [epoch: 6.13 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15335593570259845		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.15335593570259845 | validation: 0.12268340503230848]
	TIME [epoch: 6.13 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17072623350422053		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.17072623350422053 | validation: 0.10493550931351622]
	TIME [epoch: 6.14 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11012991666076802		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.11012991666076802 | validation: 0.10962992161795301]
	TIME [epoch: 6.14 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14752746142714376		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.14752746142714376 | validation: 0.212550213262054]
	TIME [epoch: 6.14 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17355536566836485		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.17355536566836485 | validation: 0.14376335831817394]
	TIME [epoch: 6.13 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13349144512671338		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.13349144512671338 | validation: 0.16257906856565582]
	TIME [epoch: 6.14 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14184304891473676		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.14184304891473676 | validation: 0.07511083111552455]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_198.pth
	Model improved!!!
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13918700122158062		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.13918700122158062 | validation: 0.17190971805300145]
	TIME [epoch: 6.14 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1437202093283652		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.1437202093283652 | validation: 0.2060306494093144]
	TIME [epoch: 6.13 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18013854690814837		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.18013854690814837 | validation: 0.21478835082249856]
	TIME [epoch: 428 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14363113320079407		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.14363113320079407 | validation: 0.11166742568701692]
	TIME [epoch: 12.1 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13856728640069238		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.13856728640069238 | validation: 0.31370456195900287]
	TIME [epoch: 12.1 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19884590406300703		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.19884590406300703 | validation: 0.16640893337165966]
	TIME [epoch: 12.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1169191636294933		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.1169191636294933 | validation: 0.14239647256585783]
	TIME [epoch: 12.1 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10400263746970563		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.10400263746970563 | validation: 0.0850548602885351]
	TIME [epoch: 12.1 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09202441546440021		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.09202441546440021 | validation: 0.08132670920777096]
	TIME [epoch: 12.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11219827209650673		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.11219827209650673 | validation: 0.16926111038030736]
	TIME [epoch: 12.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1275822622126574		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.1275822622126574 | validation: 0.10844839806467091]
	TIME [epoch: 12.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14005546427197646		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.14005546427197646 | validation: 0.13046196511033206]
	TIME [epoch: 12.1 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11736262264253833		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.11736262264253833 | validation: 0.15642016138042705]
	TIME [epoch: 12.1 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1914110994136787		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.1914110994136787 | validation: 0.27841301645306726]
	TIME [epoch: 12.1 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17644070276642335		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.17644070276642335 | validation: 0.13542822101132426]
	TIME [epoch: 12.1 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13076397600737036		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.13076397600737036 | validation: 0.0880291882945297]
	TIME [epoch: 12.1 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13061570035192804		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.13061570035192804 | validation: 0.09996482978626238]
	TIME [epoch: 12.1 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1482923590580072		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.1482923590580072 | validation: 0.11634673208891327]
	TIME [epoch: 12.1 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11106567341607143		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.11106567341607143 | validation: 0.1863092792003142]
	TIME [epoch: 12.1 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1509032119236429		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.1509032119236429 | validation: 0.13541191470627167]
	TIME [epoch: 12.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16393849440456432		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.16393849440456432 | validation: 0.13857770900534058]
	TIME [epoch: 12.1 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10029036440368125		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.10029036440368125 | validation: 0.1064067313190453]
	TIME [epoch: 12.1 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1156427466616536		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.1156427466616536 | validation: 0.1512886317730603]
	TIME [epoch: 12.1 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12294616506475363		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.12294616506475363 | validation: 0.14109141936449598]
	TIME [epoch: 12.1 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11501070574849154		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.11501070574849154 | validation: 0.11569251584885851]
	TIME [epoch: 12.1 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12413961980388043		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.12413961980388043 | validation: 0.08425171366640372]
	TIME [epoch: 12.1 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09970425936784699		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.09970425936784699 | validation: 0.08176625306042172]
	TIME [epoch: 12.1 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11155143265103468		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.11155143265103468 | validation: 0.08566049352710325]
	TIME [epoch: 12.1 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09073806202338093		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.09073806202338093 | validation: 0.12132942776630885]
	TIME [epoch: 12.1 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08976774770289649		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.08976774770289649 | validation: 0.08148985622542183]
	TIME [epoch: 12.1 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10597366763224211		[learning rate: 0.0053088]
	Learning Rate: 0.00530884
	LOSS [training: 0.10597366763224211 | validation: 0.13682567279576918]
	TIME [epoch: 12.1 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12139259915171056		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.12139259915171056 | validation: 0.16074944839912111]
	TIME [epoch: 12.1 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13608306364671552		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.13608306364671552 | validation: 0.15559812797264588]
	TIME [epoch: 12.1 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1173639276071688		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.1173639276071688 | validation: 0.10306314423666244]
	TIME [epoch: 12.1 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0953144030628961		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.0953144030628961 | validation: 0.12403100178478227]
	TIME [epoch: 12.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0878102443971313		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.0878102443971313 | validation: 0.18870851740994088]
	TIME [epoch: 12.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1336529111983431		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.1336529111983431 | validation: 0.17348009226295466]
	TIME [epoch: 12.1 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12501717841756263		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.12501717841756263 | validation: 0.14971108388567672]
	TIME [epoch: 12.1 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10557728472037521		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.10557728472037521 | validation: 0.12755633754210496]
	TIME [epoch: 12.1 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1502991838952304		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.1502991838952304 | validation: 0.06475806899549705]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07696659810673843		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.07696659810673843 | validation: 0.06786250898923146]
	TIME [epoch: 12.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06291135963156178		[learning rate: 0.005106]
	Learning Rate: 0.00510595
	LOSS [training: 0.06291135963156178 | validation: 0.12108604257733319]
	TIME [epoch: 12.1 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13447387906147668		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.13447387906147668 | validation: 0.08212988982448922]
	TIME [epoch: 12.1 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07971390424422442		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.07971390424422442 | validation: 0.05549413629093343]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10278681154928981		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.10278681154928981 | validation: 0.09472725713845238]
	TIME [epoch: 12.1 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07951792692109477		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.07951792692109477 | validation: 0.06494452431461435]
	TIME [epoch: 12.1 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08746834072702027		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.08746834072702027 | validation: 0.06164826688722542]
	TIME [epoch: 12.1 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08561237178906932		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.08561237178906932 | validation: 0.06466109236300557]
	TIME [epoch: 12.1 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0944763969729224		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.0944763969729224 | validation: 0.05167977195879096]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07535108827240412		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.07535108827240412 | validation: 0.05986374112904072]
	TIME [epoch: 12.1 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07508530999933057		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.07508530999933057 | validation: 0.14517252649739376]
	TIME [epoch: 12.1 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12114889643752479		[learning rate: 0.0049282]
	Learning Rate: 0.00492825
	LOSS [training: 0.12114889643752479 | validation: 0.07412486149625061]
	TIME [epoch: 12.1 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07961693967421617		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.07961693967421617 | validation: 0.0822848184221293]
	TIME [epoch: 12.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08618421200546324		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.08618421200546324 | validation: 0.0764015923306624]
	TIME [epoch: 12.1 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12129568780669138		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.12129568780669138 | validation: 0.07348287939512879]
	TIME [epoch: 12.1 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07469147546752809		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.07469147546752809 | validation: 0.05864683257274013]
	TIME [epoch: 12.1 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0607936064536581		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.0607936064536581 | validation: 0.06291997898474933]
	TIME [epoch: 12.1 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07520482475788037		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.07520482475788037 | validation: 0.1353860728914001]
	TIME [epoch: 12.1 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08668206493562375		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.08668206493562375 | validation: 0.10179509004359119]
	TIME [epoch: 12.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09809538426596535		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.09809538426596535 | validation: 0.05144898803211215]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06950463277787933		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.06950463277787933 | validation: 0.07172124337045609]
	TIME [epoch: 12.1 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12190829634176076		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.12190829634176076 | validation: 0.15444962516098035]
	TIME [epoch: 12.1 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09917890108951527		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.09917890108951527 | validation: 0.04798466252471083]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.051837950164913875		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.051837950164913875 | validation: 0.08523906110392128]
	TIME [epoch: 12.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06498143534887532		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.06498143534887532 | validation: 0.03749874393747629]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10440626192749025		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.10440626192749025 | validation: 0.11582382119113382]
	TIME [epoch: 12.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08763071893684346		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.08763071893684346 | validation: 0.08983212252270352]
	TIME [epoch: 12.1 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08453889801785446		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.08453889801785446 | validation: 0.056392304763985664]
	TIME [epoch: 12.1 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08219637694037793		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.08219637694037793 | validation: 0.053832547637203174]
	TIME [epoch: 12.1 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.059482115880703804		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.059482115880703804 | validation: 0.05869493910550086]
	TIME [epoch: 12.1 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0919811338392964		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.0919811338392964 | validation: 0.08673529267527405]
	TIME [epoch: 12.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07153177948976203		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.07153177948976203 | validation: 0.051781696725357304]
	TIME [epoch: 12.1 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044797727901495804		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.044797727901495804 | validation: 0.038004168374193945]
	TIME [epoch: 12.1 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06020059886106205		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.06020059886106205 | validation: 0.1676685817500304]
	TIME [epoch: 12.1 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1183149625513461		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.1183149625513461 | validation: 0.0926303398639198]
	TIME [epoch: 12.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0684090653865186		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.0684090653865186 | validation: 0.0387119027325526]
	TIME [epoch: 12.1 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05400775598124662		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.05400775598124662 | validation: 0.07224839963841886]
	TIME [epoch: 12.1 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046255474192904864		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.046255474192904864 | validation: 0.04795111539703839]
	TIME [epoch: 12.1 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054848034612038285		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.054848034612038285 | validation: 0.0573404291194485]
	TIME [epoch: 12.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08348022222079277		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.08348022222079277 | validation: 0.10338782592425379]
	TIME [epoch: 12.1 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08227012726496272		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.08227012726496272 | validation: 0.08706128313837508]
	TIME [epoch: 12.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10788835561725889		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.10788835561725889 | validation: 0.09152795309342289]
	TIME [epoch: 12.1 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05164568189553746		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.05164568189553746 | validation: 0.03190167816588425]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.056442969094261664		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.056442969094261664 | validation: 0.06684996188692116]
	TIME [epoch: 12.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05709817108252531		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.05709817108252531 | validation: 0.07887607034270996]
	TIME [epoch: 12.1 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09008091661258683		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.09008091661258683 | validation: 0.08802154474876894]
	TIME [epoch: 12.1 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06848667681374537		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.06848667681374537 | validation: 0.040704211679960314]
	TIME [epoch: 12.1 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054240606354950816		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.054240606354950816 | validation: 0.03467940041460178]
	TIME [epoch: 12.1 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05572862027654005		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.05572862027654005 | validation: 0.09837405866626754]
	TIME [epoch: 12.1 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0816453371659609		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.0816453371659609 | validation: 0.05997132348317709]
	TIME [epoch: 12.1 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0517951883309212		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.0517951883309212 | validation: 0.032604271552379]
	TIME [epoch: 12.1 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04659136829994721		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.04659136829994721 | validation: 0.09039900239216814]
	TIME [epoch: 12.1 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06869155197853577		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.06869155197853577 | validation: 0.11573596189883775]
	TIME [epoch: 12.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.057303069102248215		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.057303069102248215 | validation: 0.03579858399564414]
	TIME [epoch: 12.1 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038617810108116164		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.038617810108116164 | validation: 0.027978098608199102]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04621227742198955		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.04621227742198955 | validation: 0.053297317043312614]
	TIME [epoch: 12.1 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06523814649460268		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.06523814649460268 | validation: 0.02545033936354987]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_295.pth
	Model improved!!!
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040405565866689856		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.040405565866689856 | validation: 0.04075363606230803]
	TIME [epoch: 12.1 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05394224924637954		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.05394224924637954 | validation: 0.09341127742231378]
	TIME [epoch: 12.1 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05701173036032415		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.05701173036032415 | validation: 0.02774430580249387]
	TIME [epoch: 12.1 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04265057086352908		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.04265057086352908 | validation: 0.04550511418285192]
	TIME [epoch: 12.1 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05285931324062807		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.05285931324062807 | validation: 0.055815498752328524]
	TIME [epoch: 12.1 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04411862529947898		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.04411862529947898 | validation: 0.0557391571139846]
	TIME [epoch: 12.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06633356075344964		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.06633356075344964 | validation: 0.05728682064860577]
	TIME [epoch: 12.1 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04939187926526957		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.04939187926526957 | validation: 0.04539514164719152]
	TIME [epoch: 12.1 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0467617276084399		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.0467617276084399 | validation: 0.06425262246270351]
	TIME [epoch: 12.1 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1093816990823312		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.1093816990823312 | validation: 0.09577442934238267]
	TIME [epoch: 12.1 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06018482651930414		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.06018482651930414 | validation: 0.06568403852628646]
	TIME [epoch: 12.1 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04019774746745663		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.04019774746745663 | validation: 0.05093818154601429]
	TIME [epoch: 12.1 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05926849375620801		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.05926849375620801 | validation: 0.022782125000158693]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03814173523173402		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.03814173523173402 | validation: 0.03874082792419852]
	TIME [epoch: 12.1 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0468971755177856		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.0468971755177856 | validation: 0.12636749048355042]
	TIME [epoch: 12.1 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09139337439641365		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.09139337439641365 | validation: 0.045549304079949296]
	TIME [epoch: 12.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058826286639426933		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.058826286639426933 | validation: 0.02612754144645576]
	TIME [epoch: 12.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03725375905005224		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.03725375905005224 | validation: 0.06292630916333138]
	TIME [epoch: 12.1 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04853743336960154		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.04853743336960154 | validation: 0.029159041400885995]
	TIME [epoch: 12.1 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030702018015263476		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.030702018015263476 | validation: 0.02607130355680961]
	TIME [epoch: 12.1 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052275675093088315		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.052275675093088315 | validation: 0.023391765255611763]
	TIME [epoch: 12.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058155442196557654		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.058155442196557654 | validation: 0.0664605518330122]
	TIME [epoch: 12.1 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05857852681098404		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.05857852681098404 | validation: 0.027296130661185976]
	TIME [epoch: 12.1 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03056078923268325		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.03056078923268325 | validation: 0.0294724459848638]
	TIME [epoch: 12.1 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033888200043141914		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.033888200043141914 | validation: 0.020430865122333997]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09660676393524044		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.09660676393524044 | validation: 0.17835062121701067]
	TIME [epoch: 12.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08836607782278372		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.08836607782278372 | validation: 0.04522517742441043]
	TIME [epoch: 12.1 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034933635822965034		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.034933635822965034 | validation: 0.027862724777352212]
	TIME [epoch: 12.1 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02708199446909249		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.02708199446909249 | validation: 0.03704039524198788]
	TIME [epoch: 12.1 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03855864911577084		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.03855864911577084 | validation: 0.02122372221836219]
	TIME [epoch: 12.1 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03467103971680576		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.03467103971680576 | validation: 0.056796897553738804]
	TIME [epoch: 12.1 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042469371743975796		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.042469371743975796 | validation: 0.021455974693980015]
	TIME [epoch: 12.1 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03061958779448655		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.03061958779448655 | validation: 0.09340727647539547]
	TIME [epoch: 12.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05638210724779234		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.05638210724779234 | validation: 0.05188147707636919]
	TIME [epoch: 12.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05810760288202324		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.05810760288202324 | validation: 0.02009397852512277]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025323072730175197		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.025323072730175197 | validation: 0.01570892306842495]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023598533764903576		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.023598533764903576 | validation: 0.08116682651894301]
	TIME [epoch: 12.1 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05951148794906236		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.05951148794906236 | validation: 0.03217603905651915]
	TIME [epoch: 12.1 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025710140900021793		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.025710140900021793 | validation: 0.01841468715313742]
	TIME [epoch: 12.1 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03133425695745279		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.03133425695745279 | validation: 0.050652314276621174]
	TIME [epoch: 12.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06572229320147384		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.06572229320147384 | validation: 0.028996161907215212]
	TIME [epoch: 12.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027666771441680622		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.027666771441680622 | validation: 0.018950308071493806]
	TIME [epoch: 12.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0175713188436455		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.0175713188436455 | validation: 0.014896856693456598]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05479794002548466		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.05479794002548466 | validation: 0.015573121927237144]
	TIME [epoch: 12.1 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025885807364819182		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.025885807364819182 | validation: 0.04596355979004324]
	TIME [epoch: 12.1 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037431579135134875		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.037431579135134875 | validation: 0.08329171165545622]
	TIME [epoch: 12.1 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03851041084999711		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.03851041084999711 | validation: 0.04715672805765995]
	TIME [epoch: 12.1 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03217046287651262		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.03217046287651262 | validation: 0.09958236906754037]
	TIME [epoch: 12.1 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05809871028737357		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.05809871028737357 | validation: 0.02632088710741753]
	TIME [epoch: 12.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02885657524860042		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.02885657524860042 | validation: 0.018713481180336396]
	TIME [epoch: 12.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0182487337460591		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.0182487337460591 | validation: 0.01875054224846299]
	TIME [epoch: 12.1 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05153833391349546		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.05153833391349546 | validation: 0.06955377528902018]
	TIME [epoch: 12.1 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03999329585974063		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.03999329585974063 | validation: 0.03882687520960611]
	TIME [epoch: 12.1 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03952662869393096		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.03952662869393096 | validation: 0.026859135905695775]
	TIME [epoch: 12.1 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02756834164346353		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.02756834164346353 | validation: 0.028792637732260172]
	TIME [epoch: 12.1 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02929020977009614		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.02929020977009614 | validation: 0.03259915419960629]
	TIME [epoch: 12.1 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03130313364906115		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.03130313364906115 | validation: 0.05576768552487195]
	TIME [epoch: 12.1 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061375516687808634		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.061375516687808634 | validation: 0.021702727718859158]
	TIME [epoch: 12.1 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029647727391527275		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.029647727391527275 | validation: 0.03207459908660343]
	TIME [epoch: 12.1 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03370717145915016		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.03370717145915016 | validation: 0.0286577948855848]
	TIME [epoch: 12.1 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031802774163911494		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.031802774163911494 | validation: 0.01616724272718829]
	TIME [epoch: 12.1 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026806241388109997		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.026806241388109997 | validation: 0.022180731652512355]
	TIME [epoch: 12.1 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019393342702556507		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.019393342702556507 | validation: 0.01885340985040003]
	TIME [epoch: 12.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022049103453364062		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.022049103453364062 | validation: 0.032786703381291984]
	TIME [epoch: 12.1 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0690462150578189		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.0690462150578189 | validation: 0.03131829070389515]
	TIME [epoch: 12.1 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04340493201270324		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.04340493201270324 | validation: 0.023644343609791153]
	TIME [epoch: 12.1 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019863236306594385		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.019863236306594385 | validation: 0.01894019810604952]
	TIME [epoch: 12.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02021941832095114		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.02021941832095114 | validation: 0.019122739960892247]
	TIME [epoch: 12.1 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048852703545138404		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.048852703545138404 | validation: 0.029896187373972944]
	TIME [epoch: 12.1 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028163784381767587		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.028163784381767587 | validation: 0.012014519709153803]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01826958261669321		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.01826958261669321 | validation: 0.021573341863936814]
	TIME [epoch: 12.1 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02964357861219335		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.02964357861219335 | validation: 0.059331812436630646]
	TIME [epoch: 12.1 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039021967771692194		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.039021967771692194 | validation: 0.017717385238600884]
	TIME [epoch: 12.1 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01527898008418625		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.01527898008418625 | validation: 0.015390138061192424]
	TIME [epoch: 12.1 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018948852608443624		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.018948852608443624 | validation: 0.06797435554747311]
	TIME [epoch: 12.1 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04487348908508924		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.04487348908508924 | validation: 0.01734883976269394]
	TIME [epoch: 12.1 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02506969682392716		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.02506969682392716 | validation: 0.018563273982607043]
	TIME [epoch: 12.1 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018440390842347443		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.018440390842347443 | validation: 0.08414609984179576]
	TIME [epoch: 12.1 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03986134404419607		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.03986134404419607 | validation: 0.0335373998854152]
	TIME [epoch: 12.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01971068227808451		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.01971068227808451 | validation: 0.02529088366498279]
	TIME [epoch: 12.1 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03226231530430292		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.03226231530430292 | validation: 0.016607953639121154]
	TIME [epoch: 12.1 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016477883812335326		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.016477883812335326 | validation: 0.012671527844274435]
	TIME [epoch: 12.1 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02804462585679158		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.02804462585679158 | validation: 0.08868965619549546]
	TIME [epoch: 12.1 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041435679838995314		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.041435679838995314 | validation: 0.015273295627649525]
	TIME [epoch: 12.1 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015729150232768427		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.015729150232768427 | validation: 0.01366140113996479]
	TIME [epoch: 12.1 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023566681012063675		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.023566681012063675 | validation: 0.010731689557906267]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030008496747348925		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.030008496747348925 | validation: 0.018043781259393828]
	TIME [epoch: 12.1 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017445434979890574		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.017445434979890574 | validation: 0.021007929358236387]
	TIME [epoch: 12.1 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02290538433378123		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.02290538433378123 | validation: 0.03256431740549207]
	TIME [epoch: 12.1 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041419802096989665		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.041419802096989665 | validation: 0.03770788153266165]
	TIME [epoch: 12.1 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02563348582007141		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.02563348582007141 | validation: 0.016522816221915947]
	TIME [epoch: 12.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022024593139753933		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.022024593139753933 | validation: 0.01576888106881898]
	TIME [epoch: 12.1 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014852771243862533		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.014852771243862533 | validation: 0.038151286069884424]
	TIME [epoch: 12.1 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03286748999194747		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.03286748999194747 | validation: 0.012047127575611204]
	TIME [epoch: 12.1 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01489729538473578		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.01489729538473578 | validation: 0.016392462953263347]
	TIME [epoch: 12.1 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02880096278558321		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.02880096278558321 | validation: 0.026712284133139592]
	TIME [epoch: 12.1 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019132660201193412		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.019132660201193412 | validation: 0.014486472479632691]
	TIME [epoch: 12.1 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02234814588872725		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.02234814588872725 | validation: 0.03767130123383976]
	TIME [epoch: 12.1 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01816319488324235		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.01816319488324235 | validation: 0.014900019675830791]
	TIME [epoch: 12.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022605071788406483		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.022605071788406483 | validation: 0.019349782489004257]
	TIME [epoch: 12.1 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04089162806835806		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.04089162806835806 | validation: 0.019058957130533842]
	TIME [epoch: 12.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08221295738282394		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.08221295738282394 | validation: 0.07577500683574932]
	TIME [epoch: 12.1 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04711873543032958		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.04711873543032958 | validation: 0.022243328368473564]
	TIME [epoch: 12.1 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01949786795482065		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.01949786795482065 | validation: 0.011698507336264851]
	TIME [epoch: 12.1 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019088609403111932		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.019088609403111932 | validation: 0.013091623068765644]
	TIME [epoch: 12.1 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019331417736172887		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.019331417736172887 | validation: 0.014628235275777134]
	TIME [epoch: 12.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014080847745961312		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.014080847745961312 | validation: 0.038691380567715566]
	TIME [epoch: 12.1 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04639120679760165		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.04639120679760165 | validation: 0.02617826232723118]
	TIME [epoch: 12.1 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02044415405517545		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.02044415405517545 | validation: 0.01702158776251688]
	TIME [epoch: 12.1 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014140770931278416		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.014140770931278416 | validation: 0.016662161808330713]
	TIME [epoch: 12.1 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04574031216176516		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.04574031216176516 | validation: 0.019345193607852466]
	TIME [epoch: 12.1 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018330349997931938		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.018330349997931938 | validation: 0.01308499057633063]
	TIME [epoch: 12.1 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015384133679760922		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.015384133679760922 | validation: 0.024053142376731552]
	TIME [epoch: 12.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018889593607754634		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.018889593607754634 | validation: 0.008510748615660212]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012743243310915656		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.012743243310915656 | validation: 0.01714764008730306]
	TIME [epoch: 12.1 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01798498106937442		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.01798498106937442 | validation: 0.01814329902395379]
	TIME [epoch: 12.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034830373449175486		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.034830373449175486 | validation: 0.014368767885694866]
	TIME [epoch: 12.1 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021372627676811386		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.021372627676811386 | validation: 0.023118336005469656]
	TIME [epoch: 12.1 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019665660377350555		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.019665660377350555 | validation: 0.012171573295489247]
	TIME [epoch: 12.1 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018642368881623066		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.018642368881623066 | validation: 0.029304728386026224]
	TIME [epoch: 12.1 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02257685790212512		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.02257685790212512 | validation: 0.028998695932263285]
	TIME [epoch: 12.1 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021134522045519213		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.021134522045519213 | validation: 0.0327612735919067]
	TIME [epoch: 12.1 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0424804505615706		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.0424804505615706 | validation: 0.014880267768690392]
	TIME [epoch: 12.1 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017077724026608954		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.017077724026608954 | validation: 0.009752162596598925]
	TIME [epoch: 12.1 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011596383080433594		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.011596383080433594 | validation: 0.007772646774802718]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016887485371957535		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.016887485371957535 | validation: 0.05269979067803844]
	TIME [epoch: 12.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02436475085135665		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.02436475085135665 | validation: 0.01816890627686396]
	TIME [epoch: 12.1 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01373310615037423		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.01373310615037423 | validation: 0.014258991170707992]
	TIME [epoch: 12.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023748329607204445		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.023748329607204445 | validation: 0.029726837097550654]
	TIME [epoch: 12.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03379906807066326		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.03379906807066326 | validation: 0.022556855771262686]
	TIME [epoch: 12.1 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01514054839486402		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.01514054839486402 | validation: 0.008667237320160986]
	TIME [epoch: 12.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010437729097330047		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.010437729097330047 | validation: 0.01369697347793733]
	TIME [epoch: 12.1 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017559286526958078		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.017559286526958078 | validation: 0.011565637679217072]
	TIME [epoch: 12.1 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01938982291472839		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.01938982291472839 | validation: 0.02522487145968803]
	TIME [epoch: 12.1 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01989227513919379		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.01989227513919379 | validation: 0.01590951597124412]
	TIME [epoch: 12.1 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012770117188696768		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.012770117188696768 | validation: 0.015388038524995592]
	TIME [epoch: 12.1 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024819668261846416		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.024819668261846416 | validation: 0.009548335457420086]
	TIME [epoch: 12.1 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0188588135920339		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.0188588135920339 | validation: 0.013292613277252497]
	TIME [epoch: 12.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02842894357871363		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.02842894357871363 | validation: 0.022941585806238254]
	TIME [epoch: 12.1 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015993698111898446		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.015993698111898446 | validation: 0.010805210283902574]
	TIME [epoch: 12.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01139598859385088		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.01139598859385088 | validation: 0.01566918676247061]
	TIME [epoch: 12.1 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019873309190474624		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.019873309190474624 | validation: 0.009536354601767213]
	TIME [epoch: 12.1 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010689212671654775		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.010689212671654775 | validation: 0.009670007764314733]
	TIME [epoch: 12.1 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01576744355919506		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.01576744355919506 | validation: 0.04413735424819365]
	TIME [epoch: 12.1 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02363432227427959		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.02363432227427959 | validation: 0.017288056346075983]
	TIME [epoch: 12.1 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013108340079619363		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.013108340079619363 | validation: 0.019559355583674422]
	TIME [epoch: 12.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023582352921142864		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.023582352921142864 | validation: 0.010818072530642382]
	TIME [epoch: 12.1 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01311184977636896		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.01311184977636896 | validation: 0.020455742952447925]
	TIME [epoch: 12.1 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02421911806435789		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.02421911806435789 | validation: 0.01648369966502991]
	TIME [epoch: 12.1 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01592368598758555		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.01592368598758555 | validation: 0.00848459573188094]
	TIME [epoch: 12.1 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008845256128221592		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.008845256128221592 | validation: 0.09324116861125739]
	TIME [epoch: 12.1 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0842355770016191		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.0842355770016191 | validation: 0.03901543763893528]
	TIME [epoch: 12.1 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022045558641943196		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.022045558641943196 | validation: 0.013065874101136066]
	TIME [epoch: 12.1 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012099221085178113		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.012099221085178113 | validation: 0.020544118435684575]
	TIME [epoch: 12.1 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015378755175455151		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.015378755175455151 | validation: 0.016421286817099066]
	TIME [epoch: 12.1 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01890749371676408		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.01890749371676408 | validation: 0.01262785908363866]
	TIME [epoch: 12.1 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01475939046570629		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.01475939046570629 | validation: 0.05080344244960195]
	TIME [epoch: 12.1 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03354597515489832		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.03354597515489832 | validation: 0.03722398580110555]
	TIME [epoch: 12.1 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02528126263892565		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.02528126263892565 | validation: 0.01060163605303039]
	TIME [epoch: 12.1 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009404699217130127		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.009404699217130127 | validation: 0.01131098137956929]
	TIME [epoch: 12.1 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017156677886485137		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.017156677886485137 | validation: 0.020959472408050267]
	TIME [epoch: 12.1 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020879239760777157		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.020879239760777157 | validation: 0.00837776881625947]
	TIME [epoch: 12.1 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009417774825941274		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.009417774825941274 | validation: 0.010744601764739459]
	TIME [epoch: 12.1 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01211115769623994		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.01211115769623994 | validation: 0.01959529945746111]
	TIME [epoch: 12.1 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014802111726161103		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.014802111726161103 | validation: 0.01329288910480363]
	TIME [epoch: 12.1 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012195964478001121		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.012195964478001121 | validation: 0.02023596764368439]
	TIME [epoch: 12.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016401849356983213		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.016401849356983213 | validation: 0.01168017711081907]
	TIME [epoch: 12.1 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01710967611981892		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.01710967611981892 | validation: 0.011826981840447061]
	TIME [epoch: 12.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010403900692867077		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.010403900692867077 | validation: 0.013872671286674916]
	TIME [epoch: 12.1 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013101940906572004		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.013101940906572004 | validation: 0.024580168155391803]
	TIME [epoch: 12.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02608147747203589		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.02608147747203589 | validation: 0.010643300594629389]
	TIME [epoch: 12.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01408809943715843		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.01408809943715843 | validation: 0.019286184140225652]
	TIME [epoch: 12.1 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01618529055985318		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.01618529055985318 | validation: 0.016906234753655467]
	TIME [epoch: 12.1 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012176627338510323		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.012176627338510323 | validation: 0.01585823392284131]
	TIME [epoch: 12.1 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017105847879424093		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.017105847879424093 | validation: 0.00915891968004787]
	TIME [epoch: 12.1 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00882965168429872		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.00882965168429872 | validation: 0.008812276546092773]
	TIME [epoch: 12.1 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015909934779986833		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.015909934779986833 | validation: 0.014253215291684763]
	TIME [epoch: 12.1 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01099677492868808		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.01099677492868808 | validation: 0.010318870534103526]
	TIME [epoch: 12.1 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013176612627295647		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.013176612627295647 | validation: 0.015150801736981839]
	TIME [epoch: 12.1 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014425521737630616		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.014425521737630616 | validation: 0.0156562558855798]
	TIME [epoch: 12.1 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018550668989604356		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.018550668989604356 | validation: 0.008490935881772448]
	TIME [epoch: 12.1 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01069826438155841		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.01069826438155841 | validation: 0.012550614835697478]
	TIME [epoch: 12.1 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01727535109691382		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.01727535109691382 | validation: 0.02088278583574095]
	TIME [epoch: 12.1 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014848161631919215		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.014848161631919215 | validation: 0.009352534642237557]
	TIME [epoch: 12.1 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008261175219771487		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.008261175219771487 | validation: 0.012300986413240423]
	TIME [epoch: 12.1 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020371527773021494		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.020371527773021494 | validation: 0.016408897522948565]
	TIME [epoch: 12.1 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01323342782977829		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.01323342782977829 | validation: 0.0081379424661565]
	TIME [epoch: 12.1 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009903663335283893		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.009903663335283893 | validation: 0.016189546491851853]
	TIME [epoch: 12.1 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0149218211212432		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.0149218211212432 | validation: 0.014012428493991707]
	TIME [epoch: 12.1 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011050452471671473		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.011050452471671473 | validation: 0.018785544526443116]
	TIME [epoch: 12.1 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01393060828722309		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.01393060828722309 | validation: 0.00916178575791874]
	TIME [epoch: 12.1 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01157529229515048		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.01157529229515048 | validation: 0.021207182886776386]
	TIME [epoch: 12.1 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02183292810951255		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.02183292810951255 | validation: 0.009550309730788468]
	TIME [epoch: 12.1 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009604198994722434		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.009604198994722434 | validation: 0.007507385563139821]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009419523431245864		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.009419523431245864 | validation: 0.01595566863236712]
	TIME [epoch: 12.1 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01713006646603358		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.01713006646603358 | validation: 0.0077248605988948435]
	TIME [epoch: 12.1 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01159059970086496		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.01159059970086496 | validation: 0.015230110848881314]
	TIME [epoch: 12.1 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011954740732357567		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.011954740732357567 | validation: 0.016795672789247092]
	TIME [epoch: 12.1 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014916950592143277		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.014916950592143277 | validation: 0.00867826605102624]
	TIME [epoch: 12.1 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008353241449269373		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.008353241449269373 | validation: 0.009711252248904626]
	TIME [epoch: 12.1 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016114455538970503		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.016114455538970503 | validation: 0.02179509895597065]
	TIME [epoch: 12.1 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014223664644408135		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.014223664644408135 | validation: 0.0068712110815960835]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009004302244208149		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.009004302244208149 | validation: 0.013308769366410234]
	TIME [epoch: 12.1 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01345583155642683		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.01345583155642683 | validation: 0.044518158592832716]
	TIME [epoch: 12.1 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02900741031194313		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.02900741031194313 | validation: 0.01230021986663642]
	TIME [epoch: 12.1 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011214458320098604		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.011214458320098604 | validation: 0.008186312466590732]
	TIME [epoch: 449 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007526397585601301		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.007526397585601301 | validation: 0.008176560250532237]
	TIME [epoch: 26 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011766865835259967		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.011766865835259967 | validation: 0.009875575751244554]
	TIME [epoch: 25.9 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009014316622061256		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.009014316622061256 | validation: 0.014011871779838581]
	TIME [epoch: 25.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017402068798733034		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.017402068798733034 | validation: 0.006686130602688399]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020257350195132022		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.020257350195132022 | validation: 0.05005622863013276]
	TIME [epoch: 25.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026654063450676693		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.026654063450676693 | validation: 0.012531436370397712]
	TIME [epoch: 25.9 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012393063401761162		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.012393063401761162 | validation: 0.009744913496218052]
	TIME [epoch: 25.9 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007970653279324346		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.007970653279324346 | validation: 0.006589407154427222]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009864283727553727		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.009864283727553727 | validation: 0.014026564254450528]
	TIME [epoch: 25.9 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010958997672250883		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.010958997672250883 | validation: 0.014194395903163601]
	TIME [epoch: 25.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015154077049507756		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.015154077049507756 | validation: 0.017497554318313062]
	TIME [epoch: 25.9 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011300464795627543		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.011300464795627543 | validation: 0.013123557214340949]
	TIME [epoch: 25.9 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009895129422526185		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.009895129422526185 | validation: 0.01007531090536518]
	TIME [epoch: 25.9 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009462146362571434		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.009462146362571434 | validation: 0.010482384348211249]
	TIME [epoch: 25.9 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021115157016005075		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.021115157016005075 | validation: 0.008180326263946509]
	TIME [epoch: 25.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007558584388629084		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.007558584388629084 | validation: 0.00912653508465687]
	TIME [epoch: 25.9 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010580330456990791		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.010580330456990791 | validation: 0.014229531528168018]
	TIME [epoch: 25.9 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009861505113530695		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.009861505113530695 | validation: 0.006668134449707598]
	TIME [epoch: 25.9 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008349002585458036		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.008349002585458036 | validation: 0.012047275073936154]
	TIME [epoch: 25.9 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01791877702904366		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.01791877702904366 | validation: 0.006604649397287352]
	TIME [epoch: 25.9 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010361786844492049		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.010361786844492049 | validation: 0.026044512258145778]
	TIME [epoch: 25.9 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012665676362517133		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.012665676362517133 | validation: 0.010950931888213881]
	TIME [epoch: 25.9 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008233128874632982		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.008233128874632982 | validation: 0.0066877790061075455]
	TIME [epoch: 25.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01094038280803253		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.01094038280803253 | validation: 0.00876581576371101]
	TIME [epoch: 25.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009368468755525737		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.009368468755525737 | validation: 0.009516639821281633]
	TIME [epoch: 25.9 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009583385450519793		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.009583385450519793 | validation: 0.009569480049882787]
	TIME [epoch: 25.9 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01955124653887039		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.01955124653887039 | validation: 0.0146863133404166]
	TIME [epoch: 25.9 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01290230618055107		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.01290230618055107 | validation: 0.007461950469344936]
	TIME [epoch: 25.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007918468686756407		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.007918468686756407 | validation: 0.012222368997919075]
	TIME [epoch: 25.9 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01188966304321669		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.01188966304321669 | validation: 0.010016988489785772]
	TIME [epoch: 25.9 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013863969328267969		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.013863969328267969 | validation: 0.007183602762720305]
	TIME [epoch: 25.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008214790018743052		[learning rate: 0.0018085]
	Learning Rate: 0.00180846
	LOSS [training: 0.008214790018743052 | validation: 0.007339197230534717]
	TIME [epoch: 25.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013435892955394511		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.013435892955394511 | validation: 0.007440738510094165]
	TIME [epoch: 25.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00767265268095798		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.00767265268095798 | validation: 0.007888286972598189]
	TIME [epoch: 25.9 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006890312237764713		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.006890312237764713 | validation: 0.00618063194204012]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0088846075805319		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.0088846075805319 | validation: 0.025165278809028684]
	TIME [epoch: 26 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014182796102292486		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.014182796102292486 | validation: 0.006632677465835006]
	TIME [epoch: 25.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007153745295332263		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.007153745295332263 | validation: 0.01025275107255786]
	TIME [epoch: 25.9 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015935948801682632		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.015935948801682632 | validation: 0.011336685179746812]
	TIME [epoch: 25.9 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0109241733603003		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.0109241733603003 | validation: 0.007418193129170205]
	TIME [epoch: 25.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008138785801296112		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.008138785801296112 | validation: 0.010763305079058295]
	TIME [epoch: 25.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008206080620998535		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.008206080620998535 | validation: 0.008686722352287608]
	TIME [epoch: 25.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007897811629523173		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.007897811629523173 | validation: 0.006280649667753232]
	TIME [epoch: 25.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013573010473937544		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.013573010473937544 | validation: 0.012245652440739878]
	TIME [epoch: 25.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00800836588202029		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.00800836588202029 | validation: 0.009642961657304515]
	TIME [epoch: 25.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010485615824020189		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.010485615824020189 | validation: 0.010997177790501916]
	TIME [epoch: 25.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011324495057574088		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.011324495057574088 | validation: 0.009658095568583264]
	TIME [epoch: 25.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0088367598843903		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.0088367598843903 | validation: 0.007527010756488823]
	TIME [epoch: 25.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015957502492713528		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.015957502492713528 | validation: 0.011308987499594693]
	TIME [epoch: 25.9 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009892461579385047		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.009892461579385047 | validation: 0.006821461126021999]
	TIME [epoch: 25.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006008080163034503		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.006008080163034503 | validation: 0.007874122867000104]
	TIME [epoch: 25.9 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01097165545180722		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.01097165545180722 | validation: 0.02005172245405545]
	TIME [epoch: 25.9 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009624866748589114		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.009624866748589114 | validation: 0.01071931583072537]
	TIME [epoch: 25.9 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010733586288538013		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.010733586288538013 | validation: 0.009776860714695565]
	TIME [epoch: 25.9 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007662252063288496		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.007662252063288496 | validation: 0.008107073246097166]
	TIME [epoch: 26 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008498024560570416		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.008498024560570416 | validation: 0.007703883545440545]
	TIME [epoch: 25.9 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009533360756511351		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.009533360756511351 | validation: 0.009771101617007033]
	TIME [epoch: 25.9 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01676200295329746		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.01676200295329746 | validation: 0.01969348301100379]
	TIME [epoch: 25.9 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014049648011492291		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.014049648011492291 | validation: 0.00884876703041792]
	TIME [epoch: 25.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007873630850142213		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.007873630850142213 | validation: 0.007926845973110414]
	TIME [epoch: 25.9 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0068632584058180065		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.0068632584058180065 | validation: 0.00996575524453639]
	TIME [epoch: 25.9 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009294961728249177		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.009294961728249177 | validation: 0.009969384203496818]
	TIME [epoch: 25.9 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007550737105492396		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.007550737105492396 | validation: 0.006789499943930485]
	TIME [epoch: 25.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008911186032577931		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.008911186032577931 | validation: 0.018016179003948612]
	TIME [epoch: 25.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010208740501341618		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.010208740501341618 | validation: 0.006292216262426553]
	TIME [epoch: 25.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00655211611359308		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.00655211611359308 | validation: 0.011927342542578418]
	TIME [epoch: 25.9 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012735642720712391		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.012735642720712391 | validation: 0.010744245932789987]
	TIME [epoch: 25.9 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007787723917734894		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.007787723917734894 | validation: 0.01186523702627562]
	TIME [epoch: 25.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015060510521653601		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.015060510521653601 | validation: 0.014506864513905864]
	TIME [epoch: 25.9 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009415769396439708		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.009415769396439708 | validation: 0.007500240315871317]
	TIME [epoch: 25.9 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009843951013142116		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.009843951013142116 | validation: 0.009187153318625886]
	TIME [epoch: 25.9 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017248535723198767		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.017248535723198767 | validation: 0.0121631541117792]
	TIME [epoch: 25.9 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011358722483015102		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.011358722483015102 | validation: 0.00748996220433196]
	TIME [epoch: 25.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008114030680212882		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.008114030680212882 | validation: 0.017806812555405678]
	TIME [epoch: 25.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009583638532116207		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.009583638532116207 | validation: 0.007723327316499655]
	TIME [epoch: 25.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007352043011751979		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.007352043011751979 | validation: 0.006580177620397555]
	TIME [epoch: 25.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006772187981462913		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.006772187981462913 | validation: 0.0079467856501652]
	TIME [epoch: 25.9 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009350332429929656		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.009350332429929656 | validation: 0.007195051311625293]
	TIME [epoch: 25.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0059917307819555045		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.0059917307819555045 | validation: 0.006010853696150955]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013840446212023252		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.013840446212023252 | validation: 0.010223871127245578]
	TIME [epoch: 25.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009990411037192597		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.009990411037192597 | validation: 0.006359797035540422]
	TIME [epoch: 25.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007391290295422296		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.007391290295422296 | validation: 0.011927295487851914]
	TIME [epoch: 25.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008270122355726616		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.008270122355726616 | validation: 0.006401118351219506]
	TIME [epoch: 25.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007699142238289703		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.007699142238289703 | validation: 0.010278207464006412]
	TIME [epoch: 25.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00809863549118231		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.00809863549118231 | validation: 0.01784083387101003]
	TIME [epoch: 25.9 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012303362207428541		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.012303362207428541 | validation: 0.010092274476022473]
	TIME [epoch: 25.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008568993046049063		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.008568993046049063 | validation: 0.00794238830385778]
	TIME [epoch: 25.9 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006357517803854289		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.006357517803854289 | validation: 0.005164592244537334]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_589.pth
	Model improved!!!
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008007866609741614		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.008007866609741614 | validation: 0.014144990784145262]
	TIME [epoch: 25.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011279336504625887		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.011279336504625887 | validation: 0.006455417963746424]
	TIME [epoch: 25.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00602081503728837		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.00602081503728837 | validation: 0.01201133577828803]
	TIME [epoch: 25.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007858923528580525		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.007858923528580525 | validation: 0.006434840845892442]
	TIME [epoch: 25.9 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006163710808027219		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.006163710808027219 | validation: 0.009101928896697897]
	TIME [epoch: 25.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011735288099180913		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.011735288099180913 | validation: 0.00853879007361032]
	TIME [epoch: 25.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006147727925754425		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.006147727925754425 | validation: 0.005472543427775565]
	TIME [epoch: 25.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006534042291377328		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.006534042291377328 | validation: 0.007028848614706024]
	TIME [epoch: 25.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006795347878771902		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.006795347878771902 | validation: 0.005032141923741702]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01115273881199108		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.01115273881199108 | validation: 0.009046157791944632]
	TIME [epoch: 25.9 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008443270661875052		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.008443270661875052 | validation: 0.007100538717306825]
	TIME [epoch: 25.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006379800631792624		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.006379800631792624 | validation: 0.0069740962264614355]
	TIME [epoch: 25.9 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005903039344850998		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.005903039344850998 | validation: 0.007158898672170701]
	TIME [epoch: 25.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01097579647138162		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.01097579647138162 | validation: 0.011997987280681084]
	TIME [epoch: 25.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008924955000560906		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.008924955000560906 | validation: 0.006849757774199014]
	TIME [epoch: 25.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0120782333271453		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.0120782333271453 | validation: 0.024595968815572772]
	TIME [epoch: 25.9 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014758800837085145		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.014758800837085145 | validation: 0.010888103289218097]
	TIME [epoch: 25.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007236199705159288		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.007236199705159288 | validation: 0.007417381792504635]
	TIME [epoch: 25.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007122527009805502		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.007122527009805502 | validation: 0.010765392747537223]
	TIME [epoch: 25.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009751476480418393		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.009751476480418393 | validation: 0.0075837049157032495]
	TIME [epoch: 25.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006311875612254967		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.006311875612254967 | validation: 0.006320191845901211]
	TIME [epoch: 25.9 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009495825022254333		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.009495825022254333 | validation: 0.007212943073095338]
	TIME [epoch: 25.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006332808268743156		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.006332808268743156 | validation: 0.010598712287062398]
	TIME [epoch: 25.9 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009140629788841066		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.009140629788841066 | validation: 0.005968696922325928]
	TIME [epoch: 25.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008143332480078667		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.008143332480078667 | validation: 0.007591041194546146]
	TIME [epoch: 25.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007239270129970063		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.007239270129970063 | validation: 0.01018145624981855]
	TIME [epoch: 25.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009651306850755652		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.009651306850755652 | validation: 0.009463933251820048]
	TIME [epoch: 25.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007802968299523434		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.007802968299523434 | validation: 0.007713464314877275]
	TIME [epoch: 25.9 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006788071782509494		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.006788071782509494 | validation: 0.00672381498232975]
	TIME [epoch: 25.9 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062041892655442365		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.0062041892655442365 | validation: 0.006603050736752906]
	TIME [epoch: 25.9 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008373587961557028		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.008373587961557028 | validation: 0.007014122976224276]
	TIME [epoch: 25.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006799004535494297		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.006799004535494297 | validation: 0.007975209353823173]
	TIME [epoch: 25.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007028854745615328		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.007028854745615328 | validation: 0.005068490134410535]
	TIME [epoch: 25.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009813831262220267		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.009813831262220267 | validation: 0.0057093169606036945]
	TIME [epoch: 25.9 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006845271138625534		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.006845271138625534 | validation: 0.006051997181422254]
	TIME [epoch: 25.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006979188524276258		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.006979188524276258 | validation: 0.01022620605639592]
	TIME [epoch: 25.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009110387232638581		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.009110387232638581 | validation: 0.007382433927364451]
	TIME [epoch: 25.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006174902355676107		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.006174902355676107 | validation: 0.008365649378576718]
	TIME [epoch: 25.9 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008483455068674352		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.008483455068674352 | validation: 0.0068059175175372525]
	TIME [epoch: 25.9 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006644926672175875		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.006644926672175875 | validation: 0.006774965313336861]
	TIME [epoch: 26.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007034996979677674		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.007034996979677674 | validation: 0.010851269100624153]
	TIME [epoch: 25.9 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006844171119384234		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.006844171119384234 | validation: 0.005342508231961708]
	TIME [epoch: 25.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011908097764642906		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.011908097764642906 | validation: 0.009533708394465816]
	TIME [epoch: 25.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066285277250778315		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.0066285277250778315 | validation: 0.009890135000015735]
	TIME [epoch: 25.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0075090873932688665		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.0075090873932688665 | validation: 0.006222274258105603]
	TIME [epoch: 25.9 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00639407977244706		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.00639407977244706 | validation: 0.00577727007377048]
	TIME [epoch: 25.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005784892306523737		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.005784892306523737 | validation: 0.011015697358438938]
	TIME [epoch: 25.9 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009271021647559633		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.009271021647559633 | validation: 0.01013457035018945]
	TIME [epoch: 25.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006930673408764247		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.006930673408764247 | validation: 0.004784376406976072]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_638.pth
	Model improved!!!
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006700412315029112		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.006700412315029112 | validation: 0.006410547230370669]
	TIME [epoch: 26 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006587953844191403		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.006587953844191403 | validation: 0.0059548130429075904]
	TIME [epoch: 25.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056419870509418464		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.0056419870509418464 | validation: 0.007347096931120912]
	TIME [epoch: 25.9 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008982860984422646		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.008982860984422646 | validation: 0.006383695255388043]
	TIME [epoch: 25.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005820767288892166		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.005820767288892166 | validation: 0.006628908992446964]
	TIME [epoch: 25.9 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057597641281931546		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.0057597641281931546 | validation: 0.006261774630117928]
	TIME [epoch: 25.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006394603732106693		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.006394603732106693 | validation: 0.016266217276559906]
	TIME [epoch: 26 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008183454754686908		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.008183454754686908 | validation: 0.0055479984355748455]
	TIME [epoch: 25.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006608183270473554		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.006608183270473554 | validation: 0.00793219754956622]
	TIME [epoch: 25.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006107849540579384		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.006107849540579384 | validation: 0.013548391358190647]
	TIME [epoch: 25.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007649966047759563		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.007649966047759563 | validation: 0.006923977025900679]
	TIME [epoch: 25.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006451400435183047		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.006451400435183047 | validation: 0.0064080671186755285]
	TIME [epoch: 25.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005582761130186866		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.005582761130186866 | validation: 0.009352874217755484]
	TIME [epoch: 25.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0070815942362674605		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.0070815942362674605 | validation: 0.006274860289620862]
	TIME [epoch: 25.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048984849383392134		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.0048984849383392134 | validation: 0.0047037748355376555]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006648799612283025		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.006648799612283025 | validation: 0.010939812027401897]
	TIME [epoch: 25.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007127760178434407		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.007127760178434407 | validation: 0.010053586012395033]
	TIME [epoch: 25.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00783143807762571		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.00783143807762571 | validation: 0.0071453249150173655]
	TIME [epoch: 25.9 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009145744662301013		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.009145744662301013 | validation: 0.008522546165403037]
	TIME [epoch: 25.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006884389561035407		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.006884389561035407 | validation: 0.005783211711211572]
	TIME [epoch: 25.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007742253614163379		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.007742253614163379 | validation: 0.006105396441222849]
	TIME [epoch: 25.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0053659719302573405		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.0053659719302573405 | validation: 0.005223586253939915]
	TIME [epoch: 25.9 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00532555344176117		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.00532555344176117 | validation: 0.006028162140260579]
	TIME [epoch: 25.9 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005454112751387294		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.005454112751387294 | validation: 0.009648492211532902]
	TIME [epoch: 25.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.010001452656456394		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.010001452656456394 | validation: 0.006592889446543188]
	TIME [epoch: 25.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005070252184795198		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.005070252184795198 | validation: 0.007802303683860191]
	TIME [epoch: 25.9 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005406185337248998		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.005406185337248998 | validation: 0.005579706126360389]
	TIME [epoch: 25.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004904942250197089		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.004904942250197089 | validation: 0.007502466013825279]
	TIME [epoch: 25.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006622570918339728		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.006622570918339728 | validation: 0.009454865361751715]
	TIME [epoch: 25.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008219886135610649		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.008219886135610649 | validation: 0.007352209537414345]
	TIME [epoch: 25.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007141149764346557		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.007141149764346557 | validation: 0.005958617388431122]
	TIME [epoch: 25.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005254784033847856		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.005254784033847856 | validation: 0.00528406417875918]
	TIME [epoch: 25.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005961787560260595		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.005961787560260595 | validation: 0.0061423191941911565]
	TIME [epoch: 25.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0050188828515653604		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.0050188828515653604 | validation: 0.00602029629278125]
	TIME [epoch: 25.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005002272739440955		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.005002272739440955 | validation: 0.012372194333631752]
	TIME [epoch: 26 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008289067665924289		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.008289067665924289 | validation: 0.0058837852825652185]
	TIME [epoch: 25.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005091935805424177		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.005091935805424177 | validation: 0.006413009156396823]
	TIME [epoch: 25.9 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005865513952533383		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.005865513952533383 | validation: 0.009893992456076466]
	TIME [epoch: 25.9 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.009435765478987435		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.009435765478987435 | validation: 0.005173549849415711]
	TIME [epoch: 25.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066391838314612435		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.0066391838314612435 | validation: 0.005016311249952665]
	TIME [epoch: 25.9 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005563755009780931		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.005563755009780931 | validation: 0.0063683483839959264]
	TIME [epoch: 25.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005915040072714445		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.005915040072714445 | validation: 0.011369781991327314]
	TIME [epoch: 25.9 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00857870567674406		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.00857870567674406 | validation: 0.006598765610909002]
	TIME [epoch: 25.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0060276067677702775		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.0060276067677702775 | validation: 0.008207564773411332]
	TIME [epoch: 25.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006028419573404388		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.006028419573404388 | validation: 0.006139236845289588]
	TIME [epoch: 25.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005254428911486173		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.005254428911486173 | validation: 0.005280647773928051]
	TIME [epoch: 25.9 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006796915518487648		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.006796915518487648 | validation: 0.007897227466622664]
	TIME [epoch: 25.9 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005945877808501986		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.005945877808501986 | validation: 0.006099872699113027]
	TIME [epoch: 25.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062715108606427854		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.0062715108606427854 | validation: 0.004778097476813171]
	TIME [epoch: 25.9 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004738516504224262		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.004738516504224262 | validation: 0.005190344235022971]
	TIME [epoch: 25.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005289499476057468		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.005289499476057468 | validation: 0.004911379156801729]
	TIME [epoch: 25.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006740675780022044		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.006740675780022044 | validation: 0.005721035258223409]
	TIME [epoch: 25.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007155339954086107		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.007155339954086107 | validation: 0.006085361159911964]
	TIME [epoch: 25.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005904240916174496		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.005904240916174496 | validation: 0.006742477290767673]
	TIME [epoch: 25.9 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006113279010102045		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.006113279010102045 | validation: 0.006928172037171165]
	TIME [epoch: 25.9 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005478532571974531		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.005478532571974531 | validation: 0.005790234987872438]
	TIME [epoch: 25.9 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006838906226614361		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.006838906226614361 | validation: 0.005401704798494906]
	TIME [epoch: 25.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00421636440042994		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.00421636440042994 | validation: 0.00488225767651411]
	TIME [epoch: 26 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005270484434102822		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.005270484434102822 | validation: 0.00664265332727463]
	TIME [epoch: 26 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007609151499190495		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.007609151499190495 | validation: 0.006457459789298103]
	TIME [epoch: 25.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005304883234229622		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.005304883234229622 | validation: 0.005736674681228766]
	TIME [epoch: 25.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004801917738185622		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.004801917738185622 | validation: 0.006346893487294956]
	TIME [epoch: 25.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0057249592729736		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.0057249592729736 | validation: 0.004552659925184187]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006154336564159241		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.006154336564159241 | validation: 0.0058033298782461055]
	TIME [epoch: 26 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005168782416141153		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.005168782416141153 | validation: 0.0048277043517847285]
	TIME [epoch: 26 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00917031516063962		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.00917031516063962 | validation: 0.008345372415823537]
	TIME [epoch: 25.9 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005576790860056923		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.005576790860056923 | validation: 0.008307409882380176]
	TIME [epoch: 25.9 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006808310739147878		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.006808310739147878 | validation: 0.006806415425253656]
	TIME [epoch: 25.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004971480682940323		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.004971480682940323 | validation: 0.008434090897619831]
	TIME [epoch: 25.9 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005512223737415062		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.005512223737415062 | validation: 0.009169637610433483]
	TIME [epoch: 25.9 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006578512115764927		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.006578512115764927 | validation: 0.007525080430789118]
	TIME [epoch: 26 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005648167850546795		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.005648167850546795 | validation: 0.004870112857312151]
	TIME [epoch: 25.9 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004806450587902349		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.004806450587902349 | validation: 0.006028666303963845]
	TIME [epoch: 25.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006613792283930967		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.006613792283930967 | validation: 0.004374096783135209]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005138596553294827		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.005138596553294827 | validation: 0.0075467023529939194]
	TIME [epoch: 25.9 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006295366726691582		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.006295366726691582 | validation: 0.004899137494329987]
	TIME [epoch: 25.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005388461313903885		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.005388461313903885 | validation: 0.005246928315332267]
	TIME [epoch: 25.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004618724696899259		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.004618724696899259 | validation: 0.006619153052698823]
	TIME [epoch: 25.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005728564349629314		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.005728564349629314 | validation: 0.008599767218062417]
	TIME [epoch: 25.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0066937358167116585		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.0066937358167116585 | validation: 0.005335523053263459]
	TIME [epoch: 25.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004529865288162954		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.004529865288162954 | validation: 0.006471933500617511]
	TIME [epoch: 25.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058647055916699435		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.0058647055916699435 | validation: 0.006802827244424336]
	TIME [epoch: 25.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004870101813182317		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.004870101813182317 | validation: 0.005410849361430301]
	TIME [epoch: 25.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0058957148134766825		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.0058957148134766825 | validation: 0.005704648873849331]
	TIME [epoch: 25.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005564598539377657		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.005564598539377657 | validation: 0.006087630683520096]
	TIME [epoch: 25.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0062438741580878275		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.0062438741580878275 | validation: 0.0058417403845275745]
	TIME [epoch: 26 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005915823968088697		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.005915823968088697 | validation: 0.004747540005853411]
	TIME [epoch: 26 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004753394250807306		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.004753394250807306 | validation: 0.006979391062855811]
	TIME [epoch: 25.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0051151359869208365		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.0051151359869208365 | validation: 0.004696141498859858]
	TIME [epoch: 25.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004476330681552368		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.004476330681552368 | validation: 0.006667756366808794]
	TIME [epoch: 26 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005456842273645985		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.005456842273645985 | validation: 0.0058320416883541485]
	TIME [epoch: 25.9 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00554859873627042		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.00554859873627042 | validation: 0.0050145517350278475]
	TIME [epoch: 25.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007165332690924411		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.007165332690924411 | validation: 0.007457963178815209]
	TIME [epoch: 25.9 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005553308170092227		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.005553308170092227 | validation: 0.00472817570843308]
	TIME [epoch: 25.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044585356990440046		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.0044585356990440046 | validation: 0.0047670291372984]
	TIME [epoch: 25.9 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005040058359819076		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.005040058359819076 | validation: 0.006668704274569111]
	TIME [epoch: 25.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004922089161834398		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.004922089161834398 | validation: 0.005200831570759503]
	TIME [epoch: 25.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004694990677511815		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.004694990677511815 | validation: 0.004760579222167586]
	TIME [epoch: 25.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005883908039547655		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.005883908039547655 | validation: 0.006127383815283521]
	TIME [epoch: 26 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0047980925809231415		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.0047980925809231415 | validation: 0.005578576269429872]
	TIME [epoch: 26 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004499503906981729		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.004499503906981729 | validation: 0.006358315922792578]
	TIME [epoch: 25.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004568427260883377		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.004568427260883377 | validation: 0.005004856143983874]
	TIME [epoch: 25.9 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00514264795215013		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.00514264795215013 | validation: 0.007025563442913359]
	TIME [epoch: 25.9 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005589635431560987		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.005589635431560987 | validation: 0.00743203053148506]
	TIME [epoch: 25.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004970259127201919		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.004970259127201919 | validation: 0.004530166291891615]
	TIME [epoch: 25.9 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005091110426175374		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.005091110426175374 | validation: 0.006333386619091668]
	TIME [epoch: 26 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006557562287784873		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.006557562287784873 | validation: 0.0056666410234489295]
	TIME [epoch: 26 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.007342142382269037		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.007342142382269037 | validation: 0.005612047595471561]
	TIME [epoch: 25.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005001246585107319		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.005001246585107319 | validation: 0.005178457566373661]
	TIME [epoch: 25.9 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0048194493088967085		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.0048194493088967085 | validation: 0.004350277072662969]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004261899370779815		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.004261899370779815 | validation: 0.005038094759807336]
	TIME [epoch: 25.9 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004233687539207985		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.004233687539207985 | validation: 0.004937699679558109]
	TIME [epoch: 25.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008457542298310904		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.008457542298310904 | validation: 0.006452216899353334]
	TIME [epoch: 25.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0056426134062518354		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.0056426134062518354 | validation: 0.006513909424798899]
	TIME [epoch: 25.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042115369081802845		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.0042115369081802845 | validation: 0.004657666039355762]
	TIME [epoch: 25.9 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004717805618285223		[learning rate: 0.00082662]
	Learning Rate: 0.000826624
	LOSS [training: 0.004717805618285223 | validation: 0.005708660202582587]
	TIME [epoch: 25.9 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005546450360535309		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.005546450360535309 | validation: 0.004696581150155561]
	TIME [epoch: 26 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004734507254931791		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.004734507254931791 | validation: 0.004330576424745031]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_756.pth
	Model improved!!!
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004688526358524135		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.004688526358524135 | validation: 0.005633297889048286]
	TIME [epoch: 25.9 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004843334163183119		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.004843334163183119 | validation: 0.00693193726463268]
	TIME [epoch: 25.9 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005848549212827727		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.005848549212827727 | validation: 0.004736224933041317]
	TIME [epoch: 25.9 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004914305103455301		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.004914305103455301 | validation: 0.004151446576612216]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004397974700767579		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.004397974700767579 | validation: 0.0057229253369462645]
	TIME [epoch: 25.9 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005021137210230699		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.005021137210230699 | validation: 0.004757010178817066]
	TIME [epoch: 25.9 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004291058248876529		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.004291058248876529 | validation: 0.005346808660203546]
	TIME [epoch: 25.9 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004638464391723808		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.004638464391723808 | validation: 0.005100653748040965]
	TIME [epoch: 25.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004471227661127272		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.004471227661127272 | validation: 0.008252868025680753]
	TIME [epoch: 25.9 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004620732500075313		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.004620732500075313 | validation: 0.004658958672881011]
	TIME [epoch: 25.9 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004379981801652826		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.004379981801652826 | validation: 0.007259465521421286]
	TIME [epoch: 25.9 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005986754613256869		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.005986754613256869 | validation: 0.006029671979410178]
	TIME [epoch: 25.9 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004060591416296322		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.004060591416296322 | validation: 0.00430508551459003]
	TIME [epoch: 25.9 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004922954634977035		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.004922954634977035 | validation: 0.005905156772859608]
	TIME [epoch: 25.9 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0052109835124415525		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.0052109835124415525 | validation: 0.004403471249878352]
	TIME [epoch: 25.9 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004054977547105736		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.004054977547105736 | validation: 0.006040183486739126]
	TIME [epoch: 25.9 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006798579223137717		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.006798579223137717 | validation: 0.006131879076707883]
	TIME [epoch: 25.9 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004782626717317952		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.004782626717317952 | validation: 0.0060387463512532845]
	TIME [epoch: 25.9 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005612831571195842		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.005612831571195842 | validation: 0.00574882444601553]
	TIME [epoch: 25.9 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004816542839091765		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.004816542839091765 | validation: 0.005646535906561719]
	TIME [epoch: 25.9 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004774851318122891		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.004774851318122891 | validation: 0.006897441624326085]
	TIME [epoch: 25.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004819176959887082		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.004819176959887082 | validation: 0.005854103810408903]
	TIME [epoch: 25.9 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004265534746590732		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.004265534746590732 | validation: 0.004788346803249506]
	TIME [epoch: 25.9 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004282176799448608		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.004282176799448608 | validation: 0.009175196954242156]
	TIME [epoch: 25.9 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00587441783584825		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.00587441783584825 | validation: 0.005028745313112449]
	TIME [epoch: 25.9 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004311165371265139		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.004311165371265139 | validation: 0.005108342084835365]
	TIME [epoch: 25.9 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005028720798597314		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.005028720798597314 | validation: 0.004973600122519262]
	TIME [epoch: 25.9 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0042522716674706605		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.0042522716674706605 | validation: 0.0039826869969517825]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004373078011528994		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.004373078011528994 | validation: 0.005002432389831954]
	TIME [epoch: 25.9 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005048086219513835		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.005048086219513835 | validation: 0.004964528554104119]
	TIME [epoch: 25.9 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00548798427834448		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.00548798427834448 | validation: 0.0049568576224401605]
	TIME [epoch: 25.9 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005083759764545885		[learning rate: 0.00073282]
	Learning Rate: 0.000732825
	LOSS [training: 0.005083759764545885 | validation: 0.005039984223742232]
	TIME [epoch: 26 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004030499866088201		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.004030499866088201 | validation: 0.0042920636804025]
	TIME [epoch: 26 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004181571748390792		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.004181571748390792 | validation: 0.006097089476071363]
	TIME [epoch: 26 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005109672242607615		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.005109672242607615 | validation: 0.0043353022354418996]
	TIME [epoch: 25.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004316505075620753		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.004316505075620753 | validation: 0.005493226361071662]
	TIME [epoch: 26 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004150677057767415		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.004150677057767415 | validation: 0.005944612849502706]
	TIME [epoch: 25.9 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004389720929844947		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.004389720929844947 | validation: 0.004794606748581247]
	TIME [epoch: 25.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004338561265268454		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.004338561265268454 | validation: 0.004461023123838904]
	TIME [epoch: 26 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0043534192302651245		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.0043534192302651245 | validation: 0.00553562862658313]
	TIME [epoch: 26 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004811326521291756		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.004811326521291756 | validation: 0.0046579878937307514]
	TIME [epoch: 25.9 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.006300372596152666		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.006300372596152666 | validation: 0.004704627336133431]
	TIME [epoch: 25.9 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038896713936221955		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.0038896713936221955 | validation: 0.004770619577617538]
	TIME [epoch: 25.9 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00397972250987729		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.00397972250987729 | validation: 0.005124538485365114]
	TIME [epoch: 26 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00525258333951034		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.00525258333951034 | validation: 0.004567672824051742]
	TIME [epoch: 25.9 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00462415283411317		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.00462415283411317 | validation: 0.00598749262375278]
	TIME [epoch: 25.9 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004579476869725813		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.004579476869725813 | validation: 0.004395886205775327]
	TIME [epoch: 25.9 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004106209274902289		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.004106209274902289 | validation: 0.0041869106910548725]
	TIME [epoch: 25.9 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004080047675993034		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.004080047675993034 | validation: 0.004538025750075444]
	TIME [epoch: 25.9 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004829012645885199		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.004829012645885199 | validation: 0.006551479821975679]
	TIME [epoch: 25.9 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004142100394980246		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.004142100394980246 | validation: 0.004906395765589537]
	TIME [epoch: 25.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039247989975859186		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.0039247989975859186 | validation: 0.005089496628587226]
	TIME [epoch: 25.9 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034185222893147868		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.0034185222893147868 | validation: 0.0050538391836635905]
	TIME [epoch: 26 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003996944939829104		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.003996944939829104 | validation: 0.004284840189650622]
	TIME [epoch: 25.9 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004536577607309011		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.004536577607309011 | validation: 0.006070940183687345]
	TIME [epoch: 25.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004432379625099607		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.004432379625099607 | validation: 0.006409845264344112]
	TIME [epoch: 26 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004301167779144175		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.004301167779144175 | validation: 0.0050518350409325665]
	TIME [epoch: 25.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034532842890105516		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.0034532842890105516 | validation: 0.004146522237748794]
	TIME [epoch: 25.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004151725714760616		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.004151725714760616 | validation: 0.006668150238729231]
	TIME [epoch: 25.9 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005954195144755126		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.005954195144755126 | validation: 0.0046449557323836885]
	TIME [epoch: 25.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005005592122717579		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.005005592122717579 | validation: 0.004403720336181577]
	TIME [epoch: 25.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004001364446296372		[learning rate: 0.00065894]
	Learning Rate: 0.00065894
	LOSS [training: 0.004001364446296372 | validation: 0.003770357815073732]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_818.pth
	Model improved!!!
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037654935290152665		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.0037654935290152665 | validation: 0.004988995529308371]
	TIME [epoch: 25.9 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004377961835482459		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.004377961835482459 | validation: 0.007638957328277721]
	TIME [epoch: 25.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004785045163670957		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.004785045163670957 | validation: 0.006331700915782661]
	TIME [epoch: 25.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004162750718241061		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.004162750718241061 | validation: 0.00461499618806951]
	TIME [epoch: 25.9 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003981605427572087		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.003981605427572087 | validation: 0.004036777772825589]
	TIME [epoch: 25.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004073004356823842		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.004073004356823842 | validation: 0.00392446099681415]
	TIME [epoch: 25.9 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003956293241737555		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.003956293241737555 | validation: 0.0060012063581404256]
	TIME [epoch: 25.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004773555038559323		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.004773555038559323 | validation: 0.004287272562242726]
	TIME [epoch: 25.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036960440746428343		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.0036960440746428343 | validation: 0.004429451986899079]
	TIME [epoch: 25.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003913966724790883		[learning rate: 0.00063601]
	Learning Rate: 0.000636007
	LOSS [training: 0.003913966724790883 | validation: 0.005385604998438088]
	TIME [epoch: 25.9 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004577247031850079		[learning rate: 0.00063376]
	Learning Rate: 0.000633758
	LOSS [training: 0.004577247031850079 | validation: 0.003937013018971331]
	TIME [epoch: 25.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004032006362172361		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.004032006362172361 | validation: 0.013982430581338152]
	TIME [epoch: 25.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.008479354142882745		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.008479354142882745 | validation: 0.0055800553983179]
	TIME [epoch: 25.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0044911627564467385		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.0044911627564467385 | validation: 0.005268063726375134]
	TIME [epoch: 25.9 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004207898067914052		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.004207898067914052 | validation: 0.005203570378595578]
	TIME [epoch: 25.8 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003820313654886047		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.003820313654886047 | validation: 0.004131616991980827]
	TIME [epoch: 25.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037546291294144325		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.0037546291294144325 | validation: 0.005524830453449839]
	TIME [epoch: 25.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036452622349258715		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.0036452622349258715 | validation: 0.0053272114549782895]
	TIME [epoch: 25.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004176546532873049		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.004176546532873049 | validation: 0.004213547437266402]
	TIME [epoch: 25.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003900774016069697		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.003900774016069697 | validation: 0.0048296475441337375]
	TIME [epoch: 25.8 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038135319468964157		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.0038135319468964157 | validation: 0.004699984394089593]
	TIME [epoch: 25.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003810989397674533		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.003810989397674533 | validation: 0.004428887057140408]
	TIME [epoch: 25.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003928513392926914		[learning rate: 0.00060738]
	Learning Rate: 0.000607382
	LOSS [training: 0.003928513392926914 | validation: 0.004211398769882901]
	TIME [epoch: 25.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004083566291931418		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.004083566291931418 | validation: 0.005669471291602819]
	TIME [epoch: 25.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.005223412991929756		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.005223412991929756 | validation: 0.004308547440666621]
	TIME [epoch: 25.9 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003947792201016742		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.003947792201016742 | validation: 0.004139393107524102]
	TIME [epoch: 25.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003684638107398598		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.003684638107398598 | validation: 0.004568998886010196]
	TIME [epoch: 25.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038496520935978806		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.0038496520935978806 | validation: 0.0050687284678854845]
	TIME [epoch: 25.9 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004766024936997871		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.004766024936997871 | validation: 0.005946695807376504]
	TIME [epoch: 25.9 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004537814416525639		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.004537814416525639 | validation: 0.0037525989965744326]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003257511134728184		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.003257511134728184 | validation: 0.0046970677216504955]
	TIME [epoch: 25.9 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003773699642008822		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.003773699642008822 | validation: 0.005531777969228838]
	TIME [epoch: 25.9 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004522345757508602		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.004522345757508602 | validation: 0.004990811726749396]
	TIME [epoch: 25.9 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040553531761429		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.0040553531761429 | validation: 0.004638461614709985]
	TIME [epoch: 25.9 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0041957509380842065		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.0041957509380842065 | validation: 0.00430365959327256]
	TIME [epoch: 25.9 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039015578962953244		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.0039015578962953244 | validation: 0.004833089738230864]
	TIME [epoch: 26 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036824015762099162		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.0036824015762099162 | validation: 0.003914929941075779]
	TIME [epoch: 25.9 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037674831312760086		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.0037674831312760086 | validation: 0.0048088775965704714]
	TIME [epoch: 25.9 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003977335495278087		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.003977335495278087 | validation: 0.0041284910943538535]
	TIME [epoch: 25.9 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00396738653479708		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.00396738653479708 | validation: 0.004829551571185208]
	TIME [epoch: 25.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036823151325297617		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.0036823151325297617 | validation: 0.005084302819699757]
	TIME [epoch: 25.9 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004040824462647359		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.004040824462647359 | validation: 0.0050184184763306106]
	TIME [epoch: 25.9 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040435120303202385		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.0040435120303202385 | validation: 0.004333021038405956]
	TIME [epoch: 25.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003945128937704896		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.003945128937704896 | validation: 0.0033875532119713087]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_862.pth
	Model improved!!!
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003943509409831621		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.003943509409831621 | validation: 0.004600130837122331]
	TIME [epoch: 25.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004178938656454038		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.004178938656454038 | validation: 0.004056846789161716]
	TIME [epoch: 25.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037023516457340335		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.0037023516457340335 | validation: 0.005250045576964391]
	TIME [epoch: 25.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004044185671689984		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.004044185671689984 | validation: 0.004166392306105372]
	TIME [epoch: 25.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004284193360243335		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.004284193360243335 | validation: 0.0044583851016703245]
	TIME [epoch: 25.9 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003739889631296383		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.003739889631296383 | validation: 0.004564210975068603]
	TIME [epoch: 25.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0040340088648725195		[learning rate: 0.00055003]
	Learning Rate: 0.000550028
	LOSS [training: 0.0040340088648725195 | validation: 0.004505562220328727]
	TIME [epoch: 25.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004141084743695746		[learning rate: 0.00054808]
	Learning Rate: 0.000548083
	LOSS [training: 0.004141084743695746 | validation: 0.0036762904109598066]
	TIME [epoch: 25.9 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003841656816988896		[learning rate: 0.00054614]
	Learning Rate: 0.000546145
	LOSS [training: 0.003841656816988896 | validation: 0.004722970535821986]
	TIME [epoch: 25.9 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003518832786692276		[learning rate: 0.00054421]
	Learning Rate: 0.000544214
	LOSS [training: 0.003518832786692276 | validation: 0.004895673381463504]
	TIME [epoch: 25.9 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003688132147084896		[learning rate: 0.00054229]
	Learning Rate: 0.000542289
	LOSS [training: 0.003688132147084896 | validation: 0.004790654248453445]
	TIME [epoch: 25.9 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004478624616097213		[learning rate: 0.00054037]
	Learning Rate: 0.000540371
	LOSS [training: 0.004478624616097213 | validation: 0.004515355071694621]
	TIME [epoch: 26 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003591782824119226		[learning rate: 0.00053846]
	Learning Rate: 0.000538461
	LOSS [training: 0.003591782824119226 | validation: 0.0043744672286403375]
	TIME [epoch: 25.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038659006855699396		[learning rate: 0.00053656]
	Learning Rate: 0.000536556
	LOSS [training: 0.0038659006855699396 | validation: 0.005530711708419121]
	TIME [epoch: 25.9 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035854122446209256		[learning rate: 0.00053466]
	Learning Rate: 0.000534659
	LOSS [training: 0.0035854122446209256 | validation: 0.0042575778089444195]
	TIME [epoch: 25.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003698513872756965		[learning rate: 0.00053277]
	Learning Rate: 0.000532768
	LOSS [training: 0.003698513872756965 | validation: 0.0037241125850942906]
	TIME [epoch: 25.9 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037435427064445486		[learning rate: 0.00053088]
	Learning Rate: 0.000530885
	LOSS [training: 0.0037435427064445486 | validation: 0.004529856476819083]
	TIME [epoch: 25.9 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036013849163158743		[learning rate: 0.00052901]
	Learning Rate: 0.000529007
	LOSS [training: 0.0036013849163158743 | validation: 0.004701158822522433]
	TIME [epoch: 25.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003951432179980038		[learning rate: 0.00052714]
	Learning Rate: 0.000527137
	LOSS [training: 0.003951432179980038 | validation: 0.004287556188086356]
	TIME [epoch: 25.9 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0037321100488814		[learning rate: 0.00052527]
	Learning Rate: 0.000525273
	LOSS [training: 0.0037321100488814 | validation: 0.003839764236135092]
	TIME [epoch: 25.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033327490130235387		[learning rate: 0.00052341]
	Learning Rate: 0.000523415
	LOSS [training: 0.0033327490130235387 | validation: 0.0059679473643207345]
	TIME [epoch: 25.9 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038290036139598475		[learning rate: 0.00052156]
	Learning Rate: 0.000521564
	LOSS [training: 0.0038290036139598475 | validation: 0.005073562321340848]
	TIME [epoch: 25.9 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004190202400878604		[learning rate: 0.00051972]
	Learning Rate: 0.00051972
	LOSS [training: 0.004190202400878604 | validation: 0.005012425505304739]
	TIME [epoch: 25.9 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0039414801913557574		[learning rate: 0.00051788]
	Learning Rate: 0.000517882
	LOSS [training: 0.0039414801913557574 | validation: 0.005065615160178202]
	TIME [epoch: 25.9 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003769911297938704		[learning rate: 0.00051605]
	Learning Rate: 0.000516051
	LOSS [training: 0.003769911297938704 | validation: 0.004423343816697845]
	TIME [epoch: 25.9 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003421332086253185		[learning rate: 0.00051423]
	Learning Rate: 0.000514226
	LOSS [training: 0.003421332086253185 | validation: 0.004463713281453273]
	TIME [epoch: 26 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004121087621531314		[learning rate: 0.00051241]
	Learning Rate: 0.000512407
	LOSS [training: 0.004121087621531314 | validation: 0.004676346152644713]
	TIME [epoch: 25.9 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004309971056997574		[learning rate: 0.0005106]
	Learning Rate: 0.000510596
	LOSS [training: 0.004309971056997574 | validation: 0.004420427955112331]
	TIME [epoch: 25.9 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033349011318572507		[learning rate: 0.00050879]
	Learning Rate: 0.00050879
	LOSS [training: 0.0033349011318572507 | validation: 0.004212271162323464]
	TIME [epoch: 25.9 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032749513503950653		[learning rate: 0.00050699]
	Learning Rate: 0.000506991
	LOSS [training: 0.0032749513503950653 | validation: 0.0038338075201863717]
	TIME [epoch: 26 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033404231660757295		[learning rate: 0.0005052]
	Learning Rate: 0.000505198
	LOSS [training: 0.0033404231660757295 | validation: 0.004230230089409318]
	TIME [epoch: 25.9 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038778980558717563		[learning rate: 0.00050341]
	Learning Rate: 0.000503412
	LOSS [training: 0.0038778980558717563 | validation: 0.004566218220775884]
	TIME [epoch: 25.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004189290564855116		[learning rate: 0.00050163]
	Learning Rate: 0.000501631
	LOSS [training: 0.004189290564855116 | validation: 0.003677857187897855]
	TIME [epoch: 25.9 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003733560577817358		[learning rate: 0.00049986]
	Learning Rate: 0.000499857
	LOSS [training: 0.003733560577817358 | validation: 0.003717738229641058]
	TIME [epoch: 25.9 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038588860119623467		[learning rate: 0.00049809]
	Learning Rate: 0.00049809
	LOSS [training: 0.0038588860119623467 | validation: 0.004672592484184537]
	TIME [epoch: 25.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035294304590055463		[learning rate: 0.00049633]
	Learning Rate: 0.000496329
	LOSS [training: 0.0035294304590055463 | validation: 0.005057288791072652]
	TIME [epoch: 26 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00370233723538025		[learning rate: 0.00049457]
	Learning Rate: 0.000494573
	LOSS [training: 0.00370233723538025 | validation: 0.004576473400691656]
	TIME [epoch: 25.9 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003775523155639022		[learning rate: 0.00049282]
	Learning Rate: 0.000492825
	LOSS [training: 0.003775523155639022 | validation: 0.00405067726576427]
	TIME [epoch: 25.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033359281712534486		[learning rate: 0.00049108]
	Learning Rate: 0.000491082
	LOSS [training: 0.0033359281712534486 | validation: 0.004535635440544365]
	TIME [epoch: 26 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003290247418293429		[learning rate: 0.00048935]
	Learning Rate: 0.000489345
	LOSS [training: 0.003290247418293429 | validation: 0.004559944827287537]
	TIME [epoch: 26 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035777723634452113		[learning rate: 0.00048761]
	Learning Rate: 0.000487615
	LOSS [training: 0.0035777723634452113 | validation: 0.00435896974588416]
	TIME [epoch: 26 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003195373522364657		[learning rate: 0.00048589]
	Learning Rate: 0.000485891
	LOSS [training: 0.003195373522364657 | validation: 0.004121788228361809]
	TIME [epoch: 25.9 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003516339217615559		[learning rate: 0.00048417]
	Learning Rate: 0.000484172
	LOSS [training: 0.003516339217615559 | validation: 0.0033088222801884158]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_905.pth
	Model improved!!!
EPOCH 906/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003759442275201114		[learning rate: 0.00048246]
	Learning Rate: 0.00048246
	LOSS [training: 0.003759442275201114 | validation: 0.005248828530756059]
	TIME [epoch: 25.9 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003662192096547879		[learning rate: 0.00048075]
	Learning Rate: 0.000480754
	LOSS [training: 0.003662192096547879 | validation: 0.0041344550734794]
	TIME [epoch: 25.9 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00335876387113525		[learning rate: 0.00047905]
	Learning Rate: 0.000479054
	LOSS [training: 0.00335876387113525 | validation: 0.005477024484300914]
	TIME [epoch: 26 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034244751338754955		[learning rate: 0.00047736]
	Learning Rate: 0.00047736
	LOSS [training: 0.0034244751338754955 | validation: 0.0040905866876134915]
	TIME [epoch: 25.9 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035082798454666793		[learning rate: 0.00047567]
	Learning Rate: 0.000475672
	LOSS [training: 0.0035082798454666793 | validation: 0.004058968870187767]
	TIME [epoch: 25.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003631203196052645		[learning rate: 0.00047399]
	Learning Rate: 0.00047399
	LOSS [training: 0.003631203196052645 | validation: 0.005883172498768985]
	TIME [epoch: 26 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004012326161737718		[learning rate: 0.00047231]
	Learning Rate: 0.000472314
	LOSS [training: 0.004012326161737718 | validation: 0.0037995410753780983]
	TIME [epoch: 25.9 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032929347751719873		[learning rate: 0.00047064]
	Learning Rate: 0.000470644
	LOSS [training: 0.0032929347751719873 | validation: 0.003708902349002837]
	TIME [epoch: 26 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032381552741584784		[learning rate: 0.00046898]
	Learning Rate: 0.00046898
	LOSS [training: 0.0032381552741584784 | validation: 0.0047004065425446315]
	TIME [epoch: 25.9 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035034023063433193		[learning rate: 0.00046732]
	Learning Rate: 0.000467321
	LOSS [training: 0.0035034023063433193 | validation: 0.004123245269972003]
	TIME [epoch: 26 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032655321992838444		[learning rate: 0.00046567]
	Learning Rate: 0.000465669
	LOSS [training: 0.0032655321992838444 | validation: 0.0035750537357312176]
	TIME [epoch: 25.9 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038604032926601835		[learning rate: 0.00046402]
	Learning Rate: 0.000464022
	LOSS [training: 0.0038604032926601835 | validation: 0.003916474967829544]
	TIME [epoch: 25.9 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003387975273101723		[learning rate: 0.00046238]
	Learning Rate: 0.000462381
	LOSS [training: 0.003387975273101723 | validation: 0.0037503037913944864]
	TIME [epoch: 25.9 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003573944362343372		[learning rate: 0.00046075]
	Learning Rate: 0.000460746
	LOSS [training: 0.003573944362343372 | validation: 0.0049409506103409865]
	TIME [epoch: 25.9 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003557181258999031		[learning rate: 0.00045912]
	Learning Rate: 0.000459117
	LOSS [training: 0.003557181258999031 | validation: 0.0037100204734247565]
	TIME [epoch: 26 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033015409105070313		[learning rate: 0.00045749]
	Learning Rate: 0.000457493
	LOSS [training: 0.0033015409105070313 | validation: 0.004690049034334957]
	TIME [epoch: 26 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035846717734028736		[learning rate: 0.00045588]
	Learning Rate: 0.000455876
	LOSS [training: 0.0035846717734028736 | validation: 0.0037860726864765108]
	TIME [epoch: 25.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0038724671498018472		[learning rate: 0.00045426]
	Learning Rate: 0.000454263
	LOSS [training: 0.0038724671498018472 | validation: 0.004222214600912526]
	TIME [epoch: 25.9 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036653635976633273		[learning rate: 0.00045266]
	Learning Rate: 0.000452657
	LOSS [training: 0.0036653635976633273 | validation: 0.005484652898354393]
	TIME [epoch: 25.9 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003777471638513048		[learning rate: 0.00045106]
	Learning Rate: 0.000451056
	LOSS [training: 0.003777471638513048 | validation: 0.003874393351276749]
	TIME [epoch: 25.9 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031706796178562324		[learning rate: 0.00044946]
	Learning Rate: 0.000449461
	LOSS [training: 0.0031706796178562324 | validation: 0.0034642137040893674]
	TIME [epoch: 26 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034486067228631425		[learning rate: 0.00044787]
	Learning Rate: 0.000447872
	LOSS [training: 0.0034486067228631425 | validation: 0.00452297025230375]
	TIME [epoch: 25.9 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003613908735276314		[learning rate: 0.00044629]
	Learning Rate: 0.000446288
	LOSS [training: 0.003613908735276314 | validation: 0.0043136015664951765]
	TIME [epoch: 25.9 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033558207970321276		[learning rate: 0.00044471]
	Learning Rate: 0.00044471
	LOSS [training: 0.0033558207970321276 | validation: 0.004732775460727904]
	TIME [epoch: 25.9 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003505650158725965		[learning rate: 0.00044314]
	Learning Rate: 0.000443138
	LOSS [training: 0.003505650158725965 | validation: 0.003953399891975885]
	TIME [epoch: 25.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004304893745151363		[learning rate: 0.00044157]
	Learning Rate: 0.000441571
	LOSS [training: 0.004304893745151363 | validation: 0.005672841870577816]
	TIME [epoch: 26 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003817081123691595		[learning rate: 0.00044001]
	Learning Rate: 0.000440009
	LOSS [training: 0.003817081123691595 | validation: 0.0035873268767095514]
	TIME [epoch: 26 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003083158690677702		[learning rate: 0.00043845]
	Learning Rate: 0.000438453
	LOSS [training: 0.003083158690677702 | validation: 0.003977758504428279]
	TIME [epoch: 26 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033763771735202657		[learning rate: 0.0004369]
	Learning Rate: 0.000436903
	LOSS [training: 0.0033763771735202657 | validation: 0.004634329383507734]
	TIME [epoch: 25.9 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003441779912132161		[learning rate: 0.00043536]
	Learning Rate: 0.000435358
	LOSS [training: 0.003441779912132161 | validation: 0.003949969739626105]
	TIME [epoch: 25.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034562004025279697		[learning rate: 0.00043382]
	Learning Rate: 0.000433818
	LOSS [training: 0.0034562004025279697 | validation: 0.003945586441623358]
	TIME [epoch: 25.9 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003586318234450999		[learning rate: 0.00043228]
	Learning Rate: 0.000432284
	LOSS [training: 0.003586318234450999 | validation: 0.003913902527947024]
	TIME [epoch: 25.9 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030931697144638308		[learning rate: 0.00043076]
	Learning Rate: 0.000430755
	LOSS [training: 0.0030931697144638308 | validation: 0.004823556504356934]
	TIME [epoch: 26 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035648778348714725		[learning rate: 0.00042923]
	Learning Rate: 0.000429232
	LOSS [training: 0.0035648778348714725 | validation: 0.004424052398997006]
	TIME [epoch: 25.9 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034142379302769194		[learning rate: 0.00042771]
	Learning Rate: 0.000427714
	LOSS [training: 0.0034142379302769194 | validation: 0.0042207909314008005]
	TIME [epoch: 25.9 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029199008056760055		[learning rate: 0.0004262]
	Learning Rate: 0.000426202
	LOSS [training: 0.0029199008056760055 | validation: 0.004793771720923189]
	TIME [epoch: 26 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003511556965651399		[learning rate: 0.00042469]
	Learning Rate: 0.000424695
	LOSS [training: 0.003511556965651399 | validation: 0.00399735452229517]
	TIME [epoch: 25.9 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003141617057292602		[learning rate: 0.00042319]
	Learning Rate: 0.000423193
	LOSS [training: 0.003141617057292602 | validation: 0.0030326179418101356]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_943.pth
	Model improved!!!
EPOCH 944/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033425839019311154		[learning rate: 0.0004217]
	Learning Rate: 0.000421697
	LOSS [training: 0.0033425839019311154 | validation: 0.004537679256632572]
	TIME [epoch: 25.9 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003464549083514585		[learning rate: 0.00042021]
	Learning Rate: 0.000420205
	LOSS [training: 0.003464549083514585 | validation: 0.003946832509163167]
	TIME [epoch: 26 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003340255216682968		[learning rate: 0.00041872]
	Learning Rate: 0.000418719
	LOSS [training: 0.003340255216682968 | validation: 0.003447021873620888]
	TIME [epoch: 25.9 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003776663145899367		[learning rate: 0.00041724]
	Learning Rate: 0.000417239
	LOSS [training: 0.003776663145899367 | validation: 0.0030590010041460867]
	TIME [epoch: 25.9 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003446809182244679		[learning rate: 0.00041576]
	Learning Rate: 0.000415763
	LOSS [training: 0.003446809182244679 | validation: 0.00373279869146949]
	TIME [epoch: 25.9 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003699774662988061		[learning rate: 0.00041429]
	Learning Rate: 0.000414293
	LOSS [training: 0.003699774662988061 | validation: 0.0038115652466878193]
	TIME [epoch: 25.9 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003408790855641563		[learning rate: 0.00041283]
	Learning Rate: 0.000412828
	LOSS [training: 0.003408790855641563 | validation: 0.004051048807949639]
	TIME [epoch: 25.9 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002959106073007328		[learning rate: 0.00041137]
	Learning Rate: 0.000411368
	LOSS [training: 0.002959106073007328 | validation: 0.004461470511046523]
	TIME [epoch: 25.9 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035672971497958525		[learning rate: 0.00040991]
	Learning Rate: 0.000409914
	LOSS [training: 0.0035672971497958525 | validation: 0.003711198800788458]
	TIME [epoch: 25.9 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0035691788668800995		[learning rate: 0.00040846]
	Learning Rate: 0.000408464
	LOSS [training: 0.0035691788668800995 | validation: 0.00388409661798321]
	TIME [epoch: 25.9 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032076909218044726		[learning rate: 0.00040702]
	Learning Rate: 0.00040702
	LOSS [training: 0.0032076909218044726 | validation: 0.004206295145301353]
	TIME [epoch: 25.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003508537300672193		[learning rate: 0.00040558]
	Learning Rate: 0.00040558
	LOSS [training: 0.003508537300672193 | validation: 0.0045211510630129995]
	TIME [epoch: 25.9 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034222368350022136		[learning rate: 0.00040415]
	Learning Rate: 0.000404146
	LOSS [training: 0.0034222368350022136 | validation: 0.004575165246401915]
	TIME [epoch: 25.9 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00391386169272164		[learning rate: 0.00040272]
	Learning Rate: 0.000402717
	LOSS [training: 0.00391386169272164 | validation: 0.004233605942168629]
	TIME [epoch: 25.9 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003137582403344		[learning rate: 0.00040129]
	Learning Rate: 0.000401293
	LOSS [training: 0.003137582403344 | validation: 0.005535196070923487]
	TIME [epoch: 26 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003349612830911946		[learning rate: 0.00039987]
	Learning Rate: 0.000399874
	LOSS [training: 0.003349612830911946 | validation: 0.0035950241761458185]
	TIME [epoch: 25.9 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003189430305382963		[learning rate: 0.00039846]
	Learning Rate: 0.00039846
	LOSS [training: 0.003189430305382963 | validation: 0.003966804422970727]
	TIME [epoch: 25.9 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033253153279205177		[learning rate: 0.00039705]
	Learning Rate: 0.000397051
	LOSS [training: 0.0033253153279205177 | validation: 0.0046596660794100235]
	TIME [epoch: 25.9 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003332014596885368		[learning rate: 0.00039565]
	Learning Rate: 0.000395647
	LOSS [training: 0.003332014596885368 | validation: 0.004851880796531578]
	TIME [epoch: 25.9 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003583715906678007		[learning rate: 0.00039425]
	Learning Rate: 0.000394248
	LOSS [training: 0.003583715906678007 | validation: 0.00380456986101075]
	TIME [epoch: 25.9 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003363625733118898		[learning rate: 0.00039285]
	Learning Rate: 0.000392854
	LOSS [training: 0.003363625733118898 | validation: 0.0036898598007206857]
	TIME [epoch: 25.9 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003417790916212687		[learning rate: 0.00039146]
	Learning Rate: 0.000391464
	LOSS [training: 0.003417790916212687 | validation: 0.004073361620872786]
	TIME [epoch: 25.9 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003319979332827094		[learning rate: 0.00039008]
	Learning Rate: 0.00039008
	LOSS [training: 0.003319979332827094 | validation: 0.0036204559323141355]
	TIME [epoch: 25.9 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003380005649505125		[learning rate: 0.0003887]
	Learning Rate: 0.000388701
	LOSS [training: 0.003380005649505125 | validation: 0.0035090237511042404]
	TIME [epoch: 25.9 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033972326506354197		[learning rate: 0.00038733]
	Learning Rate: 0.000387326
	LOSS [training: 0.0033972326506354197 | validation: 0.004632724165110265]
	TIME [epoch: 25.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002970891087411612		[learning rate: 0.00038596]
	Learning Rate: 0.000385957
	LOSS [training: 0.002970891087411612 | validation: 0.003681161367011145]
	TIME [epoch: 26 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031485512524798403		[learning rate: 0.00038459]
	Learning Rate: 0.000384592
	LOSS [training: 0.0031485512524798403 | validation: 0.004401233814736833]
	TIME [epoch: 25.9 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034776689242391424		[learning rate: 0.00038323]
	Learning Rate: 0.000383232
	LOSS [training: 0.0034776689242391424 | validation: 0.004164730389775095]
	TIME [epoch: 25.9 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003014191546946462		[learning rate: 0.00038188]
	Learning Rate: 0.000381877
	LOSS [training: 0.003014191546946462 | validation: 0.003923323066667387]
	TIME [epoch: 25.9 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003362942112943792		[learning rate: 0.00038053]
	Learning Rate: 0.000380526
	LOSS [training: 0.003362942112943792 | validation: 0.0040022292797076]
	TIME [epoch: 25.9 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036996460601185213		[learning rate: 0.00037918]
	Learning Rate: 0.000379181
	LOSS [training: 0.0036996460601185213 | validation: 0.0035497867656207544]
	TIME [epoch: 25.9 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003049429345267637		[learning rate: 0.00037784]
	Learning Rate: 0.00037784
	LOSS [training: 0.003049429345267637 | validation: 0.004447559239741056]
	TIME [epoch: 25.9 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031561467182303774		[learning rate: 0.0003765]
	Learning Rate: 0.000376504
	LOSS [training: 0.0031561467182303774 | validation: 0.003922705308484216]
	TIME [epoch: 25.9 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030397020934340322		[learning rate: 0.00037517]
	Learning Rate: 0.000375172
	LOSS [training: 0.0030397020934340322 | validation: 0.0042657127209837515]
	TIME [epoch: 25.9 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033148013849201518		[learning rate: 0.00037385]
	Learning Rate: 0.000373846
	LOSS [training: 0.0033148013849201518 | validation: 0.003368363329562105]
	TIME [epoch: 25.9 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030244907444814976		[learning rate: 0.00037252]
	Learning Rate: 0.000372524
	LOSS [training: 0.0030244907444814976 | validation: 0.00392054478264503]
	TIME [epoch: 25.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003409708429925552		[learning rate: 0.00037121]
	Learning Rate: 0.000371206
	LOSS [training: 0.003409708429925552 | validation: 0.004679932346815317]
	TIME [epoch: 25.9 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003212191741419648		[learning rate: 0.00036989]
	Learning Rate: 0.000369894
	LOSS [training: 0.003212191741419648 | validation: 0.003741762474855326]
	TIME [epoch: 25.9 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00309957443611854		[learning rate: 0.00036859]
	Learning Rate: 0.000368586
	LOSS [training: 0.00309957443611854 | validation: 0.0040970760566469205]
	TIME [epoch: 26 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003503698188673545		[learning rate: 0.00036728]
	Learning Rate: 0.000367282
	LOSS [training: 0.003503698188673545 | validation: 0.004123296397698998]
	TIME [epoch: 25.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003380962290025325		[learning rate: 0.00036598]
	Learning Rate: 0.000365984
	LOSS [training: 0.003380962290025325 | validation: 0.0034322057901934036]
	TIME [epoch: 25.9 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029834000694438836		[learning rate: 0.00036469]
	Learning Rate: 0.000364689
	LOSS [training: 0.0029834000694438836 | validation: 0.0033512721170549428]
	TIME [epoch: 25.9 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.004255384649567491		[learning rate: 0.0003634]
	Learning Rate: 0.0003634
	LOSS [training: 0.004255384649567491 | validation: 0.005019314647897473]
	TIME [epoch: 25.9 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029830621962561588		[learning rate: 0.00036211]
	Learning Rate: 0.000362115
	LOSS [training: 0.0029830621962561588 | validation: 0.003491767602181976]
	TIME [epoch: 25.9 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030116019475721232		[learning rate: 0.00036083]
	Learning Rate: 0.000360834
	LOSS [training: 0.0030116019475721232 | validation: 0.003730547451795255]
	TIME [epoch: 25.9 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030816259980689883		[learning rate: 0.00035956]
	Learning Rate: 0.000359558
	LOSS [training: 0.0030816259980689883 | validation: 0.0031343867188738436]
	TIME [epoch: 25.9 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030573060308179164		[learning rate: 0.00035829]
	Learning Rate: 0.000358287
	LOSS [training: 0.0030573060308179164 | validation: 0.005069515742246399]
	TIME [epoch: 25.9 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0036037441073281658		[learning rate: 0.00035702]
	Learning Rate: 0.00035702
	LOSS [training: 0.0036037441073281658 | validation: 0.0039851347425501425]
	TIME [epoch: 25.9 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030406340092468337		[learning rate: 0.00035576]
	Learning Rate: 0.000355757
	LOSS [training: 0.0030406340092468337 | validation: 0.0038567213969501425]
	TIME [epoch: 25.9 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002945419542196356		[learning rate: 0.0003545]
	Learning Rate: 0.000354499
	LOSS [training: 0.002945419542196356 | validation: 0.0039483334549016815]
	TIME [epoch: 25.9 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003319530373802524		[learning rate: 0.00035325]
	Learning Rate: 0.000353246
	LOSS [training: 0.003319530373802524 | validation: 0.0032613223832495004]
	TIME [epoch: 25.9 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002943579383548699		[learning rate: 0.000352]
	Learning Rate: 0.000351997
	LOSS [training: 0.002943579383548699 | validation: 0.004574371316419713]
	TIME [epoch: 25.9 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003349301849753718		[learning rate: 0.00035075]
	Learning Rate: 0.000350752
	LOSS [training: 0.003349301849753718 | validation: 0.003499655284205163]
	TIME [epoch: 25.9 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030037559459462665		[learning rate: 0.00034951]
	Learning Rate: 0.000349512
	LOSS [training: 0.0030037559459462665 | validation: 0.003987980705293659]
	TIME [epoch: 25.9 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028946920077079106		[learning rate: 0.00034828]
	Learning Rate: 0.000348276
	LOSS [training: 0.0028946920077079106 | validation: 0.0032586191560279748]
	TIME [epoch: 25.9 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002807039731539115		[learning rate: 0.00034704]
	Learning Rate: 0.000347044
	LOSS [training: 0.002807039731539115 | validation: 0.004181637715278519]
	TIME [epoch: 26 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003465563985140206		[learning rate: 0.00034582]
	Learning Rate: 0.000345817
	LOSS [training: 0.003465563985140206 | validation: 0.0035654676030537824]
	TIME [epoch: 25.9 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003552066361232217		[learning rate: 0.00034459]
	Learning Rate: 0.000344594
	LOSS [training: 0.003552066361232217 | validation: 0.0038671039265959503]
	TIME [epoch: 481 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003331432927115186		[learning rate: 0.00034338]
	Learning Rate: 0.000343375
	LOSS [training: 0.003331432927115186 | validation: 0.0046961943429248065]
	TIME [epoch: 55 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003233424391600714		[learning rate: 0.00034216]
	Learning Rate: 0.000342161
	LOSS [training: 0.003233424391600714 | validation: 0.00360656389328498]
	TIME [epoch: 54.9 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031223623776607894		[learning rate: 0.00034095]
	Learning Rate: 0.000340951
	LOSS [training: 0.0031223623776607894 | validation: 0.0032624285548414457]
	TIME [epoch: 54.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027436195462913946		[learning rate: 0.00033975]
	Learning Rate: 0.000339746
	LOSS [training: 0.0027436195462913946 | validation: 0.0036905634738842756]
	TIME [epoch: 54.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030580848817235084		[learning rate: 0.00033854]
	Learning Rate: 0.000338544
	LOSS [training: 0.0030580848817235084 | validation: 0.0037955634960100283]
	TIME [epoch: 55 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003348464471155326		[learning rate: 0.00033735]
	Learning Rate: 0.000337347
	LOSS [training: 0.003348464471155326 | validation: 0.004154208104557371]
	TIME [epoch: 54.9 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0034439265262170816		[learning rate: 0.00033615]
	Learning Rate: 0.000336154
	LOSS [training: 0.0034439265262170816 | validation: 0.003358699364895813]
	TIME [epoch: 55 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003211215267912942		[learning rate: 0.00033497]
	Learning Rate: 0.000334966
	LOSS [training: 0.003211215267912942 | validation: 0.0035807171368072432]
	TIME [epoch: 54.9 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033676574275207814		[learning rate: 0.00033378]
	Learning Rate: 0.000333781
	LOSS [training: 0.0033676574275207814 | validation: 0.004738157668762261]
	TIME [epoch: 54.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003011269570348693		[learning rate: 0.0003326]
	Learning Rate: 0.000332601
	LOSS [training: 0.003011269570348693 | validation: 0.003278754675256903]
	TIME [epoch: 54.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002974385362235601		[learning rate: 0.00033142]
	Learning Rate: 0.000331425
	LOSS [training: 0.002974385362235601 | validation: 0.00345148229978268]
	TIME [epoch: 54.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027970583234418274		[learning rate: 0.00033025]
	Learning Rate: 0.000330253
	LOSS [training: 0.0027970583234418274 | validation: 0.004368325955880332]
	TIME [epoch: 55 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032689045995813156		[learning rate: 0.00032908]
	Learning Rate: 0.000329085
	LOSS [training: 0.0032689045995813156 | validation: 0.0034971285516807306]
	TIME [epoch: 54.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003146756077888435		[learning rate: 0.00032792]
	Learning Rate: 0.000327921
	LOSS [training: 0.003146756077888435 | validation: 0.0037502928174557885]
	TIME [epoch: 55 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0033390687530899994		[learning rate: 0.00032676]
	Learning Rate: 0.000326761
	LOSS [training: 0.0033390687530899994 | validation: 0.004176603554865541]
	TIME [epoch: 55 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00273204988326796		[learning rate: 0.00032561]
	Learning Rate: 0.000325606
	LOSS [training: 0.00273204988326796 | validation: 0.0036361010480658353]
	TIME [epoch: 54.9 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002895128641579028		[learning rate: 0.00032445]
	Learning Rate: 0.000324455
	LOSS [training: 0.002895128641579028 | validation: 0.0038297739392866036]
	TIME [epoch: 54.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003137465880832661		[learning rate: 0.00032331]
	Learning Rate: 0.000323307
	LOSS [training: 0.003137465880832661 | validation: 0.0034728303182894383]
	TIME [epoch: 54.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030854995035890865		[learning rate: 0.00032216]
	Learning Rate: 0.000322164
	LOSS [training: 0.0030854995035890865 | validation: 0.003727354806820902]
	TIME [epoch: 55 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031459188148542853		[learning rate: 0.00032102]
	Learning Rate: 0.000321025
	LOSS [training: 0.0031459188148542853 | validation: 0.0033147000107538807]
	TIME [epoch: 55 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003099952978828581		[learning rate: 0.00031989]
	Learning Rate: 0.00031989
	LOSS [training: 0.003099952978828581 | validation: 0.0029566537888921973]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003270301511679463		[learning rate: 0.00031876]
	Learning Rate: 0.000318758
	LOSS [training: 0.003270301511679463 | validation: 0.0036513972152803316]
	TIME [epoch: 55 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003102455067265183		[learning rate: 0.00031763]
	Learning Rate: 0.000317631
	LOSS [training: 0.003102455067265183 | validation: 0.003363913694563729]
	TIME [epoch: 55 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032717131988268967		[learning rate: 0.00031651]
	Learning Rate: 0.000316508
	LOSS [training: 0.0032717131988268967 | validation: 0.004007538549274188]
	TIME [epoch: 55 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003399787316863842		[learning rate: 0.00031539]
	Learning Rate: 0.000315389
	LOSS [training: 0.003399787316863842 | validation: 0.003046642390711238]
	TIME [epoch: 54.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029189849091216073		[learning rate: 0.00031427]
	Learning Rate: 0.000314274
	LOSS [training: 0.0029189849091216073 | validation: 0.004102300719615854]
	TIME [epoch: 54.9 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002834160786000439		[learning rate: 0.00031316]
	Learning Rate: 0.000313162
	LOSS [training: 0.002834160786000439 | validation: 0.0033252393393752534]
	TIME [epoch: 54.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002840085900597372		[learning rate: 0.00031205]
	Learning Rate: 0.000312055
	LOSS [training: 0.002840085900597372 | validation: 0.003751018565227482]
	TIME [epoch: 55 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002898576674216839		[learning rate: 0.00031095]
	Learning Rate: 0.000310951
	LOSS [training: 0.002898576674216839 | validation: 0.0036810852336552485]
	TIME [epoch: 54.9 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002907495950900019		[learning rate: 0.00030985]
	Learning Rate: 0.000309852
	LOSS [training: 0.002907495950900019 | validation: 0.002936187899423983]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_1031.pth
	Model improved!!!
EPOCH 1032/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003067008609019609		[learning rate: 0.00030876]
	Learning Rate: 0.000308756
	LOSS [training: 0.003067008609019609 | validation: 0.004081370673089884]
	TIME [epoch: 54.9 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003205357226914583		[learning rate: 0.00030766]
	Learning Rate: 0.000307664
	LOSS [training: 0.003205357226914583 | validation: 0.003589426053051452]
	TIME [epoch: 55 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030198848337238855		[learning rate: 0.00030658]
	Learning Rate: 0.000306576
	LOSS [training: 0.0030198848337238855 | validation: 0.004039137230991822]
	TIME [epoch: 55 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030137975062148664		[learning rate: 0.00030549]
	Learning Rate: 0.000305492
	LOSS [training: 0.0030137975062148664 | validation: 0.0036299041958681054]
	TIME [epoch: 54.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029090693952643254		[learning rate: 0.00030441]
	Learning Rate: 0.000304412
	LOSS [training: 0.0029090693952643254 | validation: 0.00268652327696089]
	TIME [epoch: 54.9 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_1036.pth
	Model improved!!!
EPOCH 1037/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002892630760039103		[learning rate: 0.00030334]
	Learning Rate: 0.000303335
	LOSS [training: 0.002892630760039103 | validation: 0.0036090958848194986]
	TIME [epoch: 55 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002942521640126433		[learning rate: 0.00030226]
	Learning Rate: 0.000302263
	LOSS [training: 0.002942521640126433 | validation: 0.003868195839611301]
	TIME [epoch: 55 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030609758513313653		[learning rate: 0.00030119]
	Learning Rate: 0.000301194
	LOSS [training: 0.0030609758513313653 | validation: 0.003214464315663893]
	TIME [epoch: 54.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028656818783326585		[learning rate: 0.00030013]
	Learning Rate: 0.000300129
	LOSS [training: 0.0028656818783326585 | validation: 0.0038025290812240425]
	TIME [epoch: 55 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028831322575865828		[learning rate: 0.00029907]
	Learning Rate: 0.000299068
	LOSS [training: 0.0028831322575865828 | validation: 0.0036496937381370444]
	TIME [epoch: 54.9 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028711460634370453		[learning rate: 0.00029801]
	Learning Rate: 0.00029801
	LOSS [training: 0.0028711460634370453 | validation: 0.003766671228149664]
	TIME [epoch: 54.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028786927580300293		[learning rate: 0.00029696]
	Learning Rate: 0.000296956
	LOSS [training: 0.0028786927580300293 | validation: 0.003324841137632773]
	TIME [epoch: 54.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002856756127005613		[learning rate: 0.00029591]
	Learning Rate: 0.000295906
	LOSS [training: 0.002856756127005613 | validation: 0.0033040684077665014]
	TIME [epoch: 54.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003048094321727029		[learning rate: 0.00029486]
	Learning Rate: 0.00029486
	LOSS [training: 0.003048094321727029 | validation: 0.00398919293578508]
	TIME [epoch: 54.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029832626364671034		[learning rate: 0.00029382]
	Learning Rate: 0.000293817
	LOSS [training: 0.0029832626364671034 | validation: 0.0035954558702124325]
	TIME [epoch: 54.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002940805265984045		[learning rate: 0.00029278]
	Learning Rate: 0.000292778
	LOSS [training: 0.002940805265984045 | validation: 0.003743790777503344]
	TIME [epoch: 55 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028795111323600313		[learning rate: 0.00029174]
	Learning Rate: 0.000291743
	LOSS [training: 0.0028795111323600313 | validation: 0.0031858256866231375]
	TIME [epoch: 55 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002943254378403396		[learning rate: 0.00029071]
	Learning Rate: 0.000290711
	LOSS [training: 0.002943254378403396 | validation: 0.003209529393033433]
	TIME [epoch: 55 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003152118139721846		[learning rate: 0.00028968]
	Learning Rate: 0.000289683
	LOSS [training: 0.003152118139721846 | validation: 0.003276232208185433]
	TIME [epoch: 55 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026932576045770127		[learning rate: 0.00028866]
	Learning Rate: 0.000288659
	LOSS [training: 0.0026932576045770127 | validation: 0.0034371331684695835]
	TIME [epoch: 55 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029279560996300744		[learning rate: 0.00028764]
	Learning Rate: 0.000287638
	LOSS [training: 0.0029279560996300744 | validation: 0.003432467798979636]
	TIME [epoch: 55 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003139388079734854		[learning rate: 0.00028662]
	Learning Rate: 0.000286621
	LOSS [training: 0.003139388079734854 | validation: 0.0036312266838313]
	TIME [epoch: 54.9 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031459122538743723		[learning rate: 0.00028561]
	Learning Rate: 0.000285607
	LOSS [training: 0.0031459122538743723 | validation: 0.003354438085924837]
	TIME [epoch: 55 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028592254491731214		[learning rate: 0.0002846]
	Learning Rate: 0.000284597
	LOSS [training: 0.0028592254491731214 | validation: 0.0035907425231197303]
	TIME [epoch: 55 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002889478801425754		[learning rate: 0.00028359]
	Learning Rate: 0.000283591
	LOSS [training: 0.002889478801425754 | validation: 0.003601082857870543]
	TIME [epoch: 54.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003017340918284961		[learning rate: 0.00028259]
	Learning Rate: 0.000282588
	LOSS [training: 0.003017340918284961 | validation: 0.0036501189835763067]
	TIME [epoch: 55 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031095364681254147		[learning rate: 0.00028159]
	Learning Rate: 0.000281589
	LOSS [training: 0.0031095364681254147 | validation: 0.0040387876159671005]
	TIME [epoch: 54.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002954298683670085		[learning rate: 0.00028059]
	Learning Rate: 0.000280593
	LOSS [training: 0.002954298683670085 | validation: 0.003897765864430398]
	TIME [epoch: 55 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030689911299184415		[learning rate: 0.0002796]
	Learning Rate: 0.000279601
	LOSS [training: 0.0030689911299184415 | validation: 0.0032882089517087655]
	TIME [epoch: 55 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003108547982809966		[learning rate: 0.00027861]
	Learning Rate: 0.000278612
	LOSS [training: 0.003108547982809966 | validation: 0.0031854268341079397]
	TIME [epoch: 55 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003217664665706314		[learning rate: 0.00027763]
	Learning Rate: 0.000277627
	LOSS [training: 0.003217664665706314 | validation: 0.00402183105368126]
	TIME [epoch: 55 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003005288775274448		[learning rate: 0.00027665]
	Learning Rate: 0.000276645
	LOSS [training: 0.003005288775274448 | validation: 0.003098037354576461]
	TIME [epoch: 55 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026569708285684415		[learning rate: 0.00027567]
	Learning Rate: 0.000275667
	LOSS [training: 0.0026569708285684415 | validation: 0.0033325106026318094]
	TIME [epoch: 54.9 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028465052730596583		[learning rate: 0.00027469]
	Learning Rate: 0.000274692
	LOSS [training: 0.0028465052730596583 | validation: 0.0035551418409173257]
	TIME [epoch: 54.9 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002847967442976362		[learning rate: 0.00027372]
	Learning Rate: 0.000273721
	LOSS [training: 0.002847967442976362 | validation: 0.0036988232367973902]
	TIME [epoch: 55 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029222572351630784		[learning rate: 0.00027275]
	Learning Rate: 0.000272753
	LOSS [training: 0.0029222572351630784 | validation: 0.003670292441066148]
	TIME [epoch: 55 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029708224480198327		[learning rate: 0.00027179]
	Learning Rate: 0.000271788
	LOSS [training: 0.0029708224480198327 | validation: 0.003991354586440701]
	TIME [epoch: 55 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029842745450601752		[learning rate: 0.00027083]
	Learning Rate: 0.000270827
	LOSS [training: 0.0029842745450601752 | validation: 0.0042291613682960034]
	TIME [epoch: 55 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029467769358321756		[learning rate: 0.00026987]
	Learning Rate: 0.00026987
	LOSS [training: 0.0029467769358321756 | validation: 0.004193207103570174]
	TIME [epoch: 54.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028555129573438007		[learning rate: 0.00026892]
	Learning Rate: 0.000268915
	LOSS [training: 0.0028555129573438007 | validation: 0.003213713598160826]
	TIME [epoch: 54.9 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027258968340008677		[learning rate: 0.00026796]
	Learning Rate: 0.000267964
	LOSS [training: 0.0027258968340008677 | validation: 0.0035417796295895087]
	TIME [epoch: 54.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030738044573706924		[learning rate: 0.00026702]
	Learning Rate: 0.000267017
	LOSS [training: 0.0030738044573706924 | validation: 0.0035881831788757354]
	TIME [epoch: 55 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028418507878796833		[learning rate: 0.00026607]
	Learning Rate: 0.000266073
	LOSS [training: 0.0028418507878796833 | validation: 0.0031509570291230328]
	TIME [epoch: 55 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028625485957484483		[learning rate: 0.00026513]
	Learning Rate: 0.000265132
	LOSS [training: 0.0028625485957484483 | validation: 0.0034764631496387536]
	TIME [epoch: 55 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029991605153597985		[learning rate: 0.00026419]
	Learning Rate: 0.000264194
	LOSS [training: 0.0029991605153597985 | validation: 0.003466230845901113]
	TIME [epoch: 55 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029838762098834035		[learning rate: 0.00026326]
	Learning Rate: 0.00026326
	LOSS [training: 0.0029838762098834035 | validation: 0.003919118500948184]
	TIME [epoch: 55 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029544852558553046		[learning rate: 0.00026233]
	Learning Rate: 0.000262329
	LOSS [training: 0.0029544852558553046 | validation: 0.002901207848718167]
	TIME [epoch: 55 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028127394424010545		[learning rate: 0.0002614]
	Learning Rate: 0.000261401
	LOSS [training: 0.0028127394424010545 | validation: 0.0034137158574652302]
	TIME [epoch: 55 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029910360033514258		[learning rate: 0.00026048]
	Learning Rate: 0.000260477
	LOSS [training: 0.0029910360033514258 | validation: 0.003739749419683455]
	TIME [epoch: 55 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002936291027301665		[learning rate: 0.00025956]
	Learning Rate: 0.000259556
	LOSS [training: 0.002936291027301665 | validation: 0.0033968548702136073]
	TIME [epoch: 55 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002944008366483758		[learning rate: 0.00025864]
	Learning Rate: 0.000258638
	LOSS [training: 0.002944008366483758 | validation: 0.0036942337079379018]
	TIME [epoch: 55 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029826268805826666		[learning rate: 0.00025772]
	Learning Rate: 0.000257723
	LOSS [training: 0.0029826268805826666 | validation: 0.003254344038655106]
	TIME [epoch: 55 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003014097460721118		[learning rate: 0.00025681]
	Learning Rate: 0.000256812
	LOSS [training: 0.003014097460721118 | validation: 0.003559780185382079]
	TIME [epoch: 55 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0032279503803098685		[learning rate: 0.0002559]
	Learning Rate: 0.000255904
	LOSS [training: 0.0032279503803098685 | validation: 0.0035844789506150657]
	TIME [epoch: 55 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003061067457072227		[learning rate: 0.000255]
	Learning Rate: 0.000254999
	LOSS [training: 0.003061067457072227 | validation: 0.0037109313400524755]
	TIME [epoch: 55 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027820022321377123		[learning rate: 0.0002541]
	Learning Rate: 0.000254097
	LOSS [training: 0.0027820022321377123 | validation: 0.0038800679436907324]
	TIME [epoch: 55.1 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029016941501959718		[learning rate: 0.0002532]
	Learning Rate: 0.000253199
	LOSS [training: 0.0029016941501959718 | validation: 0.003410225926466646]
	TIME [epoch: 55 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002759550273764166		[learning rate: 0.0002523]
	Learning Rate: 0.000252303
	LOSS [training: 0.002759550273764166 | validation: 0.003214232006848905]
	TIME [epoch: 55 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002786855763324235		[learning rate: 0.00025141]
	Learning Rate: 0.000251411
	LOSS [training: 0.002786855763324235 | validation: 0.0034047018918398494]
	TIME [epoch: 55 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028210609221576703		[learning rate: 0.00025052]
	Learning Rate: 0.000250522
	LOSS [training: 0.0028210609221576703 | validation: 0.003208535622260518]
	TIME [epoch: 54.9 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003027580717420087		[learning rate: 0.00024964]
	Learning Rate: 0.000249636
	LOSS [training: 0.003027580717420087 | validation: 0.0032979896355830735]
	TIME [epoch: 55 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002820648610528833		[learning rate: 0.00024875]
	Learning Rate: 0.000248754
	LOSS [training: 0.002820648610528833 | validation: 0.002960179601081884]
	TIME [epoch: 55 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027648065821647767		[learning rate: 0.00024787]
	Learning Rate: 0.000247874
	LOSS [training: 0.0027648065821647767 | validation: 0.0034886985474403415]
	TIME [epoch: 55 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002820172810677305		[learning rate: 0.000247]
	Learning Rate: 0.000246997
	LOSS [training: 0.002820172810677305 | validation: 0.002814270365674559]
	TIME [epoch: 55 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025488203068481747		[learning rate: 0.00024612]
	Learning Rate: 0.000246124
	LOSS [training: 0.0025488203068481747 | validation: 0.003481818253415316]
	TIME [epoch: 55 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031322720433592756		[learning rate: 0.00024525]
	Learning Rate: 0.000245254
	LOSS [training: 0.0031322720433592756 | validation: 0.0031064242385468897]
	TIME [epoch: 55 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002731222724831922		[learning rate: 0.00024439]
	Learning Rate: 0.000244386
	LOSS [training: 0.002731222724831922 | validation: 0.003041542303604969]
	TIME [epoch: 55 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003190417864347557		[learning rate: 0.00024352]
	Learning Rate: 0.000243522
	LOSS [training: 0.003190417864347557 | validation: 0.003190472917523542]
	TIME [epoch: 55 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026428778139468098		[learning rate: 0.00024266]
	Learning Rate: 0.000242661
	LOSS [training: 0.0026428778139468098 | validation: 0.003128992098230334]
	TIME [epoch: 55 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026944288530241544		[learning rate: 0.0002418]
	Learning Rate: 0.000241803
	LOSS [training: 0.0026944288530241544 | validation: 0.002978466224970191]
	TIME [epoch: 55 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002821178086363216		[learning rate: 0.00024095]
	Learning Rate: 0.000240948
	LOSS [training: 0.002821178086363216 | validation: 0.003371178505872245]
	TIME [epoch: 55 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029468364509228056		[learning rate: 0.0002401]
	Learning Rate: 0.000240096
	LOSS [training: 0.0029468364509228056 | validation: 0.002874518022476077]
	TIME [epoch: 55 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002781922561656505		[learning rate: 0.00023925]
	Learning Rate: 0.000239247
	LOSS [training: 0.002781922561656505 | validation: 0.0034894231188318265]
	TIME [epoch: 55 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002948062754881163		[learning rate: 0.0002384]
	Learning Rate: 0.000238401
	LOSS [training: 0.002948062754881163 | validation: 0.0031208153917859213]
	TIME [epoch: 55 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027452381216915085		[learning rate: 0.00023756]
	Learning Rate: 0.000237558
	LOSS [training: 0.0027452381216915085 | validation: 0.0035307454185611285]
	TIME [epoch: 55 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0030842413170332787		[learning rate: 0.00023672]
	Learning Rate: 0.000236718
	LOSS [training: 0.0030842413170332787 | validation: 0.0032354905838645775]
	TIME [epoch: 55 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0031911890696504847		[learning rate: 0.00023588]
	Learning Rate: 0.000235881
	LOSS [training: 0.0031911890696504847 | validation: 0.0036294745228906316]
	TIME [epoch: 54.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002535990265424456		[learning rate: 0.00023505]
	Learning Rate: 0.000235047
	LOSS [training: 0.002535990265424456 | validation: 0.0032558743688197557]
	TIME [epoch: 55 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.003027629823259865		[learning rate: 0.00023422]
	Learning Rate: 0.000234215
	LOSS [training: 0.003027629823259865 | validation: 0.0034111184323330507]
	TIME [epoch: 55 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002843319808390467		[learning rate: 0.00023339]
	Learning Rate: 0.000233387
	LOSS [training: 0.002843319808390467 | validation: 0.004057295518437694]
	TIME [epoch: 54.9 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002941600541895335		[learning rate: 0.00023256]
	Learning Rate: 0.000232562
	LOSS [training: 0.002941600541895335 | validation: 0.003377231689633362]
	TIME [epoch: 55 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025638325035435474		[learning rate: 0.00023174]
	Learning Rate: 0.00023174
	LOSS [training: 0.0025638325035435474 | validation: 0.003064998859679557]
	TIME [epoch: 55 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002712382006330348		[learning rate: 0.00023092]
	Learning Rate: 0.00023092
	LOSS [training: 0.002712382006330348 | validation: 0.0034273739376906214]
	TIME [epoch: 55 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027933558716540666		[learning rate: 0.0002301]
	Learning Rate: 0.000230103
	LOSS [training: 0.0027933558716540666 | validation: 0.0034924410760796792]
	TIME [epoch: 55 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027988006205978302		[learning rate: 0.00022929]
	Learning Rate: 0.00022929
	LOSS [training: 0.0027988006205978302 | validation: 0.0030153715574242736]
	TIME [epoch: 55 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002643945665777316		[learning rate: 0.00022848]
	Learning Rate: 0.000228479
	LOSS [training: 0.002643945665777316 | validation: 0.003636078926040882]
	TIME [epoch: 55 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002648629468112359		[learning rate: 0.00022767]
	Learning Rate: 0.000227671
	LOSS [training: 0.002648629468112359 | validation: 0.0037806451644303006]
	TIME [epoch: 55 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028131995306683835		[learning rate: 0.00022687]
	Learning Rate: 0.000226866
	LOSS [training: 0.0028131995306683835 | validation: 0.003204044064547264]
	TIME [epoch: 55 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027383218208939185		[learning rate: 0.00022606]
	Learning Rate: 0.000226064
	LOSS [training: 0.0027383218208939185 | validation: 0.003024416128297352]
	TIME [epoch: 55 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002684887962457939		[learning rate: 0.00022526]
	Learning Rate: 0.000225264
	LOSS [training: 0.002684887962457939 | validation: 0.0033676241081452337]
	TIME [epoch: 54.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026991990711636173		[learning rate: 0.00022447]
	Learning Rate: 0.000224468
	LOSS [training: 0.0026991990711636173 | validation: 0.0036970383780743302]
	TIME [epoch: 55 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027521113229914533		[learning rate: 0.00022367]
	Learning Rate: 0.000223674
	LOSS [training: 0.0027521113229914533 | validation: 0.002715916660644184]
	TIME [epoch: 55 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027551798908841685		[learning rate: 0.00022288]
	Learning Rate: 0.000222883
	LOSS [training: 0.0027551798908841685 | validation: 0.003357929833572076]
	TIME [epoch: 54.9 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002532005330983692		[learning rate: 0.00022209]
	Learning Rate: 0.000222095
	LOSS [training: 0.002532005330983692 | validation: 0.0033785147686797006]
	TIME [epoch: 54.9 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002568283709619619		[learning rate: 0.00022131]
	Learning Rate: 0.00022131
	LOSS [training: 0.002568283709619619 | validation: 0.0035398517728539533]
	TIME [epoch: 55 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002527139847760074		[learning rate: 0.00022053]
	Learning Rate: 0.000220527
	LOSS [training: 0.002527139847760074 | validation: 0.0031552166921617773]
	TIME [epoch: 54.9 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028433865879728968		[learning rate: 0.00021975]
	Learning Rate: 0.000219747
	LOSS [training: 0.0028433865879728968 | validation: 0.0031987418885033184]
	TIME [epoch: 54.9 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002513469518464419		[learning rate: 0.00021897]
	Learning Rate: 0.00021897
	LOSS [training: 0.002513469518464419 | validation: 0.003484736959231169]
	TIME [epoch: 55 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002790192958086504		[learning rate: 0.0002182]
	Learning Rate: 0.000218196
	LOSS [training: 0.002790192958086504 | validation: 0.003241466011952866]
	TIME [epoch: 54.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002646072077780223		[learning rate: 0.00021742]
	Learning Rate: 0.000217424
	LOSS [training: 0.002646072077780223 | validation: 0.003176855473761186]
	TIME [epoch: 54.9 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002877963560737246		[learning rate: 0.00021666]
	Learning Rate: 0.000216655
	LOSS [training: 0.002877963560737246 | validation: 0.002590179192655204]
	TIME [epoch: 55 sec]
	Saving model to: out/model_training/model_phi1_1a_v_mmd1_20241205_124553/states/model_phi1_1a_v_mmd1_1132.pth
	Model improved!!!
EPOCH 1133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028619814918380785		[learning rate: 0.00021589]
	Learning Rate: 0.000215889
	LOSS [training: 0.0028619814918380785 | validation: 0.0033735965794717284]
	TIME [epoch: 55 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026078894057164144		[learning rate: 0.00021513]
	Learning Rate: 0.000215126
	LOSS [training: 0.0026078894057164144 | validation: 0.0033009614927516006]
	TIME [epoch: 55 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026425952457858415		[learning rate: 0.00021436]
	Learning Rate: 0.000214365
	LOSS [training: 0.0026425952457858415 | validation: 0.003541371567215087]
	TIME [epoch: 54.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002841840903876351		[learning rate: 0.00021361]
	Learning Rate: 0.000213607
	LOSS [training: 0.002841840903876351 | validation: 0.0037881572108466365]
	TIME [epoch: 55 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026739457308789356		[learning rate: 0.00021285]
	Learning Rate: 0.000212852
	LOSS [training: 0.0026739457308789356 | validation: 0.004031022708337288]
	TIME [epoch: 55 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026395053125720025		[learning rate: 0.0002121]
	Learning Rate: 0.000212099
	LOSS [training: 0.0026395053125720025 | validation: 0.0029352094695548926]
	TIME [epoch: 54.9 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026608971705914157		[learning rate: 0.00021135]
	Learning Rate: 0.000211349
	LOSS [training: 0.0026608971705914157 | validation: 0.0034664164081311555]
	TIME [epoch: 55 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027137988354049114		[learning rate: 0.0002106]
	Learning Rate: 0.000210602
	LOSS [training: 0.0027137988354049114 | validation: 0.003184124742932572]
	TIME [epoch: 55 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0029026768261342737		[learning rate: 0.00020986]
	Learning Rate: 0.000209857
	LOSS [training: 0.0029026768261342737 | validation: 0.0030184264320749763]
	TIME [epoch: 54.9 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028961425231138413		[learning rate: 0.00020911]
	Learning Rate: 0.000209115
	LOSS [training: 0.0028961425231138413 | validation: 0.003734527833777152]
	TIME [epoch: 55 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027192408789397816		[learning rate: 0.00020838]
	Learning Rate: 0.000208375
	LOSS [training: 0.0027192408789397816 | validation: 0.0033127389419466387]
	TIME [epoch: 54.9 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027356496026170697		[learning rate: 0.00020764]
	Learning Rate: 0.000207638
	LOSS [training: 0.0027356496026170697 | validation: 0.0036073856809460763]
	TIME [epoch: 55 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026591208545060727		[learning rate: 0.0002069]
	Learning Rate: 0.000206904
	LOSS [training: 0.0026591208545060727 | validation: 0.003086308417258487]
	TIME [epoch: 55 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002674965283746622		[learning rate: 0.00020617]
	Learning Rate: 0.000206173
	LOSS [training: 0.002674965283746622 | validation: 0.003726011409449101]
	TIME [epoch: 55 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025109670314019603		[learning rate: 0.00020544]
	Learning Rate: 0.000205444
	LOSS [training: 0.0025109670314019603 | validation: 0.0032250474913514495]
	TIME [epoch: 55 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0028133521328850292		[learning rate: 0.00020472]
	Learning Rate: 0.000204717
	LOSS [training: 0.0028133521328850292 | validation: 0.0032515765885444492]
	TIME [epoch: 55 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002894763140623123		[learning rate: 0.00020399]
	Learning Rate: 0.000203993
	LOSS [training: 0.002894763140623123 | validation: 0.0035876345404682163]
	TIME [epoch: 55 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025301483216651505		[learning rate: 0.00020327]
	Learning Rate: 0.000203272
	LOSS [training: 0.0025301483216651505 | validation: 0.0028522485337061153]
	TIME [epoch: 55 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002586551739492747		[learning rate: 0.00020255]
	Learning Rate: 0.000202553
	LOSS [training: 0.002586551739492747 | validation: 0.0036855299106345624]
	TIME [epoch: 55 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002639286305804906		[learning rate: 0.00020184]
	Learning Rate: 0.000201837
	LOSS [training: 0.002639286305804906 | validation: 0.003185021017285008]
	TIME [epoch: 55 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026966711556628007		[learning rate: 0.00020112]
	Learning Rate: 0.000201123
	LOSS [training: 0.0026966711556628007 | validation: 0.003546461181408689]
	TIME [epoch: 55 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002775098227337246		[learning rate: 0.00020041]
	Learning Rate: 0.000200412
	LOSS [training: 0.002775098227337246 | validation: 0.003267639840443187]
	TIME [epoch: 55 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002468296905352007		[learning rate: 0.0001997]
	Learning Rate: 0.000199703
	LOSS [training: 0.002468296905352007 | validation: 0.0036719702204000945]
	TIME [epoch: 55 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002733090143910434		[learning rate: 0.000199]
	Learning Rate: 0.000198997
	LOSS [training: 0.002733090143910434 | validation: 0.0028695524802170055]
	TIME [epoch: 55 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0026833059241560455		[learning rate: 0.00019829]
	Learning Rate: 0.000198293
	LOSS [training: 0.0026833059241560455 | validation: 0.003477642666014428]
	TIME [epoch: 54.9 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0024597686622989286		[learning rate: 0.00019759]
	Learning Rate: 0.000197592
	LOSS [training: 0.0024597686622989286 | validation: 0.003204526254697215]
	TIME [epoch: 55 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002955242254977089		[learning rate: 0.00019689]
	Learning Rate: 0.000196893
	LOSS [training: 0.002955242254977089 | validation: 0.0035353412165231145]
	TIME [epoch: 55 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002787247918630497		[learning rate: 0.0001962]
	Learning Rate: 0.000196197
	LOSS [training: 0.002787247918630497 | validation: 0.0028865176147845804]
	TIME [epoch: 55 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0025447983312032437		[learning rate: 0.0001955]
	Learning Rate: 0.000195503
	LOSS [training: 0.0025447983312032437 | validation: 0.0030571498626883326]
	TIME [epoch: 55 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002726919530208445		[learning rate: 0.00019481]
	Learning Rate: 0.000194812
	LOSS [training: 0.002726919530208445 | validation: 0.0030353679597186958]
	TIME [epoch: 54.9 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00249308374901253		[learning rate: 0.00019412]
	Learning Rate: 0.000194123
	LOSS [training: 0.00249308374901253 | validation: 0.003631407580141498]
	TIME [epoch: 54.9 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027207227281171695		[learning rate: 0.00019344]
	Learning Rate: 0.000193437
	LOSS [training: 0.0027207227281171695 | validation: 0.003811335838664074]
	TIME [epoch: 55 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002729854448826778		[learning rate: 0.00019275]
	Learning Rate: 0.000192753
	LOSS [training: 0.002729854448826778 | validation: 0.002860831963579064]
	TIME [epoch: 54.9 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002774892701879492		[learning rate: 0.00019207]
	Learning Rate: 0.000192071
	LOSS [training: 0.002774892701879492 | validation: 0.003912916499750707]
	TIME [epoch: 54.9 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0027840429165523276		[learning rate: 0.00019139]
	Learning Rate: 0.000191392
	LOSS [training: 0.0027840429165523276 | validation: 0.0035011648745133313]
	TIME [epoch: 55 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002751400451727553		[learning rate: 0.00019071]
	Learning Rate: 0.000190715
	LOSS [training: 0.002751400451727553 | validation: 0.0030900739068434307]
	TIME [epoch: 55 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002455732963306927		[learning rate: 0.00019004]
	Learning Rate: 0.000190041
	LOSS [training: 0.002455732963306927 | validation: 0.0033667749884488682]
	TIME [epoch: 54.9 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.00292302202592774		[learning rate: 0.00018937]
	Learning Rate: 0.000189369
	LOSS [training: 0.00292302202592774 | validation: 0.0029422551627298335]
	TIME [epoch: 55 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.002633593730073132		[learning rate: 0.0001887]
	Learning Rate: 0.000188699
	LOSS [training: 0.002633593730073132 | validation: 0.0035804081710817544]
	TIME [epoch: 54.9 sec]
EPOCH 1172/2000:
	Training over batches...
