Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3988997589

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.679826973953425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.679826973953425 | validation: 6.803754228286445]
	TIME [epoch: 169 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.277674379539408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.277674379539408 | validation: 6.54859785914712]
	TIME [epoch: 0.791 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.532682192340185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.532682192340185 | validation: 6.976239585356644]
	TIME [epoch: 0.698 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.520896428021749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.520896428021749 | validation: 6.3996587197880235]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.961287066095893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.961287066095893 | validation: 6.3018398299317155]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.955913612706287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.955913612706287 | validation: 6.474984650201828]
	TIME [epoch: 0.698 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5805996269083895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5805996269083895 | validation: 6.524209606881733]
	TIME [epoch: 0.692 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.371269451866915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.371269451866915 | validation: 6.267630062205608]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.026227851663357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.026227851663357 | validation: 5.834962370291801]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.243589640316542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.243589640316542 | validation: 6.047477944453627]
	TIME [epoch: 0.702 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.13551374781692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.13551374781692 | validation: 6.0435751572464005]
	TIME [epoch: 0.696 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.861608283091446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.861608283091446 | validation: 5.929212402245478]
	TIME [epoch: 0.695 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9273637834675714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9273637834675714 | validation: 6.039299400249865]
	TIME [epoch: 0.694 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8645285567157512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8645285567157512 | validation: 5.850616636048521]
	TIME [epoch: 0.694 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.762970891665854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.762970891665854 | validation: 5.805079129332833]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7553103524470792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7553103524470792 | validation: 5.889681867928947]
	TIME [epoch: 0.697 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7574034215082084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7574034215082084 | validation: 5.807859594166676]
	TIME [epoch: 0.698 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.712645274863344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.712645274863344 | validation: 5.816767474797617]
	TIME [epoch: 0.697 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.687489840504136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.687489840504136 | validation: 5.7608057905118]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6680525447130097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6680525447130097 | validation: 5.784856419143375]
	TIME [epoch: 0.694 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.647879635875188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.647879635875188 | validation: 5.736703183877889]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.636664758247101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.636664758247101 | validation: 5.746509410783389]
	TIME [epoch: 0.692 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.64056280386896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.64056280386896 | validation: 5.710908822291511]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.635048280160902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.635048280160902 | validation: 5.724012608834356]
	TIME [epoch: 0.7 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.653182697191989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.653182697191989 | validation: 5.65535408164391]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.59494796092573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.59494796092573 | validation: 5.657341992204349]
	TIME [epoch: 0.696 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5739635446613183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5739635446613183 | validation: 5.625027229229056]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.542633266524989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.542633266524989 | validation: 5.608063873993803]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5246912389172245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5246912389172245 | validation: 5.579310411839595]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5049809581535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5049809581535 | validation: 5.563413291375774]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4945080905425803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4945080905425803 | validation: 5.534518263784996]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4856161582396283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4856161582396283 | validation: 5.520487858440553]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.493115274015063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.493115274015063 | validation: 5.503138211588103]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.467095780588457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.467095780588457 | validation: 5.474347508645744]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4721052365636447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4721052365636447 | validation: 5.442823905789037]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.414932283144391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.414932283144391 | validation: 5.396568755336283]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3894385528568467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3894385528568467 | validation: 5.375141808415019]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.360834085563913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.360834085563913 | validation: 5.3377448874817]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.336706487735278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.336706487735278 | validation: 5.284062383286252]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3090978913341313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3090978913341313 | validation: 5.2450019562272026]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.268448382234621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.268448382234621 | validation: 5.191212057610835]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2308464471071976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2308464471071976 | validation: 5.135573014615993]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.165031526648434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.165031526648434 | validation: 5.081935084573928]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0902735700876076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0902735700876076 | validation: 4.992200679020467]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.012962515428925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.012962515428925 | validation: 4.892763946742116]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.934088545432537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.934088545432537 | validation: 4.745251354114887]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1065756866731737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1065756866731737 | validation: 4.688520410032086]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.89385970665012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.89385970665012 | validation: 4.373346501238346]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8116815189391957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8116815189391957 | validation: 4.1961120894891435]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5498736897133694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5498736897133694 | validation: 3.5685124879175185]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.279130008510473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.279130008510473 | validation: 1.4933086525615824]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9403409275011998		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.9403409275011998 | validation: 2.3508954093651293]
	TIME [epoch: 0.697 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1233505427014148		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 2.1233505427014148 | validation: 1.793572107673639]
	TIME [epoch: 0.699 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.240312380221308		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.240312380221308 | validation: 1.1883506125330034]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7416150378669368		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.7416150378669368 | validation: 1.673679168841381]
	TIME [epoch: 0.694 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.484112049835421		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.484112049835421 | validation: 1.423757153179322]
	TIME [epoch: 0.697 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3968502986149531		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.3968502986149531 | validation: 1.07129234625225]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3144557885995345		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.3144557885995345 | validation: 1.5306183147606662]
	TIME [epoch: 0.695 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2925063918415662		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.2925063918415662 | validation: 0.9162454827519995]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.279065179424225		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.279065179424225 | validation: 1.8217829442888016]
	TIME [epoch: 0.698 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3426257590893034		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.3426257590893034 | validation: 0.8611976698160118]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2150548676908275		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.2150548676908275 | validation: 1.1496646965267503]
	TIME [epoch: 0.696 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0887327961690292		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 1.0887327961690292 | validation: 0.9649263028534435]
	TIME [epoch: 0.695 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0477781577209397		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.0477781577209397 | validation: 1.0338300949468484]
	TIME [epoch: 0.694 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0275028571069318		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.0275028571069318 | validation: 0.9468723102088004]
	TIME [epoch: 0.694 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.009128261220199		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 1.009128261220199 | validation: 1.209612393528925]
	TIME [epoch: 0.695 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.037541793998589		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 1.037541793998589 | validation: 1.1453517011532297]
	TIME [epoch: 0.69 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3929496160630865		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.3929496160630865 | validation: 2.0812899285416058]
	TIME [epoch: 0.69 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4032589065474015		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.4032589065474015 | validation: 0.9293310132673137]
	TIME [epoch: 0.691 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0415875183892578		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.0415875183892578 | validation: 0.854595063595917]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1086995046886836		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.1086995046886836 | validation: 1.259354935555847]
	TIME [epoch: 0.699 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0343198069120192		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.0343198069120192 | validation: 0.7693416579604867]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748406460323901		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9748406460323901 | validation: 0.8683511813255809]
	TIME [epoch: 0.7 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9439063460007477		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9439063460007477 | validation: 0.9118784219161647]
	TIME [epoch: 0.696 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238803203329252		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9238803203329252 | validation: 0.7856830333805602]
	TIME [epoch: 0.695 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220320423569528		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.9220320423569528 | validation: 1.0979928332741629]
	TIME [epoch: 0.699 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9427101089187339		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.9427101089187339 | validation: 0.7472742531847069]
	TIME [epoch: 1.08 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9975335942429425		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.9975335942429425 | validation: 1.3206026179216521]
	TIME [epoch: 0.695 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0339681947355328		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.0339681947355328 | validation: 0.7361572854272209]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.043819703526567		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.043819703526567 | validation: 1.044837073760769]
	TIME [epoch: 0.698 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9359637615500691		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.9359637615500691 | validation: 0.7770480715608675]
	TIME [epoch: 0.7 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937663696399392		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.8937663696399392 | validation: 0.7710899805585026]
	TIME [epoch: 0.701 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831529401146181		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.8831529401146181 | validation: 0.8910147719294493]
	TIME [epoch: 0.7 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848109389987115		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8848109389987115 | validation: 0.762875454861619]
	TIME [epoch: 0.699 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8822824253310886		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8822824253310886 | validation: 0.9862240465699803]
	TIME [epoch: 0.699 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960250020318341		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8960250020318341 | validation: 0.725778946058397]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.920361226827014		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.920361226827014 | validation: 1.1788119988104555]
	TIME [epoch: 0.698 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9627718482981203		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.9627718482981203 | validation: 0.6839017853510047]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0191442185967428		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 1.0191442185967428 | validation: 1.1588028939313453]
	TIME [epoch: 0.697 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9893650045723505		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9893650045723505 | validation: 0.6434191984801606]
	TIME [epoch: 0.696 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0337361400012393		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 1.0337361400012393 | validation: 0.8406039195202363]
	TIME [epoch: 0.699 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620189483994102		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8620189483994102 | validation: 0.9491708075746262]
	TIME [epoch: 0.695 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857069859246227		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8857069859246227 | validation: 0.6533445284609691]
	TIME [epoch: 0.697 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9308220141589456		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.9308220141589456 | validation: 0.9672592333960236]
	TIME [epoch: 0.696 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806361163474364		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8806361163474364 | validation: 0.7828461381208832]
	TIME [epoch: 0.697 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8785869060528111		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.8785869060528111 | validation: 0.9161120091188713]
	TIME [epoch: 0.694 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917607983394615		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8917607983394615 | validation: 0.7720171338472508]
	TIME [epoch: 0.695 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8924809757013165		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8924809757013165 | validation: 1.0430473466993815]
	TIME [epoch: 0.693 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9062322015783107		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9062322015783107 | validation: 0.6480574468025343]
	TIME [epoch: 0.696 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9207558508957534		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.9207558508957534 | validation: 1.0661891554837946]
	TIME [epoch: 0.698 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927862577611253		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.927862577611253 | validation: 0.6245894191474051]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9699527076879016		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.9699527076879016 | validation: 0.8472009396544185]
	TIME [epoch: 0.703 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505682785904941		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8505682785904941 | validation: 0.808236824470294]
	TIME [epoch: 0.697 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840721096320203		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.840721096320203 | validation: 0.7234079388330364]
	TIME [epoch: 0.701 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433281992853479		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8433281992853479 | validation: 0.8393778840032404]
	TIME [epoch: 0.696 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8399322386597428		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8399322386597428 | validation: 0.6680606598653818]
	TIME [epoch: 0.696 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478999843933102		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8478999843933102 | validation: 0.9662201632390239]
	TIME [epoch: 0.694 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907153325846497		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8907153325846497 | validation: 0.6310122861677311]
	TIME [epoch: 0.697 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0296312070402382		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.0296312070402382 | validation: 1.2403147521261968]
	TIME [epoch: 0.695 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0218402869004517		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0218402869004517 | validation: 0.6216096204479755]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902161450514245		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8902161450514245 | validation: 0.780321861521872]
	TIME [epoch: 0.699 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341412960717776		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8341412960717776 | validation: 0.7933672962940577]
	TIME [epoch: 0.698 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488273114381107		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8488273114381107 | validation: 0.7382435399882045]
	TIME [epoch: 0.7 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478665451679608		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.8478665451679608 | validation: 0.8323183782749597]
	TIME [epoch: 0.698 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8439740934162062		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8439740934162062 | validation: 0.7434682808998135]
	TIME [epoch: 0.699 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519155348995591		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8519155348995591 | validation: 0.8681586767173131]
	TIME [epoch: 0.703 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8581078335526567		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8581078335526567 | validation: 0.7228452199338341]
	TIME [epoch: 0.696 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852437508052926		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.852437508052926 | validation: 0.8527090169525379]
	TIME [epoch: 0.694 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572935986434598		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8572935986434598 | validation: 0.697598333694122]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512384334608267		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8512384334608267 | validation: 0.8941092759241169]
	TIME [epoch: 0.696 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849111803677341		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.849111803677341 | validation: 0.5940036730846303]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8877296721279617		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8877296721279617 | validation: 1.134173420154378]
	TIME [epoch: 0.701 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.034825230936632		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.034825230936632 | validation: 0.6043984373823843]
	TIME [epoch: 0.702 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1118655763058083		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1118655763058083 | validation: 0.8075863629746911]
	TIME [epoch: 0.7 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475759912604105		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8475759912604105 | validation: 0.9561586117766562]
	TIME [epoch: 0.699 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9336728427095574		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.9336728427095574 | validation: 0.5963232945634519]
	TIME [epoch: 0.7 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8976087677604554		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8976087677604554 | validation: 0.8109239960559596]
	TIME [epoch: 0.703 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.851989418481154		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.851989418481154 | validation: 0.7888720930437025]
	TIME [epoch: 0.699 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408721230605538		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8408721230605538 | validation: 0.6961034947470192]
	TIME [epoch: 0.698 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210648443748636		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8210648443748636 | validation: 0.826534029293696]
	TIME [epoch: 0.698 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454339882245724		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8454339882245724 | validation: 0.7016264750668812]
	TIME [epoch: 0.701 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287099317862606		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8287099317862606 | validation: 0.8693527773406583]
	TIME [epoch: 0.701 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390215305958562		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8390215305958562 | validation: 0.6351017822804192]
	TIME [epoch: 0.699 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448596362878976		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8448596362878976 | validation: 0.8268988554432417]
	TIME [epoch: 0.699 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271723763115327		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8271723763115327 | validation: 0.6306404138889028]
	TIME [epoch: 0.703 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361090925679937		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8361090925679937 | validation: 0.8468569969138076]
	TIME [epoch: 0.701 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371439327380611		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8371439327380611 | validation: 0.6036958455531416]
	TIME [epoch: 0.7 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575541637784239		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8575541637784239 | validation: 0.9506020719328254]
	TIME [epoch: 0.698 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692940377757892		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8692940377757892 | validation: 0.5855480821624051]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860510992244155		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8860510992244155 | validation: 0.9305977215067156]
	TIME [epoch: 0.697 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729221441915289		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8729221441915289 | validation: 0.5980141902311416]
	TIME [epoch: 0.695 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621701273012607		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8621701273012607 | validation: 0.8044722616248201]
	TIME [epoch: 0.697 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120628179305434		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8120628179305434 | validation: 0.6468880631186364]
	TIME [epoch: 0.694 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041872253425604		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8041872253425604 | validation: 0.738926506709887]
	TIME [epoch: 0.692 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990925490364208		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7990925490364208 | validation: 0.672603073242624]
	TIME [epoch: 0.691 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968015887621798		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7968015887621798 | validation: 0.8519905495196686]
	TIME [epoch: 0.694 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350471761196075		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8350471761196075 | validation: 0.7165981099470179]
	TIME [epoch: 0.696 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902385126475696		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8902385126475696 | validation: 0.9233607639387243]
	TIME [epoch: 0.695 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826348113623124		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8826348113623124 | validation: 0.7138141681076379]
	TIME [epoch: 0.695 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196871961712373		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8196871961712373 | validation: 0.6040614519642288]
	TIME [epoch: 0.696 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687685712380193		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8687685712380193 | validation: 0.8952989067718736]
	TIME [epoch: 0.695 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8677165025698841		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8677165025698841 | validation: 0.5835185364564254]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806584336222227		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8806584336222227 | validation: 0.8427727762816487]
	TIME [epoch: 0.696 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192900685156309		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8192900685156309 | validation: 0.6774198534877162]
	TIME [epoch: 0.695 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921645978393481		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7921645978393481 | validation: 0.7131269225062179]
	TIME [epoch: 0.694 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924565431855878		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7924565431855878 | validation: 0.6747599838422323]
	TIME [epoch: 0.695 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855547046627285		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7855547046627285 | validation: 0.71793136504301]
	TIME [epoch: 0.694 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869144524743082		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7869144524743082 | validation: 0.6910859166446189]
	TIME [epoch: 0.695 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852194381463486		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7852194381463486 | validation: 0.7216618791330927]
	TIME [epoch: 0.695 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169833868195265		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8169833868195265 | validation: 0.7963235272210284]
	TIME [epoch: 0.694 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8622276171967034		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8622276171967034 | validation: 0.7316349918852696]
	TIME [epoch: 0.695 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500794959657105		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8500794959657105 | validation: 0.7254695434815623]
	TIME [epoch: 0.694 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7885791425705126		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7885791425705126 | validation: 0.6630362631761084]
	TIME [epoch: 0.694 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696799675540834		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7696799675540834 | validation: 0.6236208032013506]
	TIME [epoch: 0.695 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639612456847038		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7639612456847038 | validation: 0.7171210348771084]
	TIME [epoch: 0.696 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770624737440967		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.770624737440967 | validation: 0.5756553293930765]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413968384161632		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8413968384161632 | validation: 1.1172325824951277]
	TIME [epoch: 0.696 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9952623279418958		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.9952623279418958 | validation: 0.6481006446302487]
	TIME [epoch: 0.696 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539479251787631		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.9539479251787631 | validation: 0.7619584015048768]
	TIME [epoch: 0.694 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779837972729263		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.779837972729263 | validation: 0.7455509719026304]
	TIME [epoch: 0.694 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786599766908318		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.786599766908318 | validation: 0.5764771243612528]
	TIME [epoch: 0.697 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983808635932943		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7983808635932943 | validation: 0.7381701151246198]
	TIME [epoch: 0.696 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664194121569066		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7664194121569066 | validation: 0.6281984497053483]
	TIME [epoch: 0.696 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511554356069431		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7511554356069431 | validation: 0.6722799454766542]
	TIME [epoch: 0.697 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502350646819028		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7502350646819028 | validation: 0.6396690085154152]
	TIME [epoch: 0.698 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7482039662478036		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7482039662478036 | validation: 0.6969460235620266]
	TIME [epoch: 0.698 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7608073253715821		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7608073253715821 | validation: 0.7201576022428512]
	TIME [epoch: 0.696 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008652354453889		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8008652354453889 | validation: 0.832598795340016]
	TIME [epoch: 0.697 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8658612741560355		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8658612741560355 | validation: 0.6958455014626163]
	TIME [epoch: 0.697 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8101566309825522		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.8101566309825522 | validation: 0.6380234625659177]
	TIME [epoch: 0.698 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590852272366486		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7590852272366486 | validation: 0.6492859348309102]
	TIME [epoch: 0.696 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418017397317579		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7418017397317579 | validation: 0.6303289333697242]
	TIME [epoch: 0.696 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368337191749504		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7368337191749504 | validation: 0.6495240443932744]
	TIME [epoch: 0.701 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374388745345245		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7374388745345245 | validation: 0.6431757784089391]
	TIME [epoch: 0.697 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7389504778759912		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7389504778759912 | validation: 0.6549509688830002]
	TIME [epoch: 0.694 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718880717459627		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7718880717459627 | validation: 0.8061975134184629]
	TIME [epoch: 0.694 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540392997481087		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.8540392997481087 | validation: 0.6541527657799355]
	TIME [epoch: 0.695 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551737929304994		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.8551737929304994 | validation: 0.6853174794136936]
	TIME [epoch: 0.697 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7560940439165809		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7560940439165809 | validation: 0.6094769484356469]
	TIME [epoch: 0.693 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385096223673118		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7385096223673118 | validation: 0.7994971756083802]
	TIME [epoch: 0.692 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7712292914700447		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7712292914700447 | validation: 0.6131897027924448]
	TIME [epoch: 0.691 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194262367227552		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.8194262367227552 | validation: 0.8500888060825382]
	TIME [epoch: 0.69 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794766154431637		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.794766154431637 | validation: 0.6106325240225984]
	TIME [epoch: 0.689 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551268080567312		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7551268080567312 | validation: 0.6679450974954207]
	TIME [epoch: 0.69 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694185217471862		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7694185217471862 | validation: 0.6539088904149835]
	TIME [epoch: 0.691 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449887101609631		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7449887101609631 | validation: 0.6343293369208486]
	TIME [epoch: 0.691 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190158080739238		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7190158080739238 | validation: 0.5961941905776147]
	TIME [epoch: 0.69 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7141578734830378		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7141578734830378 | validation: 0.6306306259776813]
	TIME [epoch: 0.694 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222116613653816		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7222116613653816 | validation: 0.580320985822185]
	TIME [epoch: 0.697 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214614018243822		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7214614018243822 | validation: 0.6699316886767503]
	TIME [epoch: 0.698 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185597378168024		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7185597378168024 | validation: 0.5683179986974233]
	TIME [epoch: 176 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759597025051752		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7759597025051752 | validation: 0.8147411472214494]
	TIME [epoch: 1.44 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612160204170843		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.8612160204170843 | validation: 0.5421715134023565]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7772977126199107		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7772977126199107 | validation: 0.6557790992181973]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7102360098940281		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7102360098940281 | validation: 0.6410486791965243]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7022168181795666		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7022168181795666 | validation: 0.5634700602036392]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035079103823244		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7035079103823244 | validation: 0.6473717768427328]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7051673903190369		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7051673903190369 | validation: 0.5925688106345904]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7044182742264531		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7044182742264531 | validation: 0.6229684382861452]
	TIME [epoch: 1.36 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7103668510663138		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7103668510663138 | validation: 0.5981356862207562]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.720462116796314		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.720462116796314 | validation: 0.6972528636998859]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047438791488584		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.8047438791488584 | validation: 0.7898097076116496]
	TIME [epoch: 1.36 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981158231446847		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7981158231446847 | validation: 0.5222332340880874]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996601608415395		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6996601608415395 | validation: 0.5685565853529884]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6758057020859364		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.6758057020859364 | validation: 0.618278761288783]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919253167805498		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6919253167805498 | validation: 0.6238558001002137]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7113473589956173		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7113473589956173 | validation: 0.6108211170481903]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177723050415616		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7177723050415616 | validation: 0.611541958426319]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183185197304213		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7183185197304213 | validation: 0.5557561481253979]
	TIME [epoch: 1.36 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7069979105266055		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7069979105266055 | validation: 0.5736445643518099]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6972293948008991		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6972293948008991 | validation: 0.5934639850662992]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839265854314739		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6839265854314739 | validation: 0.5845740865801788]
	TIME [epoch: 1.36 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849598268072505		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6849598268072505 | validation: 0.5944197355200224]
	TIME [epoch: 1.36 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6928516099192138		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6928516099192138 | validation: 0.6210628983000253]
	TIME [epoch: 1.36 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223857920190204		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7223857920190204 | validation: 0.6283975739752159]
	TIME [epoch: 1.36 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7072049664763083		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7072049664763083 | validation: 0.5444541081184698]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880673820905773		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6880673820905773 | validation: 0.6301015067535155]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817320549848127		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6817320549848127 | validation: 0.5561096984120556]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7081807973999474		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7081807973999474 | validation: 0.5447903470712605]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107444312827531		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7107444312827531 | validation: 0.6391584576222997]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7100715010033497		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7100715010033497 | validation: 0.519738412924644]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6823078031668469		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6823078031668469 | validation: 0.5689723876832686]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6596835505309195		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6596835505309195 | validation: 0.5590051101256956]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535728428720193		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6535728428720193 | validation: 0.5065417645901714]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557989807224668		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6557989807224668 | validation: 0.5948347254481727]
	TIME [epoch: 1.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6687676575080045		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6687676575080045 | validation: 0.5585402992574849]
	TIME [epoch: 1.37 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013582012680231		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7013582012680231 | validation: 0.6797649558386981]
	TIME [epoch: 1.36 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285968472537182		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7285968472537182 | validation: 0.5692501870345098]
	TIME [epoch: 1.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7031459782218004		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7031459782218004 | validation: 0.49279788702058264]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708354527360567		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6708354527360567 | validation: 0.5898952480728632]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827197380361384		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6827197380361384 | validation: 0.5462843063837511]
	TIME [epoch: 1.37 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6881364614925098		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6881364614925098 | validation: 0.6670334399530718]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373579222602763		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7373579222602763 | validation: 0.5829358022213421]
	TIME [epoch: 1.36 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7049148800182253		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7049148800182253 | validation: 0.5315327458760584]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6801455689686502		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6801455689686502 | validation: 0.5361634726058909]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476695934566928		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6476695934566928 | validation: 0.5339281659656695]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444855049430612		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6444855049430612 | validation: 0.5582132482675509]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427711482508207		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6427711482508207 | validation: 0.5493080425420942]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6420042702241099		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6420042702241099 | validation: 0.535304674723737]
	TIME [epoch: 1.36 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409207709858139		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6409207709858139 | validation: 0.5509982849310374]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472567681014323		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6472567681014323 | validation: 0.5314152377903393]
	TIME [epoch: 1.36 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701329486178542		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6701329486178542 | validation: 0.6337178262607421]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188518870390381		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7188518870390381 | validation: 0.5896241428834442]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7041720665378383		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7041720665378383 | validation: 0.5071188448251569]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649834939553622		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.649834939553622 | validation: 0.5546285820495914]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306454136174867		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6306454136174867 | validation: 0.5274559686700463]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386389252695903		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6386389252695903 | validation: 0.5428017458823745]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293142626628123		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6293142626628123 | validation: 0.5125720877226992]
	TIME [epoch: 1.42 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226778538951545		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6226778538951545 | validation: 0.5294086652546909]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6252102716465466		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6252102716465466 | validation: 0.5128571847107913]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244206548473306		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6244206548473306 | validation: 0.5555429384993614]
	TIME [epoch: 1.36 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6480533841193216		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6480533841193216 | validation: 0.5270189678438811]
	TIME [epoch: 1.36 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931123033173302		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6931123033173302 | validation: 0.5976973261429853]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6773149504180702		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6773149504180702 | validation: 0.5383489039827613]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6279650704784699		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6279650704784699 | validation: 0.5037308598576341]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6051397936489943		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.6051397936489943 | validation: 0.6078003059036409]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615793924720052		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.615793924720052 | validation: 0.4074690899043363]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160982841825436		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6160982841825436 | validation: 0.5936483261820705]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6131511083140183		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6131511083140183 | validation: 0.459237579261232]
	TIME [epoch: 1.36 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6111032248750858		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6111032248750858 | validation: 0.5624309856858484]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854442161441622		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.5854442161441622 | validation: 0.47124656228007694]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5892850816909934		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5892850816909934 | validation: 0.5863875362545695]
	TIME [epoch: 1.36 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6159123071077439		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.6159123071077439 | validation: 0.49312294928382666]
	TIME [epoch: 1.37 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6552540067417761		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6552540067417761 | validation: 0.5368380984009947]
	TIME [epoch: 1.37 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.600770979659002		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.600770979659002 | validation: 0.4586536292543393]
	TIME [epoch: 1.37 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5561234454322156		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5561234454322156 | validation: 0.5023989990752732]
	TIME [epoch: 1.36 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.550186394225874		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.550186394225874 | validation: 0.428114244740385]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5455300050601827		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5455300050601827 | validation: 0.5879816578831335]
	TIME [epoch: 1.36 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558906059587292		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.558906059587292 | validation: 0.31580230200023324]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853450398783754		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6853450398783754 | validation: 0.6017167985554077]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843364308221825		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5843364308221825 | validation: 0.5371739278029393]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593743664991693		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5593743664991693 | validation: 0.46750164151453133]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381093296005632		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.6381093296005632 | validation: 0.46036509243301477]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5358260670096427		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.5358260670096427 | validation: 0.5053406571285585]
	TIME [epoch: 1.36 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5174728337245789		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5174728337245789 | validation: 0.4380162585385861]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5228287687017124		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.5228287687017124 | validation: 0.5260043760642138]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384742246124475		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.5384742246124475 | validation: 0.44664520254246787]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5666734847255722		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.5666734847255722 | validation: 0.43193795537649227]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5141652049287945		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.5141652049287945 | validation: 0.5323332294310265]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5096307270875013		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.5096307270875013 | validation: 0.32022231883176433]
	TIME [epoch: 1.36 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5714608265830706		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5714608265830706 | validation: 0.6361423812865425]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5596460048329194		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5596460048329194 | validation: 0.38992374196445506]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48227893233363384		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.48227893233363384 | validation: 0.41609438311165187]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46039994556198865		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.46039994556198865 | validation: 0.4912689082043331]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47250807260846345		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.47250807260846345 | validation: 0.39234020805895503]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5201482954110699		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.5201482954110699 | validation: 0.4943018783457033]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139447394282518		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5139447394282518 | validation: 0.4865641200440151]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5178182028161943		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.5178182028161943 | validation: 0.3731316347520104]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44308879113306043		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.44308879113306043 | validation: 0.5332450040526758]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45372592886666924		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.45372592886666924 | validation: 0.3009482594988046]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311889079538592		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5311889079538592 | validation: 0.5942203045491251]
	TIME [epoch: 1.37 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4976502727452373		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.4976502727452373 | validation: 0.3639626578563999]
	TIME [epoch: 1.36 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453223454084908		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.453223454084908 | validation: 0.42470702608429234]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44308475746278236		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.44308475746278236 | validation: 0.4615767685095562]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45483828468087945		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.45483828468087945 | validation: 0.3933897506903477]
	TIME [epoch: 1.36 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44546320208048423		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.44546320208048423 | validation: 0.4082639678182136]
	TIME [epoch: 1.37 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3925610562016509		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.3925610562016509 | validation: 0.4204048274997895]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.385075313780864		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.385075313780864 | validation: 0.41136122513944173]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4018611684269527		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.4018611684269527 | validation: 0.43731691636864944]
	TIME [epoch: 1.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414234230040149		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.4414234230040149 | validation: 0.4103480370664234]
	TIME [epoch: 1.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40635940138231064		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.40635940138231064 | validation: 0.3652540508903619]
	TIME [epoch: 1.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39730490933405127		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.39730490933405127 | validation: 0.4808935765593114]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.391462875578424		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.391462875578424 | validation: 0.27001376229910273]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574364747540815		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.574364747540815 | validation: 0.6330457197408825]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5670708903391531		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.5670708903391531 | validation: 0.3833479140061695]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40763208005809703		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.40763208005809703 | validation: 0.3076682776328144]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3899962692167197		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.3899962692167197 | validation: 0.5219050245941453]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40039138019145215		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.40039138019145215 | validation: 0.32678458300634033]
	TIME [epoch: 1.43 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36857083420460657		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.36857083420460657 | validation: 0.3970771781868142]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3393981914097692		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.3393981914097692 | validation: 0.3689572430363341]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32389562801401517		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.32389562801401517 | validation: 0.3505440576888734]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34808839633313093		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.34808839633313093 | validation: 0.4374538336532737]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37487797432044445		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.37487797432044445 | validation: 0.37511902911755174]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.451323037823239		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.451323037823239 | validation: 0.4559100277188788]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3588951229616211		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.3588951229616211 | validation: 0.2720757218497771]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36125810077112563		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.36125810077112563 | validation: 0.5373202684629644]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3971330869329938		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.3971330869329938 | validation: 0.2991561404687274]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3947716842990589		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.3947716842990589 | validation: 0.3582852814512827]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3105643965816308		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.3105643965816308 | validation: 0.4113447350715112]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31857449431953433		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.31857449431953433 | validation: 0.29233027816428175]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3548412064727486		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.3548412064727486 | validation: 0.4539327070461939]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3325011133820645		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.3325011133820645 | validation: 0.32120641658850896]
	TIME [epoch: 1.36 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474849970441657		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.3474849970441657 | validation: 0.39168283533453385]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3235036493049827		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.3235036493049827 | validation: 0.3948500259712872]
	TIME [epoch: 1.36 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652304906656299		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.3652304906656299 | validation: 0.33195879822479757]
	TIME [epoch: 1.36 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3379306362304672		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.3379306362304672 | validation: 0.42180631071198127]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31160699380383644		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.31160699380383644 | validation: 0.2764650402638856]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143214472171751		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.3143214472171751 | validation: 0.5383059941750408]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3752074668958736		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.3752074668958736 | validation: 0.24544999329803852]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3636310527520219		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.3636310527520219 | validation: 0.42783947539238815]
	TIME [epoch: 1.4 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29971202813214587		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.29971202813214587 | validation: 0.30366682762182506]
	TIME [epoch: 1.37 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2665442183655174		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.2665442183655174 | validation: 0.3357821030784929]
	TIME [epoch: 1.37 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25765242398555277		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.25765242398555277 | validation: 0.3448008554752714]
	TIME [epoch: 1.37 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26746371853043516		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.26746371853043516 | validation: 0.3542856839826126]
	TIME [epoch: 1.37 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35265474448978623		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.35265474448978623 | validation: 0.3530454127083205]
	TIME [epoch: 1.37 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4454013738355081		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.4454013738355081 | validation: 0.4480086984495345]
	TIME [epoch: 1.37 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32213411858932745		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.32213411858932745 | validation: 0.26167139461721695]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3663340997521658		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.3663340997521658 | validation: 0.40677493363771106]
	TIME [epoch: 1.36 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29167807278028784		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.29167807278028784 | validation: 0.3237198632624331]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2763838381819681		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2763838381819681 | validation: 0.3261136328937595]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2747975269144145		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.2747975269144145 | validation: 0.4313289723703515]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32123038300677326		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.32123038300677326 | validation: 0.3266075940225189]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3527105159799777		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.3527105159799777 | validation: 0.41560361690969716]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2688562032194849		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.2688562032194849 | validation: 0.24807866268361561]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3131538042278277		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.3131538042278277 | validation: 0.4814820966253063]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29622679388498774		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.29622679388498774 | validation: 0.2635897697591627]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25674842819043375		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.25674842819043375 | validation: 0.3587663599871089]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24540405017527725		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.24540405017527725 | validation: 0.3578134093864242]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2712730672039562		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.2712730672039562 | validation: 0.31775483975348967]
	TIME [epoch: 1.36 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3703854866730687		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.3703854866730687 | validation: 0.34832789950931314]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25298409270714667		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.25298409270714667 | validation: 0.29785146583795236]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23594029919787837		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.23594029919787837 | validation: 0.33956729652832535]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23914105702526822		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.23914105702526822 | validation: 0.3259853647773744]
	TIME [epoch: 1.37 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29227877118947926		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.29227877118947926 | validation: 0.3756647451437608]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106427282030374		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.3106427282030374 | validation: 0.3430864276998762]
	TIME [epoch: 1.36 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3323905270713782		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.3323905270713782 | validation: 0.3714584530235309]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23439619960274208		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.23439619960274208 | validation: 0.25405649887008636]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2383632045388324		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.2383632045388324 | validation: 0.4697923996125821]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29383287820861254		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.29383287820861254 | validation: 0.23792603808783894]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3336000296306442		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.3336000296306442 | validation: 0.4174339124501938]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24875610365738263		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.24875610365738263 | validation: 0.2634836429222967]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22947401837107095		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.22947401837107095 | validation: 0.4085442888287757]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2960518723463644		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.2960518723463644 | validation: 0.3296323434323241]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3351430361966676		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.3351430361966676 | validation: 0.2991295153942685]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2395489643419853		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2395489643419853 | validation: 0.33534048959408963]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20851584991267239		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.20851584991267239 | validation: 0.29568827850291174]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21105904229915426		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.21105904229915426 | validation: 0.31006648254480473]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22367160684177997		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.22367160684177997 | validation: 0.35999167257624554]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27217577938072807		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.27217577938072807 | validation: 0.29138860042331977]
	TIME [epoch: 1.37 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3709886233100275		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.3709886233100275 | validation: 0.48586671072326926]
	TIME [epoch: 1.37 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3093427150244493		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.3093427150244493 | validation: 0.27173335695717216]
	TIME [epoch: 1.37 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2946046300905746		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.2946046300905746 | validation: 0.3135811856208942]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25238342312614576		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.25238342312614576 | validation: 0.4744323362056353]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3105195993902194		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3105195993902194 | validation: 0.2527690873528376]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25123716832355797		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.25123716832355797 | validation: 0.3284415053053346]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21294542246955794		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.21294542246955794 | validation: 0.32359198783835813]
	TIME [epoch: 1.37 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21623011427361627		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.21623011427361627 | validation: 0.26359887550535505]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25232548766860485		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.25232548766860485 | validation: 0.3765734411641478]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24499442316134615		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.24499442316134615 | validation: 0.2563010922494974]
	TIME [epoch: 1.37 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2101565074824942		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.2101565074824942 | validation: 0.3621475410260757]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20768694314160535		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.20768694314160535 | validation: 0.25250946986559347]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22054727263125584		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.22054727263125584 | validation: 0.48535234902885194]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353244391731905		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.3353244391731905 | validation: 0.32643523394424356]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183561416751521		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.3183561416751521 | validation: 0.25663630443274926]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21313845740133844		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.21313845740133844 | validation: 0.4064743934184212]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22895531261839566		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.22895531261839566 | validation: 0.2794255917033054]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25642176981615483		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.25642176981615483 | validation: 0.3034926119363578]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2335785460570486		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.2335785460570486 | validation: 0.30255098551364584]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20026189445200115		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.20026189445200115 | validation: 0.30767360335181515]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18900376439922661		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.18900376439922661 | validation: 0.27908601093123175]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1978172470712515		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.1978172470712515 | validation: 0.3248903887468558]
	TIME [epoch: 1.37 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091687531965534		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.2091687531965534 | validation: 0.25296450324037023]
	TIME [epoch: 1.37 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24584627452661456		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.24584627452661456 | validation: 0.40714676358512086]
	TIME [epoch: 1.37 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.275443996061887		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.275443996061887 | validation: 0.26814313861587386]
	TIME [epoch: 1.37 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25278243650721277		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.25278243650721277 | validation: 0.3094851685794673]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2060091472021073		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.2060091472021073 | validation: 0.29083212208958076]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1934425899538389		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.1934425899538389 | validation: 0.27397033743651944]
	TIME [epoch: 1.35 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21265470075431248		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.21265470075431248 | validation: 0.3038133495785271]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21646721142776518		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.21646721142776518 | validation: 0.2890689118560322]
	TIME [epoch: 1.35 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22036045966037868		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.22036045966037868 | validation: 0.30846066219089757]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2097807269911825		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.2097807269911825 | validation: 0.28034701771506165]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20154999612987878		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.20154999612987878 | validation: 0.28371003824951024]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20834627617177212		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.20834627617177212 | validation: 0.2981985588959927]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22009984110443157		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.22009984110443157 | validation: 0.259677007264909]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19927183636137372		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.19927183636137372 | validation: 0.34913916638374193]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20881285124702004		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.20881285124702004 | validation: 0.21966683990504512]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062003789456083		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.3062003789456083 | validation: 0.5157634498966993]
	TIME [epoch: 1.37 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30438458275992725		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.30438458275992725 | validation: 0.25004425377642103]
	TIME [epoch: 1.37 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1684759167786261		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.1684759167786261 | validation: 0.20826290172939013]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_419.pth
	Model improved!!!
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19976485057109142		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.19976485057109142 | validation: 0.41153153064992476]
	TIME [epoch: 1.37 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2351848921337289		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2351848921337289 | validation: 0.24358169609588132]
	TIME [epoch: 1.37 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17775688857939081		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.17775688857939081 | validation: 0.25526240727925853]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2156017081824635		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.2156017081824635 | validation: 0.37419151815743]
	TIME [epoch: 1.36 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26591757006363004		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.26591757006363004 | validation: 0.27093853491149195]
	TIME [epoch: 1.36 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23415301852703835		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.23415301852703835 | validation: 0.26507734193424926]
	TIME [epoch: 1.37 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18645319002693833		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.18645319002693833 | validation: 0.2523351720572441]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16260576766611237		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.16260576766611237 | validation: 0.27648246714191643]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15454640813324674		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.15454640813324674 | validation: 0.23311685821696476]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518213599380548		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.1518213599380548 | validation: 0.28781969846871835]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15673693213339124		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.15673693213339124 | validation: 0.24119878869408834]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17623016772562122		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.17623016772562122 | validation: 0.3535628777358235]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27726050048293005		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.27726050048293005 | validation: 0.2772501800741752]
	TIME [epoch: 1.37 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3105960535326274		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.3105960535326274 | validation: 0.27114960949179484]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614156303591029		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.1614156303591029 | validation: 0.3046978768366281]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20383189388653655		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.20383189388653655 | validation: 0.2847336242265294]
	TIME [epoch: 1.36 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2452836493584842		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.2452836493584842 | validation: 0.2817121156098396]
	TIME [epoch: 1.36 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1713747637160325		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.1713747637160325 | validation: 0.24124892784293472]
	TIME [epoch: 1.36 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17503728732481677		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.17503728732481677 | validation: 0.29022329077516507]
	TIME [epoch: 1.36 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16645719628132752		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.16645719628132752 | validation: 0.2222102567824713]
	TIME [epoch: 1.36 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638340041331946		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1638340041331946 | validation: 0.3355780784022771]
	TIME [epoch: 1.36 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1857480508826374		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.1857480508826374 | validation: 0.21373749377768547]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20355651453027918		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.20355651453027918 | validation: 0.41686544098729117]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23586726793167564		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.23586726793167564 | validation: 0.22460330121340194]
	TIME [epoch: 1.36 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1655396067745445		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.1655396067745445 | validation: 0.24212394616381722]
	TIME [epoch: 1.36 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1568074727572086		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.1568074727572086 | validation: 0.28226536086210036]
	TIME [epoch: 1.36 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19791119305709437		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.19791119305709437 | validation: 0.2755217422405765]
	TIME [epoch: 1.36 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24833141872068593		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.24833141872068593 | validation: 0.26047347165117934]
	TIME [epoch: 1.5 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21122891976292052		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.21122891976292052 | validation: 0.24941587929177889]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16486151320623854		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.16486151320623854 | validation: 0.28691924547907205]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18424686506073493		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.18424686506073493 | validation: 0.24669557220233324]
	TIME [epoch: 1.37 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18280135803248648		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.18280135803248648 | validation: 0.2526635365330579]
	TIME [epoch: 1.37 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444144241718593		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.1444144241718593 | validation: 0.21774696640429894]
	TIME [epoch: 1.37 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15476547646984432		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.15476547646984432 | validation: 0.34447799449800726]
	TIME [epoch: 1.37 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19492428199846296		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.19492428199846296 | validation: 0.19848105086349108]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2065659231447132		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.2065659231447132 | validation: 0.3387477931746865]
	TIME [epoch: 1.37 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18314302478935687		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.18314302478935687 | validation: 0.19068168887857573]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15521139772026452		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.15521139772026452 | validation: 0.28037309478126243]
	TIME [epoch: 1.37 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1547654290976194		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.1547654290976194 | validation: 0.20590753674970694]
	TIME [epoch: 1.37 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14357909573053795		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.14357909573053795 | validation: 0.23953483946835363]
	TIME [epoch: 1.37 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1373995928172066		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.1373995928172066 | validation: 0.21579478215891387]
	TIME [epoch: 1.37 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15713325505489326		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.15713325505489326 | validation: 0.24304659091002379]
	TIME [epoch: 1.36 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21848982738882028		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.21848982738882028 | validation: 0.3302533788473121]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2197100156653324		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.2197100156653324 | validation: 0.20151087918520388]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1906844813182227		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.1906844813182227 | validation: 0.3429557955032693]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1981955210364876		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.1981955210364876 | validation: 0.2064021446087109]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15261255717609404		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.15261255717609404 | validation: 0.22455047857991534]
	TIME [epoch: 1.37 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14629250040796984		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.14629250040796984 | validation: 0.25129358252773976]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15934824360866875		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.15934824360866875 | validation: 0.19669342176085602]
	TIME [epoch: 1.37 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1725423385581615		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.1725423385581615 | validation: 0.2607538011401538]
	TIME [epoch: 1.37 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16414659279568633		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.16414659279568633 | validation: 0.18586412586661463]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_470.pth
	Model improved!!!
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14081017871456397		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.14081017871456397 | validation: 0.27939641247192953]
	TIME [epoch: 1.37 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697275281451302		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.14697275281451302 | validation: 0.17261199223321433]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15901518020378683		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.15901518020378683 | validation: 0.3104991744224893]
	TIME [epoch: 1.36 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16570246439431402		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.16570246439431402 | validation: 0.18260787986901797]
	TIME [epoch: 1.37 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16432436566461445		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.16432436566461445 | validation: 0.3302929557978951]
	TIME [epoch: 1.37 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21973813180404403		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.21973813180404403 | validation: 0.26136449533239503]
	TIME [epoch: 1.37 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23629437693120292		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.23629437693120292 | validation: 0.20503181358303835]
	TIME [epoch: 1.36 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14332961563993354		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.14332961563993354 | validation: 0.21530168624216933]
	TIME [epoch: 1.37 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11761543334823793		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.11761543334823793 | validation: 0.218447289852478]
	TIME [epoch: 1.36 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13382360886823252		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.13382360886823252 | validation: 0.23293202421773201]
	TIME [epoch: 1.37 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15581197412259024		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.15581197412259024 | validation: 0.2214090652278542]
	TIME [epoch: 1.37 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1671517780816803		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.1671517780816803 | validation: 0.27059429205836655]
	TIME [epoch: 1.37 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1641939155104746		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.1641939155104746 | validation: 0.18695220261885753]
	TIME [epoch: 1.37 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14770942061596068		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.14770942061596068 | validation: 0.27469734175032584]
	TIME [epoch: 1.37 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16416635707014238		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.16416635707014238 | validation: 0.1881835853848528]
	TIME [epoch: 1.37 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15364392249751935		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.15364392249751935 | validation: 0.26773759493670835]
	TIME [epoch: 1.37 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16550978764558663		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.16550978764558663 | validation: 0.21963504175034407]
	TIME [epoch: 1.37 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1539329971013831		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.1539329971013831 | validation: 0.20059327016615747]
	TIME [epoch: 1.37 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15713180463432266		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.15713180463432266 | validation: 0.24750187764820072]
	TIME [epoch: 1.37 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13505079628515756		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.13505079628515756 | validation: 0.18009632734221914]
	TIME [epoch: 1.37 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13913559975512763		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.13913559975512763 | validation: 0.28788508621063896]
	TIME [epoch: 1.37 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1470965109178884		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.1470965109178884 | validation: 0.16869546848290382]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14312012701786234		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.14312012701786234 | validation: 0.25589746022008747]
	TIME [epoch: 1.37 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380422537177302		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1380422537177302 | validation: 0.18855610998993805]
	TIME [epoch: 1.37 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14931744337297467		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.14931744337297467 | validation: 0.23607970028365424]
	TIME [epoch: 1.37 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16362732677860947		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.16362732677860947 | validation: 0.2129040326182325]
	TIME [epoch: 1.37 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17683704797913904		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.17683704797913904 | validation: 0.23352644745986925]
	TIME [epoch: 1.37 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1457382562805162		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.1457382562805162 | validation: 0.1992038837062301]
	TIME [epoch: 1.37 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1474363443073474		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.1474363443073474 | validation: 0.2154440540782431]
	TIME [epoch: 1.37 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14229972855391798		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.14229972855391798 | validation: 0.20600964216487744]
	TIME [epoch: 1.37 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13038495765897212		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.13038495765897212 | validation: 0.1961622268019154]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12262135288060216		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.12262135288060216 | validation: 0.19156400951877958]
	TIME [epoch: 2.72 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12991757874295856		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.12991757874295856 | validation: 0.27813487290621736]
	TIME [epoch: 2.71 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15022272591173652		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.15022272591173652 | validation: 0.1774961305854173]
	TIME [epoch: 2.7 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13702282624057321		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.13702282624057321 | validation: 0.22640566295083797]
	TIME [epoch: 2.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15414592591588908		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.15414592591588908 | validation: 0.19811719117954998]
	TIME [epoch: 2.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13950032304171153		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.13950032304171153 | validation: 0.21706384220614147]
	TIME [epoch: 2.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12153890818713844		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.12153890818713844 | validation: 0.18332020926338624]
	TIME [epoch: 2.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1271537123674618		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.1271537123674618 | validation: 0.2820494877446437]
	TIME [epoch: 2.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17022133265276843		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.17022133265276843 | validation: 0.17415149880360448]
	TIME [epoch: 2.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12652111895613113		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.12652111895613113 | validation: 0.19068106301328086]
	TIME [epoch: 2.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11618826972327653		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.11618826972327653 | validation: 0.21639420873575566]
	TIME [epoch: 2.71 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12416378879256534		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.12416378879256534 | validation: 0.171907312262234]
	TIME [epoch: 2.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1281578694088743		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.1281578694088743 | validation: 0.23126533453269166]
	TIME [epoch: 2.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12693453208633984		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.12693453208633984 | validation: 0.15734234348586018]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1540842826891867		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.1540842826891867 | validation: 0.23336003254774892]
	TIME [epoch: 2.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1435594549871148		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.1435594549871148 | validation: 0.16656166917086623]
	TIME [epoch: 2.71 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12187934004380996		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.12187934004380996 | validation: 0.17410852653497988]
	TIME [epoch: 2.71 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10951321750038505		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.10951321750038505 | validation: 0.23777802186720978]
	TIME [epoch: 2.71 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14500235919723575		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.14500235919723575 | validation: 0.18956607610669454]
	TIME [epoch: 2.71 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19488213112199654		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.19488213112199654 | validation: 0.24274770357578013]
	TIME [epoch: 2.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1517667023657306		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1517667023657306 | validation: 0.17167465019028016]
	TIME [epoch: 2.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12269293993612199		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.12269293993612199 | validation: 0.2082532710691469]
	TIME [epoch: 2.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1134976346444897		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1134976346444897 | validation: 0.18590390975361948]
	TIME [epoch: 2.71 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11964747920107925		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.11964747920107925 | validation: 0.2301443318222261]
	TIME [epoch: 2.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13731131248505699		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.13731131248505699 | validation: 0.18488720627482014]
	TIME [epoch: 2.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13993404297271722		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.13993404297271722 | validation: 0.2095272331755342]
	TIME [epoch: 2.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13572792445144724		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.13572792445144724 | validation: 0.17713158810608945]
	TIME [epoch: 2.71 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11858951728056694		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.11858951728056694 | validation: 0.20593705389868383]
	TIME [epoch: 2.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696853807482554		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.10696853807482554 | validation: 0.17152386129903086]
	TIME [epoch: 2.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1071342035261886		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.1071342035261886 | validation: 0.19353737055402426]
	TIME [epoch: 2.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10128594250468002		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.10128594250468002 | validation: 0.15932729114292724]
	TIME [epoch: 2.7 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10034288843885307		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.10034288843885307 | validation: 0.1994439442179964]
	TIME [epoch: 2.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09976809268745232		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.09976809268745232 | validation: 0.1500346055427557]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12319940276564198		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.12319940276564198 | validation: 0.29159939841872107]
	TIME [epoch: 2.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17464670926221085		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.17464670926221085 | validation: 0.14616501741687146]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14616279807426705		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.14616279807426705 | validation: 0.20561668661889243]
	TIME [epoch: 2.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179777991027622		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.1179777991027622 | validation: 0.20847125149326384]
	TIME [epoch: 2.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18461487805083962		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.18461487805083962 | validation: 0.22736124514021322]
	TIME [epoch: 2.7 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18435752654851997		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.18435752654851997 | validation: 0.15719845496648782]
	TIME [epoch: 2.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10646070842275665		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.10646070842275665 | validation: 0.20694167619417334]
	TIME [epoch: 2.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10521517601465458		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.10521517601465458 | validation: 0.1606107261079408]
	TIME [epoch: 2.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11459867599518671		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.11459867599518671 | validation: 0.17280892326988123]
	TIME [epoch: 2.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11603327622834103		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.11603327622834103 | validation: 0.20950270883140334]
	TIME [epoch: 2.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1082837985451818		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.1082837985451818 | validation: 0.15412601900410305]
	TIME [epoch: 2.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584131822436298		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09584131822436298 | validation: 0.18816665129230714]
	TIME [epoch: 2.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11533072487097114		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.11533072487097114 | validation: 0.17204291844126995]
	TIME [epoch: 2.7 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13989783896832406		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.13989783896832406 | validation: 0.21449603308713283]
	TIME [epoch: 2.7 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13133400680496343		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.13133400680496343 | validation: 0.1563769346228916]
	TIME [epoch: 2.7 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11063191246072468		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.11063191246072468 | validation: 0.1985393961018372]
	TIME [epoch: 2.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10414937242206002		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.10414937242206002 | validation: 0.18617077172398688]
	TIME [epoch: 2.7 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1025549411792881		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1025549411792881 | validation: 0.15113656397542324]
	TIME [epoch: 2.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09787196963449853		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.09787196963449853 | validation: 0.18572678252265146]
	TIME [epoch: 2.71 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771668369553092		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.09771668369553092 | validation: 0.16325967033484712]
	TIME [epoch: 2.71 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09915838117149188		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.09915838117149188 | validation: 0.16371220932593405]
	TIME [epoch: 2.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11111387536684862		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.11111387536684862 | validation: 0.22334746510435777]
	TIME [epoch: 2.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508922957549482		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.1508922957549482 | validation: 0.16225339470531627]
	TIME [epoch: 2.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1267333919666985		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.1267333919666985 | validation: 0.20467317213534747]
	TIME [epoch: 2.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11210107633569984		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.11210107633569984 | validation: 0.14749412983048454]
	TIME [epoch: 2.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117227863505514		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.117227863505514 | validation: 0.1914380992875807]
	TIME [epoch: 2.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10416316111011474		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.10416316111011474 | validation: 0.15064823512459793]
	TIME [epoch: 2.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889792077530355		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.0889792077530355 | validation: 0.2107423384034806]
	TIME [epoch: 2.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1052907295204146		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1052907295204146 | validation: 0.14333789253984547]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09335428334391047		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.09335428334391047 | validation: 0.19256990822793704]
	TIME [epoch: 2.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09120080027421743		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.09120080027421743 | validation: 0.13892854936086274]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10033047912037242		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.10033047912037242 | validation: 0.21556478882522156]
	TIME [epoch: 2.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10701185116613783		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.10701185116613783 | validation: 0.13523814561588546]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09962348713666827		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.09962348713666827 | validation: 0.2046441172524955]
	TIME [epoch: 2.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11637574834228083		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.11637574834228083 | validation: 0.16569265374213746]
	TIME [epoch: 2.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14510996478439328		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.14510996478439328 | validation: 0.17805019181882564]
	TIME [epoch: 2.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15113169320185782		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.15113169320185782 | validation: 0.16507588817636482]
	TIME [epoch: 2.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10417882122899143		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.10417882122899143 | validation: 0.15073438100980152]
	TIME [epoch: 2.7 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880072413391176		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.0880072413391176 | validation: 0.15999265220645453]
	TIME [epoch: 2.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09724230432429753		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.09724230432429753 | validation: 0.19328454335597428]
	TIME [epoch: 2.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10872899767497206		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.10872899767497206 | validation: 0.16724058957692584]
	TIME [epoch: 2.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245961705253855		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.10245961705253855 | validation: 0.1489527427117258]
	TIME [epoch: 2.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10589481547738774		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.10589481547738774 | validation: 0.17202497791880086]
	TIME [epoch: 2.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022798308530973		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.11022798308530973 | validation: 0.15133297280921074]
	TIME [epoch: 2.71 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12654522574626667		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.12654522574626667 | validation: 0.15080469936250807]
	TIME [epoch: 2.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09014097147774347		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.09014097147774347 | validation: 0.19151467673409286]
	TIME [epoch: 2.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10111528748136354		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.10111528748136354 | validation: 0.14393125304953855]
	TIME [epoch: 2.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858821020974167		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.0858821020974167 | validation: 0.17088806677236845]
	TIME [epoch: 2.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08515100821356249		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.08515100821356249 | validation: 0.1384133054880021]
	TIME [epoch: 2.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10775736957664772		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.10775736957664772 | validation: 0.3048406729620856]
	TIME [epoch: 2.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17581074212287753		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.17581074212287753 | validation: 0.16815759301938066]
	TIME [epoch: 2.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10830444125230734		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.10830444125230734 | validation: 0.1368836075359969]
	TIME [epoch: 2.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11364288019391694		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.11364288019391694 | validation: 0.2053365521298198]
	TIME [epoch: 2.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10031612805889491		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.10031612805889491 | validation: 0.14841056075249287]
	TIME [epoch: 2.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08879638317335176		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.08879638317335176 | validation: 0.14607113411271783]
	TIME [epoch: 2.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09293921710983671		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.09293921710983671 | validation: 0.153970566456946]
	TIME [epoch: 2.71 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08481410702479718		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.08481410702479718 | validation: 0.1553090131215562]
	TIME [epoch: 2.71 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09626565929565789		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.09626565929565789 | validation: 0.1507945337269047]
	TIME [epoch: 2.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09758287086744777		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.09758287086744777 | validation: 0.189307471877043]
	TIME [epoch: 2.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10850884874018721		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.10850884874018721 | validation: 0.14855069960502496]
	TIME [epoch: 2.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09481146477322827		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.09481146477322827 | validation: 0.13966044813233655]
	TIME [epoch: 2.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09423398073080823		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.09423398073080823 | validation: 0.14297077234326702]
	TIME [epoch: 2.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08962830891914436		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.08962830891914436 | validation: 0.1421410058668416]
	TIME [epoch: 2.7 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08699403272619048		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.08699403272619048 | validation: 0.18820510629332307]
	TIME [epoch: 2.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10165312509888728		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.10165312509888728 | validation: 0.13377159483713238]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0834742285309529		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0834742285309529 | validation: 0.19841954685776875]
	TIME [epoch: 2.71 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970517343714607		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.0970517343714607 | validation: 0.14917944920891854]
	TIME [epoch: 2.71 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10621106422897793		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.10621106422897793 | validation: 0.1462375701023141]
	TIME [epoch: 2.71 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11734154726666443		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.11734154726666443 | validation: 0.1452396723819018]
	TIME [epoch: 2.71 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09918446225342863		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.09918446225342863 | validation: 0.17582578262120915]
	TIME [epoch: 2.71 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.095803705992042		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.095803705992042 | validation: 0.13936789269348057]
	TIME [epoch: 2.71 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07722284978471798		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07722284978471798 | validation: 0.14355369346202876]
	TIME [epoch: 2.71 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07629793220381939		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.07629793220381939 | validation: 0.1436550502410063]
	TIME [epoch: 2.71 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515706111230566		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.07515706111230566 | validation: 0.12574416448839787]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748074086434841		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.0748074086434841 | validation: 0.16945217719886874]
	TIME [epoch: 2.68 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08914012906927134		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.08914012906927134 | validation: 0.1291742278049883]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13818718937699148		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.13818718937699148 | validation: 0.15734822415521432]
	TIME [epoch: 2.68 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10325420715445674		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.10325420715445674 | validation: 0.19918331749928556]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11059617172481723		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.11059617172481723 | validation: 0.1386686459746114]
	TIME [epoch: 2.68 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07566218700218084		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.07566218700218084 | validation: 0.1283491639370951]
	TIME [epoch: 2.68 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08284112829343751		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.08284112829343751 | validation: 0.16867402455251634]
	TIME [epoch: 2.68 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08962841809472542		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08962841809472542 | validation: 0.12232048505712903]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_616.pth
	Model improved!!!
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07941348038263263		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.07941348038263263 | validation: 0.14118283844232252]
	TIME [epoch: 2.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08512637562144335		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.08512637562144335 | validation: 0.15695672968932584]
	TIME [epoch: 2.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1020891768698743		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.1020891768698743 | validation: 0.15157075831591266]
	TIME [epoch: 2.69 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10430741498149004		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10430741498149004 | validation: 0.16801063978167766]
	TIME [epoch: 2.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11157623566019396		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.11157623566019396 | validation: 0.1345585139998059]
	TIME [epoch: 2.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10167226940523576		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.10167226940523576 | validation: 0.14551765678328785]
	TIME [epoch: 2.69 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0932126705076212		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.0932126705076212 | validation: 0.1236832253133676]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215643651887826		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.07215643651887826 | validation: 0.1289026721898039]
	TIME [epoch: 2.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07006283680872881		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.07006283680872881 | validation: 0.13226567736738568]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07330157552375657		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07330157552375657 | validation: 0.1465339320741039]
	TIME [epoch: 2.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081384172714111		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.08081384172714111 | validation: 0.12452765679191989]
	TIME [epoch: 2.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08894602776880418		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.08894602776880418 | validation: 0.1494393754572709]
	TIME [epoch: 2.7 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10847290278117848		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.10847290278117848 | validation: 0.19709255458336267]
	TIME [epoch: 2.69 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11630103614828458		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.11630103614828458 | validation: 0.12966490526066063]
	TIME [epoch: 2.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760391671849515		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.07760391671849515 | validation: 0.11967395451805383]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_631.pth
	Model improved!!!
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0824780132089506		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.0824780132089506 | validation: 0.16885761282608197]
	TIME [epoch: 2.7 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08640709242527833		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.08640709242527833 | validation: 0.12471404894631971]
	TIME [epoch: 2.7 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07052520463289538		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.07052520463289538 | validation: 0.13247140107964236]
	TIME [epoch: 2.7 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07324658309781402		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.07324658309781402 | validation: 0.135944175071247]
	TIME [epoch: 2.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0733120204663945		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.0733120204663945 | validation: 0.13084717757160857]
	TIME [epoch: 2.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07539557026735266		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.07539557026735266 | validation: 0.14411396142903765]
	TIME [epoch: 2.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12133098697263868		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.12133098697263868 | validation: 0.16867365033908494]
	TIME [epoch: 2.7 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1120845327761563		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.1120845327761563 | validation: 0.13624167544855692]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.079201187627026		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.079201187627026 | validation: 0.13707422011511883]
	TIME [epoch: 2.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08718546269402454		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.08718546269402454 | validation: 0.14206152794656732]
	TIME [epoch: 2.68 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08123287885843614		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08123287885843614 | validation: 0.12524909467303183]
	TIME [epoch: 2.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07614483566766955		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.07614483566766955 | validation: 0.1433870949528265]
	TIME [epoch: 2.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07840898521447892		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.07840898521447892 | validation: 0.1129692581561969]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777692459934243		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.07777692459934243 | validation: 0.13037577459050428]
	TIME [epoch: 2.68 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06970752514031764		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.06970752514031764 | validation: 0.12189221390910301]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083152229955687		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.07083152229955687 | validation: 0.12983505093526046]
	TIME [epoch: 2.68 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095700566839543		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.07095700566839543 | validation: 0.12582713851164776]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07898014597469202		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.07898014597469202 | validation: 0.13162484707859856]
	TIME [epoch: 2.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07914372858985612		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.07914372858985612 | validation: 0.16963499120854372]
	TIME [epoch: 2.69 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10399822605447984		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.10399822605447984 | validation: 0.12093984247184385]
	TIME [epoch: 2.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587523371319372		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.08587523371319372 | validation: 0.13268816374703452]
	TIME [epoch: 2.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08294325318848918		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.08294325318848918 | validation: 0.11956427541989088]
	TIME [epoch: 2.69 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0709944644488849		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.0709944644488849 | validation: 0.14400935236938275]
	TIME [epoch: 2.68 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116345692156692		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.07116345692156692 | validation: 0.11532413552390319]
	TIME [epoch: 2.69 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07535409796390471		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.07535409796390471 | validation: 0.1412857598459765]
	TIME [epoch: 2.69 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076571377173168		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.076571377173168 | validation: 0.12135279616084767]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946404102359291		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.06946404102359291 | validation: 0.12826662940914196]
	TIME [epoch: 2.69 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760917683961012		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.07760917683961012 | validation: 0.16480827091572212]
	TIME [epoch: 2.68 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292863856436594		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.1292863856436594 | validation: 0.11956340932690919]
	TIME [epoch: 2.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09190973977486401		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.09190973977486401 | validation: 0.12416934056484252]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06971692459068299		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.06971692459068299 | validation: 0.13314291069493278]
	TIME [epoch: 2.69 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06831720056026278		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.06831720056026278 | validation: 0.11899602379408068]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06507878461604694		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06507878461604694 | validation: 0.11938732318952446]
	TIME [epoch: 2.69 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06518377616195298		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06518377616195298 | validation: 0.13091741855315095]
	TIME [epoch: 2.68 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06860971677107457		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.06860971677107457 | validation: 0.12155952539511153]
	TIME [epoch: 2.69 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937215262862215		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.06937215262862215 | validation: 0.11608828944901728]
	TIME [epoch: 2.68 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805214139401095		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.07805214139401095 | validation: 0.16072802352969884]
	TIME [epoch: 2.69 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955442450730872		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.0955442450730872 | validation: 0.11839417833339926]
	TIME [epoch: 2.69 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07525277710004344		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.07525277710004344 | validation: 0.12008171824610914]
	TIME [epoch: 2.69 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07025198406912911		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.07025198406912911 | validation: 0.12015363567308551]
	TIME [epoch: 2.69 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07099162362868766		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.07099162362868766 | validation: 0.11491276588013688]
	TIME [epoch: 2.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07208242381728636		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.07208242381728636 | validation: 0.12486263990575544]
	TIME [epoch: 2.69 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07264534932309676		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.07264534932309676 | validation: 0.13718219695971304]
	TIME [epoch: 2.69 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08597655462508967		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.08597655462508967 | validation: 0.11773599278391039]
	TIME [epoch: 2.69 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0611435678600377		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.0611435678600377 | validation: 0.11593419829756875]
	TIME [epoch: 2.69 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06451636027067695		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06451636027067695 | validation: 0.1428397622961893]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08518975902535125		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.08518975902535125 | validation: 0.12601609145507067]
	TIME [epoch: 2.69 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07158948433979409		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.07158948433979409 | validation: 0.11149351567065185]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_679.pth
	Model improved!!!
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748051003477591		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.0748051003477591 | validation: 0.14445738511281342]
	TIME [epoch: 2.71 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08669359822901913		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.08669359822901913 | validation: 0.09999880944088575]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_681.pth
	Model improved!!!
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498551117483704		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.06498551117483704 | validation: 0.11553927263676307]
	TIME [epoch: 2.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058727713533720714		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.058727713533720714 | validation: 0.12342515074828371]
	TIME [epoch: 2.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06100527593772908		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06100527593772908 | validation: 0.10810002830307625]
	TIME [epoch: 2.71 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137484797762273		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.06137484797762273 | validation: 0.12097417435568264]
	TIME [epoch: 2.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122305542921314		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.07122305542921314 | validation: 0.11291704899793319]
	TIME [epoch: 2.7 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172343554459118		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.07172343554459118 | validation: 0.12404832265124109]
	TIME [epoch: 2.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09129901477596807		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.09129901477596807 | validation: 0.15090695753009303]
	TIME [epoch: 2.71 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0978648168053702		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.0978648168053702 | validation: 0.11602372783456172]
	TIME [epoch: 2.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553117524231127		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06553117524231127 | validation: 0.10730965328540965]
	TIME [epoch: 2.71 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06243881482249165		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.06243881482249165 | validation: 0.130761098561806]
	TIME [epoch: 2.71 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149759362855693		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.07149759362855693 | validation: 0.10923268689377004]
	TIME [epoch: 2.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06293181730961657		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.06293181730961657 | validation: 0.11400106368550707]
	TIME [epoch: 2.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05948546360043532		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.05948546360043532 | validation: 0.13712833502354704]
	TIME [epoch: 2.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07420347374615897		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.07420347374615897 | validation: 0.12339810865599864]
	TIME [epoch: 2.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08137028293025172		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08137028293025172 | validation: 0.10924941285639891]
	TIME [epoch: 2.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07818677366676592		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.07818677366676592 | validation: 0.11671847228801778]
	TIME [epoch: 2.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07180144686472069		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.07180144686472069 | validation: 0.1308133140004369]
	TIME [epoch: 2.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06867799447573525		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.06867799447573525 | validation: 0.1060710261376062]
	TIME [epoch: 2.71 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06054489289869603		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.06054489289869603 | validation: 0.11719295498820093]
	TIME [epoch: 2.71 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05862724405597138		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.05862724405597138 | validation: 0.10044153028195729]
	TIME [epoch: 2.68 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060397994676979344		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.060397994676979344 | validation: 0.1208198929869016]
	TIME [epoch: 2.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059113182810243615		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.059113182810243615 | validation: 0.10161128756027851]
	TIME [epoch: 2.69 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06244532940755706		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06244532940755706 | validation: 0.12549102066943255]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06702410793463319		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.06702410793463319 | validation: 0.12580355321372488]
	TIME [epoch: 2.69 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08307026489383555		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.08307026489383555 | validation: 0.11003562502311776]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06913373344755445		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06913373344755445 | validation: 0.10581950859799893]
	TIME [epoch: 2.69 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0701693510925585		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0701693510925585 | validation: 0.11459212096915601]
	TIME [epoch: 2.68 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06492943715403149		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.06492943715403149 | validation: 0.11279059886094947]
	TIME [epoch: 2.69 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05893436733934543		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.05893436733934543 | validation: 0.10431007599675911]
	TIME [epoch: 2.68 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06921840456472739		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.06921840456472739 | validation: 0.13621921777772109]
	TIME [epoch: 2.69 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09167357190559076		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.09167357190559076 | validation: 0.10230601807255338]
	TIME [epoch: 2.68 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06842413052907155		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.06842413052907155 | validation: 0.09649359481939182]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_713.pth
	Model improved!!!
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05825697546903178		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.05825697546903178 | validation: 0.12653902787619162]
	TIME [epoch: 2.69 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06171777851485814		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.06171777851485814 | validation: 0.10046161555756372]
	TIME [epoch: 2.69 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06321703132087983		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.06321703132087983 | validation: 0.12171721527793121]
	TIME [epoch: 2.69 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060463382379248386		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.060463382379248386 | validation: 0.11208126422490933]
	TIME [epoch: 2.69 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06120241151933481		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.06120241151933481 | validation: 0.11400150497144823]
	TIME [epoch: 2.69 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07599369202212045		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.07599369202212045 | validation: 0.1207848209172071]
	TIME [epoch: 2.69 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718289816185572		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.0718289816185572 | validation: 0.11411490965897793]
	TIME [epoch: 2.69 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976263772826212		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06976263772826212 | validation: 0.10504647129440679]
	TIME [epoch: 2.69 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06231137794769683		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.06231137794769683 | validation: 0.1097577534386332]
	TIME [epoch: 2.69 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431514202942515		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.06431514202942515 | validation: 0.11841203417438582]
	TIME [epoch: 2.69 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06746033134202392		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.06746033134202392 | validation: 0.10160679475527075]
	TIME [epoch: 3.08 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879634639765758		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.06879634639765758 | validation: 0.10939077991237803]
	TIME [epoch: 2.69 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058834753214273804		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.058834753214273804 | validation: 0.10572459325013997]
	TIME [epoch: 2.69 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05400883596274918		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.05400883596274918 | validation: 0.10190864141497952]
	TIME [epoch: 2.69 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396272175298378		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.05396272175298378 | validation: 0.11385364782874241]
	TIME [epoch: 2.69 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058195769556913524		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.058195769556913524 | validation: 0.09333491417224443]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_729.pth
	Model improved!!!
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286405811933385		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.06286405811933385 | validation: 0.112287010559915]
	TIME [epoch: 2.69 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056777694736447		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.06056777694736447 | validation: 0.09430338365934247]
	TIME [epoch: 2.69 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657868124494662		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.05657868124494662 | validation: 0.10162171329342384]
	TIME [epoch: 2.68 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05568546109665107		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.05568546109665107 | validation: 0.1144573336363374]
	TIME [epoch: 2.69 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0685750443988713		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.0685750443988713 | validation: 0.1182651782359953]
	TIME [epoch: 2.69 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08177749126981375		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.08177749126981375 | validation: 0.11446358909666708]
	TIME [epoch: 2.68 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521512836921083		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.06521512836921083 | validation: 0.1015792308962005]
	TIME [epoch: 2.68 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05799941755578003		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.05799941755578003 | validation: 0.11275083615563959]
	TIME [epoch: 2.68 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055965341583560556		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.055965341583560556 | validation: 0.1039426314684841]
	TIME [epoch: 2.68 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05676146630208399		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.05676146630208399 | validation: 0.11644849039178325]
	TIME [epoch: 2.68 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06198010485012073		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.06198010485012073 | validation: 0.09668455159281993]
	TIME [epoch: 2.68 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057507302665714306		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.057507302665714306 | validation: 0.10271895281597115]
	TIME [epoch: 2.68 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05059194588475682		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.05059194588475682 | validation: 0.09550415301827797]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05594200766348793		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.05594200766348793 | validation: 0.11594595680168339]
	TIME [epoch: 2.68 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236421925743702		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.06236421925743702 | validation: 0.09998604430540113]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061634548717995945		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.061634548717995945 | validation: 0.10920325888569285]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05810548179734526		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05810548179734526 | validation: 0.10376659616220492]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05668495848599675		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.05668495848599675 | validation: 0.10186392476775837]
	TIME [epoch: 2.69 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062075625618990764		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.062075625618990764 | validation: 0.10852214828670773]
	TIME [epoch: 2.68 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062109815253381856		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.062109815253381856 | validation: 0.1079898575954464]
	TIME [epoch: 2.69 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060074429334026076		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.060074429334026076 | validation: 0.09482071762517963]
	TIME [epoch: 2.69 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06117344560959754		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.06117344560959754 | validation: 0.10926656518746831]
	TIME [epoch: 2.69 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251355895382918		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06251355895382918 | validation: 0.09280806886800637]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05858761219446566		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.05858761219446566 | validation: 0.0979364900481749]
	TIME [epoch: 2.68 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05501496567494863		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.05501496567494863 | validation: 0.10285178184367627]
	TIME [epoch: 2.69 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093921076584596		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.05093921076584596 | validation: 0.10208169785949038]
	TIME [epoch: 2.69 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05061564966850371		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.05061564966850371 | validation: 0.09446187646124764]
	TIME [epoch: 2.69 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054447212917488885		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.054447212917488885 | validation: 0.10080670702799135]
	TIME [epoch: 2.69 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05244290906084441		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.05244290906084441 | validation: 0.11086183878155116]
	TIME [epoch: 2.69 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05348208602048079		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.05348208602048079 | validation: 0.09194646119391015]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142721986604897		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.06142721986604897 | validation: 0.11429298131775241]
	TIME [epoch: 2.69 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07277588586627386		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.07277588586627386 | validation: 0.09072732965470559]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06644915613849214		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.06644915613849214 | validation: 0.10051731745472192]
	TIME [epoch: 2.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055560298502833255		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.055560298502833255 | validation: 0.12557346469955089]
	TIME [epoch: 2.69 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058621398606654974		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.058621398606654974 | validation: 0.09083722416727603]
	TIME [epoch: 2.69 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05373489162568665		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.05373489162568665 | validation: 0.1007464657042812]
	TIME [epoch: 2.69 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0498415785226062		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.0498415785226062 | validation: 0.09856055280977148]
	TIME [epoch: 2.68 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05004560795724643		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.05004560795724643 | validation: 0.10091501425013943]
	TIME [epoch: 2.69 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05072276893221405		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.05072276893221405 | validation: 0.10770710247521685]
	TIME [epoch: 2.69 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739035580482575		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05739035580482575 | validation: 0.09972878412684871]
	TIME [epoch: 2.69 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371328791110964		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05371328791110964 | validation: 0.10049824491261826]
	TIME [epoch: 2.69 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054492273108837884		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.054492273108837884 | validation: 0.1031831154147249]
	TIME [epoch: 2.69 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056415500698255114		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.056415500698255114 | validation: 0.10125587810668546]
	TIME [epoch: 2.69 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06596454241826152		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.06596454241826152 | validation: 0.09570713240416119]
	TIME [epoch: 2.69 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05498424412056824		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.05498424412056824 | validation: 0.0919430237950068]
	TIME [epoch: 2.68 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05082833085927331		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.05082833085927331 | validation: 0.09416591738956112]
	TIME [epoch: 2.69 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05041180798616288		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.05041180798616288 | validation: 0.10368357540106454]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05266908576321347		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.05266908576321347 | validation: 0.09105389463516944]
	TIME [epoch: 2.68 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05569502452871879		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.05569502452871879 | validation: 0.10670406171811275]
	TIME [epoch: 2.69 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05387149676713168		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.05387149676713168 | validation: 0.08632157974321764]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_779.pth
	Model improved!!!
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058666678245104416		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.058666678245104416 | validation: 0.09631343133433809]
	TIME [epoch: 2.68 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05976701430642557		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.05976701430642557 | validation: 0.08830184479690598]
	TIME [epoch: 2.69 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926420525522217		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05926420525522217 | validation: 0.10078208297129809]
	TIME [epoch: 2.69 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05203620704871957		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05203620704871957 | validation: 0.09272964410481892]
	TIME [epoch: 2.69 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05738341252996577		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.05738341252996577 | validation: 0.09655409614602403]
	TIME [epoch: 2.69 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050006991936073		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.05050006991936073 | validation: 0.09512155689682948]
	TIME [epoch: 2.69 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05432735173241321		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.05432735173241321 | validation: 0.09561628006835884]
	TIME [epoch: 2.69 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05497776609993773		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.05497776609993773 | validation: 0.09709959097350673]
	TIME [epoch: 2.69 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05436459650739124		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.05436459650739124 | validation: 0.0999643539391451]
	TIME [epoch: 2.69 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055274252869606605		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.055274252869606605 | validation: 0.09397721009691964]
	TIME [epoch: 2.69 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709461326967392		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.04709461326967392 | validation: 0.09691191674451066]
	TIME [epoch: 2.69 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05078555860865389		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.05078555860865389 | validation: 0.09459700266954588]
	TIME [epoch: 2.69 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056742274322161314		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.056742274322161314 | validation: 0.09888301081298538]
	TIME [epoch: 2.69 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05377939341402124		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.05377939341402124 | validation: 0.08339173792099307]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_793.pth
	Model improved!!!
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058886306999039066		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.058886306999039066 | validation: 0.09994834289783623]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05020981031736037		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05020981031736037 | validation: 0.09462119569344418]
	TIME [epoch: 2.69 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048430320114481056		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.048430320114481056 | validation: 0.08092710976453672]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_796.pth
	Model improved!!!
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05317261910139063		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.05317261910139063 | validation: 0.0976229297741897]
	TIME [epoch: 2.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04999272932693176		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.04999272932693176 | validation: 0.08415087823217851]
	TIME [epoch: 2.69 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049578360411850826		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.049578360411850826 | validation: 0.09790554363323051]
	TIME [epoch: 2.69 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05098538602882776		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.05098538602882776 | validation: 0.09969178778878429]
	TIME [epoch: 2.68 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0628784462268175		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.0628784462268175 | validation: 0.09450816353146066]
	TIME [epoch: 2.69 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05831987686390195		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.05831987686390195 | validation: 0.0958411553124488]
	TIME [epoch: 2.69 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873262035423103		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.04873262035423103 | validation: 0.08749904808156667]
	TIME [epoch: 2.69 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04655126158211026		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.04655126158211026 | validation: 0.09392385416065054]
	TIME [epoch: 2.69 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048715745579636566		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.048715745579636566 | validation: 0.09291798790187451]
	TIME [epoch: 2.69 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878820716834739		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.04878820716834739 | validation: 0.08994329058047018]
	TIME [epoch: 2.69 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642319370586455		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.04642319370586455 | validation: 0.09080099086199261]
	TIME [epoch: 2.69 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04698504311542875		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.04698504311542875 | validation: 0.09096983575070111]
	TIME [epoch: 2.69 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04987540208175984		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.04987540208175984 | validation: 0.08556575465140494]
	TIME [epoch: 2.69 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048628320667084765		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.048628320667084765 | validation: 0.09371676033299993]
	TIME [epoch: 2.69 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046369291645161965		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.046369291645161965 | validation: 0.07774213908742349]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049701733681126646		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.049701733681126646 | validation: 0.09202556949364105]
	TIME [epoch: 2.69 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05672055610964243		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05672055610964243 | validation: 0.08178019927925552]
	TIME [epoch: 2.68 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05698726231892048		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.05698726231892048 | validation: 0.09862238582560723]
	TIME [epoch: 2.68 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054787118461618725		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.054787118461618725 | validation: 0.0909645918318623]
	TIME [epoch: 2.69 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05363299768909938		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05363299768909938 | validation: 0.0923411409336606]
	TIME [epoch: 2.68 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04575720580056367		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.04575720580056367 | validation: 0.07243022062526332]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04625357391015853		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.04625357391015853 | validation: 0.08764585653595057]
	TIME [epoch: 2.68 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455871009326229		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.0455871009326229 | validation: 0.08746356464555421]
	TIME [epoch: 2.69 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045161065771907455		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.045161065771907455 | validation: 0.08447481215060469]
	TIME [epoch: 2.69 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04557873810982881		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.04557873810982881 | validation: 0.08727502369899165]
	TIME [epoch: 2.69 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532032153263831		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.04532032153263831 | validation: 0.09299025489088253]
	TIME [epoch: 2.69 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046469257297968296		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.046469257297968296 | validation: 0.09169841038675625]
	TIME [epoch: 2.69 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044334275728211026		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.044334275728211026 | validation: 0.08506452635792076]
	TIME [epoch: 2.68 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189776769391691		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.05189776769391691 | validation: 0.09519412961352733]
	TIME [epoch: 2.69 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05501279270201099		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.05501279270201099 | validation: 0.09638663498963174]
	TIME [epoch: 2.69 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048249911826035684		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.048249911826035684 | validation: 0.08332403711164525]
	TIME [epoch: 2.69 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05348308273790961		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.05348308273790961 | validation: 0.10410345262668713]
	TIME [epoch: 2.68 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420424975264195		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05420424975264195 | validation: 0.09289423798246203]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773752139120648		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.04773752139120648 | validation: 0.08941621893177674]
	TIME [epoch: 2.68 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04461720849465832		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.04461720849465832 | validation: 0.09174390586568372]
	TIME [epoch: 2.69 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951884598007997		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.04951884598007997 | validation: 0.08307276735112139]
	TIME [epoch: 2.68 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344608663413735		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.04344608663413735 | validation: 0.08499431708926419]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04296252553112858		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.04296252553112858 | validation: 0.09910758104587813]
	TIME [epoch: 2.68 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04747124302755526		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.04747124302755526 | validation: 0.08248641563811261]
	TIME [epoch: 2.69 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04809516207618192		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.04809516207618192 | validation: 0.09233230358654394]
	TIME [epoch: 2.68 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05047920013490605		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.05047920013490605 | validation: 0.08154826391440426]
	TIME [epoch: 2.68 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04833541323458913		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.04833541323458913 | validation: 0.08932218035683391]
	TIME [epoch: 2.69 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04431714172413373		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.04431714172413373 | validation: 0.08622869696139271]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04306063767886067		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.04306063767886067 | validation: 0.08685709911693479]
	TIME [epoch: 2.69 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396528595979864		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.04396528595979864 | validation: 0.07874685081636155]
	TIME [epoch: 2.69 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04254726411849859		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.04254726411849859 | validation: 0.08600229335834204]
	TIME [epoch: 2.68 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042076383259534986		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.042076383259534986 | validation: 0.08162502698119536]
	TIME [epoch: 2.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04201579211168556		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.04201579211168556 | validation: 0.09332053387760458]
	TIME [epoch: 2.71 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04411493187620716		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.04411493187620716 | validation: 0.07930085172272905]
	TIME [epoch: 2.69 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05099460635097957		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.05099460635097957 | validation: 0.09929173171518857]
	TIME [epoch: 2.68 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049918415571832214		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.049918415571832214 | validation: 0.07626481866658058]
	TIME [epoch: 2.69 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04467880582442113		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.04467880582442113 | validation: 0.08768115909707982]
	TIME [epoch: 2.68 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04242666504223509		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.04242666504223509 | validation: 0.0838464652239343]
	TIME [epoch: 2.69 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04469362800870002		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.04469362800870002 | validation: 0.09968459079290193]
	TIME [epoch: 2.69 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049567336092739146		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.049567336092739146 | validation: 0.09178929723042732]
	TIME [epoch: 2.68 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058991700555794474		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.058991700555794474 | validation: 0.09475470888801305]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05014447826444468		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.05014447826444468 | validation: 0.08847096404175983]
	TIME [epoch: 2.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04511433075628453		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.04511433075628453 | validation: 0.08186352011856302]
	TIME [epoch: 2.68 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04199432198111553		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.04199432198111553 | validation: 0.08859158741181018]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04284244980055899		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.04284244980055899 | validation: 0.08281499390874805]
	TIME [epoch: 2.68 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324551764343166		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.04324551764343166 | validation: 0.07802939733623698]
	TIME [epoch: 2.69 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042409222770148976		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.042409222770148976 | validation: 0.0867986233200494]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04199775378490724		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.04199775378490724 | validation: 0.08142811379799886]
	TIME [epoch: 2.68 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041015491852421974		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.041015491852421974 | validation: 0.08886817631112569]
	TIME [epoch: 2.68 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186501624669704		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.04186501624669704 | validation: 0.08804148942381392]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043571147587122716		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.043571147587122716 | validation: 0.07584814955980534]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04606927705439268		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.04606927705439268 | validation: 0.08981696303829186]
	TIME [epoch: 2.68 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395835095577602		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.0395835095577602 | validation: 0.08038121781048407]
	TIME [epoch: 2.68 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042640092609703716		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.042640092609703716 | validation: 0.0838792667863682]
	TIME [epoch: 2.68 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346390507366522		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.04346390507366522 | validation: 0.07324969362131546]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04595109797639313		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.04595109797639313 | validation: 0.09914067744861567]
	TIME [epoch: 2.68 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05303602962064155		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.05303602962064155 | validation: 0.09081421262184541]
	TIME [epoch: 2.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06008081673927688		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.06008081673927688 | validation: 0.08517351183046251]
	TIME [epoch: 2.68 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042466104132698065		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.042466104132698065 | validation: 0.09294924744379118]
	TIME [epoch: 2.68 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726106827932073		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.04726106827932073 | validation: 0.08687629613560566]
	TIME [epoch: 2.68 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04380753314237817		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.04380753314237817 | validation: 0.07729878843609166]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04164723201589036		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.04164723201589036 | validation: 0.08735835942023978]
	TIME [epoch: 2.68 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041344590019580164		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.041344590019580164 | validation: 0.08316769026817948]
	TIME [epoch: 2.68 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865253542711421		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.03865253542711421 | validation: 0.08445853763131561]
	TIME [epoch: 2.68 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400507549770231		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.0400507549770231 | validation: 0.07926018085739356]
	TIME [epoch: 2.68 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04064003825445866		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.04064003825445866 | validation: 0.07678206344668354]
	TIME [epoch: 2.68 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041872726707950864		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.041872726707950864 | validation: 0.08437516665070117]
	TIME [epoch: 2.68 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0389827071689867		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.0389827071689867 | validation: 0.09021123576449064]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04145686083468679		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.04145686083468679 | validation: 0.07570828548190615]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048011041045041315		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.048011041045041315 | validation: 0.08619441368774766]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04721983417692785		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.04721983417692785 | validation: 0.08545767827213628]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04302016137762575		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.04302016137762575 | validation: 0.08020364036718894]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04168345227460691		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.04168345227460691 | validation: 0.08549376480820782]
	TIME [epoch: 2.68 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03950980743966542		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.03950980743966542 | validation: 0.07911601839794809]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989276494038723		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.03989276494038723 | validation: 0.10096184918100522]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0580289763293344		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.0580289763293344 | validation: 0.07362291495661834]
	TIME [epoch: 2.68 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044331002748625434		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.044331002748625434 | validation: 0.08184107634940907]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989426613446793		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.03989426613446793 | validation: 0.09181164785773721]
	TIME [epoch: 2.68 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04671361236667305		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.04671361236667305 | validation: 0.07568903293046156]
	TIME [epoch: 2.68 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282770648641401		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.04282770648641401 | validation: 0.0907861853203875]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040342639949986984		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.040342639949986984 | validation: 0.07977004125116577]
	TIME [epoch: 2.68 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860135195712514		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.03860135195712514 | validation: 0.08636497943992807]
	TIME [epoch: 2.68 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040222844678632505		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.040222844678632505 | validation: 0.08124333492306454]
	TIME [epoch: 2.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040458215123301916		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.040458215123301916 | validation: 0.08361135172704541]
	TIME [epoch: 2.68 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04135079755600817		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.04135079755600817 | validation: 0.08307694559244128]
	TIME [epoch: 2.68 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044117011865297456		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.044117011865297456 | validation: 0.07622504186845608]
	TIME [epoch: 2.68 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04218237065234524		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.04218237065234524 | validation: 0.0782749532359974]
	TIME [epoch: 2.68 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04112938293121679		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.04112938293121679 | validation: 0.08711565952513577]
	TIME [epoch: 2.68 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0394092744157743		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.0394092744157743 | validation: 0.0791686796601894]
	TIME [epoch: 2.68 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037516892869610366		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.037516892869610366 | validation: 0.07898913748535596]
	TIME [epoch: 2.68 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04055855978472691		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.04055855978472691 | validation: 0.08215104966856175]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03938054495331364		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.03938054495331364 | validation: 0.07622534802977088]
	TIME [epoch: 2.68 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236627654131087		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.04236627654131087 | validation: 0.10013452626856321]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859681105513834		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.04859681105513834 | validation: 0.07115865119843374]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_905.pth
	Model improved!!!
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04914471086406465		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.04914471086406465 | validation: 0.08284299317436411]
	TIME [epoch: 2.68 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699653422677792		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.03699653422677792 | validation: 0.08665748356229099]
	TIME [epoch: 2.68 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04403094010716155		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.04403094010716155 | validation: 0.07967932428691848]
	TIME [epoch: 2.68 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03711170585957807		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03711170585957807 | validation: 0.07518897152571147]
	TIME [epoch: 2.68 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03682601761210019		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.03682601761210019 | validation: 0.08686488596382476]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03774485479067526		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.03774485479067526 | validation: 0.08141387566497757]
	TIME [epoch: 2.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719776464432074		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03719776464432074 | validation: 0.07823906122631827]
	TIME [epoch: 2.69 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740415657282269		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.03740415657282269 | validation: 0.0739637200161629]
	TIME [epoch: 2.68 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038463042607900386		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.038463042607900386 | validation: 0.08222202639934173]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780971289714553		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.03780971289714553 | validation: 0.07715030722866052]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038581561504728655		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.038581561504728655 | validation: 0.08575084884957439]
	TIME [epoch: 2.68 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038733020012255384		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.038733020012255384 | validation: 0.0787094836360394]
	TIME [epoch: 2.68 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03691600159913274		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.03691600159913274 | validation: 0.07932560917266437]
	TIME [epoch: 2.68 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037444871446288525		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.037444871446288525 | validation: 0.07087279343920987]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039243035377642184		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.039243035377642184 | validation: 0.07576471481473357]
	TIME [epoch: 2.68 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927008050783355		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.03927008050783355 | validation: 0.0791158615243166]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04038809669454341		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.04038809669454341 | validation: 0.08259042288667763]
	TIME [epoch: 2.69 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03762403605689655		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03762403605689655 | validation: 0.06840666047276289]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780408021448574		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.03780408021448574 | validation: 0.08889790808835779]
	TIME [epoch: 2.69 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03920692248930868		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.03920692248930868 | validation: 0.07994644481019388]
	TIME [epoch: 2.69 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278692637017922		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.04278692637017922 | validation: 0.0862649037589151]
	TIME [epoch: 2.69 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04089979111049557		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.04089979111049557 | validation: 0.07305637330379665]
	TIME [epoch: 2.68 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038320753675493244		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.038320753675493244 | validation: 0.08085511694267347]
	TIME [epoch: 2.68 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03481564785629745		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.03481564785629745 | validation: 0.07744907583564845]
	TIME [epoch: 2.68 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579548780653822		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.03579548780653822 | validation: 0.08038518104701682]
	TIME [epoch: 2.68 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039875809045896875		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.039875809045896875 | validation: 0.07757840521637745]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03826590136611654		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.03826590136611654 | validation: 0.07676600424608072]
	TIME [epoch: 2.68 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03584875516687025		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.03584875516687025 | validation: 0.0838771756737251]
	TIME [epoch: 2.68 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512678743718003		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03512678743718003 | validation: 0.0730737483857855]
	TIME [epoch: 2.68 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037508853119656765		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.037508853119656765 | validation: 0.08569618849015068]
	TIME [epoch: 2.68 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036374640668553865		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.036374640668553865 | validation: 0.0725189625294742]
	TIME [epoch: 2.68 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438524141149426		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.03438524141149426 | validation: 0.07822730265989912]
	TIME [epoch: 2.68 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034416882471200526		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.034416882471200526 | validation: 0.07954116485848428]
	TIME [epoch: 2.68 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603052111292495		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.03603052111292495 | validation: 0.07348585789181403]
	TIME [epoch: 2.68 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038465375163004446		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.038465375163004446 | validation: 0.07658836702478107]
	TIME [epoch: 2.68 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03763983825387071		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.03763983825387071 | validation: 0.0772526183946451]
	TIME [epoch: 2.68 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0382465784282292		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.0382465784282292 | validation: 0.09269365849530296]
	TIME [epoch: 2.69 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04609929320250822		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.04609929320250822 | validation: 0.0752804415252253]
	TIME [epoch: 2.68 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04772570824705162		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.04772570824705162 | validation: 0.07650562571943652]
	TIME [epoch: 2.69 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537810317386646		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.03537810317386646 | validation: 0.07218092551761406]
	TIME [epoch: 2.68 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035036018368272484		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.035036018368272484 | validation: 0.07637382836395036]
	TIME [epoch: 2.68 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719921413243623		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.03719921413243623 | validation: 0.07877869091568858]
	TIME [epoch: 2.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0359241958832457		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.0359241958832457 | validation: 0.07775257765841664]
	TIME [epoch: 2.69 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03458435817434914		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.03458435817434914 | validation: 0.07345552854632048]
	TIME [epoch: 2.69 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520117635021941		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.03520117635021941 | validation: 0.08056025584686036]
	TIME [epoch: 2.69 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034340930742678875		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.034340930742678875 | validation: 0.07714107149562809]
	TIME [epoch: 2.69 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034511144196715624		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.034511144196715624 | validation: 0.0678392230864735]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03580845072516821		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.03580845072516821 | validation: 0.07865762079725036]
	TIME [epoch: 2.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03625056686981076		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.03625056686981076 | validation: 0.06625579363984119]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_954.pth
	Model improved!!!
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04040868859334356		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.04040868859334356 | validation: 0.08614445460742147]
	TIME [epoch: 2.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04014675772913993		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.04014675772913993 | validation: 0.07097543833661299]
	TIME [epoch: 2.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035979551640866164		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.035979551640866164 | validation: 0.07632410754641429]
	TIME [epoch: 2.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03620475336967046		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.03620475336967046 | validation: 0.07286661552571128]
	TIME [epoch: 2.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03559321303676763		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.03559321303676763 | validation: 0.06929379670912766]
	TIME [epoch: 2.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034529644415085416		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.034529644415085416 | validation: 0.0794555932871771]
	TIME [epoch: 2.69 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033537904398400376		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.033537904398400376 | validation: 0.07518981664923474]
	TIME [epoch: 2.69 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509131565079167		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.03509131565079167 | validation: 0.07205505190015886]
	TIME [epoch: 2.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03701041411979453		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.03701041411979453 | validation: 0.0800584748540006]
	TIME [epoch: 2.69 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03445379073892959		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.03445379073892959 | validation: 0.07273395230454258]
	TIME [epoch: 2.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400169355302728		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.03400169355302728 | validation: 0.08288152063309451]
	TIME [epoch: 2.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03738864772859988		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.03738864772859988 | validation: 0.07573129743423401]
	TIME [epoch: 2.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03875952209547618		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.03875952209547618 | validation: 0.07787432409238404]
	TIME [epoch: 2.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035399620220967556		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.035399620220967556 | validation: 0.08010461130960214]
	TIME [epoch: 2.69 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353233565219473		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.03353233565219473 | validation: 0.07483294092853791]
	TIME [epoch: 2.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03300880100750125		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.03300880100750125 | validation: 0.06734584517303738]
	TIME [epoch: 2.68 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03440685836651086		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.03440685836651086 | validation: 0.06992760878991128]
	TIME [epoch: 2.68 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03404354335872827		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.03404354335872827 | validation: 0.08196518466382516]
	TIME [epoch: 2.68 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037296131230656886		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.037296131230656886 | validation: 0.07231760801286051]
	TIME [epoch: 2.68 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036045248837177975		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.036045248837177975 | validation: 0.08376867690461368]
	TIME [epoch: 2.68 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03752241235721409		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.03752241235721409 | validation: 0.07180573785162983]
	TIME [epoch: 2.68 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034210649885720305		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.034210649885720305 | validation: 0.07512431392162544]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033769030411151583		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.033769030411151583 | validation: 0.07197785038226655]
	TIME [epoch: 2.69 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033086566097401356		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.033086566097401356 | validation: 0.07957342845501074]
	TIME [epoch: 2.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035256751522527684		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.035256751522527684 | validation: 0.0758163885291265]
	TIME [epoch: 2.7 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035564231743922974		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.035564231743922974 | validation: 0.07253701138693369]
	TIME [epoch: 2.7 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03953010962418399		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.03953010962418399 | validation: 0.07785003690985781]
	TIME [epoch: 2.7 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552249604005435		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.03552249604005435 | validation: 0.07469584989905155]
	TIME [epoch: 2.7 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03275270233907066		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.03275270233907066 | validation: 0.06987175052571332]
	TIME [epoch: 2.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034261248090593784		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.034261248090593784 | validation: 0.08740372057160689]
	TIME [epoch: 2.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037052524597365764		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.037052524597365764 | validation: 0.06872789020909593]
	TIME [epoch: 2.7 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03443037797837907		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03443037797837907 | validation: 0.07728419760865196]
	TIME [epoch: 2.7 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03147023442010481		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.03147023442010481 | validation: 0.08084840121691117]
	TIME [epoch: 2.7 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663437866874458		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.03663437866874458 | validation: 0.07164725728100127]
	TIME [epoch: 2.7 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035182425914782114		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.035182425914782114 | validation: 0.07637338465263711]
	TIME [epoch: 2.7 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0324994755703154		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.0324994755703154 | validation: 0.07847164758274937]
	TIME [epoch: 2.7 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0330388432738771		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0330388432738771 | validation: 0.07178857462559161]
	TIME [epoch: 2.7 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03444282837261087		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03444282837261087 | validation: 0.0735732675909339]
	TIME [epoch: 2.7 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0328529828140291		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.0328529828140291 | validation: 0.07526812207235284]
	TIME [epoch: 2.7 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03220578509211295		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.03220578509211295 | validation: 0.07110708743783932]
	TIME [epoch: 2.7 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03294512456659691		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.03294512456659691 | validation: 0.07186793248221654]
	TIME [epoch: 2.7 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03310894813820501		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.03310894813820501 | validation: 0.07054249296321478]
	TIME [epoch: 2.7 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033421872891546786		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.033421872891546786 | validation: 0.0777571046061033]
	TIME [epoch: 2.7 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03395656006212644		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.03395656006212644 | validation: 0.07183960071053302]
	TIME [epoch: 2.71 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036888218650495096		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.036888218650495096 | validation: 0.06971573510632466]
	TIME [epoch: 2.71 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214170166761042		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.03214170166761042 | validation: 0.07231925975348096]
	TIME [epoch: 2.71 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035239354589599356		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.035239354589599356 | validation: 0.08153139235300473]
	TIME [epoch: 182 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498890706995608		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.03498890706995608 | validation: 0.07478672908510249]
	TIME [epoch: 5.81 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03332649676910039		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.03332649676910039 | validation: 0.08168150334585887]
	TIME [epoch: 5.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0363934958673343		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0363934958673343 | validation: 0.07574349464530285]
	TIME [epoch: 5.81 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0357048983138163		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.0357048983138163 | validation: 0.08087332624286431]
	TIME [epoch: 5.81 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03417871464776979		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.03417871464776979 | validation: 0.07179969835465859]
	TIME [epoch: 5.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031963542600927645		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.031963542600927645 | validation: 0.0702773769635946]
	TIME [epoch: 5.81 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032476695434602465		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.032476695434602465 | validation: 0.06998146691095437]
	TIME [epoch: 5.8 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03161400043640822		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.03161400043640822 | validation: 0.08363902346094625]
	TIME [epoch: 5.81 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549775192439471		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.03549775192439471 | validation: 0.06884427551736591]
	TIME [epoch: 5.8 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039807021204174696		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.039807021204174696 | validation: 0.07646133081249396]
	TIME [epoch: 5.81 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031561143251213716		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.031561143251213716 | validation: 0.07743224180839138]
	TIME [epoch: 5.8 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03244638777495829		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.03244638777495829 | validation: 0.07036317524972363]
	TIME [epoch: 5.81 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03195826553561242		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.03195826553561242 | validation: 0.07447293646378209]
	TIME [epoch: 5.8 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0313580277904234		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.0313580277904234 | validation: 0.07800055624956087]
	TIME [epoch: 5.8 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03215909946880382		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.03215909946880382 | validation: 0.07360494040042646]
	TIME [epoch: 5.8 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201075092636125		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.03201075092636125 | validation: 0.07218640178361467]
	TIME [epoch: 5.8 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032204998701568		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.032204998701568 | validation: 0.06907748187816064]
	TIME [epoch: 5.8 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434046882628209		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.03434046882628209 | validation: 0.07502350266066161]
	TIME [epoch: 5.8 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03433940309170936		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03433940309170936 | validation: 0.0686833637892827]
	TIME [epoch: 5.8 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034781392459674466		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.034781392459674466 | validation: 0.0700036449890702]
	TIME [epoch: 5.81 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256183175754528		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.03256183175754528 | validation: 0.07354016410379835]
	TIME [epoch: 5.81 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030995444008138976		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.030995444008138976 | validation: 0.06633409938705957]
	TIME [epoch: 5.8 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194136239013926		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.03194136239013926 | validation: 0.07562956185997571]
	TIME [epoch: 5.8 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034435869439760895		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.034435869439760895 | validation: 0.06520905593329009]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1025.pth
	Model improved!!!
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03384683396569298		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.03384683396569298 | validation: 0.0701878958611053]
	TIME [epoch: 5.8 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031119676958122292		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.031119676958122292 | validation: 0.07196662043281399]
	TIME [epoch: 5.8 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03066519257867541		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.03066519257867541 | validation: 0.06825873013512947]
	TIME [epoch: 5.8 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03239929903142579		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.03239929903142579 | validation: 0.07146061652137413]
	TIME [epoch: 5.81 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0295635819798789		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.0295635819798789 | validation: 0.06534303350266397]
	TIME [epoch: 5.81 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03176678558926867		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.03176678558926867 | validation: 0.06956539614771622]
	TIME [epoch: 5.81 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03176052917733062		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.03176052917733062 | validation: 0.0788478857155112]
	TIME [epoch: 5.8 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033716328347437645		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.033716328347437645 | validation: 0.0644786216242974]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1033.pth
	Model improved!!!
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030228991676660107		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.030228991676660107 | validation: 0.06977209833627655]
	TIME [epoch: 5.8 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031554351314693155		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.031554351314693155 | validation: 0.07357281109902696]
	TIME [epoch: 5.81 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168718764443598		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.03168718764443598 | validation: 0.06631969082168786]
	TIME [epoch: 5.8 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194956797926226		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.03194956797926226 | validation: 0.07829446569726398]
	TIME [epoch: 5.81 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03443607351983241		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.03443607351983241 | validation: 0.0639142934394329]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1038.pth
	Model improved!!!
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601566498978888		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.03601566498978888 | validation: 0.0752298076178702]
	TIME [epoch: 5.81 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031360043434131374		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.031360043434131374 | validation: 0.07400284155582507]
	TIME [epoch: 5.81 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02928870170287472		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.02928870170287472 | validation: 0.06602963879549094]
	TIME [epoch: 5.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030809516284546805		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.030809516284546805 | validation: 0.07159965102547204]
	TIME [epoch: 5.79 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031220631669880587		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.031220631669880587 | validation: 0.06350886519362085]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1043.pth
	Model improved!!!
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030069771774597884		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.030069771774597884 | validation: 0.07174948115009035]
	TIME [epoch: 5.79 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030998979701146238		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.030998979701146238 | validation: 0.06920521104617398]
	TIME [epoch: 5.8 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03118667186801879		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.03118667186801879 | validation: 0.08008308059631257]
	TIME [epoch: 5.81 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031215050093578924		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.031215050093578924 | validation: 0.06877187008449846]
	TIME [epoch: 5.81 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029829751941705143		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.029829751941705143 | validation: 0.06628528727110526]
	TIME [epoch: 5.81 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03361474433891617		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.03361474433891617 | validation: 0.07353772706723798]
	TIME [epoch: 5.8 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030553280247550052		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.030553280247550052 | validation: 0.07336363042766084]
	TIME [epoch: 5.81 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029591377108336436		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.029591377108336436 | validation: 0.06871042670740364]
	TIME [epoch: 5.8 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033065050902304584		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.033065050902304584 | validation: 0.07384454887601363]
	TIME [epoch: 5.81 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03140907014480777		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.03140907014480777 | validation: 0.07581420535784152]
	TIME [epoch: 5.8 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030890123292525048		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.030890123292525048 | validation: 0.06725081657055942]
	TIME [epoch: 5.81 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033140964039306116		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.033140964039306116 | validation: 0.07569920339620023]
	TIME [epoch: 5.81 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03260021905100568		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.03260021905100568 | validation: 0.06773967637855889]
	TIME [epoch: 5.81 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03313118922930312		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.03313118922930312 | validation: 0.07296749356901633]
	TIME [epoch: 5.8 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03017067396297895		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.03017067396297895 | validation: 0.07128771551971246]
	TIME [epoch: 5.81 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030888126253137672		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.030888126253137672 | validation: 0.06901846128167378]
	TIME [epoch: 5.8 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03070965853766107		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.03070965853766107 | validation: 0.07106086589681877]
	TIME [epoch: 5.81 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02999203972437308		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.02999203972437308 | validation: 0.07371597664359401]
	TIME [epoch: 5.8 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030722187166112684		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.030722187166112684 | validation: 0.06848276820247509]
	TIME [epoch: 5.81 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03084824809212833		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.03084824809212833 | validation: 0.08077671450405714]
	TIME [epoch: 5.81 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02995701052510159		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.02995701052510159 | validation: 0.06586655771415258]
	TIME [epoch: 5.81 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030431669621510452		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.030431669621510452 | validation: 0.06467386684797376]
	TIME [epoch: 5.8 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885988120999218		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.02885988120999218 | validation: 0.07308720531399433]
	TIME [epoch: 5.81 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030318562015600365		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.030318562015600365 | validation: 0.06367699637563982]
	TIME [epoch: 5.81 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029397140264509677		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.029397140264509677 | validation: 0.06578442425349096]
	TIME [epoch: 5.81 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029701480034195447		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.029701480034195447 | validation: 0.06531007857153101]
	TIME [epoch: 5.8 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03305145286450081		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.03305145286450081 | validation: 0.07234812899426402]
	TIME [epoch: 5.81 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03104301152878785		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.03104301152878785 | validation: 0.06323073718799216]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1071.pth
	Model improved!!!
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030099243134883828		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.030099243134883828 | validation: 0.06751203852255096]
	TIME [epoch: 5.81 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029921148288050713		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.029921148288050713 | validation: 0.07290888262074292]
	TIME [epoch: 5.81 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030050441205826735		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.030050441205826735 | validation: 0.06717280333067992]
	TIME [epoch: 5.81 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030596548606404275		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.030596548606404275 | validation: 0.0650574428174465]
	TIME [epoch: 5.8 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029869970439129943		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.029869970439129943 | validation: 0.07260185868709183]
	TIME [epoch: 5.81 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029702260406077843		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.029702260406077843 | validation: 0.07326100971770202]
	TIME [epoch: 5.81 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03093038902841868		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.03093038902841868 | validation: 0.06298138038618838]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1078.pth
	Model improved!!!
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030546864742953286		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.030546864742953286 | validation: 0.07370802915884499]
	TIME [epoch: 5.79 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029514817604872238		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.029514817604872238 | validation: 0.07217283947671789]
	TIME [epoch: 5.81 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02969606942743828		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.02969606942743828 | validation: 0.06442523739128429]
	TIME [epoch: 5.8 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02921805820789517		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.02921805820789517 | validation: 0.07334966082993796]
	TIME [epoch: 5.8 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029775193434575895		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.029775193434575895 | validation: 0.06717198810010364]
	TIME [epoch: 5.8 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030799581649404797		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.030799581649404797 | validation: 0.07912474928667008]
	TIME [epoch: 5.8 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028904408303257325		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.028904408303257325 | validation: 0.06705149548294922]
	TIME [epoch: 5.81 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029274530123190715		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.029274530123190715 | validation: 0.06946988192375673]
	TIME [epoch: 5.8 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02907893393446763		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.02907893393446763 | validation: 0.06879536946558823]
	TIME [epoch: 5.8 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027751501858041757		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.027751501858041757 | validation: 0.06420174135658184]
	TIME [epoch: 5.8 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027854223028023525		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.027854223028023525 | validation: 0.06681706865549211]
	TIME [epoch: 5.8 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02813350201627163		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.02813350201627163 | validation: 0.07028871025496804]
	TIME [epoch: 5.8 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028322114725003535		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.028322114725003535 | validation: 0.06387796655946595]
	TIME [epoch: 5.8 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028001496942965658		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.028001496942965658 | validation: 0.06678721824664222]
	TIME [epoch: 5.81 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027842673607455286		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.027842673607455286 | validation: 0.07142158444781822]
	TIME [epoch: 5.81 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0278955056298168		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.0278955056298168 | validation: 0.0690962550305739]
	TIME [epoch: 5.8 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028608124806277724		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.028608124806277724 | validation: 0.06471224318813255]
	TIME [epoch: 5.8 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032236856088842344		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.032236856088842344 | validation: 0.07355517389739165]
	TIME [epoch: 5.8 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028981105714234102		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.028981105714234102 | validation: 0.06643113055049903]
	TIME [epoch: 5.8 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02921523213103889		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.02921523213103889 | validation: 0.06819650711226377]
	TIME [epoch: 5.81 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02802419311739557		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.02802419311739557 | validation: 0.07145524521865596]
	TIME [epoch: 5.8 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02770306893537609		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.02770306893537609 | validation: 0.06147866128915833]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1100.pth
	Model improved!!!
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02841734834968701		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.02841734834968701 | validation: 0.06927040519013393]
	TIME [epoch: 5.8 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028725807427032173		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.028725807427032173 | validation: 0.0714467892022565]
	TIME [epoch: 5.8 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02871036977317706		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.02871036977317706 | validation: 0.059073639157925066]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1103.pth
	Model improved!!!
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02816454561791458		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.02816454561791458 | validation: 0.06252240793623572]
	TIME [epoch: 5.8 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026818511730225286		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.026818511730225286 | validation: 0.06681204996920752]
	TIME [epoch: 5.8 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028839430425869707		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.028839430425869707 | validation: 0.06235722776151577]
	TIME [epoch: 5.82 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811317199822889		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.02811317199822889 | validation: 0.07081701560819408]
	TIME [epoch: 5.8 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02832872881423401		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.02832872881423401 | validation: 0.06669365276094923]
	TIME [epoch: 5.81 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026982678534425653		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.026982678534425653 | validation: 0.06892751559272892]
	TIME [epoch: 5.8 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027826417207594777		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.027826417207594777 | validation: 0.07117985654228139]
	TIME [epoch: 5.81 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032050730154855704		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.032050730154855704 | validation: 0.07017517890021938]
	TIME [epoch: 5.8 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0274271691081342		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.0274271691081342 | validation: 0.07089188974253707]
	TIME [epoch: 5.8 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151922622697257		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.03151922622697257 | validation: 0.06633868561172794]
	TIME [epoch: 5.8 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02652500750899499		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.02652500750899499 | validation: 0.06696647583003815]
	TIME [epoch: 5.8 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026981415841200364		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.026981415841200364 | validation: 0.0717799879422639]
	TIME [epoch: 5.8 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028122643159705696		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.028122643159705696 | validation: 0.06118371005452214]
	TIME [epoch: 5.81 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027365530856558178		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.027365530856558178 | validation: 0.0674697115101127]
	TIME [epoch: 5.8 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027221922939753878		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.027221922939753878 | validation: 0.06795916824037307]
	TIME [epoch: 5.81 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885601186001642		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02885601186001642 | validation: 0.06212082885009293]
	TIME [epoch: 5.8 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02872675432394939		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.02872675432394939 | validation: 0.06745404596045872]
	TIME [epoch: 5.8 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026194757175854885		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.026194757175854885 | validation: 0.06910415401089914]
	TIME [epoch: 5.8 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02748408825892244		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.02748408825892244 | validation: 0.06741382348888278]
	TIME [epoch: 5.8 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0263494966901577		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.0263494966901577 | validation: 0.06970795968892156]
	TIME [epoch: 5.8 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02648174433149946		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.02648174433149946 | validation: 0.06653621540808882]
	TIME [epoch: 5.81 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027612403630892863		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.027612403630892863 | validation: 0.06272998162448494]
	TIME [epoch: 5.8 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030662775760703448		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.030662775760703448 | validation: 0.0778273793128052]
	TIME [epoch: 5.81 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142376589505912		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.03142376589505912 | validation: 0.06291548504113618]
	TIME [epoch: 5.8 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02825093179349201		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.02825093179349201 | validation: 0.0659034011459172]
	TIME [epoch: 5.81 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02706707891775317		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.02706707891775317 | validation: 0.06526519005863603]
	TIME [epoch: 5.8 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027719041486804832		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.027719041486804832 | validation: 0.06259241826523819]
	TIME [epoch: 5.81 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02680834506359205		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.02680834506359205 | validation: 0.06697671340912469]
	TIME [epoch: 5.8 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02713188592059314		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.02713188592059314 | validation: 0.06629859530477021]
	TIME [epoch: 5.81 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027723890921993545		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.027723890921993545 | validation: 0.06312080588782643]
	TIME [epoch: 5.79 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027185876468527086		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.027185876468527086 | validation: 0.07141485586678724]
	TIME [epoch: 5.8 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028940491933237382		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.028940491933237382 | validation: 0.06521033207199714]
	TIME [epoch: 5.79 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02528708925638509		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.02528708925638509 | validation: 0.06424006739982946]
	TIME [epoch: 5.8 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02706780942973718		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.02706780942973718 | validation: 0.07453443340753428]
	TIME [epoch: 5.79 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031152254606252004		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.031152254606252004 | validation: 0.06202155729188596]
	TIME [epoch: 5.8 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02805997834645927		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.02805997834645927 | validation: 0.06559361484051499]
	TIME [epoch: 5.8 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028422452229745186		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.028422452229745186 | validation: 0.06522652291223584]
	TIME [epoch: 5.8 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027243775883750355		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.027243775883750355 | validation: 0.05771496438571316]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025932483316856984		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.025932483316856984 | validation: 0.06343472517180732]
	TIME [epoch: 5.77 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02577084509376126		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.02577084509376126 | validation: 0.07126030260022516]
	TIME [epoch: 5.78 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0260338255964965		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.0260338255964965 | validation: 0.06057545315578892]
	TIME [epoch: 5.77 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027432488190523678		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.027432488190523678 | validation: 0.06466992237935085]
	TIME [epoch: 5.78 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027397229392961027		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.027397229392961027 | validation: 0.06765190766952853]
	TIME [epoch: 5.78 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0262283168701197		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.0262283168701197 | validation: 0.0635788282066956]
	TIME [epoch: 5.79 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02598195219405573		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.02598195219405573 | validation: 0.0625695485992782]
	TIME [epoch: 5.78 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02588228996136071		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.02588228996136071 | validation: 0.06758450214802499]
	TIME [epoch: 5.79 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02674885201170467		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.02674885201170467 | validation: 0.06629573155759637]
	TIME [epoch: 5.77 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026406191925970223		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.026406191925970223 | validation: 0.0700239125161765]
	TIME [epoch: 5.78 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026689122616930287		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.026689122616930287 | validation: 0.07249679129075506]
	TIME [epoch: 5.78 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02794591029031329		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.02794591029031329 | validation: 0.06379734519065237]
	TIME [epoch: 5.78 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026452694024837353		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.026452694024837353 | validation: 0.06265575690647418]
	TIME [epoch: 5.78 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026394139123995464		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.026394139123995464 | validation: 0.058320748460591446]
	TIME [epoch: 5.78 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027260857503817713		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.027260857503817713 | validation: 0.07421349467978641]
	TIME [epoch: 5.78 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028396979516307548		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.028396979516307548 | validation: 0.06501684539609491]
	TIME [epoch: 5.79 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026484397336235914		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.026484397336235914 | validation: 0.06913296016207855]
	TIME [epoch: 5.78 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025646608484169314		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.025646608484169314 | validation: 0.07603983725796086]
	TIME [epoch: 5.78 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028425442489478356		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.028425442489478356 | validation: 0.05861065600716806]
	TIME [epoch: 5.77 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027315713144545683		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.027315713144545683 | validation: 0.06257014483850758]
	TIME [epoch: 5.78 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02789075119433494		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.02789075119433494 | validation: 0.07751608808273905]
	TIME [epoch: 5.78 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027877043096322173		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.027877043096322173 | validation: 0.0657921594479502]
	TIME [epoch: 5.78 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025543693233131264		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.025543693233131264 | validation: 0.07017219066257073]
	TIME [epoch: 5.78 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025340702712176574		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.025340702712176574 | validation: 0.0675575479242241]
	TIME [epoch: 5.78 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024957811391207292		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.024957811391207292 | validation: 0.06487681316007779]
	TIME [epoch: 5.78 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025403576508638885		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.025403576508638885 | validation: 0.06616775614925752]
	TIME [epoch: 5.78 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025121333578284763		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.025121333578284763 | validation: 0.07288593923117853]
	TIME [epoch: 5.78 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025587371800666375		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.025587371800666375 | validation: 0.06648742212121818]
	TIME [epoch: 5.78 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025719850925040536		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.025719850925040536 | validation: 0.06440110762914235]
	TIME [epoch: 5.78 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02528660499464789		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.02528660499464789 | validation: 0.06426028244765107]
	TIME [epoch: 5.78 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02439582260233348		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02439582260233348 | validation: 0.06213021026517297]
	TIME [epoch: 5.78 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025981404363545		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.025981404363545 | validation: 0.06354765619684151]
	TIME [epoch: 5.79 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02536925837923877		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02536925837923877 | validation: 0.06921330966704524]
	TIME [epoch: 5.78 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026508974745142914		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.026508974745142914 | validation: 0.05794080383131768]
	TIME [epoch: 5.78 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024892987641056616		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.024892987641056616 | validation: 0.06733091744747502]
	TIME [epoch: 5.78 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02698403197664323		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.02698403197664323 | validation: 0.06536329525775947]
	TIME [epoch: 5.78 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02573305397786452		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.02573305397786452 | validation: 0.058653211450605924]
	TIME [epoch: 5.79 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026076300781462454		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.026076300781462454 | validation: 0.06506479627246613]
	TIME [epoch: 5.78 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02626004866581236		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.02626004866581236 | validation: 0.06278200215595897]
	TIME [epoch: 5.78 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025601797078245385		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.025601797078245385 | validation: 0.05973046785536075]
	TIME [epoch: 5.78 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024666001493513		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.024666001493513 | validation: 0.07264580108924318]
	TIME [epoch: 5.79 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02621216105569638		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.02621216105569638 | validation: 0.061414648478419334]
	TIME [epoch: 5.78 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02626910490101345		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.02626910490101345 | validation: 0.06450782358901598]
	TIME [epoch: 5.79 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02457922009193826		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.02457922009193826 | validation: 0.06316033786969276]
	TIME [epoch: 5.78 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025566656117668604		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.025566656117668604 | validation: 0.06534427412289791]
	TIME [epoch: 5.79 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028535273160723		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.028535273160723 | validation: 0.06750022909772313]
	TIME [epoch: 5.78 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02501496210473852		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02501496210473852 | validation: 0.06574101851488977]
	TIME [epoch: 5.78 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026586556552420242		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.026586556552420242 | validation: 0.06666430170852063]
	TIME [epoch: 5.83 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02507372401206375		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.02507372401206375 | validation: 0.06962863382061714]
	TIME [epoch: 5.78 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025077397632877033		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.025077397632877033 | validation: 0.06384094440228931]
	TIME [epoch: 5.79 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027749854205212712		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.027749854205212712 | validation: 0.05942643733670405]
	TIME [epoch: 5.78 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02751755137719252		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.02751755137719252 | validation: 0.06932937483363368]
	TIME [epoch: 5.78 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024785984255861065		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.024785984255861065 | validation: 0.07503742419136301]
	TIME [epoch: 5.78 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025150489422533454		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.025150489422533454 | validation: 0.07357966923555798]
	TIME [epoch: 5.78 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025644388299907388		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.025644388299907388 | validation: 0.06547660954611823]
	TIME [epoch: 5.78 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024901177528551317		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.024901177528551317 | validation: 0.05864567988726776]
	TIME [epoch: 5.78 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023893719281461574		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.023893719281461574 | validation: 0.0629128191692646]
	TIME [epoch: 5.78 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025159219110636562		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.025159219110636562 | validation: 0.057305966082992536]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02432519415952415		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.02432519415952415 | validation: 0.057118958927741395]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1200.pth
	Model improved!!!
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025253275054099182		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.025253275054099182 | validation: 0.07063678081971662]
	TIME [epoch: 5.76 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027621408775829522		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.027621408775829522 | validation: 0.06546980336307014]
	TIME [epoch: 5.77 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023661238168765		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.023661238168765 | validation: 0.05710235359791915]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1203.pth
	Model improved!!!
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024257021882395785		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.024257021882395785 | validation: 0.060288227576948794]
	TIME [epoch: 5.79 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02539663268562144		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.02539663268562144 | validation: 0.06546615580647423]
	TIME [epoch: 5.78 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027544967646768895		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.027544967646768895 | validation: 0.06031219167906219]
	TIME [epoch: 5.79 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02589223897863457		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.02589223897863457 | validation: 0.06092440553900698]
	TIME [epoch: 5.78 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025317069236489024		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.025317069236489024 | validation: 0.0575792271662569]
	TIME [epoch: 5.79 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02500384251396297		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02500384251396297 | validation: 0.07046038809507564]
	TIME [epoch: 5.78 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024928167790797772		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.024928167790797772 | validation: 0.0648702767128624]
	TIME [epoch: 5.78 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025033427024488027		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.025033427024488027 | validation: 0.056953260958300826]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1211.pth
	Model improved!!!
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025287687181864957		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.025287687181864957 | validation: 0.06129127399413488]
	TIME [epoch: 5.78 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023529508116738054		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.023529508116738054 | validation: 0.06391362801106495]
	TIME [epoch: 5.78 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024847552644278285		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.024847552644278285 | validation: 0.05782599944618652]
	TIME [epoch: 5.79 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02536130505437666		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.02536130505437666 | validation: 0.06847541832081853]
	TIME [epoch: 5.78 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0240651678870971		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.0240651678870971 | validation: 0.06105335932199213]
	TIME [epoch: 5.78 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024257122016882047		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.024257122016882047 | validation: 0.05956302637022103]
	TIME [epoch: 5.78 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023645325864458278		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.023645325864458278 | validation: 0.05501029673371645]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1218.pth
	Model improved!!!
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024259601703988217		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.024259601703988217 | validation: 0.06288797651792422]
	TIME [epoch: 5.77 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024496246097082102		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.024496246097082102 | validation: 0.0656494172070592]
	TIME [epoch: 5.78 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02402702759862483		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.02402702759862483 | validation: 0.06255336278723393]
	TIME [epoch: 5.78 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024738346818076438		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.024738346818076438 | validation: 0.06405713784273999]
	TIME [epoch: 5.79 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024961065797155114		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.024961065797155114 | validation: 0.06272006592516981]
	TIME [epoch: 5.78 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02445267949592588		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.02445267949592588 | validation: 0.0630577166358516]
	TIME [epoch: 5.78 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025442085633106588		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.025442085633106588 | validation: 0.06946976549006571]
	TIME [epoch: 5.78 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02406832153263594		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.02406832153263594 | validation: 0.05480396383730943]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1226.pth
	Model improved!!!
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02390930442074324		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.02390930442074324 | validation: 0.060487254926151826]
	TIME [epoch: 5.78 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025109225120392386		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.025109225120392386 | validation: 0.05340775999900486]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1228.pth
	Model improved!!!
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02532888997268829		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.02532888997268829 | validation: 0.06769446985381336]
	TIME [epoch: 5.74 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024165530509429597		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.024165530509429597 | validation: 0.059373141456287075]
	TIME [epoch: 5.75 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025867317661826164		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.025867317661826164 | validation: 0.058752035739463894]
	TIME [epoch: 5.74 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02408355336191947		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.02408355336191947 | validation: 0.05813005641644094]
	TIME [epoch: 5.74 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024528267402633072		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.024528267402633072 | validation: 0.06589439890222114]
	TIME [epoch: 5.75 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024916326934477685		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.024916326934477685 | validation: 0.057426123539959864]
	TIME [epoch: 5.74 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024189583867080398		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.024189583867080398 | validation: 0.05966941301920318]
	TIME [epoch: 5.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023484962783973308		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.023484962783973308 | validation: 0.06880248060182488]
	TIME [epoch: 5.74 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024036170009423197		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.024036170009423197 | validation: 0.06294904916859155]
	TIME [epoch: 5.74 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02317878025731543		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.02317878025731543 | validation: 0.055784044745671474]
	TIME [epoch: 5.75 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02286045702345512		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.02286045702345512 | validation: 0.06507120489924755]
	TIME [epoch: 5.74 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02368634406276488		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.02368634406276488 | validation: 0.06093866831237924]
	TIME [epoch: 5.75 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0231672507223288		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.0231672507223288 | validation: 0.05923287341948724]
	TIME [epoch: 5.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024475046512371482		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.024475046512371482 | validation: 0.06482784264812531]
	TIME [epoch: 5.74 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024015162001765775		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.024015162001765775 | validation: 0.06288865848992119]
	TIME [epoch: 5.74 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024219214914278045		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.024219214914278045 | validation: 0.06187489111378957]
	TIME [epoch: 5.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023483123200154693		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.023483123200154693 | validation: 0.06327687716044891]
	TIME [epoch: 5.74 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024575190026853152		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.024575190026853152 | validation: 0.05860943174404501]
	TIME [epoch: 5.74 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024133891947848398		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.024133891947848398 | validation: 0.06272831443394114]
	TIME [epoch: 5.74 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02424823015894984		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.02424823015894984 | validation: 0.0562432638575213]
	TIME [epoch: 5.75 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02238740259612821		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.02238740259612821 | validation: 0.06071417881939502]
	TIME [epoch: 5.75 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0237511346829292		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.0237511346829292 | validation: 0.05863121505620981]
	TIME [epoch: 5.74 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023378966492317563		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.023378966492317563 | validation: 0.060706550744568155]
	TIME [epoch: 5.74 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02323187793337966		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.02323187793337966 | validation: 0.06564361959734866]
	TIME [epoch: 5.74 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024413447390973286		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.024413447390973286 | validation: 0.057759382092019]
	TIME [epoch: 5.74 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02162866213828756		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.02162866213828756 | validation: 0.0540645085997025]
	TIME [epoch: 5.74 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023148383311590814		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.023148383311590814 | validation: 0.06454976089786908]
	TIME [epoch: 5.74 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023563407725500536		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.023563407725500536 | validation: 0.05848276675115582]
	TIME [epoch: 5.74 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023030627518179296		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.023030627518179296 | validation: 0.062280757045698715]
	TIME [epoch: 5.75 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022734093891216216		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.022734093891216216 | validation: 0.0596381072170854]
	TIME [epoch: 5.74 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022538044375576955		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.022538044375576955 | validation: 0.06503911949541812]
	TIME [epoch: 5.75 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02434652001800358		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.02434652001800358 | validation: 0.05375661280849869]
	TIME [epoch: 5.74 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024968870825268507		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.024968870825268507 | validation: 0.06226607444017895]
	TIME [epoch: 5.75 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02330244722936428		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.02330244722936428 | validation: 0.0629092548972641]
	TIME [epoch: 5.74 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02425365990732245		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.02425365990732245 | validation: 0.06272679706372346]
	TIME [epoch: 5.75 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024127433037630264		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.024127433037630264 | validation: 0.05456573061105775]
	TIME [epoch: 5.75 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023975713012501468		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.023975713012501468 | validation: 0.06300605271662793]
	TIME [epoch: 5.74 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024754807982875146		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.024754807982875146 | validation: 0.05775169248119616]
	TIME [epoch: 5.75 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023101998179205323		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.023101998179205323 | validation: 0.055254619496492456]
	TIME [epoch: 5.75 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02292131076312224		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.02292131076312224 | validation: 0.05666557582016845]
	TIME [epoch: 5.74 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02264075856411845		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.02264075856411845 | validation: 0.06260758049341575]
	TIME [epoch: 5.75 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022634179785492422		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.022634179785492422 | validation: 0.06294264919588638]
	TIME [epoch: 5.75 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02354174673401987		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.02354174673401987 | validation: 0.05776503318915519]
	TIME [epoch: 5.75 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02350979895090982		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.02350979895090982 | validation: 0.06035536820797108]
	TIME [epoch: 5.75 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02160309396163634		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.02160309396163634 | validation: 0.05603095540816018]
	TIME [epoch: 5.76 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021891316146217848		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.021891316146217848 | validation: 0.0609513665176348]
	TIME [epoch: 5.75 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02297681553253939		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.02297681553253939 | validation: 0.055706346506286065]
	TIME [epoch: 5.75 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024179590344908092		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.024179590344908092 | validation: 0.06323687609990664]
	TIME [epoch: 5.75 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023064907260781208		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.023064907260781208 | validation: 0.0636736553222177]
	TIME [epoch: 5.76 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023876347499976733		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.023876347499976733 | validation: 0.05424641479399259]
	TIME [epoch: 5.75 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02253590863460114		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.02253590863460114 | validation: 0.05840831748510427]
	TIME [epoch: 5.76 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02201067783502774		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.02201067783502774 | validation: 0.05954128776306253]
	TIME [epoch: 5.74 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0230182234945004		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.0230182234945004 | validation: 0.058697011665306775]
	TIME [epoch: 5.76 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023695514607758556		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.023695514607758556 | validation: 0.05846734425041104]
	TIME [epoch: 5.75 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021988618686348275		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.021988618686348275 | validation: 0.062299640164230154]
	TIME [epoch: 5.76 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022424781148855957		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.022424781148855957 | validation: 0.06190622192236442]
	TIME [epoch: 5.75 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02385627399408067		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.02385627399408067 | validation: 0.058350542684995114]
	TIME [epoch: 5.75 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022214172070398375		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.022214172070398375 | validation: 0.05126524703424617]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1286.pth
	Model improved!!!
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023288977318017975		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.023288977318017975 | validation: 0.061698598935585984]
	TIME [epoch: 5.79 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022849431977057973		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.022849431977057973 | validation: 0.06422846579157368]
	TIME [epoch: 5.79 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021128108310072877		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.021128108310072877 | validation: 0.05926409623013849]
	TIME [epoch: 5.79 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022076172317034706		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.022076172317034706 | validation: 0.057153424681268286]
	TIME [epoch: 5.77 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023573503764070747		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.023573503764070747 | validation: 0.0562522096932384]
	TIME [epoch: 5.76 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022537831316367073		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.022537831316367073 | validation: 0.057774733071020316]
	TIME [epoch: 5.75 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022932568251409845		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.022932568251409845 | validation: 0.06079773339238838]
	TIME [epoch: 5.75 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023362282495693398		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.023362282495693398 | validation: 0.05953750665066043]
	TIME [epoch: 5.8 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022826268358334102		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.022826268358334102 | validation: 0.0673956326719251]
	TIME [epoch: 5.8 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023522952489531814		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.023522952489531814 | validation: 0.055398786384096725]
	TIME [epoch: 5.79 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02349143416514139		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.02349143416514139 | validation: 0.06021994160569308]
	TIME [epoch: 5.8 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021225948918242567		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.021225948918242567 | validation: 0.06405869397511603]
	TIME [epoch: 5.79 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02264207683367602		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.02264207683367602 | validation: 0.05787467993169624]
	TIME [epoch: 5.81 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02295544364332058		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.02295544364332058 | validation: 0.05608140798058353]
	TIME [epoch: 5.8 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02190354659864186		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.02190354659864186 | validation: 0.05653290406411472]
	TIME [epoch: 5.74 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021973280292469352		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.021973280292469352 | validation: 0.05465822638972299]
	TIME [epoch: 5.78 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022422678946202118		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.022422678946202118 | validation: 0.053128787110505706]
	TIME [epoch: 5.75 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021870750312503812		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.021870750312503812 | validation: 0.05809316721006902]
	TIME [epoch: 5.74 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021587188475400757		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.021587188475400757 | validation: 0.06081500391671473]
	TIME [epoch: 5.74 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022850871367965545		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.022850871367965545 | validation: 0.06185933033443394]
	TIME [epoch: 5.74 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021985919346023754		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.021985919346023754 | validation: 0.0548311693801509]
	TIME [epoch: 5.74 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022878625957037525		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.022878625957037525 | validation: 0.06040853775637123]
	TIME [epoch: 5.74 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02180855941062817		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.02180855941062817 | validation: 0.05921642476110808]
	TIME [epoch: 5.74 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02221323217695729		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.02221323217695729 | validation: 0.06152164910794671]
	TIME [epoch: 5.74 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021296663304254743		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.021296663304254743 | validation: 0.059087703761173165]
	TIME [epoch: 5.73 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022196598586467414		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.022196598586467414 | validation: 0.05656713062383256]
	TIME [epoch: 5.74 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021938847931258252		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.021938847931258252 | validation: 0.05910827306745429]
	TIME [epoch: 5.74 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020518887613453156		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.020518887613453156 | validation: 0.05876094771401424]
	TIME [epoch: 5.74 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021608150155726116		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.021608150155726116 | validation: 0.055672173314254864]
	TIME [epoch: 5.74 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022383561274217617		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.022383561274217617 | validation: 0.05768103753248745]
	TIME [epoch: 5.74 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02228771914181525		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.02228771914181525 | validation: 0.05970821919735095]
	TIME [epoch: 5.74 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020775079187647955		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.020775079187647955 | validation: 0.065099124711947]
	TIME [epoch: 5.74 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023301573510132942		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.023301573510132942 | validation: 0.06430863773214644]
	TIME [epoch: 5.74 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022042490156606656		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.022042490156606656 | validation: 0.05852428523322786]
	TIME [epoch: 5.74 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021977106204219145		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.021977106204219145 | validation: 0.060550311462847985]
	TIME [epoch: 5.74 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0216722423142473		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.0216722423142473 | validation: 0.055030438676110564]
	TIME [epoch: 5.74 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02210176666571786		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.02210176666571786 | validation: 0.05968560800322412]
	TIME [epoch: 5.74 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022261802903068523		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.022261802903068523 | validation: 0.06186197884950422]
	TIME [epoch: 5.74 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0224817843282159		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.0224817843282159 | validation: 0.054242373308537366]
	TIME [epoch: 5.74 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021643512453314383		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.021643512453314383 | validation: 0.060199865549431686]
	TIME [epoch: 5.74 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021486401543820213		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.021486401543820213 | validation: 0.06240982818019497]
	TIME [epoch: 5.74 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02263276955702393		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.02263276955702393 | validation: 0.05957760093164186]
	TIME [epoch: 5.74 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02191147856263961		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.02191147856263961 | validation: 0.05399880724740467]
	TIME [epoch: 5.74 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02113196266829808		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.02113196266829808 | validation: 0.05919424254294177]
	TIME [epoch: 5.74 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020811702014223325		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.020811702014223325 | validation: 0.06181669009682922]
	TIME [epoch: 5.74 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021920518467681973		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.021920518467681973 | validation: 0.05105541561008697]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1332.pth
	Model improved!!!
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02147402950104202		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.02147402950104202 | validation: 0.056630546728491715]
	TIME [epoch: 5.8 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02071496158660821		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.02071496158660821 | validation: 0.05958594413543964]
	TIME [epoch: 5.8 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020588834009230887		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.020588834009230887 | validation: 0.0683632462516638]
	TIME [epoch: 5.81 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021590908501288184		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.021590908501288184 | validation: 0.05293818982266116]
	TIME [epoch: 5.8 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0214367612590812		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.0214367612590812 | validation: 0.053288808322813956]
	TIME [epoch: 5.81 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022490407606324547		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.022490407606324547 | validation: 0.06269804030517388]
	TIME [epoch: 5.79 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02091817233873761		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.02091817233873761 | validation: 0.061853656297350124]
	TIME [epoch: 5.8 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021816086650233695		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.021816086650233695 | validation: 0.06277078716481906]
	TIME [epoch: 5.79 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02119649529037059		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.02119649529037059 | validation: 0.061253122315266265]
	TIME [epoch: 5.8 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171044028722496		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.02171044028722496 | validation: 0.05585408605009995]
	TIME [epoch: 5.79 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021247932563367135		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.021247932563367135 | validation: 0.05349436072759416]
	TIME [epoch: 5.8 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02082380467564939		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.02082380467564939 | validation: 0.057826779060491766]
	TIME [epoch: 5.79 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021167155179800386		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.021167155179800386 | validation: 0.057109794104639226]
	TIME [epoch: 5.8 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021129417298126164		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.021129417298126164 | validation: 0.056967513373867996]
	TIME [epoch: 5.79 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021891708487851727		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.021891708487851727 | validation: 0.057806955871037916]
	TIME [epoch: 5.8 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02054393753883246		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.02054393753883246 | validation: 0.058351328492817114]
	TIME [epoch: 5.79 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021901080506227907		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.021901080506227907 | validation: 0.054792393752431334]
	TIME [epoch: 5.8 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021085219245028614		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.021085219245028614 | validation: 0.06305655975621792]
	TIME [epoch: 5.79 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020859252595883407		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.020859252595883407 | validation: 0.057417148602224445]
	TIME [epoch: 5.79 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021259063584892317		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.021259063584892317 | validation: 0.05807069517463375]
	TIME [epoch: 5.79 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021224259994799634		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.021224259994799634 | validation: 0.06046850877110708]
	TIME [epoch: 5.79 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020866176820134325		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.020866176820134325 | validation: 0.05861958193760371]
	TIME [epoch: 5.79 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02056392723229732		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.02056392723229732 | validation: 0.05868859614143308]
	TIME [epoch: 5.79 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021466058590496138		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.021466058590496138 | validation: 0.06065307657043635]
	TIME [epoch: 5.78 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02129167290457501		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.02129167290457501 | validation: 0.05738308327776647]
	TIME [epoch: 5.79 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02198729367791313		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.02198729367791313 | validation: 0.05628653397046432]
	TIME [epoch: 5.79 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021254753758312123		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.021254753758312123 | validation: 0.05349968836150685]
	TIME [epoch: 5.79 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0213988297459189		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.0213988297459189 | validation: 0.056537734311490645]
	TIME [epoch: 5.79 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0205881496303118		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.0205881496303118 | validation: 0.062287079020229466]
	TIME [epoch: 5.79 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0219878098232385		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.0219878098232385 | validation: 0.062442069426832725]
	TIME [epoch: 5.78 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020671532131288705		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.020671532131288705 | validation: 0.05518602186916127]
	TIME [epoch: 5.8 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02050759699061611		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.02050759699061611 | validation: 0.054310176958940304]
	TIME [epoch: 5.78 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021546728440634263		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.021546728440634263 | validation: 0.05527968512781861]
	TIME [epoch: 5.8 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02043422716650933		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02043422716650933 | validation: 0.05059389149458139]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1366.pth
	Model improved!!!
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020868931244452812		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.020868931244452812 | validation: 0.059299713940325044]
	TIME [epoch: 5.75 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020182650923521242		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.020182650923521242 | validation: 0.055896462484097145]
	TIME [epoch: 5.74 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020944269229412615		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.020944269229412615 | validation: 0.056341501767502446]
	TIME [epoch: 6.07 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021580930875520023		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.021580930875520023 | validation: 0.0559411623407772]
	TIME [epoch: 5.74 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020584485607151745		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.020584485607151745 | validation: 0.056600247114991065]
	TIME [epoch: 5.75 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020910566196463178		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.020910566196463178 | validation: 0.060584331611360204]
	TIME [epoch: 5.75 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021176417068064442		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.021176417068064442 | validation: 0.058370109977134314]
	TIME [epoch: 5.75 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020117722730806246		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.020117722730806246 | validation: 0.052966300291976326]
	TIME [epoch: 5.74 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020475231718781354		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.020475231718781354 | validation: 0.06180002896389182]
	TIME [epoch: 5.75 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02061854038576424		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.02061854038576424 | validation: 0.05539621937893209]
	TIME [epoch: 5.75 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0199418483091131		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.0199418483091131 | validation: 0.057050281786034045]
	TIME [epoch: 5.74 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021038618247112745		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.021038618247112745 | validation: 0.051797755184455875]
	TIME [epoch: 5.74 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02101134268683521		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.02101134268683521 | validation: 0.059439726008204824]
	TIME [epoch: 5.74 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020817431302570615		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.020817431302570615 | validation: 0.05774807336707343]
	TIME [epoch: 5.75 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02031789598270498		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.02031789598270498 | validation: 0.060368270779701805]
	TIME [epoch: 5.75 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019747671961355306		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.019747671961355306 | validation: 0.052796776590112664]
	TIME [epoch: 5.75 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020382614186690847		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.020382614186690847 | validation: 0.05744387453604029]
	TIME [epoch: 5.75 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020545568053035153		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.020545568053035153 | validation: 0.052770035449130774]
	TIME [epoch: 5.75 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021334394957337074		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.021334394957337074 | validation: 0.05728315001418134]
	TIME [epoch: 5.75 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019815386163628587		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.019815386163628587 | validation: 0.0591627615223048]
	TIME [epoch: 5.75 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020424398043840233		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.020424398043840233 | validation: 0.05965640749435891]
	TIME [epoch: 5.75 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02022714136971915		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.02022714136971915 | validation: 0.060140057144305516]
	TIME [epoch: 5.75 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02109096431687786		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.02109096431687786 | validation: 0.054355746860533904]
	TIME [epoch: 5.74 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01975581091989617		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.01975581091989617 | validation: 0.061206224027168146]
	TIME [epoch: 5.75 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02068801964656713		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.02068801964656713 | validation: 0.05276527411415516]
	TIME [epoch: 5.74 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019905449494595887		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.019905449494595887 | validation: 0.055259982273143195]
	TIME [epoch: 5.75 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019931347342832254		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.019931347342832254 | validation: 0.05390364316422284]
	TIME [epoch: 5.75 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019450010570826754		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.019450010570826754 | validation: 0.05448966911406472]
	TIME [epoch: 5.75 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019157087654402265		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.019157087654402265 | validation: 0.05315406061255648]
	TIME [epoch: 5.75 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02034996430746854		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.02034996430746854 | validation: 0.05333936451080317]
	TIME [epoch: 5.75 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020643583938488887		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.020643583938488887 | validation: 0.05476000677534076]
	TIME [epoch: 5.74 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020357814350955756		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.020357814350955756 | validation: 0.06124473788523019]
	TIME [epoch: 5.75 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021088765806274914		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.021088765806274914 | validation: 0.054719387170728495]
	TIME [epoch: 5.74 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019933245832357956		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.019933245832357956 | validation: 0.05415847902714911]
	TIME [epoch: 5.75 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01885619626464427		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.01885619626464427 | validation: 0.05741207479207525]
	TIME [epoch: 5.79 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019357363494580736		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.019357363494580736 | validation: 0.060693960643275396]
	TIME [epoch: 5.79 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02129797989446721		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.02129797989446721 | validation: 0.059052659428802556]
	TIME [epoch: 5.79 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020136599982994028		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.020136599982994028 | validation: 0.05509134479450842]
	TIME [epoch: 5.77 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020283031728330254		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.020283031728330254 | validation: 0.05149968223249044]
	TIME [epoch: 5.74 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01932280722967317		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.01932280722967317 | validation: 0.05840052326605644]
	TIME [epoch: 5.75 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021005741790664594		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.021005741790664594 | validation: 0.05363937177906839]
	TIME [epoch: 5.74 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020743817957520304		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.020743817957520304 | validation: 0.053744630750209035]
	TIME [epoch: 5.75 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020983116196179995		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.020983116196179995 | validation: 0.05498045093681217]
	TIME [epoch: 5.75 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019232951852284942		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.019232951852284942 | validation: 0.05508255414416872]
	TIME [epoch: 5.75 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020412090279315374		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.020412090279315374 | validation: 0.058458191949808586]
	TIME [epoch: 5.74 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021097463594919534		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.021097463594919534 | validation: 0.05253371487503175]
	TIME [epoch: 5.75 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020799038937645092		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.020799038937645092 | validation: 0.05930158046984921]
	TIME [epoch: 5.74 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019481969114166095		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.019481969114166095 | validation: 0.05933182293761809]
	TIME [epoch: 5.75 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01942016913515054		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.01942016913515054 | validation: 0.05774499637489107]
	TIME [epoch: 5.74 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020867967816841904		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.020867967816841904 | validation: 0.05184443878380675]
	TIME [epoch: 5.75 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01996344908330586		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.01996344908330586 | validation: 0.051384107752791265]
	TIME [epoch: 5.75 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039131792145381		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.02039131792145381 | validation: 0.053484276508751906]
	TIME [epoch: 5.75 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019399238160261553		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.019399238160261553 | validation: 0.06080510505316031]
	TIME [epoch: 5.74 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02037025857846478		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.02037025857846478 | validation: 0.05974430848898465]
	TIME [epoch: 5.75 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01963340842665308		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.01963340842665308 | validation: 0.05281323087527794]
	TIME [epoch: 5.74 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020219688438199462		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.020219688438199462 | validation: 0.052971453943145924]
	TIME [epoch: 5.74 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019010410290587844		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.019010410290587844 | validation: 0.056426453526653354]
	TIME [epoch: 5.75 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019257538908296554		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.019257538908296554 | validation: 0.05288494152441863]
	TIME [epoch: 5.74 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019650741794902293		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.019650741794902293 | validation: 0.05291009569197818]
	TIME [epoch: 5.75 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019822102612923023		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.019822102612923023 | validation: 0.05719108010177668]
	TIME [epoch: 5.74 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020214029035467727		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.020214029035467727 | validation: 0.054758695402105886]
	TIME [epoch: 5.75 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019879798535491235		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.019879798535491235 | validation: 0.05984037358399424]
	TIME [epoch: 5.74 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019238272654309656		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.019238272654309656 | validation: 0.0600101342283234]
	TIME [epoch: 5.8 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019199873154674654		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.019199873154674654 | validation: 0.05660280626100625]
	TIME [epoch: 5.79 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01912744108774604		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.01912744108774604 | validation: 0.051456006415065196]
	TIME [epoch: 5.8 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019228160010504276		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.019228160010504276 | validation: 0.059982622904039266]
	TIME [epoch: 5.8 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019653448639843595		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.019653448639843595 | validation: 0.05742397341176882]
	TIME [epoch: 5.8 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020183630096483763		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.020183630096483763 | validation: 0.059478105372928275]
	TIME [epoch: 5.8 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02043698322353052		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.02043698322353052 | validation: 0.05921773452682203]
	TIME [epoch: 5.81 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019844408696211856		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.019844408696211856 | validation: 0.06042539122724582]
	TIME [epoch: 5.8 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020688227338266748		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.020688227338266748 | validation: 0.05205959398888749]
	TIME [epoch: 5.8 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020299575135096504		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.020299575135096504 | validation: 0.053884880566944876]
	TIME [epoch: 5.81 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018805930701701216		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.018805930701701216 | validation: 0.05837947862120196]
	TIME [epoch: 5.8 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018956139890829435		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.018956139890829435 | validation: 0.05386632929408852]
	TIME [epoch: 5.81 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018494080343139402		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.018494080343139402 | validation: 0.05403952746011982]
	TIME [epoch: 5.8 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019396198146212813		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.019396198146212813 | validation: 0.054139684286228444]
	TIME [epoch: 5.8 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01957230379186064		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.01957230379186064 | validation: 0.055642637456107404]
	TIME [epoch: 5.8 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019703596353610624		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.019703596353610624 | validation: 0.05753861222111515]
	TIME [epoch: 5.81 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020069415799562305		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.020069415799562305 | validation: 0.048526452614821386]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1445.pth
	Model improved!!!
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018891362737186612		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.018891362737186612 | validation: 0.05384988418864576]
	TIME [epoch: 5.75 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019566081928331792		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.019566081928331792 | validation: 0.05094953673586409]
	TIME [epoch: 5.75 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019888484422237035		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.019888484422237035 | validation: 0.05902727835630161]
	TIME [epoch: 5.75 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019946040113437955		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.019946040113437955 | validation: 0.060219785066444514]
	TIME [epoch: 5.74 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01986567425462885		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.01986567425462885 | validation: 0.05464385602490112]
	TIME [epoch: 5.75 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01936530098402202		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.01936530098402202 | validation: 0.05648614669591552]
	TIME [epoch: 5.75 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019006728672859883		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.019006728672859883 | validation: 0.06263328256236748]
	TIME [epoch: 5.75 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018376101494512093		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.018376101494512093 | validation: 0.05362824359789886]
	TIME [epoch: 5.75 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018596856353668118		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.018596856353668118 | validation: 0.049485355431743205]
	TIME [epoch: 5.74 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018721149862447377		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.018721149862447377 | validation: 0.054139005071958915]
	TIME [epoch: 5.75 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019525339097389422		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.019525339097389422 | validation: 0.053262890736667706]
	TIME [epoch: 5.75 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01890025492173553		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.01890025492173553 | validation: 0.055595253388056515]
	TIME [epoch: 5.75 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018212276850476012		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.018212276850476012 | validation: 0.054559476894937876]
	TIME [epoch: 5.75 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019034912328993986		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.019034912328993986 | validation: 0.06002815834272934]
	TIME [epoch: 5.76 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019421491095192825		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.019421491095192825 | validation: 0.05692564565627972]
	TIME [epoch: 5.75 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01860793863922852		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.01860793863922852 | validation: 0.05721274060428301]
	TIME [epoch: 5.76 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0193229881906688		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.0193229881906688 | validation: 0.05244317914576551]
	TIME [epoch: 5.74 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018669529859789286		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.018669529859789286 | validation: 0.06303717273030791]
	TIME [epoch: 5.75 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01998185120195481		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.01998185120195481 | validation: 0.05072608586544687]
	TIME [epoch: 5.74 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019245763708047574		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.019245763708047574 | validation: 0.05407353607269974]
	TIME [epoch: 5.74 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018686501729808857		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.018686501729808857 | validation: 0.05733839031230488]
	TIME [epoch: 5.75 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01836902690763321		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.01836902690763321 | validation: 0.05922840929466211]
	TIME [epoch: 5.75 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019199453291698858		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.019199453291698858 | validation: 0.05727538010901893]
	TIME [epoch: 5.75 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0183550111600466		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.0183550111600466 | validation: 0.054003432038784466]
	TIME [epoch: 5.75 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018022393904491354		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.018022393904491354 | validation: 0.052516157201342374]
	TIME [epoch: 5.75 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01929688110617445		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.01929688110617445 | validation: 0.04798145759500144]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1471.pth
	Model improved!!!
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018249435251563417		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.018249435251563417 | validation: 0.053807296750205624]
	TIME [epoch: 5.79 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01876775954623694		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.01876775954623694 | validation: 0.054267541632257936]
	TIME [epoch: 5.79 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01853818439112799		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.01853818439112799 | validation: 0.05668186035420776]
	TIME [epoch: 5.75 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019103646078138456		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.019103646078138456 | validation: 0.05454014685093748]
	TIME [epoch: 5.75 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017612326955320962		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.017612326955320962 | validation: 0.05416781880282551]
	TIME [epoch: 5.75 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019501522453672048		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.019501522453672048 | validation: 0.053883305866147306]
	TIME [epoch: 5.75 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019461986705701557		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.019461986705701557 | validation: 0.05435035603085047]
	TIME [epoch: 5.75 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018709726322656238		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.018709726322656238 | validation: 0.05557922672077316]
	TIME [epoch: 5.75 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017833143319929492		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.017833143319929492 | validation: 0.05635749180971481]
	TIME [epoch: 5.74 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019612761247973412		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.019612761247973412 | validation: 0.05805042138228814]
	TIME [epoch: 5.75 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0180467734513183		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.0180467734513183 | validation: 0.05733359299346429]
	TIME [epoch: 5.75 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019419764374970745		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.019419764374970745 | validation: 0.05346601389362798]
	TIME [epoch: 5.75 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01864915558900676		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.01864915558900676 | validation: 0.054248024694203326]
	TIME [epoch: 5.75 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018339198826512585		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.018339198826512585 | validation: 0.058602366699038205]
	TIME [epoch: 5.75 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020247661915880694		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.020247661915880694 | validation: 0.05674309778983602]
	TIME [epoch: 5.75 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01856853003718336		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.01856853003718336 | validation: 0.051970560346732135]
	TIME [epoch: 5.74 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018025238279629752		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.018025238279629752 | validation: 0.05952301258139469]
	TIME [epoch: 5.74 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01809791531126045		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.01809791531126045 | validation: 0.05386492197168168]
	TIME [epoch: 5.75 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018656761747139507		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.018656761747139507 | validation: 0.053037144256994274]
	TIME [epoch: 5.74 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018640977251015926		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.018640977251015926 | validation: 0.05075090495564997]
	TIME [epoch: 5.75 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019145363602639236		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.019145363602639236 | validation: 0.04994974070126589]
	TIME [epoch: 5.75 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018099785506250462		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.018099785506250462 | validation: 0.05840307985212648]
	TIME [epoch: 5.75 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018785076103681856		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.018785076103681856 | validation: 0.05903896676870474]
	TIME [epoch: 5.75 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017880071525685288		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.017880071525685288 | validation: 0.06213268969987208]
	TIME [epoch: 5.74 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018509457359908842		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.018509457359908842 | validation: 0.052602549603021326]
	TIME [epoch: 5.75 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019368977557792776		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.019368977557792776 | validation: 0.052593356079738064]
	TIME [epoch: 5.78 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01928886297937942		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.01928886297937942 | validation: 0.05878899292332677]
	TIME [epoch: 5.8 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01956164770076375		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.01956164770076375 | validation: 0.05359957423360895]
	TIME [epoch: 5.77 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017046719896744607		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.017046719896744607 | validation: 0.054028385085734]
	TIME [epoch: 5.77 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018627662752777323		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.018627662752777323 | validation: 0.05271878022754522]
	TIME [epoch: 5.77 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018526361112333137		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.018526361112333137 | validation: 0.05215984938555836]
	TIME [epoch: 5.77 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01901303744141573		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.01901303744141573 | validation: 0.054968534053678755]
	TIME [epoch: 5.78 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017614984522683695		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.017614984522683695 | validation: 0.05166495463363821]
	TIME [epoch: 5.77 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018792275803210336		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.018792275803210336 | validation: 0.05652482454398741]
	TIME [epoch: 5.77 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018598481581564734		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.018598481581564734 | validation: 0.05673985043532326]
	TIME [epoch: 5.77 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018472770939943105		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.018472770939943105 | validation: 0.05902691355826115]
	TIME [epoch: 5.78 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018225322705310642		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.018225322705310642 | validation: 0.05638853800207621]
	TIME [epoch: 5.77 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01853063531906587		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.01853063531906587 | validation: 0.05267015321324754]
	TIME [epoch: 5.78 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01790167601204162		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.01790167601204162 | validation: 0.05647030250002816]
	TIME [epoch: 5.77 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018483294628579023		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.018483294628579023 | validation: 0.05356491373540282]
	TIME [epoch: 5.78 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018731078171670107		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.018731078171670107 | validation: 0.05541667532777105]
	TIME [epoch: 5.78 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01863817896486006		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.01863817896486006 | validation: 0.05508742831220657]
	TIME [epoch: 5.78 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018427391278428133		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.018427391278428133 | validation: 0.053526053234702775]
	TIME [epoch: 5.77 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01779372598348059		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.01779372598348059 | validation: 0.05294845164430193]
	TIME [epoch: 5.78 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01890370713684197		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.01890370713684197 | validation: 0.05135818783186481]
	TIME [epoch: 5.77 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018866720363455613		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.018866720363455613 | validation: 0.051321536950333806]
	TIME [epoch: 5.77 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01968291161565231		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.01968291161565231 | validation: 0.05670956537908908]
	TIME [epoch: 5.77 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018320797179332736		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.018320797179332736 | validation: 0.05749335584168322]
	TIME [epoch: 5.77 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017918335370189786		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.017918335370189786 | validation: 0.053678723445208444]
	TIME [epoch: 5.77 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018787347451337755		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.018787347451337755 | validation: 0.048284727523771664]
	TIME [epoch: 5.78 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017597135244940786		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.017597135244940786 | validation: 0.05607279842907928]
	TIME [epoch: 5.77 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01855510478831489		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.01855510478831489 | validation: 0.05488626420513773]
	TIME [epoch: 5.78 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019476864302850776		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.019476864302850776 | validation: 0.060235411037994895]
	TIME [epoch: 5.77 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01861524538911793		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.01861524538911793 | validation: 0.055600948759100155]
	TIME [epoch: 5.77 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01757815389076757		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.01757815389076757 | validation: 0.053797095635814356]
	TIME [epoch: 5.78 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018747315057494964		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.018747315057494964 | validation: 0.05221742903007309]
	TIME [epoch: 5.77 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017851608354330143		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.017851608354330143 | validation: 0.05133484311593398]
	TIME [epoch: 5.78 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018159504241360064		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.018159504241360064 | validation: 0.05491215176876743]
	TIME [epoch: 5.78 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017074746118544745		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.017074746118544745 | validation: 0.05449363616613903]
	TIME [epoch: 5.77 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01786319291373491		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.01786319291373491 | validation: 0.05215358739272128]
	TIME [epoch: 5.77 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01827472016649305		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.01827472016649305 | validation: 0.05048906610409918]
	TIME [epoch: 5.78 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01813360297008903		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.01813360297008903 | validation: 0.05531256595137645]
	TIME [epoch: 5.77 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018131531083451505		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.018131531083451505 | validation: 0.058804370902988234]
	TIME [epoch: 5.78 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01898200250891844		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.01898200250891844 | validation: 0.05287324802001743]
	TIME [epoch: 5.78 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018262117231683275		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.018262117231683275 | validation: 0.05520311033099017]
	TIME [epoch: 5.79 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017828809625966925		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.017828809625966925 | validation: 0.05209674207101857]
	TIME [epoch: 5.78 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018728067560491		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.018728067560491 | validation: 0.054136531656185476]
	TIME [epoch: 5.78 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017848096037067718		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.017848096037067718 | validation: 0.055891505076047336]
	TIME [epoch: 5.78 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018363127253254587		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.018363127253254587 | validation: 0.05573270288174862]
	TIME [epoch: 5.77 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01788400151005692		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.01788400151005692 | validation: 0.05454222759388555]
	TIME [epoch: 5.77 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01764076467430673		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.01764076467430673 | validation: 0.04768835275985457]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1542.pth
	Model improved!!!
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018664388871888506		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.018664388871888506 | validation: 0.05619067941425531]
	TIME [epoch: 5.77 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018742163522240833		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.018742163522240833 | validation: 0.05564091133397967]
	TIME [epoch: 5.77 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018168267486236664		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.018168267486236664 | validation: 0.05347536086909125]
	TIME [epoch: 5.77 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017925189177547193		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.017925189177547193 | validation: 0.04811430165117057]
	TIME [epoch: 5.78 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018793192063376214		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.018793192063376214 | validation: 0.05406772059711994]
	TIME [epoch: 5.77 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017609173373186182		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.017609173373186182 | validation: 0.053041755262960025]
	TIME [epoch: 5.78 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01805062568870749		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.01805062568870749 | validation: 0.0509138643109069]
	TIME [epoch: 5.77 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01852184457258778		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.01852184457258778 | validation: 0.04756346909913858]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1550.pth
	Model improved!!!
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018238261390302463		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.018238261390302463 | validation: 0.04962600870136816]
	TIME [epoch: 5.78 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019578808503233593		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.019578808503233593 | validation: 0.05491769944692441]
	TIME [epoch: 5.79 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018214406916262662		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.018214406916262662 | validation: 0.05349495949789412]
	TIME [epoch: 5.78 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01865033641898422		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.01865033641898422 | validation: 0.04570567587810577]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1554.pth
	Model improved!!!
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0183442707200873		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.0183442707200873 | validation: 0.049359756409616144]
	TIME [epoch: 5.78 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01720770368724935		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.01720770368724935 | validation: 0.05377584458276538]
	TIME [epoch: 5.78 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018290487780674373		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.018290487780674373 | validation: 0.046086850883070254]
	TIME [epoch: 5.78 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017848269683856822		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.017848269683856822 | validation: 0.053335162928557237]
	TIME [epoch: 5.78 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017575982937223185		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.017575982937223185 | validation: 0.04890097814353672]
	TIME [epoch: 5.78 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01881400637602375		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.01881400637602375 | validation: 0.05419347187306361]
	TIME [epoch: 5.78 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018092615857587813		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.018092615857587813 | validation: 0.05310188716299803]
	TIME [epoch: 5.77 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017957096432145554		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.017957096432145554 | validation: 0.05261046803521552]
	TIME [epoch: 5.78 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01729585320844711		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.01729585320844711 | validation: 0.052055970380738795]
	TIME [epoch: 5.78 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018855714731211465		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.018855714731211465 | validation: 0.05125459329148461]
	TIME [epoch: 5.78 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017982306317879886		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.017982306317879886 | validation: 0.05354930965960611]
	TIME [epoch: 5.78 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01786126493505413		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.01786126493505413 | validation: 0.054272083871165805]
	TIME [epoch: 5.78 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0181476347505858		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.0181476347505858 | validation: 0.05040550153731223]
	TIME [epoch: 5.78 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01833813267807081		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.01833813267807081 | validation: 0.04929179229229696]
	TIME [epoch: 5.78 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01786309529642979		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.01786309529642979 | validation: 0.04948257463554541]
	TIME [epoch: 5.78 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017971336824519582		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.017971336824519582 | validation: 0.04756897906517598]
	TIME [epoch: 5.78 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017362513047098526		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.017362513047098526 | validation: 0.05313426078922243]
	TIME [epoch: 5.78 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01769823142330556		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.01769823142330556 | validation: 0.055289553100616964]
	TIME [epoch: 5.79 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0177347149721334		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.0177347149721334 | validation: 0.050935984064393314]
	TIME [epoch: 5.78 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017392676638796303		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.017392676638796303 | validation: 0.04734529422548739]
	TIME [epoch: 5.78 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01741299169965899		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.01741299169965899 | validation: 0.046236910021703465]
	TIME [epoch: 5.78 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016644791405229915		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.016644791405229915 | validation: 0.050055477617665545]
	TIME [epoch: 5.78 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01787904462728909		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.01787904462728909 | validation: 0.053954616680189076]
	TIME [epoch: 5.78 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01851100462795749		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.01851100462795749 | validation: 0.04915027826928786]
	TIME [epoch: 5.79 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01811356810341064		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.01811356810341064 | validation: 0.054440745340377544]
	TIME [epoch: 5.78 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017282975776648963		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.017282975776648963 | validation: 0.050094720934174754]
	TIME [epoch: 5.79 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018275964120056724		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.018275964120056724 | validation: 0.05081795695297642]
	TIME [epoch: 5.78 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01748248246838717		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.01748248246838717 | validation: 0.0549686422449148]
	TIME [epoch: 5.78 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01775183353476995		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.01775183353476995 | validation: 0.055812904119704236]
	TIME [epoch: 5.78 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017945741754266224		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.017945741754266224 | validation: 0.05016566061484368]
	TIME [epoch: 5.79 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01719676954200317		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.01719676954200317 | validation: 0.05070284105997702]
	TIME [epoch: 5.78 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017406303186286422		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.017406303186286422 | validation: 0.05240839358075436]
	TIME [epoch: 5.79 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018032013272556578		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.018032013272556578 | validation: 0.05140684601808463]
	TIME [epoch: 5.78 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01729834194985222		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.01729834194985222 | validation: 0.04918940209074895]
	TIME [epoch: 5.79 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017752800005738832		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.017752800005738832 | validation: 0.05389894426950821]
	TIME [epoch: 5.77 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016907682064341997		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.016907682064341997 | validation: 0.05031279911960961]
	TIME [epoch: 5.78 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018072535649652003		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.018072535649652003 | validation: 0.05135318415273652]
	TIME [epoch: 5.78 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017446488350857745		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.017446488350857745 | validation: 0.05331935927368922]
	TIME [epoch: 5.79 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01807970729678519		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.01807970729678519 | validation: 0.054510126109182426]
	TIME [epoch: 5.79 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017904027186946843		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.017904027186946843 | validation: 0.05437198169248463]
	TIME [epoch: 5.78 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01659883006667974		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.01659883006667974 | validation: 0.055260358862732534]
	TIME [epoch: 5.79 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01727680643963155		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.01727680643963155 | validation: 0.058053674902855235]
	TIME [epoch: 5.78 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018627590520279083		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.018627590520279083 | validation: 0.05283583225161456]
	TIME [epoch: 5.78 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016247769505513657		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.016247769505513657 | validation: 0.04780032421075047]
	TIME [epoch: 5.77 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017417940268312112		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.017417940268312112 | validation: 0.050724904778818795]
	TIME [epoch: 5.78 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017708089223652836		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.017708089223652836 | validation: 0.05079988538517364]
	TIME [epoch: 5.78 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017479999717131028		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.017479999717131028 | validation: 0.05029495943070755]
	TIME [epoch: 5.84 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01764779299107775		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.01764779299107775 | validation: 0.05525553772328756]
	TIME [epoch: 5.77 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017533238708727646		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.017533238708727646 | validation: 0.054364994367374644]
	TIME [epoch: 5.77 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017598203425868687		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.017598203425868687 | validation: 0.05069941850717827]
	TIME [epoch: 5.77 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017233293194125657		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.017233293194125657 | validation: 0.05907332678419991]
	TIME [epoch: 5.77 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017432346914213862		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.017432346914213862 | validation: 0.0554747197265022]
	TIME [epoch: 5.77 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017524843195795944		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.017524843195795944 | validation: 0.05422517539948734]
	TIME [epoch: 5.77 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016713668623737108		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.016713668623737108 | validation: 0.04968180988272421]
	TIME [epoch: 5.78 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017382115024934017		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.017382115024934017 | validation: 0.05579421597703769]
	TIME [epoch: 5.78 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016820372069703615		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.016820372069703615 | validation: 0.04934978861661156]
	TIME [epoch: 5.78 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018470889091530017		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.018470889091530017 | validation: 0.05099257487464492]
	TIME [epoch: 5.77 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01706627463411758		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.01706627463411758 | validation: 0.061255131812645795]
	TIME [epoch: 5.78 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016531250124437236		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.016531250124437236 | validation: 0.05562742432283531]
	TIME [epoch: 5.79 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017084379110645274		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.017084379110645274 | validation: 0.05576677957570018]
	TIME [epoch: 5.78 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017353389830059164		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.017353389830059164 | validation: 0.05555203955116529]
	TIME [epoch: 5.78 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016830375449505306		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.016830375449505306 | validation: 0.05167024948954239]
	TIME [epoch: 5.77 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017786554078886498		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.017786554078886498 | validation: 0.04791869975380808]
	TIME [epoch: 5.78 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0169895259604422		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.0169895259604422 | validation: 0.05311284284468646]
	TIME [epoch: 5.78 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016575270037238816		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.016575270037238816 | validation: 0.05131664165961809]
	TIME [epoch: 5.78 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017800737507908258		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.017800737507908258 | validation: 0.05667585957158643]
	TIME [epoch: 5.79 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017526062443204416		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.017526062443204416 | validation: 0.04822153039808927]
	TIME [epoch: 5.78 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016788922787728436		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.016788922787728436 | validation: 0.053173185504703714]
	TIME [epoch: 5.78 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017016688293137402		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.017016688293137402 | validation: 0.0550015677572328]
	TIME [epoch: 5.78 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017132719154346155		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.017132719154346155 | validation: 0.053214722361547895]
	TIME [epoch: 5.77 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016651975584820924		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.016651975584820924 | validation: 0.05279852258577026]
	TIME [epoch: 5.78 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016208130647364573		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.016208130647364573 | validation: 0.046789341891317796]
	TIME [epoch: 5.78 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017519003686251963		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.017519003686251963 | validation: 0.052014208856102175]
	TIME [epoch: 5.78 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01688644192025561		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.01688644192025561 | validation: 0.05219373940696053]
	TIME [epoch: 5.77 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018109414626443943		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.018109414626443943 | validation: 0.049219630554799436]
	TIME [epoch: 5.79 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01681210919728469		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.01681210919728469 | validation: 0.05259113959866927]
	TIME [epoch: 5.78 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016337867914796272		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.016337867914796272 | validation: 0.050412919693610295]
	TIME [epoch: 5.78 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016415276816151722		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.016415276816151722 | validation: 0.04777815937213681]
	TIME [epoch: 5.75 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017552663066994188		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.017552663066994188 | validation: 0.04849321858960997]
	TIME [epoch: 5.76 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017093869325623054		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.017093869325623054 | validation: 0.05442336365244212]
	TIME [epoch: 5.75 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01692394271347285		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.01692394271347285 | validation: 0.05414467534744867]
	TIME [epoch: 5.75 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016333752782436057		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.016333752782436057 | validation: 0.05638674935079522]
	TIME [epoch: 5.75 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016687463981392092		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.016687463981392092 | validation: 0.047465874416964426]
	TIME [epoch: 5.75 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016833384953753642		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.016833384953753642 | validation: 0.05732713093133482]
	TIME [epoch: 5.75 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016967654730970172		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.016967654730970172 | validation: 0.055129657247899255]
	TIME [epoch: 5.75 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01740863780205502		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.01740863780205502 | validation: 0.05408310317425261]
	TIME [epoch: 5.76 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017529877885880688		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.017529877885880688 | validation: 0.05457085587032667]
	TIME [epoch: 5.75 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01665058599950131		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.01665058599950131 | validation: 0.04581135945073266]
	TIME [epoch: 5.75 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01675443580924361		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.01675443580924361 | validation: 0.05229761752043828]
	TIME [epoch: 5.75 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017222721956980963		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.017222721956980963 | validation: 0.047669649370369875]
	TIME [epoch: 5.75 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016932161927537288		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.016932161927537288 | validation: 0.048866674732930285]
	TIME [epoch: 5.76 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01698203188005926		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.01698203188005926 | validation: 0.05051793729024842]
	TIME [epoch: 5.76 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015528217988766286		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.015528217988766286 | validation: 0.05330079682003928]
	TIME [epoch: 5.76 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01711510765326679		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.01711510765326679 | validation: 0.05108717021900865]
	TIME [epoch: 5.75 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016598956161719157		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.016598956161719157 | validation: 0.047426103923265996]
	TIME [epoch: 5.76 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015992314027521656		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.015992314027521656 | validation: 0.049088124917018465]
	TIME [epoch: 5.75 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016873355142362483		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.016873355142362483 | validation: 0.05454439284518284]
	TIME [epoch: 5.76 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016980232742491753		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.016980232742491753 | validation: 0.05246092318677478]
	TIME [epoch: 5.75 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01725782198360805		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.01725782198360805 | validation: 0.05374242638315943]
	TIME [epoch: 5.75 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01708140248423195		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.01708140248423195 | validation: 0.055573855189404886]
	TIME [epoch: 5.76 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01596447919454846		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.01596447919454846 | validation: 0.050142209268599726]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131526/states/model_phi1_4a_v_mmd1_1655.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6500.550 seconds.
