Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3458801221

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.3449803184609594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3449803184609594 | validation: 6.044787626430711]
	TIME [epoch: 161 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.741244146356007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.741244146356007 | validation: 6.173903385340807]
	TIME [epoch: 0.71 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.734395065377793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.734395065377793 | validation: 5.945920899403028]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.821832486541613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.821832486541613 | validation: 5.786101754233299]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.006421011900677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.006421011900677 | validation: 6.112483775043947]
	TIME [epoch: 0.684 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.313925896905903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.313925896905903 | validation: 5.9728266955725715]
	TIME [epoch: 0.688 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.033417214883375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033417214883375 | validation: 5.797700475545594]
	TIME [epoch: 0.689 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7764620409259546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7764620409259546 | validation: 5.731969348112894]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.633599347229755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.633599347229755 | validation: 5.75763168566969]
	TIME [epoch: 0.689 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6268138937980217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6268138937980217 | validation: 5.705286156158836]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6280577667318767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6280577667318767 | validation: 5.69061160785668]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6211846699983767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6211846699983767 | validation: 5.624563693765083]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5432967022163746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5432967022163746 | validation: 5.561603636769851]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.503849485720134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.503849485720134 | validation: 5.474854042771755]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.44020134256566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.44020134256566 | validation: 5.395602612317714]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3509592953468443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3509592953468443 | validation: 5.255742248297169]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.203961199350079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.203961199350079 | validation: 5.064753173545701]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1168785227874616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1168785227874616 | validation: 5.095881658694054]
	TIME [epoch: 0.689 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.741350038880462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.741350038880462 | validation: 4.83294129042941]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9125356786799474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9125356786799474 | validation: 4.711456314963782]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8343170022241257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8343170022241257 | validation: 4.473618795938789]
	TIME [epoch: 0.685 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8594099748350916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8594099748350916 | validation: 4.822785734612864]
	TIME [epoch: 0.684 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5141680267053017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5141680267053017 | validation: 4.115978824103983]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.069015100087762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.069015100087762 | validation: 3.851200654267677]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.75255541173784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.75255541173784 | validation: 3.9474892919513156]
	TIME [epoch: 0.688 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5181671305352054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5181671305352054 | validation: 3.2749000416041647]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2728367696623133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2728367696623133 | validation: 2.733358546327871]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9605945227292398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9605945227292398 | validation: 1.800847189021951]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6875716682274198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6875716682274198 | validation: 1.0480200907404262]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.682157483219861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.682157483219861 | validation: 1.7101600562282073]
	TIME [epoch: 0.688 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8575423937338504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8575423937338504 | validation: 0.9574567596123149]
	TIME [epoch: 0.682 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4564368570865966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4564368570865966 | validation: 1.3160376669574967]
	TIME [epoch: 0.689 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4072793407721695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4072793407721695 | validation: 1.802327215178397]
	TIME [epoch: 0.685 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4060245796652413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4060245796652413 | validation: 0.9924129170498143]
	TIME [epoch: 0.686 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2987325038739967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2987325038739967 | validation: 1.7081235443312395]
	TIME [epoch: 0.685 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3494620111090103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3494620111090103 | validation: 0.828468796182381]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2303283061661012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2303283061661012 | validation: 1.141582070445643]
	TIME [epoch: 0.687 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.135722681881041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.135722681881041 | validation: 0.9875599877073583]
	TIME [epoch: 0.687 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108794785550813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1108794785550813 | validation: 1.0365653890604642]
	TIME [epoch: 0.688 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0419468939873477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0419468939873477 | validation: 0.8527810318390934]
	TIME [epoch: 0.689 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038830245970088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.038830245970088 | validation: 1.4406212473883155]
	TIME [epoch: 0.687 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1511282806577088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1511282806577088 | validation: 0.9584566030226189]
	TIME [epoch: 0.69 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4034197359069684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4034197359069684 | validation: 1.3212661284656424]
	TIME [epoch: 0.688 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.12802518973943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.12802518973943 | validation: 0.7738722511854231]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0366308621480753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0366308621480753 | validation: 0.8835180898855595]
	TIME [epoch: 0.685 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.97741651559805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.97741651559805 | validation: 1.1759656071165583]
	TIME [epoch: 0.687 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9950543800640316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9950543800640316 | validation: 0.6984994982841467]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0540460042590163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0540460042590163 | validation: 1.3344118832642022]
	TIME [epoch: 0.693 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048166821823274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.048166821823274 | validation: 0.6877405610582326]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586303643614883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0586303643614883 | validation: 1.0280508013774217]
	TIME [epoch: 0.693 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9544790559737362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9544790559737362 | validation: 0.783502980406932]
	TIME [epoch: 0.69 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9217888562264739		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.9217888562264739 | validation: 1.0298707241503242]
	TIME [epoch: 0.688 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9164814758180276		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9164814758180276 | validation: 0.6781685411059934]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542073584836896		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.9542073584836896 | validation: 1.398791842263413]
	TIME [epoch: 0.69 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0692406550280975		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0692406550280975 | validation: 0.8115154932816261]
	TIME [epoch: 0.691 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4139778108220857		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.4139778108220857 | validation: 0.9111687647880997]
	TIME [epoch: 0.687 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014150443367308		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9014150443367308 | validation: 1.2336451924340872]
	TIME [epoch: 0.688 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9990597387727752		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.9990597387727752 | validation: 0.662888641521199]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1746668102282636		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.1746668102282636 | validation: 0.7885268821608391]
	TIME [epoch: 0.689 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881803287827466		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.881803287827466 | validation: 1.2537675580506447]
	TIME [epoch: 0.684 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0642772791451103		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.0642772791451103 | validation: 0.6062690816428795]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.093446333680908		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.093446333680908 | validation: 0.6687771081524427]
	TIME [epoch: 0.688 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9061394729661203		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9061394729661203 | validation: 1.1389118385724246]
	TIME [epoch: 0.689 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9696580672998323		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9696580672998323 | validation: 0.6451733007990704]
	TIME [epoch: 0.685 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9020080492711725		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9020080492711725 | validation: 0.7834553679120868]
	TIME [epoch: 0.684 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687815074404316		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.8687815074404316 | validation: 0.9417040072327526]
	TIME [epoch: 0.687 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717017549994981		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.8717017549994981 | validation: 0.7192352010389365]
	TIME [epoch: 0.689 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576673043577876		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.8576673043577876 | validation: 0.8467285021911217]
	TIME [epoch: 0.687 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364168781591158		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.8364168781591158 | validation: 0.7867830700015859]
	TIME [epoch: 0.684 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276244689941848		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8276244689941848 | validation: 0.7834467399927569]
	TIME [epoch: 0.687 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336353298889438		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.8336353298889438 | validation: 0.7989915148007699]
	TIME [epoch: 0.684 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468259634420869		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8468259634420869 | validation: 0.8579252706343082]
	TIME [epoch: 0.683 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502029832273698		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.8502029832273698 | validation: 0.7396882387451185]
	TIME [epoch: 0.688 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8475748212132074		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.8475748212132074 | validation: 0.8831146969352028]
	TIME [epoch: 0.684 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591406180526633		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8591406180526633 | validation: 0.6941370507461483]
	TIME [epoch: 0.687 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560295284270225		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.8560295284270225 | validation: 0.8941011489019497]
	TIME [epoch: 0.687 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345815439641046		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8345815439641046 | validation: 0.6052101194315913]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864425277616239		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.864425277616239 | validation: 1.2050227491981742]
	TIME [epoch: 0.692 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.140691632928045		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.140691632928045 | validation: 0.6462722727342656]
	TIME [epoch: 0.688 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2893507151560644		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.2893507151560644 | validation: 0.7544581424542269]
	TIME [epoch: 0.693 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.892322784603073		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.892322784603073 | validation: 1.1008988625509228]
	TIME [epoch: 0.69 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.04588761281286		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.04588761281286 | validation: 0.6286644967595163]
	TIME [epoch: 0.688 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691611395505743		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.8691611395505743 | validation: 0.7359189167785319]
	TIME [epoch: 0.692 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278744793629826		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8278744793629826 | validation: 0.7727747762308714]
	TIME [epoch: 0.689 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961372988030256		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.7961372988030256 | validation: 0.6720913466269208]
	TIME [epoch: 0.689 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7956400869532372		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.7956400869532372 | validation: 0.801784195921786]
	TIME [epoch: 0.687 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960298596014045		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.7960298596014045 | validation: 0.6942021538311199]
	TIME [epoch: 0.686 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784470115508699		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7784470115508699 | validation: 0.7254778812626725]
	TIME [epoch: 0.685 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678754922736137		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.7678754922736137 | validation: 0.7009637868910157]
	TIME [epoch: 0.687 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577168078249449		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.7577168078249449 | validation: 0.6905522912080722]
	TIME [epoch: 0.686 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614304384559256		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.7614304384559256 | validation: 0.7173438315061853]
	TIME [epoch: 0.688 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605069894953854		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.7605069894953854 | validation: 0.6652800712927619]
	TIME [epoch: 0.688 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605872743144824		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.7605872743144824 | validation: 0.7832116282943119]
	TIME [epoch: 0.686 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827058206900404		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.7827058206900404 | validation: 0.5949578960920382]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9581887043082193		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.9581887043082193 | validation: 1.0473023648639441]
	TIME [epoch: 0.683 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9999461795558793		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9999461795558793 | validation: 0.5887898134051823]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424308103416366		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.9424308103416366 | validation: 0.8084510227383703]
	TIME [epoch: 0.691 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739882047460149		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.7739882047460149 | validation: 0.7478411421222647]
	TIME [epoch: 0.693 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854727824724002		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.7854727824724002 | validation: 0.5962259463377011]
	TIME [epoch: 0.69 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7905413015289016		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.7905413015289016 | validation: 0.7275835859068689]
	TIME [epoch: 0.688 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379532737054786		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7379532737054786 | validation: 0.6566603208358871]
	TIME [epoch: 0.688 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284506829096872		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7284506829096872 | validation: 0.6414736024665881]
	TIME [epoch: 0.685 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150535396665093		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7150535396665093 | validation: 0.6703812216797318]
	TIME [epoch: 0.686 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117096482356948		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7117096482356948 | validation: 0.64079543266208]
	TIME [epoch: 0.688 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7288294531728892		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7288294531728892 | validation: 0.6925661780300243]
	TIME [epoch: 0.694 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904670105851068		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7904670105851068 | validation: 0.7241060823815086]
	TIME [epoch: 0.686 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228010772024885		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8228010772024885 | validation: 0.6708893897367179]
	TIME [epoch: 0.689 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758061738857613		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.758061738857613 | validation: 0.6534303519333235]
	TIME [epoch: 0.687 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982050695274685		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.6982050695274685 | validation: 0.6059670483029587]
	TIME [epoch: 0.691 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816508159658891		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.6816508159658891 | validation: 0.6078305588654658]
	TIME [epoch: 0.687 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678965137441894		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.678965137441894 | validation: 0.5628593568317467]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6802631941868333		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.6802631941868333 | validation: 0.7358956332971356]
	TIME [epoch: 0.689 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7115443795578011		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7115443795578011 | validation: 0.48137188535533587]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754489025225252		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7754489025225252 | validation: 0.8882328307792653]
	TIME [epoch: 0.69 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8719301665372179		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8719301665372179 | validation: 0.532271602745497]
	TIME [epoch: 0.689 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433413405897014		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7433413405897014 | validation: 0.5879372295135389]
	TIME [epoch: 0.69 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6660441928113283		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.6660441928113283 | validation: 0.6145796265607204]
	TIME [epoch: 0.691 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.662425997473881		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.662425997473881 | validation: 0.5338279556168112]
	TIME [epoch: 0.688 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708415572183529		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.6708415572183529 | validation: 0.5941205662443968]
	TIME [epoch: 0.689 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533498372852284		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.6533498372852284 | validation: 0.520968442783235]
	TIME [epoch: 0.691 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6508970139867427		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.6508970139867427 | validation: 0.6340173092403146]
	TIME [epoch: 0.692 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710492011938977		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.6710492011938977 | validation: 0.5631885988126094]
	TIME [epoch: 0.689 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222094080964527		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7222094080964527 | validation: 0.6927016632878169]
	TIME [epoch: 0.703 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744311400548349		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.744311400548349 | validation: 0.5676403357634507]
	TIME [epoch: 0.692 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7090130016578414		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.7090130016578414 | validation: 0.5709971785309442]
	TIME [epoch: 0.692 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.689940624914507		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.689940624914507 | validation: 0.5748906971309747]
	TIME [epoch: 0.692 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819649463731527		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.6819649463731527 | validation: 0.5696605152161979]
	TIME [epoch: 0.689 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434465577515533		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.6434465577515533 | validation: 0.5397700411255948]
	TIME [epoch: 0.691 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399069923789219		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.6399069923789219 | validation: 0.5324612736191962]
	TIME [epoch: 0.692 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229010002084239		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.6229010002084239 | validation: 0.5432659011626205]
	TIME [epoch: 0.69 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269854031007484		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.6269854031007484 | validation: 0.5567464954172242]
	TIME [epoch: 0.692 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403308922630931		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.6403308922630931 | validation: 0.5382111175015214]
	TIME [epoch: 0.689 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912331197952907		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.6912331197952907 | validation: 0.6283328620167644]
	TIME [epoch: 0.692 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6862160051951264		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.6862160051951264 | validation: 0.5379616929577292]
	TIME [epoch: 0.693 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453854678196865		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6453854678196865 | validation: 0.5341984396749606]
	TIME [epoch: 0.689 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6056370311526614		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.6056370311526614 | validation: 0.49341187682460785]
	TIME [epoch: 0.692 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6034649808261747		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.6034649808261747 | validation: 0.5490058500624108]
	TIME [epoch: 0.69 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5918884687901407		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.5918884687901407 | validation: 0.4469447463171946]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941496490716539		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.5941496490716539 | validation: 0.656278957101]
	TIME [epoch: 0.692 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612264041097482		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.612264041097482 | validation: 0.50597820927593]
	TIME [epoch: 0.685 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594607144954869		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.6594607144954869 | validation: 0.5247335290470326]
	TIME [epoch: 0.688 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133687875758482		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6133687875758482 | validation: 0.5610684997229417]
	TIME [epoch: 0.686 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965501739586089		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.5965501739586089 | validation: 0.451801170730864]
	TIME [epoch: 0.688 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.557323888484412		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.557323888484412 | validation: 0.5826072881197863]
	TIME [epoch: 0.685 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689863100427397		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.5689863100427397 | validation: 0.47339298715298206]
	TIME [epoch: 0.685 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6186522925159281		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.6186522925159281 | validation: 0.5869956120781831]
	TIME [epoch: 0.686 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.580158056847137		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.580158056847137 | validation: 0.4344388123760132]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815195830065892		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.5815195830065892 | validation: 0.4710203340867892]
	TIME [epoch: 0.69 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5158990337365134		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.5158990337365134 | validation: 0.4926176422272816]
	TIME [epoch: 0.689 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5009127768413885		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.5009127768413885 | validation: 0.4209104574736358]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4825044550146147		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.4825044550146147 | validation: 0.5881962325796489]
	TIME [epoch: 0.692 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5501288748903113		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.5501288748903113 | validation: 0.5347247435065393]
	TIME [epoch: 0.689 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7235092199050419		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7235092199050419 | validation: 0.472095621999468]
	TIME [epoch: 0.692 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037280657752138		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.6037280657752138 | validation: 0.562072744719074]
	TIME [epoch: 0.691 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523703996251266		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.5523703996251266 | validation: 0.4267668831942413]
	TIME [epoch: 0.69 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5297242843536576		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.5297242843536576 | validation: 0.5430466088123341]
	TIME [epoch: 0.689 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019406778673776		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.5019406778673776 | validation: 0.4084106201960374]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4557436131356684		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.4557436131356684 | validation: 0.39489451374955564]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_158.pth
	Model improved!!!
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4314409439355501		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.4314409439355501 | validation: 0.4593073868675247]
	TIME [epoch: 0.687 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43848142504355225		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.43848142504355225 | validation: 0.3777569166804031]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49483292537841045		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.49483292537841045 | validation: 0.6348788544389121]
	TIME [epoch: 0.689 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855176864651672		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5855176864651672 | validation: 0.44181733497542997]
	TIME [epoch: 0.692 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5363198411345332		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.5363198411345332 | validation: 0.35946950638405795]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41247615187220976		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.41247615187220976 | validation: 0.5326636757770132]
	TIME [epoch: 0.687 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4772086476055853		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.4772086476055853 | validation: 0.41145802646347]
	TIME [epoch: 0.687 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4941598383813321		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.4941598383813321 | validation: 0.4212313148870158]
	TIME [epoch: 0.692 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42557305029625453		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.42557305029625453 | validation: 0.4184517052745232]
	TIME [epoch: 0.686 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3803289019236466		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.3803289019236466 | validation: 0.3511185518957235]
	TIME [epoch: 0.684 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698919467528542		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.3698919467528542 | validation: 0.4327403024803793]
	TIME [epoch: 0.685 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886668488306593		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.3886668488306593 | validation: 0.3685039797698524]
	TIME [epoch: 0.688 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46469807026810883		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.46469807026810883 | validation: 0.46301898929415836]
	TIME [epoch: 0.686 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42781888575893845		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.42781888575893845 | validation: 0.41361489656218037]
	TIME [epoch: 0.686 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4391271902771301		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.4391271902771301 | validation: 0.3889035289812779]
	TIME [epoch: 0.688 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3493158000786582		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.3493158000786582 | validation: 0.3619262561243982]
	TIME [epoch: 0.686 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33767417691124385		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.33767417691124385 | validation: 0.4737192719901775]
	TIME [epoch: 0.685 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3565309509724836		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.3565309509724836 | validation: 0.3248794686250879]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37951582170243703		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.37951582170243703 | validation: 0.43089868584145413]
	TIME [epoch: 0.686 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.366043190308933		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.366043190308933 | validation: 0.39705801769423843]
	TIME [epoch: 0.687 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920984708023977		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.3920984708023977 | validation: 0.33148065429257556]
	TIME [epoch: 0.686 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36760857797230745		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.36760857797230745 | validation: 0.4550706964429792]
	TIME [epoch: 0.687 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33569412401433524		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.33569412401433524 | validation: 0.316644571868195]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30922010741410183		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.30922010741410183 | validation: 0.36062325419555497]
	TIME [epoch: 0.687 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706914357399663		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.2706914357399663 | validation: 0.31454170114217705]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25878953151263057		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.25878953151263057 | validation: 0.344231834808746]
	TIME [epoch: 0.689 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2591243688734364		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.2591243688734364 | validation: 0.3241084947176914]
	TIME [epoch: 0.686 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3744888459320585		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.3744888459320585 | validation: 0.7087183257426197]
	TIME [epoch: 0.687 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625119972430694		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6625119972430694 | validation: 0.4057201083215891]
	TIME [epoch: 0.686 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30796598799286273		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.30796598799286273 | validation: 0.3083638931727093]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3559145849849824		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.3559145849849824 | validation: 0.5095851096887486]
	TIME [epoch: 0.684 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3589903203840381		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.3589903203840381 | validation: 0.4080330309173077]
	TIME [epoch: 0.683 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32387698352458827		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.32387698352458827 | validation: 0.3158818574244543]
	TIME [epoch: 0.685 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28205592070426627		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.28205592070426627 | validation: 0.40888343698469615]
	TIME [epoch: 0.686 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714214112871267		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.2714214112871267 | validation: 0.30154224355659637]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2690218264641679		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.2690218264641679 | validation: 0.5429431304027292]
	TIME [epoch: 0.686 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4278123085020495		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.4278123085020495 | validation: 0.43497907896915966]
	TIME [epoch: 0.686 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3396623865946091		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.3396623865946091 | validation: 0.2762306477155289]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28294901224132935		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.28294901224132935 | validation: 0.3517232824542971]
	TIME [epoch: 0.684 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23458997557838268		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.23458997557838268 | validation: 0.32364510961243764]
	TIME [epoch: 0.682 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22422377559586842		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.22422377559586842 | validation: 0.2783343774456774]
	TIME [epoch: 0.687 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22156486052016505		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.22156486052016505 | validation: 0.31557295136017843]
	TIME [epoch: 0.686 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21860123556823038		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.21860123556823038 | validation: 0.3997960582918696]
	TIME [epoch: 176 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3614808360595299		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.3614808360595299 | validation: 0.41412303112647675]
	TIME [epoch: 1.36 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581195965291675		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.5581195965291675 | validation: 0.3796705534412639]
	TIME [epoch: 1.35 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3311945118879045		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.3311945118879045 | validation: 0.6194864573036603]
	TIME [epoch: 1.35 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5348764898226885		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5348764898226885 | validation: 0.335160642515652]
	TIME [epoch: 1.35 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26024667487111386		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.26024667487111386 | validation: 0.4016224309739316]
	TIME [epoch: 1.35 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3942929475989603		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.3942929475989603 | validation: 0.47266403340110996]
	TIME [epoch: 1.35 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3234794316811688		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.3234794316811688 | validation: 0.3654756947152505]
	TIME [epoch: 1.35 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24919331892470856		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.24919331892470856 | validation: 0.3294968587529975]
	TIME [epoch: 1.35 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27389413722455525		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.27389413722455525 | validation: 0.39254092829750864]
	TIME [epoch: 1.35 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2536806656118544		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.2536806656118544 | validation: 0.3165039023220706]
	TIME [epoch: 1.35 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22258307439803132		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.22258307439803132 | validation: 0.30449331271800206]
	TIME [epoch: 1.34 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22666985675817827		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.22666985675817827 | validation: 0.34272382775950716]
	TIME [epoch: 1.35 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23072164025896486		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.23072164025896486 | validation: 0.29392937747055353]
	TIME [epoch: 1.34 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2600834203440052		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.2600834203440052 | validation: 0.40019502689488573]
	TIME [epoch: 1.35 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29056298426927596		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.29056298426927596 | validation: 0.3106205387769818]
	TIME [epoch: 1.34 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533918235159877		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.2533918235159877 | validation: 0.31226590213370436]
	TIME [epoch: 1.35 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24468733743295987		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.24468733743295987 | validation: 0.4011625313013971]
	TIME [epoch: 1.35 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568727291891999		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.2568727291891999 | validation: 0.2717918923912401]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25385636848878557		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.25385636848878557 | validation: 0.39989689354804475]
	TIME [epoch: 1.35 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26396877882304215		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.26396877882304215 | validation: 0.27435502326981714]
	TIME [epoch: 1.35 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22026847336758998		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.22026847336758998 | validation: 0.3287232844294434]
	TIME [epoch: 1.35 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2030370002279278		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.2030370002279278 | validation: 0.32223334261070263]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2024137762209778		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.2024137762209778 | validation: 0.27805161590827493]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2373285767652437		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.2373285767652437 | validation: 0.3442386172823707]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24261388909944268		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.24261388909944268 | validation: 0.3225376817059578]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2947792829646151		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.2947792829646151 | validation: 0.3079269402333093]
	TIME [epoch: 1.34 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22773724354854427		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.22773724354854427 | validation: 0.27893919285693397]
	TIME [epoch: 1.35 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17997256471533413		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.17997256471533413 | validation: 0.27809305817913]
	TIME [epoch: 1.35 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17711302871337117		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.17711302871337117 | validation: 0.2869982513952811]
	TIME [epoch: 1.35 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1926637678663958		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.1926637678663958 | validation: 0.436030180735108]
	TIME [epoch: 1.35 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29275469112092223		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.29275469112092223 | validation: 0.2910862973389851]
	TIME [epoch: 1.35 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2627845610330895		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.2627845610330895 | validation: 0.35680629279893417]
	TIME [epoch: 1.35 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3916580617508443		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.3916580617508443 | validation: 0.27754223685214846]
	TIME [epoch: 1.34 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23186669980232494		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.23186669980232494 | validation: 0.3751579994647286]
	TIME [epoch: 1.35 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26378096545416585		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.26378096545416585 | validation: 0.3003267320489239]
	TIME [epoch: 1.35 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21130522770291763		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.21130522770291763 | validation: 0.2911777115584693]
	TIME [epoch: 1.35 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1984675064204853		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.1984675064204853 | validation: 0.2711068894675037]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17995880830496574		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.17995880830496574 | validation: 0.26943685561335046]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20082067305502135		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.20082067305502135 | validation: 0.42504020476445137]
	TIME [epoch: 1.35 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24733274926627144		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.24733274926627144 | validation: 0.2709923321453353]
	TIME [epoch: 1.35 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18279439596720154		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.18279439596720154 | validation: 0.2714639168002225]
	TIME [epoch: 1.35 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17854189467699472		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.17854189467699472 | validation: 0.2805559586129675]
	TIME [epoch: 1.35 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20804816387004274		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.20804816387004274 | validation: 0.38530173061204787]
	TIME [epoch: 1.35 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25835581968093185		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.25835581968093185 | validation: 0.31896953471486256]
	TIME [epoch: 1.35 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21076840757961043		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.21076840757961043 | validation: 0.24700020874564013]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21501327886832278		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.21501327886832278 | validation: 0.32641654276146137]
	TIME [epoch: 1.35 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20034480683965739		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.20034480683965739 | validation: 0.30166417647801896]
	TIME [epoch: 1.34 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24683844417392245		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.24683844417392245 | validation: 0.26025014698964605]
	TIME [epoch: 1.34 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17792949641567282		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.17792949641567282 | validation: 0.3061821648601717]
	TIME [epoch: 1.35 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1762123772589883		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.1762123772589883 | validation: 0.222653113988645]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18961187001571997		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.18961187001571997 | validation: 0.4075712199589383]
	TIME [epoch: 1.34 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23146619633322932		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.23146619633322932 | validation: 0.24925948239208628]
	TIME [epoch: 1.34 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18607157174907948		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.18607157174907948 | validation: 0.2537560400565191]
	TIME [epoch: 1.34 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15780793317251224		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.15780793317251224 | validation: 0.2671398551308461]
	TIME [epoch: 1.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15769976396949306		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.15769976396949306 | validation: 0.4075845511978843]
	TIME [epoch: 1.34 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27948662414696257		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.27948662414696257 | validation: 0.33662046535041645]
	TIME [epoch: 1.35 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19384022745525162		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.19384022745525162 | validation: 0.2128539937271012]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20032331973512613		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.20032331973512613 | validation: 0.3830653497598636]
	TIME [epoch: 1.35 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19359227940784735		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.19359227940784735 | validation: 0.28117781132679504]
	TIME [epoch: 1.35 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22196655640109023		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.22196655640109023 | validation: 0.2947254012844799]
	TIME [epoch: 1.35 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22032575410324298		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.22032575410324298 | validation: 0.28174183403774455]
	TIME [epoch: 1.35 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1962190686635425		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.1962190686635425 | validation: 0.24497516340848147]
	TIME [epoch: 1.35 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17348281711280217		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.17348281711280217 | validation: 0.3049577840079699]
	TIME [epoch: 1.35 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19111681411056775		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.19111681411056775 | validation: 0.2708833808694206]
	TIME [epoch: 1.35 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17899117000158632		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.17899117000158632 | validation: 0.26582575486866994]
	TIME [epoch: 1.35 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15723056568227484		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.15723056568227484 | validation: 0.24448773349687408]
	TIME [epoch: 1.34 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17356993761287245		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.17356993761287245 | validation: 0.30367194496970457]
	TIME [epoch: 1.35 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20475821499508942		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.20475821499508942 | validation: 0.23789320597910277]
	TIME [epoch: 1.35 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15397960277039235		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.15397960277039235 | validation: 0.23072878188670487]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1383681792344165		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.1383681792344165 | validation: 0.23518334629042625]
	TIME [epoch: 1.34 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13894195182243077		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.13894195182243077 | validation: 0.24841647489115548]
	TIME [epoch: 1.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16362161905990807		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.16362161905990807 | validation: 0.25989570906022225]
	TIME [epoch: 1.35 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17797068176278735		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.17797068176278735 | validation: 0.2452577288722888]
	TIME [epoch: 1.34 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21100224699747816		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.21100224699747816 | validation: 0.2829910886303222]
	TIME [epoch: 1.34 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.179472715200673		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.179472715200673 | validation: 0.25189393380031766]
	TIME [epoch: 1.34 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659799755929396		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.1659799755929396 | validation: 0.24992734625223836]
	TIME [epoch: 1.34 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17018840613106542		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.17018840613106542 | validation: 0.2422251638691385]
	TIME [epoch: 1.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18296774399880775		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.18296774399880775 | validation: 0.24472715759484276]
	TIME [epoch: 1.34 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15958813813253114		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.15958813813253114 | validation: 0.2446283482504565]
	TIME [epoch: 1.34 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135136062101089		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.135136062101089 | validation: 0.2218600681368073]
	TIME [epoch: 1.35 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13572294492951786		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.13572294492951786 | validation: 0.22217225619522032]
	TIME [epoch: 1.34 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14019803427424773		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.14019803427424773 | validation: 0.22232089863824625]
	TIME [epoch: 1.35 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16873852323042088		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.16873852323042088 | validation: 0.29774430698058335]
	TIME [epoch: 1.34 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22746455605508772		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.22746455605508772 | validation: 0.2046317931769881]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1758801332552496		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.1758801332552496 | validation: 0.22556781364835155]
	TIME [epoch: 1.34 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12765422569536747		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.12765422569536747 | validation: 0.1847578184584479]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12334944960312812		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.12334944960312812 | validation: 0.23032938107433024]
	TIME [epoch: 1.34 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12075045233816965		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.12075045233816965 | validation: 0.18679811654086134]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11967688177619662		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.11967688177619662 | validation: 0.20442597235329513]
	TIME [epoch: 1.34 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15165195952693675		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.15165195952693675 | validation: 0.3275485118436441]
	TIME [epoch: 1.34 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23773467183916594		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.23773467183916594 | validation: 0.2323802579297245]
	TIME [epoch: 1.34 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17442395054550366		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.17442395054550366 | validation: 0.24426774323135964]
	TIME [epoch: 1.35 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14426308663713291		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.14426308663713291 | validation: 0.20769154495219155]
	TIME [epoch: 1.34 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315704726049032		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.1315704726049032 | validation: 0.1940979328732092]
	TIME [epoch: 1.35 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12149613362149074		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.12149613362149074 | validation: 0.31529479394962073]
	TIME [epoch: 1.34 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3052266398161305		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.3052266398161305 | validation: 0.33033166291894694]
	TIME [epoch: 1.34 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24607050626473947		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.24607050626473947 | validation: 0.1803085989474365]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16503208793727214		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.16503208793727214 | validation: 0.16065026247779055]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17371513610543154		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.17371513610543154 | validation: 0.3121965879488009]
	TIME [epoch: 1.35 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2135272746865431		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.2135272746865431 | validation: 0.22063141113503876]
	TIME [epoch: 1.35 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13696431759357525		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.13696431759357525 | validation: 0.1817205249425328]
	TIME [epoch: 1.35 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11515859647487499		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.11515859647487499 | validation: 0.23844083797364993]
	TIME [epoch: 1.35 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12152210806431452		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.12152210806431452 | validation: 0.19995525294399366]
	TIME [epoch: 1.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12802593644995816		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.12802593644995816 | validation: 0.22367260963616387]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452467455285511		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.1452467455285511 | validation: 0.2289804826819194]
	TIME [epoch: 1.34 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16413003538742707		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.16413003538742707 | validation: 0.21164920246961075]
	TIME [epoch: 1.35 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128621667200262		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.1128621667200262 | validation: 0.19712504235073366]
	TIME [epoch: 1.34 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10713814616512046		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.10713814616512046 | validation: 0.1971404147498131]
	TIME [epoch: 1.34 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10907303875530674		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.10907303875530674 | validation: 0.22078720577793823]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13669226613754717		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.13669226613754717 | validation: 0.1967528824578178]
	TIME [epoch: 1.35 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22025811373899043		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.22025811373899043 | validation: 0.2139454230081812]
	TIME [epoch: 1.34 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13333736236319502		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.13333736236319502 | validation: 0.16076744207704244]
	TIME [epoch: 1.35 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10205050461316331		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.10205050461316331 | validation: 0.20053556441720746]
	TIME [epoch: 1.34 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241320965560351		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.10241320965560351 | validation: 0.14263519827831983]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11750364037351378		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.11750364037351378 | validation: 0.2568896323589523]
	TIME [epoch: 1.34 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13374153203791223		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.13374153203791223 | validation: 0.17329891565489097]
	TIME [epoch: 1.35 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18515658525982093		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.18515658525982093 | validation: 0.2237964656144268]
	TIME [epoch: 1.35 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12628997438332795		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.12628997438332795 | validation: 0.20407062788461588]
	TIME [epoch: 1.35 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12202398465585067		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.12202398465585067 | validation: 0.17417073558620202]
	TIME [epoch: 1.34 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12159852940037424		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.12159852940037424 | validation: 0.20066623294193564]
	TIME [epoch: 1.35 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11794571335447711		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.11794571335447711 | validation: 0.1732169745130636]
	TIME [epoch: 1.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11818029931711539		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.11818029931711539 | validation: 0.21559415051197506]
	TIME [epoch: 1.35 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12290682919586876		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.12290682919586876 | validation: 0.1679444147042026]
	TIME [epoch: 1.34 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09353938799814841		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.09353938799814841 | validation: 0.16235934886443282]
	TIME [epoch: 1.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09939672714254329		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.09939672714254329 | validation: 0.44129679514017744]
	TIME [epoch: 1.35 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33995838704899023		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.33995838704899023 | validation: 0.41248173409462896]
	TIME [epoch: 1.35 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2975100604141953		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.2975100604141953 | validation: 0.29309695700143573]
	TIME [epoch: 1.35 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.193084935931377		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.193084935931377 | validation: 0.2038973424157251]
	TIME [epoch: 1.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12307570645601512		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.12307570645601512 | validation: 0.19068478482375742]
	TIME [epoch: 1.34 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12633794480381827		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.12633794480381827 | validation: 0.21907569566501356]
	TIME [epoch: 1.35 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11603830675763951		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.11603830675763951 | validation: 0.1739218578776982]
	TIME [epoch: 1.35 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10668655297401806		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.10668655297401806 | validation: 0.20801367448195318]
	TIME [epoch: 1.35 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10460407626984798		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.10460407626984798 | validation: 0.1911676226701419]
	TIME [epoch: 1.35 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10445663331658579		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.10445663331658579 | validation: 0.19581019614382172]
	TIME [epoch: 1.35 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824142470994919		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.10824142470994919 | validation: 0.25965752146335824]
	TIME [epoch: 1.35 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12955979180039603		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.12955979180039603 | validation: 0.22719495652335875]
	TIME [epoch: 1.35 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1365315470067345		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.1365315470067345 | validation: 0.1875846112476563]
	TIME [epoch: 1.35 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11411718697009943		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.11411718697009943 | validation: 0.1952742202506836]
	TIME [epoch: 1.35 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10070991590687595		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.10070991590687595 | validation: 0.14058819375882023]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10043125729008914		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.10043125729008914 | validation: 0.21044586106992239]
	TIME [epoch: 1.35 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10379824664600593		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.10379824664600593 | validation: 0.14028318725760097]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959259625001694		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.0959259625001694 | validation: 0.1915471048757679]
	TIME [epoch: 1.35 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532566838746141		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.10532566838746141 | validation: 0.16720209609882264]
	TIME [epoch: 1.35 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11487110619378063		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.11487110619378063 | validation: 0.203683545680999]
	TIME [epoch: 1.35 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14994418991139416		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.14994418991139416 | validation: 0.1860462124971815]
	TIME [epoch: 1.35 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08430282312551227		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.08430282312551227 | validation: 0.1468249084362975]
	TIME [epoch: 1.35 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937937484009797		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.0937937484009797 | validation: 0.18815378550322498]
	TIME [epoch: 1.35 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09866895292596418		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.09866895292596418 | validation: 0.13610428998847782]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08376642041257784		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.08376642041257784 | validation: 0.1758734715794201]
	TIME [epoch: 1.35 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08201750622015143		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.08201750622015143 | validation: 0.1416851761385395]
	TIME [epoch: 1.35 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889084559950214		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.0889084559950214 | validation: 0.2040423785751968]
	TIME [epoch: 1.35 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10603001627920983		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.10603001627920983 | validation: 0.16617480990941536]
	TIME [epoch: 1.35 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13561124499721314		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.13561124499721314 | validation: 0.15878714717640433]
	TIME [epoch: 1.35 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09305976271827746		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.09305976271827746 | validation: 0.15830542950504]
	TIME [epoch: 1.35 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08121685812672726		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.08121685812672726 | validation: 0.12801104627422571]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10573079930753926		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.10573079930753926 | validation: 0.20760189966152637]
	TIME [epoch: 1.35 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369064203838295		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.1369064203838295 | validation: 0.2077684047828215]
	TIME [epoch: 1.35 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13501620512616275		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.13501620512616275 | validation: 0.18223546864663734]
	TIME [epoch: 1.35 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284246995873204		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.1284246995873204 | validation: 0.17666844985235142]
	TIME [epoch: 1.35 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09555754803252972		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.09555754803252972 | validation: 0.12363362446675263]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07793634935799597		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.07793634935799597 | validation: 0.12010778878113584]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642630334467808		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.0642630334467808 | validation: 0.13450298842115094]
	TIME [epoch: 1.35 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07419341836821503		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.07419341836821503 | validation: 0.1458230211265538]
	TIME [epoch: 1.35 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09744910099481219		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.09744910099481219 | validation: 0.14936341529858913]
	TIME [epoch: 1.34 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771089646904454		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.08771089646904454 | validation: 0.135808537315299]
	TIME [epoch: 1.35 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08588281803543055		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.08588281803543055 | validation: 0.15928672908313166]
	TIME [epoch: 1.35 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08866650660933369		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.08866650660933369 | validation: 0.14274336459006473]
	TIME [epoch: 1.35 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09466767756839574		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.09466767756839574 | validation: 0.1673308048657366]
	TIME [epoch: 1.35 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10147134825505287		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.10147134825505287 | validation: 0.1581166127273821]
	TIME [epoch: 1.35 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10197105717470269		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.10197105717470269 | validation: 0.12667149840444464]
	TIME [epoch: 1.35 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09509605036130847		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.09509605036130847 | validation: 0.14905728139551572]
	TIME [epoch: 1.35 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07901667712855677		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.07901667712855677 | validation: 0.11189710412053665]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06760309869956861		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.06760309869956861 | validation: 0.11618043242337546]
	TIME [epoch: 1.34 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365426229635		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.07365426229635 | validation: 0.11817073510426068]
	TIME [epoch: 1.35 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732306250215219		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.0732306250215219 | validation: 0.1284637431728651]
	TIME [epoch: 1.34 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0953864236447736		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.0953864236447736 | validation: 0.13537092557895952]
	TIME [epoch: 1.35 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08049672005161558		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.08049672005161558 | validation: 0.10279170467741042]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_378.pth
	Model improved!!!
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449184226018989		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.07449184226018989 | validation: 0.16472511484551308]
	TIME [epoch: 1.35 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07620223576006564		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.07620223576006564 | validation: 0.14768714729363416]
	TIME [epoch: 1.35 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09721030586316402		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.09721030586316402 | validation: 0.2123364907024352]
	TIME [epoch: 1.35 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11115805647039878		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.11115805647039878 | validation: 0.11372801037964862]
	TIME [epoch: 1.35 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07881754891293627		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.07881754891293627 | validation: 0.11912512655439433]
	TIME [epoch: 1.35 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07863728604480075		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.07863728604480075 | validation: 0.163248040789024]
	TIME [epoch: 1.35 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10012143325878019		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.10012143325878019 | validation: 0.11592171563074836]
	TIME [epoch: 1.35 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09625525185348206		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.09625525185348206 | validation: 0.12217015318676572]
	TIME [epoch: 1.35 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835876904105599		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.07835876904105599 | validation: 0.12360809046610988]
	TIME [epoch: 1.35 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911158538278853		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.06911158538278853 | validation: 0.12706878868902216]
	TIME [epoch: 1.35 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07334774321284476		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.07334774321284476 | validation: 0.1457793966705497]
	TIME [epoch: 1.35 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07233967622138733		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.07233967622138733 | validation: 0.1029540706225043]
	TIME [epoch: 1.35 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636398031958194		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.08636398031958194 | validation: 0.16463600847558968]
	TIME [epoch: 1.35 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08446767296599633		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.08446767296599633 | validation: 0.132420883522662]
	TIME [epoch: 1.35 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638175056007547		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.07638175056007547 | validation: 0.1280387148749258]
	TIME [epoch: 1.35 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07006660668158243		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.07006660668158243 | validation: 0.12555092917896868]
	TIME [epoch: 1.35 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07096140185081597		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.07096140185081597 | validation: 0.1170397633985084]
	TIME [epoch: 1.35 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08235516014103127		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.08235516014103127 | validation: 0.1594909201610818]
	TIME [epoch: 1.35 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09506754831742545		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.09506754831742545 | validation: 0.11208947449436064]
	TIME [epoch: 1.35 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07251112722654657		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.07251112722654657 | validation: 0.09701659814709299]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07189008708868715		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.07189008708868715 | validation: 0.13323511808589275]
	TIME [epoch: 1.34 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431128571720434		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.07431128571720434 | validation: 0.11029542054109749]
	TIME [epoch: 1.34 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477640132373235		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.08477640132373235 | validation: 0.13190407671438326]
	TIME [epoch: 1.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07013609462268354		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.07013609462268354 | validation: 0.09391732500166507]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07442142330937446		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.07442142330937446 | validation: 0.14490565791192653]
	TIME [epoch: 1.34 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06685951563657294		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.06685951563657294 | validation: 0.12153513345631627]
	TIME [epoch: 1.35 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06896969852478713		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.06896969852478713 | validation: 0.12561835512647343]
	TIME [epoch: 1.34 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754010497545399		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.06754010497545399 | validation: 0.10711393520064388]
	TIME [epoch: 1.34 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064289034188144		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.07064289034188144 | validation: 0.13295930331906866]
	TIME [epoch: 1.34 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06933367607052311		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.06933367607052311 | validation: 0.1261927907937312]
	TIME [epoch: 1.34 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014380205705766		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.08014380205705766 | validation: 0.14584107898848028]
	TIME [epoch: 1.34 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10055071579268393		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.10055071579268393 | validation: 0.1260316024294089]
	TIME [epoch: 1.34 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07803967817616089		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.07803967817616089 | validation: 0.11248019357280699]
	TIME [epoch: 1.35 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054877409130610344		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.054877409130610344 | validation: 0.10536545788006975]
	TIME [epoch: 1.34 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413957084190777		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.07413957084190777 | validation: 0.14430223136649922]
	TIME [epoch: 1.35 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811665865258688		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.09811665865258688 | validation: 0.1043087155037869]
	TIME [epoch: 1.35 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056390120199144834		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.056390120199144834 | validation: 0.10381431246761963]
	TIME [epoch: 1.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055402726053029934		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.055402726053029934 | validation: 0.10689715861428253]
	TIME [epoch: 1.35 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060379608732792446		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.060379608732792446 | validation: 0.1097892563520206]
	TIME [epoch: 1.34 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0579889403838912		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.0579889403838912 | validation: 0.11212480598715602]
	TIME [epoch: 1.35 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059648126693528404		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.059648126693528404 | validation: 0.10206141953363893]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06453903524942523		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.06453903524942523 | validation: 0.12440112019738854]
	TIME [epoch: 1.34 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369137735385515		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.08369137735385515 | validation: 0.13133681115880047]
	TIME [epoch: 1.34 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10163077415619629		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.10163077415619629 | validation: 0.11559048464793445]
	TIME [epoch: 1.34 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06672731774769579		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.06672731774769579 | validation: 0.08834227349851163]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0544958212728656		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.0544958212728656 | validation: 0.11660963393682824]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05541063853827795		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.05541063853827795 | validation: 0.09976113998950907]
	TIME [epoch: 1.34 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07547991665765545		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.07547991665765545 | validation: 0.11718714935032776]
	TIME [epoch: 1.35 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119074788812162		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.07119074788812162 | validation: 0.08743690895957229]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05331654216692934		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.05331654216692934 | validation: 0.11386926128368347]
	TIME [epoch: 1.34 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047622141086104346		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.047622141086104346 | validation: 0.08754445817920772]
	TIME [epoch: 1.35 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04830084567975061		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.04830084567975061 | validation: 0.11142895692817252]
	TIME [epoch: 1.34 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04937388151736087		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.04937388151736087 | validation: 0.09452860395215573]
	TIME [epoch: 1.34 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05763722028401822		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.05763722028401822 | validation: 0.12940236622926196]
	TIME [epoch: 1.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325035126474874		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.08325035126474874 | validation: 0.09211670009920808]
	TIME [epoch: 1.34 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06868673227063675		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.06868673227063675 | validation: 0.09537110695198775]
	TIME [epoch: 1.34 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058218630109835835		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.058218630109835835 | validation: 0.08935292122865922]
	TIME [epoch: 1.34 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05583575353037205		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.05583575353037205 | validation: 0.10989787092985404]
	TIME [epoch: 1.34 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0669573562153659		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.0669573562153659 | validation: 0.1439955998307054]
	TIME [epoch: 1.34 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07999715324689237		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.07999715324689237 | validation: 0.13180332338906933]
	TIME [epoch: 1.34 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09544637959245737		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.09544637959245737 | validation: 0.1127250814709139]
	TIME [epoch: 1.34 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06978367649335858		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.06978367649335858 | validation: 0.08656506504730005]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050383545249667316		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.050383545249667316 | validation: 0.0783590245634665]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045281949630449056		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.045281949630449056 | validation: 0.08248739244154896]
	TIME [epoch: 1.34 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04354705012469478		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.04354705012469478 | validation: 0.07935184891837724]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041720703352967306		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.041720703352967306 | validation: 0.07886352670424246]
	TIME [epoch: 1.34 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166987464466492		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.04166987464466492 | validation: 0.08091634904836098]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626626017852864		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.04626626017852864 | validation: 0.09631306569398564]
	TIME [epoch: 1.34 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06156183499368423		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.06156183499368423 | validation: 0.11346965641745839]
	TIME [epoch: 1.34 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08495780596921332		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.08495780596921332 | validation: 0.10945661982329394]
	TIME [epoch: 1.34 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07075663478069481		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.07075663478069481 | validation: 0.09994288775306935]
	TIME [epoch: 1.34 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06254071102085097		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.06254071102085097 | validation: 0.11570484857137919]
	TIME [epoch: 1.35 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664439495374622		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.06664439495374622 | validation: 0.10373938316143323]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060057481200064315		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.060057481200064315 | validation: 0.09680414187468964]
	TIME [epoch: 1.34 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06148393192293917		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.06148393192293917 | validation: 0.10392701004359117]
	TIME [epoch: 1.34 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848669433310802		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.05848669433310802 | validation: 0.07816781141703429]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05390618254252143		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.05390618254252143 | validation: 0.09531024442651148]
	TIME [epoch: 1.35 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04880313878037479		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.04880313878037479 | validation: 0.08833993993646798]
	TIME [epoch: 1.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051492756057332445		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.051492756057332445 | validation: 0.10163036466275431]
	TIME [epoch: 1.34 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056010128322152		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.06056010128322152 | validation: 0.09380139848140497]
	TIME [epoch: 1.34 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06398437792123737		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.06398437792123737 | validation: 0.10472555219583098]
	TIME [epoch: 1.34 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05657280512390155		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.05657280512390155 | validation: 0.07981367443847062]
	TIME [epoch: 1.34 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112512784239165		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.05112512784239165 | validation: 0.09080485260607962]
	TIME [epoch: 1.34 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443676602951067		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.04443676602951067 | validation: 0.07317908667702523]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04195542077604752		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.04195542077604752 | validation: 0.09096603847555931]
	TIME [epoch: 1.34 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051390458990066126		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.051390458990066126 | validation: 0.08279304161549349]
	TIME [epoch: 1.34 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940384480503935		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.06940384480503935 | validation: 0.09580488003155616]
	TIME [epoch: 1.34 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04865754729081196		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.04865754729081196 | validation: 0.08249024883660934]
	TIME [epoch: 1.34 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04554714796703742		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.04554714796703742 | validation: 0.10478361619130877]
	TIME [epoch: 1.34 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049329160748583754		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.049329160748583754 | validation: 0.09510359112944211]
	TIME [epoch: 1.34 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0679937927275216		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.0679937927275216 | validation: 0.10894537362662296]
	TIME [epoch: 1.34 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697736858377332		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.0697736858377332 | validation: 0.07664225299779238]
	TIME [epoch: 1.37 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04205294994692977		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.04205294994692977 | validation: 0.06783799061195683]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030730182531894		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.05030730182531894 | validation: 0.08294283068124696]
	TIME [epoch: 1.35 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05737042808137621		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.05737042808137621 | validation: 0.07961762201425417]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05592843917814502		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.05592843917814502 | validation: 0.10867280659632568]
	TIME [epoch: 1.34 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05941430604451162		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.05941430604451162 | validation: 0.09975786747616]
	TIME [epoch: 1.34 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0489593741428091		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.0489593741428091 | validation: 0.08005546710723122]
	TIME [epoch: 1.34 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05802851063253327		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.05802851063253327 | validation: 0.10941508169436409]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056758221874836715		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.056758221874836715 | validation: 0.08094108591195981]
	TIME [epoch: 1.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05703694907275974		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.05703694907275974 | validation: 0.07666318314787393]
	TIME [epoch: 1.34 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042940212898045046		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.042940212898045046 | validation: 0.07373069061981713]
	TIME [epoch: 1.34 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040343732783607304		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.040343732783607304 | validation: 0.07846760159832936]
	TIME [epoch: 1.34 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020483706914726		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.04020483706914726 | validation: 0.07481443880486734]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04341316537320709		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.04341316537320709 | validation: 0.08257861486366258]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04782160479859309		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.04782160479859309 | validation: 0.0856051736220545]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05730900770291072		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.05730900770291072 | validation: 0.09657659751951858]
	TIME [epoch: 1.35 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06150232571191305		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.06150232571191305 | validation: 0.08308490338115199]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319480606628627		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.05319480606628627 | validation: 0.0841306095114511]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04258908577635534		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.04258908577635534 | validation: 0.06585097808231678]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047200454375462		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.04047200454375462 | validation: 0.07529431179491036]
	TIME [epoch: 1.34 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038766849430070485		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.038766849430070485 | validation: 0.07230794509859155]
	TIME [epoch: 1.34 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04093167975894163		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.04093167975894163 | validation: 0.07195382839775284]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04330601832428208		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.04330601832428208 | validation: 0.059650742771255275]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322536886557602		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.04322536886557602 | validation: 0.08910366050227425]
	TIME [epoch: 1.34 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265969496952151		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.04265969496952151 | validation: 0.06596594960390954]
	TIME [epoch: 1.34 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042803189449230956		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.042803189449230956 | validation: 0.10123196005924746]
	TIME [epoch: 1.34 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04798595623730055		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.04798595623730055 | validation: 0.07542280901897999]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05584769285288923		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.05584769285288923 | validation: 0.11149909168002284]
	TIME [epoch: 1.34 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425839988641506		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.06425839988641506 | validation: 0.09316344785257268]
	TIME [epoch: 1.34 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06102344627690528		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.06102344627690528 | validation: 0.06668861853995418]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04456576803309448		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.04456576803309448 | validation: 0.0712819815194834]
	TIME [epoch: 1.34 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037797140984894444		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.037797140984894444 | validation: 0.07137639958870794]
	TIME [epoch: 166 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036141048167975684		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.036141048167975684 | validation: 0.07270537071066516]
	TIME [epoch: 2.69 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03591589094027931		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.03591589094027931 | validation: 0.07450414280993613]
	TIME [epoch: 2.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146029220472716		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.04146029220472716 | validation: 0.06876339200590408]
	TIME [epoch: 2.66 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504823553675004		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.04504823553675004 | validation: 0.07236075319190527]
	TIME [epoch: 2.66 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03950800055229469		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.03950800055229469 | validation: 0.0660327404785231]
	TIME [epoch: 2.67 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03731309911958414		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.03731309911958414 | validation: 0.0706554868849788]
	TIME [epoch: 2.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034569650667886684		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.034569650667886684 | validation: 0.07573289570983005]
	TIME [epoch: 2.68 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040256377906781396		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.040256377906781396 | validation: 0.09373761508034352]
	TIME [epoch: 2.66 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05146034646090838		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.05146034646090838 | validation: 0.08450745479625604]
	TIME [epoch: 2.67 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06739884937527692		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.06739884937527692 | validation: 0.08739661770229071]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060474541415436754		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.060474541415436754 | validation: 0.05930529965907758]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046188617068919975		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.046188617068919975 | validation: 0.0557311912976326]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574374308886004		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.03574374308886004 | validation: 0.057955047342172196]
	TIME [epoch: 2.67 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034130312744106925		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.034130312744106925 | validation: 0.0611019918901006]
	TIME [epoch: 2.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03093234054279209		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.03093234054279209 | validation: 0.0648808219469766]
	TIME [epoch: 2.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461201794370915		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.03461201794370915 | validation: 0.07396739922098837]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04195148550202986		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.04195148550202986 | validation: 0.06904853986377275]
	TIME [epoch: 2.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03825264177127332		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.03825264177127332 | validation: 0.07047665529487768]
	TIME [epoch: 2.66 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033444663396025164		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.033444663396025164 | validation: 0.06146708989461781]
	TIME [epoch: 2.67 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033679608363504906		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.033679608363504906 | validation: 0.08146089828640296]
	TIME [epoch: 2.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045834886198474034		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.045834886198474034 | validation: 0.08971381967464287]
	TIME [epoch: 2.67 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054873800944815976		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.054873800944815976 | validation: 0.10163845063184834]
	TIME [epoch: 2.67 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04602621675644592		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.04602621675644592 | validation: 0.07160988068761732]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03190852078503236		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.03190852078503236 | validation: 0.06772213160539703]
	TIME [epoch: 2.67 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03023946918508459		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.03023946918508459 | validation: 0.0655004525303566]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03419703904171335		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.03419703904171335 | validation: 0.06968493768731858]
	TIME [epoch: 2.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579732477814447		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.04579732477814447 | validation: 0.083590113077403]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07031154720772026		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.07031154720772026 | validation: 0.1206777456891865]
	TIME [epoch: 2.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07277467409094918		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.07277467409094918 | validation: 0.05379311442576811]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677621177302596		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.03677621177302596 | validation: 0.056069958684969334]
	TIME [epoch: 2.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03017913119140247		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.03017913119140247 | validation: 0.060975213390415034]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331120607045837		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.0331120607045837 | validation: 0.05795886823765905]
	TIME [epoch: 2.66 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034364168030812985		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.034364168030812985 | validation: 0.055337512351379015]
	TIME [epoch: 2.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033296927572998486		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.033296927572998486 | validation: 0.06272329510363332]
	TIME [epoch: 2.66 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035714326945725496		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.035714326945725496 | validation: 0.06565378059839325]
	TIME [epoch: 2.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040972192344886235		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.040972192344886235 | validation: 0.06389953164652958]
	TIME [epoch: 2.67 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038296458298183025		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.038296458298183025 | validation: 0.05743648244728897]
	TIME [epoch: 2.67 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04177822137362591		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.04177822137362591 | validation: 0.06854355531881985]
	TIME [epoch: 2.67 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04154686209702236		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.04154686209702236 | validation: 0.07219887584031896]
	TIME [epoch: 2.67 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042429180698867495		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.042429180698867495 | validation: 0.07748153855492192]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043029611571554265		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.043029611571554265 | validation: 0.06409082995849376]
	TIME [epoch: 2.67 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384470673877868		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.0384470673877868 | validation: 0.06795093372077032]
	TIME [epoch: 2.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03713800397398167		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.03713800397398167 | validation: 0.057125617457130774]
	TIME [epoch: 2.66 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034529259728506186		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.034529259728506186 | validation: 0.06308932139971447]
	TIME [epoch: 2.67 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035739030182099604		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.035739030182099604 | validation: 0.050320110492861385]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033675601161109715		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.033675601161109715 | validation: 0.06960879415065462]
	TIME [epoch: 2.66 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03274304869187148		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.03274304869187148 | validation: 0.05627541691592476]
	TIME [epoch: 2.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03655672968127958		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.03655672968127958 | validation: 0.0709085387505363]
	TIME [epoch: 2.66 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041669611597225556		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.041669611597225556 | validation: 0.05135369531978701]
	TIME [epoch: 2.67 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0445845391351472		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.0445845391351472 | validation: 0.06839743458083093]
	TIME [epoch: 2.65 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037392122690894966		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.037392122690894966 | validation: 0.0641282463333598]
	TIME [epoch: 2.66 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03611079321628731		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.03611079321628731 | validation: 0.056146122073094544]
	TIME [epoch: 2.65 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036363595997119254		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.036363595997119254 | validation: 0.065340114686539]
	TIME [epoch: 2.66 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03384040500477251		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.03384040500477251 | validation: 0.06402954884203033]
	TIME [epoch: 2.66 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035054513539817166		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.035054513539817166 | validation: 0.04801176680103887]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_556.pth
	Model improved!!!
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034039860946848556		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.034039860946848556 | validation: 0.06515722822419032]
	TIME [epoch: 2.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032865435494457994		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.032865435494457994 | validation: 0.053468856212742735]
	TIME [epoch: 2.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03756679878345176		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.03756679878345176 | validation: 0.06849545424576271]
	TIME [epoch: 2.67 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03827361405261822		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.03827361405261822 | validation: 0.057295119440621226]
	TIME [epoch: 2.67 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036869547558661835		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.036869547558661835 | validation: 0.06792636244464105]
	TIME [epoch: 2.66 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03589072879455442		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.03589072879455442 | validation: 0.051978937712334174]
	TIME [epoch: 2.66 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03309354884543459		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.03309354884543459 | validation: 0.051512929750556186]
	TIME [epoch: 2.66 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03103500975935674		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.03103500975935674 | validation: 0.0562298708768556]
	TIME [epoch: 2.66 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030524192302689747		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.030524192302689747 | validation: 0.04570862889919561]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03049117695692763		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.03049117695692763 | validation: 0.04876253062117053]
	TIME [epoch: 2.66 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162140326577736		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.03162140326577736 | validation: 0.050238744559325546]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151641680865584		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.03151641680865584 | validation: 0.055703248878863146]
	TIME [epoch: 2.66 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036309142250229826		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.036309142250229826 | validation: 0.0599121665048475]
	TIME [epoch: 2.67 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05008036220358659		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.05008036220358659 | validation: 0.05289362714608592]
	TIME [epoch: 2.67 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02970720673130324		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.02970720673130324 | validation: 0.059609642908890814]
	TIME [epoch: 2.67 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027741380382247504		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.027741380382247504 | validation: 0.04502665013406454]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02693648594873419		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.02693648594873419 | validation: 0.052975056256470615]
	TIME [epoch: 2.66 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026739235116580336		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.026739235116580336 | validation: 0.0407791694797277]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_574.pth
	Model improved!!!
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02582859189649595		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.02582859189649595 | validation: 0.05423205824377019]
	TIME [epoch: 2.67 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026446212158670008		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.026446212158670008 | validation: 0.045709172081558364]
	TIME [epoch: 2.67 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033220423537888934		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.033220423537888934 | validation: 0.09038323788564827]
	TIME [epoch: 2.67 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059826122591273824		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.059826122591273824 | validation: 0.08755325884588588]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015725916467827		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.07015725916467827 | validation: 0.061398288111452363]
	TIME [epoch: 2.67 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03357062456238671		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.03357062456238671 | validation: 0.04453418623036229]
	TIME [epoch: 2.67 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025559050911267346		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.025559050911267346 | validation: 0.044786118772787514]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029648977405188088		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.029648977405188088 | validation: 0.06515825418487235]
	TIME [epoch: 2.67 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029336037997776766		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.029336037997776766 | validation: 0.04019004332980142]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027230336505904767		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.027230336505904767 | validation: 0.048816491227605376]
	TIME [epoch: 2.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030352417067764634		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.030352417067764634 | validation: 0.06452390250442744]
	TIME [epoch: 2.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035427604934144465		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.035427604934144465 | validation: 0.04948357307263025]
	TIME [epoch: 2.65 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02923708294646706		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.02923708294646706 | validation: 0.04605678187555029]
	TIME [epoch: 2.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026872355094792667		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.026872355094792667 | validation: 0.03995724920997593]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027836286794449334		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.027836286794449334 | validation: 0.04722669319056974]
	TIME [epoch: 2.66 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026680178853674633		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.026680178853674633 | validation: 0.049920130490416395]
	TIME [epoch: 2.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02694293954235749		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.02694293954235749 | validation: 0.046409419546409626]
	TIME [epoch: 2.65 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02890692712715082		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.02890692712715082 | validation: 0.06489846504291764]
	TIME [epoch: 2.64 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03479450183086835		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.03479450183086835 | validation: 0.0638847359940378]
	TIME [epoch: 2.65 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0398035560820703		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.0398035560820703 | validation: 0.06862703284526633]
	TIME [epoch: 2.65 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04748088479833217		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.04748088479833217 | validation: 0.05449816782661379]
	TIME [epoch: 2.65 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020260000837899		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.04020260000837899 | validation: 0.04858120464022449]
	TIME [epoch: 2.65 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023955852912187072		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.023955852912187072 | validation: 0.03865538332688656]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022953088571423374		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.022953088571423374 | validation: 0.05021172002120231]
	TIME [epoch: 2.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026840928022322595		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.026840928022322595 | validation: 0.047281522564359114]
	TIME [epoch: 2.67 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029472420145278164		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.029472420145278164 | validation: 0.042732602075959636]
	TIME [epoch: 2.67 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026261479977072275		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.026261479977072275 | validation: 0.04454295305717634]
	TIME [epoch: 2.67 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026478226236425657		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.026478226236425657 | validation: 0.05608115349434753]
	TIME [epoch: 2.67 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02710736588594215		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.02710736588594215 | validation: 0.05137414046973441]
	TIME [epoch: 2.66 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02530256199101953		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.02530256199101953 | validation: 0.0423292629882499]
	TIME [epoch: 2.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026233803923709758		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.026233803923709758 | validation: 0.05054658852125733]
	TIME [epoch: 2.66 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031090151150967645		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.031090151150967645 | validation: 0.059460480939850784]
	TIME [epoch: 2.67 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036363354135841075		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.036363354135841075 | validation: 0.05797125074719886]
	TIME [epoch: 2.66 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700075744574415		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.03700075744574415 | validation: 0.037963249734471766]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03146194522458468		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.03146194522458468 | validation: 0.047447540252238185]
	TIME [epoch: 2.66 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03264087163971487		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.03264087163971487 | validation: 0.05020596650372044]
	TIME [epoch: 2.68 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653572519048256		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.03653572519048256 | validation: 0.04897970705105434]
	TIME [epoch: 2.67 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025363285702155892		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.025363285702155892 | validation: 0.04807703333633398]
	TIME [epoch: 2.67 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025268183310633702		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.025268183310633702 | validation: 0.04436882506731618]
	TIME [epoch: 2.66 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02611532861734267		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.02611532861734267 | validation: 0.04740204246477956]
	TIME [epoch: 2.66 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026695627303447635		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.026695627303447635 | validation: 0.04398823470390992]
	TIME [epoch: 2.67 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026604602690253314		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.026604602690253314 | validation: 0.04682074129096774]
	TIME [epoch: 2.66 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0244388872946986		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.0244388872946986 | validation: 0.039426905839309606]
	TIME [epoch: 2.66 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023608420000385086		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.023608420000385086 | validation: 0.04504978374689668]
	TIME [epoch: 2.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02240031155200243		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.02240031155200243 | validation: 0.044979015930967495]
	TIME [epoch: 2.66 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026440499469548354		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.026440499469548354 | validation: 0.057303733812059804]
	TIME [epoch: 2.67 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039022874673125985		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.039022874673125985 | validation: 0.0771558784278294]
	TIME [epoch: 2.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05355706527684197		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.05355706527684197 | validation: 0.051595848739398424]
	TIME [epoch: 2.67 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037044146706847574		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.037044146706847574 | validation: 0.046663751879191175]
	TIME [epoch: 2.66 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02373633734938765		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.02373633734938765 | validation: 0.04922508433113201]
	TIME [epoch: 2.67 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025958862711073775		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.025958862711073775 | validation: 0.03712766202135074]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030651383716088904		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.030651383716088904 | validation: 0.050502657957835956]
	TIME [epoch: 2.66 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02722678518334269		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.02722678518334269 | validation: 0.03996518315947807]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022901482629754232		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.022901482629754232 | validation: 0.03901131318486595]
	TIME [epoch: 2.66 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023668386020917302		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.023668386020917302 | validation: 0.04488157811910942]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024860936934352415		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.024860936934352415 | validation: 0.04213643260684391]
	TIME [epoch: 2.66 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025973946580881424		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.025973946580881424 | validation: 0.03950493668796021]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026547509584908008		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.026547509584908008 | validation: 0.04464998515659274]
	TIME [epoch: 2.67 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026119258150613493		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.026119258150613493 | validation: 0.03617225508643214]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024240584129645627		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.024240584129645627 | validation: 0.049635779136177484]
	TIME [epoch: 2.66 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023673773904603106		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.023673773904603106 | validation: 0.03616361661693592]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02436987026421197		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.02436987026421197 | validation: 0.047091561060427826]
	TIME [epoch: 2.66 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02628097719760228		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.02628097719760228 | validation: 0.04422353286283422]
	TIME [epoch: 2.67 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026995507900197037		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.026995507900197037 | validation: 0.04618920147663033]
	TIME [epoch: 2.66 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030475533606366262		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.030475533606366262 | validation: 0.05339835565702469]
	TIME [epoch: 2.68 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03250714761219694		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.03250714761219694 | validation: 0.04867177587861324]
	TIME [epoch: 2.67 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030260775325081757		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.030260775325081757 | validation: 0.03857094527469512]
	TIME [epoch: 2.66 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025270197219272105		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.025270197219272105 | validation: 0.04167817654998821]
	TIME [epoch: 2.66 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022355756961816936		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.022355756961816936 | validation: 0.035953226189123964]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020917751362654637		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.020917751362654637 | validation: 0.047754645203696024]
	TIME [epoch: 2.65 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026057094916527548		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.026057094916527548 | validation: 0.05001214305512161]
	TIME [epoch: 2.66 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042427020313179895		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.042427020313179895 | validation: 0.05072883803261938]
	TIME [epoch: 2.65 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028805414623381757		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.028805414623381757 | validation: 0.050438160369904686]
	TIME [epoch: 2.66 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032183958760464974		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.032183958760464974 | validation: 0.04485319624153109]
	TIME [epoch: 2.66 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02901838428312034		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.02901838428312034 | validation: 0.04517273993350601]
	TIME [epoch: 2.66 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022725793951945312		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.022725793951945312 | validation: 0.030475366551767327]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022070967254098895		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.022070967254098895 | validation: 0.046412215789607886]
	TIME [epoch: 2.66 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023987446820917892		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.023987446820917892 | validation: 0.03162859444644648]
	TIME [epoch: 2.65 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022787556472465376		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.022787556472465376 | validation: 0.03981824108942189]
	TIME [epoch: 2.65 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023429265499021863		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.023429265499021863 | validation: 0.03986870473325208]
	TIME [epoch: 2.65 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02778394432490936		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.02778394432490936 | validation: 0.049382830820849766]
	TIME [epoch: 2.66 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461431053776003		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.03461431053776003 | validation: 0.03999042669193045]
	TIME [epoch: 2.65 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031272500972322516		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.031272500972322516 | validation: 0.044222790773016885]
	TIME [epoch: 2.65 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022906272546276866		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.022906272546276866 | validation: 0.0422110261017769]
	TIME [epoch: 2.65 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022332782309094405		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.022332782309094405 | validation: 0.03781732633511843]
	TIME [epoch: 2.66 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023957400666023962		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.023957400666023962 | validation: 0.03685364217490951]
	TIME [epoch: 2.66 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02088989356338614		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.02088989356338614 | validation: 0.04500439054826388]
	TIME [epoch: 2.65 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025671996127187287		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.025671996127187287 | validation: 0.0385898601664769]
	TIME [epoch: 2.65 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021829560614119118		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.021829560614119118 | validation: 0.03182916965420433]
	TIME [epoch: 2.65 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023155009884637527		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.023155009884637527 | validation: 0.04836492726757827]
	TIME [epoch: 2.65 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02263541071100268		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.02263541071100268 | validation: 0.03709291732211472]
	TIME [epoch: 2.65 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022939424452231735		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.022939424452231735 | validation: 0.0504452991226944]
	TIME [epoch: 2.65 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026586975598702198		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.026586975598702198 | validation: 0.04176035458580282]
	TIME [epoch: 2.65 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028875444368305907		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.028875444368305907 | validation: 0.0505378838357303]
	TIME [epoch: 2.66 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030017977640394797		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.030017977640394797 | validation: 0.041902733168709065]
	TIME [epoch: 2.65 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035601709174086074		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.035601709174086074 | validation: 0.047372052798146186]
	TIME [epoch: 2.66 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0287621412548888		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.0287621412548888 | validation: 0.03940535785039483]
	TIME [epoch: 2.65 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022138049056421773		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.022138049056421773 | validation: 0.04039494733483943]
	TIME [epoch: 2.66 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022039046896379685		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.022039046896379685 | validation: 0.03815792776771117]
	TIME [epoch: 2.65 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021300687044227364		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.021300687044227364 | validation: 0.034924537272401905]
	TIME [epoch: 2.66 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020759740695971935		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.020759740695971935 | validation: 0.038298439696199706]
	TIME [epoch: 2.66 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02234449759260305		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.02234449759260305 | validation: 0.037557911548536896]
	TIME [epoch: 2.65 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02263136963147059		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.02263136963147059 | validation: 0.04000947660627457]
	TIME [epoch: 2.65 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02127131910086452		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.02127131910086452 | validation: 0.03207688738193861]
	TIME [epoch: 2.65 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020645346708964826		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.020645346708964826 | validation: 0.03402749613284361]
	TIME [epoch: 2.65 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021324285701632743		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.021324285701632743 | validation: 0.03877446473630145]
	TIME [epoch: 2.66 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021604366840116386		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.021604366840116386 | validation: 0.03832206117203168]
	TIME [epoch: 2.65 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02531188113542526		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.02531188113542526 | validation: 0.04086914883875584]
	TIME [epoch: 2.65 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0241341159378697		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.0241341159378697 | validation: 0.03393216797922455]
	TIME [epoch: 2.65 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022406868657252305		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.022406868657252305 | validation: 0.041005201131279735]
	TIME [epoch: 2.65 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023335987409604585		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.023335987409604585 | validation: 0.028767515947956568]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025013156499008055		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.025013156499008055 | validation: 0.05222444758058823]
	TIME [epoch: 2.66 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030743552506446663		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.030743552506446663 | validation: 0.04768421972927474]
	TIME [epoch: 2.68 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034699845323706305		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.034699845323706305 | validation: 0.03942074386720064]
	TIME [epoch: 2.66 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023548797344411867		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.023548797344411867 | validation: 0.02970766819571712]
	TIME [epoch: 2.66 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019450884770353165		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.019450884770353165 | validation: 0.03393540355136322]
	TIME [epoch: 2.66 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020794787021403374		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.020794787021403374 | validation: 0.06282339570375231]
	TIME [epoch: 2.66 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03431449267734655		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.03431449267734655 | validation: 0.05077824327191836]
	TIME [epoch: 2.65 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025187664881120772		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.025187664881120772 | validation: 0.032400380735005445]
	TIME [epoch: 2.66 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023669331046854918		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.023669331046854918 | validation: 0.03408812153028449]
	TIME [epoch: 2.65 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02434016273025727		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.02434016273025727 | validation: 0.039281285428321246]
	TIME [epoch: 2.66 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019671251179439072		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.019671251179439072 | validation: 0.03186136270205758]
	TIME [epoch: 2.66 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019544669620596637		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.019544669620596637 | validation: 0.03393315876719567]
	TIME [epoch: 2.66 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020300632167116958		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.020300632167116958 | validation: 0.03723333806329939]
	TIME [epoch: 2.66 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020652144058477706		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.020652144058477706 | validation: 0.03457035408937447]
	TIME [epoch: 2.66 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021371473653809408		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.021371473653809408 | validation: 0.041946615993639184]
	TIME [epoch: 2.66 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022016831358182883		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.022016831358182883 | validation: 0.037942911822503256]
	TIME [epoch: 2.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026143374993489416		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.026143374993489416 | validation: 0.05158835053700509]
	TIME [epoch: 2.65 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03183143225075628		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.03183143225075628 | validation: 0.038835369162922434]
	TIME [epoch: 2.65 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02986614536279113		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.02986614536279113 | validation: 0.04071565143575018]
	TIME [epoch: 2.66 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021445849324344058		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.021445849324344058 | validation: 0.032129890158632636]
	TIME [epoch: 2.65 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019047125284924592		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.019047125284924592 | validation: 0.032443760545725765]
	TIME [epoch: 2.65 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01915320676555182		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.01915320676555182 | validation: 0.02944854683609768]
	TIME [epoch: 2.66 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018224571144407533		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.018224571144407533 | validation: 0.03359368189689447]
	TIME [epoch: 2.66 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019181921353458677		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.019181921353458677 | validation: 0.03589696674477568]
	TIME [epoch: 2.66 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019351133047413403		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.019351133047413403 | validation: 0.03254245839813421]
	TIME [epoch: 2.66 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018644715543938137		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.018644715543938137 | validation: 0.030404522024585136]
	TIME [epoch: 2.66 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019739918461597936		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.019739918461597936 | validation: 0.03744072855926505]
	TIME [epoch: 2.66 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02113691744651664		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.02113691744651664 | validation: 0.03745932008958517]
	TIME [epoch: 2.66 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022972975963768		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.022972975963768 | validation: 0.042820678755084475]
	TIME [epoch: 2.66 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026779965115850343		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.026779965115850343 | validation: 0.044592477325109625]
	TIME [epoch: 2.66 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025489183638032493		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.025489183638032493 | validation: 0.04020400207341699]
	TIME [epoch: 2.66 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022601480461065672		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.022601480461065672 | validation: 0.037083843512441796]
	TIME [epoch: 2.66 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027037324420853256		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.027037324420853256 | validation: 0.03398238980448505]
	TIME [epoch: 2.66 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02589624357307068		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.02589624357307068 | validation: 0.03247280379962321]
	TIME [epoch: 2.66 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020548545048655713		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.020548545048655713 | validation: 0.03873135800262986]
	TIME [epoch: 2.68 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0218353850643519		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.0218353850643519 | validation: 0.026988619416892567]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018832057154448325		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.018832057154448325 | validation: 0.03206230402564144]
	TIME [epoch: 2.66 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01868908633261317		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.01868908633261317 | validation: 0.03450901590472938]
	TIME [epoch: 2.65 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019472095315997175		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.019472095315997175 | validation: 0.02727585813178365]
	TIME [epoch: 2.64 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02006440441728097		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.02006440441728097 | validation: 0.03461041606773673]
	TIME [epoch: 2.65 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018718081791332417		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.018718081791332417 | validation: 0.03171506544063909]
	TIME [epoch: 2.66 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018974598030322264		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.018974598030322264 | validation: 0.030687945821564644]
	TIME [epoch: 2.65 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0191502257443543		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.0191502257443543 | validation: 0.035257542037402]
	TIME [epoch: 2.65 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020856507611803972		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.020856507611803972 | validation: 0.041339512824623614]
	TIME [epoch: 2.65 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023494427717448307		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.023494427717448307 | validation: 0.034895930525152975]
	TIME [epoch: 2.65 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024879335724968234		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.024879335724968234 | validation: 0.041284115364963306]
	TIME [epoch: 2.66 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025617979946183846		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.025617979946183846 | validation: 0.026344920145965026]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_732.pth
	Model improved!!!
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025456180434056216		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.025456180434056216 | validation: 0.03542608558675092]
	TIME [epoch: 2.66 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018308142816046157		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.018308142816046157 | validation: 0.029434731249441716]
	TIME [epoch: 2.66 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018244567215349485		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.018244567215349485 | validation: 0.03317618992896361]
	TIME [epoch: 2.65 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020689422160464686		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.020689422160464686 | validation: 0.032742886590338784]
	TIME [epoch: 2.66 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01811669035760373		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.01811669035760373 | validation: 0.033206042969054515]
	TIME [epoch: 2.66 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0199780967690372		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.0199780967690372 | validation: 0.03274731163558584]
	TIME [epoch: 2.66 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02113793488375873		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.02113793488375873 | validation: 0.03261297442884029]
	TIME [epoch: 2.66 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01938105650050503		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.01938105650050503 | validation: 0.028163596337453967]
	TIME [epoch: 2.66 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017883184137334043		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.017883184137334043 | validation: 0.029626513533608623]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018295508418638688		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.018295508418638688 | validation: 0.03643478186871513]
	TIME [epoch: 2.65 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02057549824656956		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.02057549824656956 | validation: 0.03324107250406788]
	TIME [epoch: 2.66 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020682461140281702		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.020682461140281702 | validation: 0.02913926946763087]
	TIME [epoch: 2.66 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018771376691505783		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.018771376691505783 | validation: 0.03369446519679741]
	TIME [epoch: 2.66 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02073355912723445		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.02073355912723445 | validation: 0.034175383937029145]
	TIME [epoch: 2.66 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02069927495147472		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.02069927495147472 | validation: 0.030029223030563403]
	TIME [epoch: 2.66 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019472882475116585		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.019472882475116585 | validation: 0.03207897121414204]
	TIME [epoch: 2.65 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021196188311943732		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.021196188311943732 | validation: 0.04075855990481702]
	TIME [epoch: 2.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026214740261991318		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.026214740261991318 | validation: 0.04070856885986573]
	TIME [epoch: 2.66 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029886784084906238		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.029886784084906238 | validation: 0.03531972675272068]
	TIME [epoch: 2.66 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019706415919119903		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.019706415919119903 | validation: 0.028000231245017205]
	TIME [epoch: 2.66 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01859104650728525		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.01859104650728525 | validation: 0.02977889466893796]
	TIME [epoch: 2.66 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0194544341495971		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.0194544341495971 | validation: 0.03028178950769893]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02106262236513203		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.02106262236513203 | validation: 0.030081501692149928]
	TIME [epoch: 2.66 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017999428268670917		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.017999428268670917 | validation: 0.03280491916705974]
	TIME [epoch: 2.66 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01870878875942621		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.01870878875942621 | validation: 0.032079629254334906]
	TIME [epoch: 2.66 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019526633395654504		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.019526633395654504 | validation: 0.027769753768177943]
	TIME [epoch: 2.66 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01676870891610682		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.01676870891610682 | validation: 0.029564960009159938]
	TIME [epoch: 2.65 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018030100028565758		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.018030100028565758 | validation: 0.031630783474055145]
	TIME [epoch: 2.65 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017728528827204378		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.017728528827204378 | validation: 0.026123645866376555]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017350965739690236		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.017350965739690236 | validation: 0.02769640577001288]
	TIME [epoch: 2.66 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018235701911554724		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.018235701911554724 | validation: 0.036023973608793336]
	TIME [epoch: 2.66 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020381267329513682		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.020381267329513682 | validation: 0.029245580126233184]
	TIME [epoch: 2.68 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022170233655641382		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.022170233655641382 | validation: 0.029358296677725406]
	TIME [epoch: 2.66 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02025836615267849		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.02025836615267849 | validation: 0.03257173816383896]
	TIME [epoch: 2.66 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019767171147833572		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.019767171147833572 | validation: 0.024672707311134825]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_767.pth
	Model improved!!!
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02195524841087605		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.02195524841087605 | validation: 0.03755158664365732]
	TIME [epoch: 2.65 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022290158754280667		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.022290158754280667 | validation: 0.031178464563205735]
	TIME [epoch: 2.65 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022239215484869766		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.022239215484869766 | validation: 0.027355661395504206]
	TIME [epoch: 2.65 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018190878570444043		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.018190878570444043 | validation: 0.03166920767440538]
	TIME [epoch: 2.66 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016783832344511276		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.016783832344511276 | validation: 0.028832180319290624]
	TIME [epoch: 2.66 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017762984317674375		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.017762984317674375 | validation: 0.029978639254351416]
	TIME [epoch: 2.66 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017822315453219103		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.017822315453219103 | validation: 0.029186275376764215]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018495984229666515		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.018495984229666515 | validation: 0.03004937562306278]
	TIME [epoch: 2.66 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019636630868811306		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.019636630868811306 | validation: 0.026252656884720296]
	TIME [epoch: 2.66 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0194935854220567		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.0194935854220567 | validation: 0.03587548405433292]
	TIME [epoch: 2.66 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01880072022728339		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.01880072022728339 | validation: 0.028379659834357654]
	TIME [epoch: 2.66 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01753706584741901		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.01753706584741901 | validation: 0.03026482181245308]
	TIME [epoch: 2.66 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01680407497848385		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.01680407497848385 | validation: 0.02714496793904636]
	TIME [epoch: 2.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016389765151356487		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.016389765151356487 | validation: 0.02649271230860889]
	TIME [epoch: 2.66 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017046334240578366		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.017046334240578366 | validation: 0.030365823016472216]
	TIME [epoch: 2.66 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017748123833856017		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.017748123833856017 | validation: 0.029640046006563493]
	TIME [epoch: 2.65 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018486473738934056		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.018486473738934056 | validation: 0.026250617584850734]
	TIME [epoch: 2.65 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018956472180010963		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.018956472180010963 | validation: 0.026939885492691763]
	TIME [epoch: 2.65 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018495493987224727		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.018495493987224727 | validation: 0.03604990592629068]
	TIME [epoch: 2.65 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021762639049384226		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.021762639049384226 | validation: 0.03135520623157683]
	TIME [epoch: 2.65 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0211454474123983		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.0211454474123983 | validation: 0.027173839050581086]
	TIME [epoch: 2.65 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01835657674606723		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.01835657674606723 | validation: 0.023885806414701352]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_789.pth
	Model improved!!!
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01679778200094228		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.01679778200094228 | validation: 0.0280877651550016]
	TIME [epoch: 2.67 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018092828646195252		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.018092828646195252 | validation: 0.025325769892897268]
	TIME [epoch: 2.66 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017736351359113407		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.017736351359113407 | validation: 0.03380339361099988]
	TIME [epoch: 2.66 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01743160085179914		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.01743160085179914 | validation: 0.027003317517624227]
	TIME [epoch: 2.65 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020037479194963792		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.020037479194963792 | validation: 0.03217916964385494]
	TIME [epoch: 2.65 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019635325558091138		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.019635325558091138 | validation: 0.02965853254715456]
	TIME [epoch: 2.65 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018143395736019294		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.018143395736019294 | validation: 0.02970202698917154]
	TIME [epoch: 2.65 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017916956993117947		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.017916956993117947 | validation: 0.025720619959244476]
	TIME [epoch: 2.65 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018626570422612825		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.018626570422612825 | validation: 0.028069245877597994]
	TIME [epoch: 2.65 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018412449729239867		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.018412449729239867 | validation: 0.025241287234165046]
	TIME [epoch: 2.65 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017788508516514003		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.017788508516514003 | validation: 0.029106117369348962]
	TIME [epoch: 2.65 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01700289120973524		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.01700289120973524 | validation: 0.02930623260967228]
	TIME [epoch: 2.66 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016017436169256372		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.016017436169256372 | validation: 0.023954141311293888]
	TIME [epoch: 2.67 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016686036285850615		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.016686036285850615 | validation: 0.02781537067194291]
	TIME [epoch: 2.66 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017844967189303427		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.017844967189303427 | validation: 0.0266180935816588]
	TIME [epoch: 2.66 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02192936191220701		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.02192936191220701 | validation: 0.0267190058472804]
	TIME [epoch: 2.66 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016330777057960055		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.016330777057960055 | validation: 0.02562725757357015]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016183876191269948		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.016183876191269948 | validation: 0.024460102426340294]
	TIME [epoch: 2.68 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018512312846031845		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.018512312846031845 | validation: 0.02802365699173003]
	TIME [epoch: 2.66 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015561526317752863		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.015561526317752863 | validation: 0.030782747063261697]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017350920572738415		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.017350920572738415 | validation: 0.03050874861302665]
	TIME [epoch: 2.66 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018666604328259444		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.018666604328259444 | validation: 0.0323599982697786]
	TIME [epoch: 2.66 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021102270085586187		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.021102270085586187 | validation: 0.02517521081817863]
	TIME [epoch: 2.66 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019613138966678618		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.019613138966678618 | validation: 0.030053237927809007]
	TIME [epoch: 2.66 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020859128132884576		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.020859128132884576 | validation: 0.021635017350801755]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_814.pth
	Model improved!!!
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017454272610961333		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.017454272610961333 | validation: 0.024681943447618728]
	TIME [epoch: 2.66 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016230606504557855		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.016230606504557855 | validation: 0.025939485095174353]
	TIME [epoch: 2.64 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015767015968490113		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.015767015968490113 | validation: 0.02466972317430154]
	TIME [epoch: 2.66 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016607979597793775		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.016607979597793775 | validation: 0.029168860957814603]
	TIME [epoch: 2.65 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016253266428338263		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.016253266428338263 | validation: 0.02752878280647643]
	TIME [epoch: 2.65 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01595412664467934		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.01595412664467934 | validation: 0.026367203161045096]
	TIME [epoch: 2.66 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016551546532730353		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.016551546532730353 | validation: 0.026557483023216467]
	TIME [epoch: 2.65 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017169021572852713		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.017169021572852713 | validation: 0.027532330282699892]
	TIME [epoch: 2.66 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0165351028945697		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.0165351028945697 | validation: 0.027145281345351314]
	TIME [epoch: 2.66 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01715791855774462		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.01715791855774462 | validation: 0.033313546691020546]
	TIME [epoch: 2.66 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01912157474009484		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.01912157474009484 | validation: 0.028594162964301974]
	TIME [epoch: 2.65 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022603918834533544		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.022603918834533544 | validation: 0.028651198614459396]
	TIME [epoch: 2.65 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01841318400154941		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.01841318400154941 | validation: 0.02345561030201783]
	TIME [epoch: 2.65 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014633771894812036		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.014633771894812036 | validation: 0.024133459525032364]
	TIME [epoch: 2.65 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015985129674022845		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.015985129674022845 | validation: 0.022102092754982053]
	TIME [epoch: 2.66 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01636002452625635		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.01636002452625635 | validation: 0.023326660709150338]
	TIME [epoch: 2.66 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016431069153914946		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.016431069153914946 | validation: 0.024235529442352178]
	TIME [epoch: 2.66 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016628842677949177		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.016628842677949177 | validation: 0.024325297951182503]
	TIME [epoch: 2.64 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0171112986460087		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.0171112986460087 | validation: 0.025163589714474945]
	TIME [epoch: 2.65 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017102724037240256		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.017102724037240256 | validation: 0.028265618925796224]
	TIME [epoch: 2.65 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0168407088376964		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.0168407088376964 | validation: 0.02765146008791132]
	TIME [epoch: 2.65 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017040206255282134		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.017040206255282134 | validation: 0.029313894822474252]
	TIME [epoch: 2.65 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017207881865124664		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.017207881865124664 | validation: 0.026694508241819927]
	TIME [epoch: 2.66 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017170643143959018		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.017170643143959018 | validation: 0.02941340314651767]
	TIME [epoch: 2.65 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01703082729936338		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.01703082729936338 | validation: 0.028104838694850698]
	TIME [epoch: 2.65 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017570799590707058		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.017570799590707058 | validation: 0.024024984951004892]
	TIME [epoch: 2.65 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016556658897258766		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.016556658897258766 | validation: 0.028952411259999145]
	TIME [epoch: 2.65 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015309147766129811		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.015309147766129811 | validation: 0.026881565062006643]
	TIME [epoch: 2.65 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015474623036700347		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.015474623036700347 | validation: 0.021593467548018577]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01527170285898617		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.01527170285898617 | validation: 0.021222485402759506]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01605116834794875		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.01605116834794875 | validation: 0.025773534775604845]
	TIME [epoch: 2.66 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015346842832085863		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.015346842832085863 | validation: 0.026072225750052726]
	TIME [epoch: 2.67 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018521873808622424		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.018521873808622424 | validation: 0.02856235681079762]
	TIME [epoch: 2.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018333762403136934		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.018333762403136934 | validation: 0.02266178697422169]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01684029358451836		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.01684029358451836 | validation: 0.025038660302473037]
	TIME [epoch: 2.66 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015134908489774643		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.015134908489774643 | validation: 0.024051193629107084]
	TIME [epoch: 2.66 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017334351417231993		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.017334351417231993 | validation: 0.030512657257750223]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01775843024707336		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.01775843024707336 | validation: 0.02438457075232251]
	TIME [epoch: 2.66 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018228209071975667		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.018228209071975667 | validation: 0.027358267777796832]
	TIME [epoch: 2.66 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01537230825997737		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.01537230825997737 | validation: 0.02217182391785205]
	TIME [epoch: 2.66 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015269752259194299		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.015269752259194299 | validation: 0.02131047199604921]
	TIME [epoch: 2.67 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014799387258458044		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.014799387258458044 | validation: 0.024900813778944677]
	TIME [epoch: 2.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015185754920676963		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.015185754920676963 | validation: 0.023003857083770774]
	TIME [epoch: 2.66 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01486782442770328		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.01486782442770328 | validation: 0.022566923965478925]
	TIME [epoch: 2.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014684154337297924		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.014684154337297924 | validation: 0.020038531325967557]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016041822876069204		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.016041822876069204 | validation: 0.027533484702380907]
	TIME [epoch: 2.67 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01601641695857015		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.01601641695857015 | validation: 0.02317656795727424]
	TIME [epoch: 2.68 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015338374140807803		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.015338374140807803 | validation: 0.023662444247903702]
	TIME [epoch: 2.66 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01674800291916922		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.01674800291916922 | validation: 0.03121050743813333]
	TIME [epoch: 2.67 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01858105656216095		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.01858105656216095 | validation: 0.02967895984804425]
	TIME [epoch: 2.66 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017489723390069507		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.017489723390069507 | validation: 0.025565974070842958]
	TIME [epoch: 2.67 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017892593237446436		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.017892593237446436 | validation: 0.02637940340029178]
	TIME [epoch: 2.66 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015803424747464004		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.015803424747464004 | validation: 0.022194992637887757]
	TIME [epoch: 2.66 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015741276494767857		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.015741276494767857 | validation: 0.02189773653593339]
	TIME [epoch: 2.67 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015024759394933815		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.015024759394933815 | validation: 0.02446821496213225]
	TIME [epoch: 2.66 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015783602991784062		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.015783602991784062 | validation: 0.022130936803147806]
	TIME [epoch: 2.67 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014663307751060521		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.014663307751060521 | validation: 0.026221089463233583]
	TIME [epoch: 2.66 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015363614489101108		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.015363614489101108 | validation: 0.022374037564207184]
	TIME [epoch: 2.66 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01611397634935316		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.01611397634935316 | validation: 0.0235244603959369]
	TIME [epoch: 2.66 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014449664227404525		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.014449664227404525 | validation: 0.024083028465833302]
	TIME [epoch: 2.67 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01573605741031106		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.01573605741031106 | validation: 0.026265308669433874]
	TIME [epoch: 2.66 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015097285025208863		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.015097285025208863 | validation: 0.02206730192536941]
	TIME [epoch: 2.66 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01442641504015505		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.01442641504015505 | validation: 0.021991763957922707]
	TIME [epoch: 2.65 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016126692631500735		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.016126692631500735 | validation: 0.022704924899528457]
	TIME [epoch: 2.66 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015583858826114093		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.015583858826114093 | validation: 0.024498144618174247]
	TIME [epoch: 2.66 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01400001562233029		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.01400001562233029 | validation: 0.022085087807748063]
	TIME [epoch: 2.66 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014526266232457192		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.014526266232457192 | validation: 0.029898859808374548]
	TIME [epoch: 2.66 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014881320281347898		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.014881320281347898 | validation: 0.02299766771073568]
	TIME [epoch: 2.65 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013994189639336175		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.013994189639336175 | validation: 0.02520909120324524]
	TIME [epoch: 2.65 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015079420253161993		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.015079420253161993 | validation: 0.027571684775785223]
	TIME [epoch: 2.65 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018446974028556464		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.018446974028556464 | validation: 0.025205203483536456]
	TIME [epoch: 2.65 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017854233823623358		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.017854233823623358 | validation: 0.022504825443202772]
	TIME [epoch: 2.65 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015654614965511418		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.015654614965511418 | validation: 0.028567462777608823]
	TIME [epoch: 2.65 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016236488175490237		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.016236488175490237 | validation: 0.023219471666535554]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015300938466217136		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.015300938466217136 | validation: 0.020327157006241603]
	TIME [epoch: 2.65 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015480836113179906		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.015480836113179906 | validation: 0.022739409164042313]
	TIME [epoch: 2.65 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014679995730164468		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.014679995730164468 | validation: 0.021336678672604848]
	TIME [epoch: 2.65 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01517770597455335		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.01517770597455335 | validation: 0.021519101565029022]
	TIME [epoch: 2.66 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014604009776746502		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.014604009776746502 | validation: 0.017477593980068852]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_893.pth
	Model improved!!!
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014531746734729276		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.014531746734729276 | validation: 0.02479119622905558]
	TIME [epoch: 2.65 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01629821926895829		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.01629821926895829 | validation: 0.022772649989225038]
	TIME [epoch: 2.65 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01631835314025375		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.01631835314025375 | validation: 0.021019705193369754]
	TIME [epoch: 2.65 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015766428138127		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.015766428138127 | validation: 0.02141932052398339]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014992250483591785		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.014992250483591785 | validation: 0.025324816171973]
	TIME [epoch: 2.65 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0164624220161185		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.0164624220161185 | validation: 0.01981185728638285]
	TIME [epoch: 2.66 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014500443898428108		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.014500443898428108 | validation: 0.02046796331008002]
	TIME [epoch: 2.66 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016415281075068445		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.016415281075068445 | validation: 0.01911813245296186]
	TIME [epoch: 2.66 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015240424662013724		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.015240424662013724 | validation: 0.022720138223837817]
	TIME [epoch: 2.65 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014090496715863668		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.014090496715863668 | validation: 0.024698686263622507]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014407243907533535		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.014407243907533535 | validation: 0.02270821258806155]
	TIME [epoch: 2.67 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014093357522361028		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.014093357522361028 | validation: 0.024646016406380002]
	TIME [epoch: 2.65 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013945439930940344		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.013945439930940344 | validation: 0.024834019180575862]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014968144861566503		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.014968144861566503 | validation: 0.019611500071327472]
	TIME [epoch: 2.65 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014029518159523557		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.014029518159523557 | validation: 0.024497997191191348]
	TIME [epoch: 2.66 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015246979807082055		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.015246979807082055 | validation: 0.024332098533842994]
	TIME [epoch: 2.65 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01616703424534072		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.01616703424534072 | validation: 0.022887621131694482]
	TIME [epoch: 2.66 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0142226580045481		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.0142226580045481 | validation: 0.024182939856363585]
	TIME [epoch: 2.66 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01580766886108274		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.01580766886108274 | validation: 0.022695077869953884]
	TIME [epoch: 2.66 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015327086302859018		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.015327086302859018 | validation: 0.021005512787414228]
	TIME [epoch: 2.65 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013867311018930444		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.013867311018930444 | validation: 0.024812933821281503]
	TIME [epoch: 2.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014052391189721954		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.014052391189721954 | validation: 0.026155840539861253]
	TIME [epoch: 2.65 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01471045322936246		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.01471045322936246 | validation: 0.019904184376456736]
	TIME [epoch: 2.65 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013797188983629851		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.013797188983629851 | validation: 0.02019450732977004]
	TIME [epoch: 2.65 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01385503442516967		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.01385503442516967 | validation: 0.02165496303625516]
	TIME [epoch: 2.66 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013792532307806566		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.013792532307806566 | validation: 0.022211872598572793]
	TIME [epoch: 2.65 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014356429615824897		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.014356429615824897 | validation: 0.024615403293269212]
	TIME [epoch: 2.66 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015044967603634914		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.015044967603634914 | validation: 0.02517386722254509]
	TIME [epoch: 2.66 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015551021488959014		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.015551021488959014 | validation: 0.020242704608510076]
	TIME [epoch: 2.65 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015618294726101208		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.015618294726101208 | validation: 0.022972401367149432]
	TIME [epoch: 2.66 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014524916137417306		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.014524916137417306 | validation: 0.01742092506112567]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_924.pth
	Model improved!!!
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014585606180379192		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.014585606180379192 | validation: 0.021059941882790468]
	TIME [epoch: 2.65 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012331951855519758		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.012331951855519758 | validation: 0.02165081844736553]
	TIME [epoch: 2.66 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013158759626315422		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.013158759626315422 | validation: 0.01992962188508951]
	TIME [epoch: 2.65 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01418715822062092		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.01418715822062092 | validation: 0.02104879869210662]
	TIME [epoch: 2.66 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013839045838569488		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.013839045838569488 | validation: 0.022971920111010548]
	TIME [epoch: 2.65 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015510411688130294		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.015510411688130294 | validation: 0.026785193186823233]
	TIME [epoch: 2.64 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012996552504492196		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.012996552504492196 | validation: 0.02121163291440259]
	TIME [epoch: 2.64 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013100869392103065		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.013100869392103065 | validation: 0.02050147442883864]
	TIME [epoch: 2.65 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015103902646574668		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.015103902646574668 | validation: 0.0258702580211016]
	TIME [epoch: 2.67 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015119035364345787		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.015119035364345787 | validation: 0.021794093614002332]
	TIME [epoch: 2.66 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0135777718731498		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.0135777718731498 | validation: 0.02107952164967184]
	TIME [epoch: 2.65 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013894623478847469		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.013894623478847469 | validation: 0.022314249121941777]
	TIME [epoch: 2.65 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013331154169914485		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.013331154169914485 | validation: 0.01905460907248918]
	TIME [epoch: 2.66 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01392439483004579		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.01392439483004579 | validation: 0.02100854755559044]
	TIME [epoch: 2.66 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01314019298396021		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.01314019298396021 | validation: 0.018083051355936575]
	TIME [epoch: 2.65 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013729976948318755		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.013729976948318755 | validation: 0.01753201322006134]
	TIME [epoch: 2.65 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012246523285704546		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.012246523285704546 | validation: 0.01937088702845781]
	TIME [epoch: 2.65 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012816900427004141		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.012816900427004141 | validation: 0.021864338461409907]
	TIME [epoch: 2.66 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015086458923370894		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.015086458923370894 | validation: 0.021282756448164443]
	TIME [epoch: 2.66 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01597826102374699		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.01597826102374699 | validation: 0.02176931772245999]
	TIME [epoch: 2.66 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014320556834100247		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.014320556834100247 | validation: 0.021960082781890614]
	TIME [epoch: 2.66 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014326339085704866		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.014326339085704866 | validation: 0.01981866200140674]
	TIME [epoch: 2.67 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012743583488104839		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.012743583488104839 | validation: 0.023263402412597237]
	TIME [epoch: 2.67 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014179024947777697		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.014179024947777697 | validation: 0.01736465230486044]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_948.pth
	Model improved!!!
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013120966184597429		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.013120966184597429 | validation: 0.018236058173789794]
	TIME [epoch: 2.66 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013394681190410753		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.013394681190410753 | validation: 0.020474285134002002]
	TIME [epoch: 2.66 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012532717131392219		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.012532717131392219 | validation: 0.018941102033009107]
	TIME [epoch: 2.66 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013096225510143094		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.013096225510143094 | validation: 0.020740851827941355]
	TIME [epoch: 2.67 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013889961797086474		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.013889961797086474 | validation: 0.024713267014545882]
	TIME [epoch: 2.66 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013370815751200667		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.013370815751200667 | validation: 0.0186619337375838]
	TIME [epoch: 2.66 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013246648030867107		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.013246648030867107 | validation: 0.019349499971439157]
	TIME [epoch: 2.65 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014858069558992236		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.014858069558992236 | validation: 0.018736404000745444]
	TIME [epoch: 2.65 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012422996883228064		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.012422996883228064 | validation: 0.019141973191499108]
	TIME [epoch: 2.65 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013260416621266874		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.013260416621266874 | validation: 0.02473856462317847]
	TIME [epoch: 2.66 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013643780739233849		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.013643780739233849 | validation: 0.01880920058223895]
	TIME [epoch: 2.66 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013223363051760015		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.013223363051760015 | validation: 0.018372432069236767]
	TIME [epoch: 2.66 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013578325466684143		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.013578325466684143 | validation: 0.02153901837913266]
	TIME [epoch: 2.66 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013796648798363573		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.013796648798363573 | validation: 0.024788098029565466]
	TIME [epoch: 2.66 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016205382646540093		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.016205382646540093 | validation: 0.024747164879576868]
	TIME [epoch: 2.66 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015119073641843539		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.015119073641843539 | validation: 0.017853981232102446]
	TIME [epoch: 2.66 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013137323542011834		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.013137323542011834 | validation: 0.019033577434829963]
	TIME [epoch: 2.66 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013632402121539617		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.013632402121539617 | validation: 0.018660457010393705]
	TIME [epoch: 2.65 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013093211368623008		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.013093211368623008 | validation: 0.016990781389358677]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_967.pth
	Model improved!!!
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012888911131114381		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.012888911131114381 | validation: 0.018562383674494875]
	TIME [epoch: 2.66 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013348119901433227		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.013348119901433227 | validation: 0.019352540658557828]
	TIME [epoch: 2.66 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013121414683616385		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.013121414683616385 | validation: 0.01861201594097848]
	TIME [epoch: 2.67 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01282833891091522		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.01282833891091522 | validation: 0.020794341804207694]
	TIME [epoch: 2.66 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013407269740283216		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.013407269740283216 | validation: 0.024339581288658808]
	TIME [epoch: 2.65 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012893106984147269		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.012893106984147269 | validation: 0.020685404910928976]
	TIME [epoch: 2.67 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013184292879265053		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.013184292879265053 | validation: 0.02266370691531494]
	TIME [epoch: 2.66 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01310101292122043		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.01310101292122043 | validation: 0.019800468723482224]
	TIME [epoch: 2.65 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013451537469433386		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.013451537469433386 | validation: 0.02184107757060995]
	TIME [epoch: 2.66 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01191124273240118		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.01191124273240118 | validation: 0.017774827335641664]
	TIME [epoch: 2.65 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012484267859445428		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.012484267859445428 | validation: 0.020630374474795345]
	TIME [epoch: 2.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012831084136312657		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.012831084136312657 | validation: 0.019793750529814847]
	TIME [epoch: 2.65 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01404640369383868		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.01404640369383868 | validation: 0.019219851218129493]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012634726309532334		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.012634726309532334 | validation: 0.022983116425313668]
	TIME [epoch: 2.65 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013996775377184828		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.013996775377184828 | validation: 0.018989636206562435]
	TIME [epoch: 2.66 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013552005376903811		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.013552005376903811 | validation: 0.02452172087414738]
	TIME [epoch: 2.66 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014718831628363236		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.014718831628363236 | validation: 0.01634404644173758]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_984.pth
	Model improved!!!
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013263155526443509		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.013263155526443509 | validation: 0.021023668118230157]
	TIME [epoch: 2.65 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01440661337476967		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.01440661337476967 | validation: 0.01929304197581454]
	TIME [epoch: 2.66 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012517505588393112		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.012517505588393112 | validation: 0.01942803312864776]
	TIME [epoch: 2.66 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013842914340081221		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.013842914340081221 | validation: 0.020518830511870312]
	TIME [epoch: 2.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012088353529111414		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.012088353529111414 | validation: 0.022045033354257573]
	TIME [epoch: 2.66 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013865901151448314		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.013865901151448314 | validation: 0.02373704214208057]
	TIME [epoch: 2.66 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013731053739335388		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.013731053739335388 | validation: 0.017248314303524893]
	TIME [epoch: 2.66 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012166243130544315		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.012166243130544315 | validation: 0.0155920259258449]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01362169254066055		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.01362169254066055 | validation: 0.019659626682309708]
	TIME [epoch: 2.65 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012572106674506438		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.012572106674506438 | validation: 0.019404445389968297]
	TIME [epoch: 2.66 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013195771581249321		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.013195771581249321 | validation: 0.024213446016772888]
	TIME [epoch: 2.65 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014331468644339727		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.014331468644339727 | validation: 0.01613182590922606]
	TIME [epoch: 2.66 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012302362749152094		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.012302362749152094 | validation: 0.018704480834817218]
	TIME [epoch: 2.66 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01251947901499206		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.01251947901499206 | validation: 0.015309500665274145]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_998.pth
	Model improved!!!
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012565226683243242		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.012565226683243242 | validation: 0.018006822031378444]
	TIME [epoch: 2.65 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012981366306025923		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.012981366306025923 | validation: 0.019198447003654542]
	TIME [epoch: 2.66 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012334433872132245		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.012334433872132245 | validation: 0.019464616890987097]
	TIME [epoch: 178 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012778937329178824		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.012778937329178824 | validation: 0.016566693993352023]
	TIME [epoch: 5.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01213731608994598		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.01213731608994598 | validation: 0.016725142643209356]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013766233642387273		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.013766233642387273 | validation: 0.016274013180948756]
	TIME [epoch: 5.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01322691589339655		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.01322691589339655 | validation: 0.0209572420092107]
	TIME [epoch: 5.69 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01343856201547994		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.01343856201547994 | validation: 0.0230961299720021]
	TIME [epoch: 5.69 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014297376597843638		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.014297376597843638 | validation: 0.020100228883592267]
	TIME [epoch: 5.73 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011889635398187938		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.011889635398187938 | validation: 0.0167293704254033]
	TIME [epoch: 5.73 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01185791733403584		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.01185791733403584 | validation: 0.01941369656828944]
	TIME [epoch: 5.73 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012270826925707596		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.012270826925707596 | validation: 0.023483074864558674]
	TIME [epoch: 5.74 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012521158317546499		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.012521158317546499 | validation: 0.019062161529222854]
	TIME [epoch: 5.71 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012168253147251895		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.012168253147251895 | validation: 0.015447417156755495]
	TIME [epoch: 5.72 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012425955292496934		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.012425955292496934 | validation: 0.018018788892575344]
	TIME [epoch: 5.71 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011938191304023777		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.011938191304023777 | validation: 0.01790722937349584]
	TIME [epoch: 5.71 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013271172191264786		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.013271172191264786 | validation: 0.0213865376858036]
	TIME [epoch: 5.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012892963069476853		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.012892963069476853 | validation: 0.020636576257076857]
	TIME [epoch: 5.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013540580126760573		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.013540580126760573 | validation: 0.018484411105398637]
	TIME [epoch: 5.73 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013210232529006653		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.013210232529006653 | validation: 0.016660372128692737]
	TIME [epoch: 5.71 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012491216410515649		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.012491216410515649 | validation: 0.01746489165341806]
	TIME [epoch: 5.72 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013709012861493758		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.013709012861493758 | validation: 0.017180095227129932]
	TIME [epoch: 5.72 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01175917558232096		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.01175917558232096 | validation: 0.017950637305288786]
	TIME [epoch: 5.74 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012735303845628243		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.012735303845628243 | validation: 0.015055379292076666]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0136133462674669		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.0136133462674669 | validation: 0.018604932234263228]
	TIME [epoch: 5.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011884297930311307		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.011884297930311307 | validation: 0.016204775901571544]
	TIME [epoch: 5.75 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012343315266868776		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.012343315266868776 | validation: 0.015257913752115215]
	TIME [epoch: 5.71 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012207291758381388		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.012207291758381388 | validation: 0.018803616855077566]
	TIME [epoch: 5.75 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011379924402089514		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.011379924402089514 | validation: 0.018563861724051924]
	TIME [epoch: 5.72 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012704328607067472		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.012704328607067472 | validation: 0.01893512646138187]
	TIME [epoch: 5.74 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012102652101462826		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.012102652101462826 | validation: 0.016120386079083737]
	TIME [epoch: 5.73 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012187130446200772		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.012187130446200772 | validation: 0.018121139268762142]
	TIME [epoch: 5.74 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012715439343291078		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.012715439343291078 | validation: 0.018610085468302586]
	TIME [epoch: 5.73 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01186602027466425		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.01186602027466425 | validation: 0.020338390726426337]
	TIME [epoch: 5.73 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012028573837987331		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.012028573837987331 | validation: 0.014903096511268456]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1033.pth
	Model improved!!!
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012250037229159195		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.012250037229159195 | validation: 0.017418457720717862]
	TIME [epoch: 5.74 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01239783412465699		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.01239783412465699 | validation: 0.021345857196917783]
	TIME [epoch: 5.72 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012080735332357869		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.012080735332357869 | validation: 0.01770816689188073]
	TIME [epoch: 5.73 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01275589871814297		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.01275589871814297 | validation: 0.018035370821688913]
	TIME [epoch: 5.73 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012213185407845297		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.012213185407845297 | validation: 0.016968712665893594]
	TIME [epoch: 5.74 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012489316075360697		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.012489316075360697 | validation: 0.01661067966569908]
	TIME [epoch: 5.73 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01203068228254582		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.01203068228254582 | validation: 0.017946544894317074]
	TIME [epoch: 5.73 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012080176223675321		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.012080176223675321 | validation: 0.017887272386931108]
	TIME [epoch: 5.73 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011850101050098014		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.011850101050098014 | validation: 0.01679344992800741]
	TIME [epoch: 5.74 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010936041272831987		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.010936041272831987 | validation: 0.016142853572538442]
	TIME [epoch: 5.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011268541048109246		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.011268541048109246 | validation: 0.01728490525267389]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011954778745972412		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.011954778745972412 | validation: 0.01833202472063297]
	TIME [epoch: 5.74 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012326573310205886		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.012326573310205886 | validation: 0.018608829908793346]
	TIME [epoch: 5.74 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012420607073557988		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.012420607073557988 | validation: 0.01600859051857917]
	TIME [epoch: 5.73 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01241175174489329		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.01241175174489329 | validation: 0.015900482888090473]
	TIME [epoch: 5.74 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012074316895114803		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.012074316895114803 | validation: 0.018386089943589624]
	TIME [epoch: 5.74 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011029880421889644		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.011029880421889644 | validation: 0.018774521502873776]
	TIME [epoch: 5.73 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012874502941997403		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.012874502941997403 | validation: 0.015343918393291613]
	TIME [epoch: 5.75 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012063412869860426		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.012063412869860426 | validation: 0.02011155151991011]
	TIME [epoch: 5.74 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011966000514984842		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.011966000514984842 | validation: 0.014958042856949039]
	TIME [epoch: 5.74 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012732492736285125		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.012732492736285125 | validation: 0.01979021820579574]
	TIME [epoch: 5.73 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013454149130044182		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.013454149130044182 | validation: 0.01891329289420961]
	TIME [epoch: 5.74 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012463439338109467		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.012463439338109467 | validation: 0.020637322611309097]
	TIME [epoch: 5.73 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012626715745439797		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.012626715745439797 | validation: 0.014719876094827855]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1057.pth
	Model improved!!!
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012347019372091227		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.012347019372091227 | validation: 0.01785129288400068]
	TIME [epoch: 5.73 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011355689783497673		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.011355689783497673 | validation: 0.019845890025146265]
	TIME [epoch: 5.75 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011936598005112002		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.011936598005112002 | validation: 0.016559851781907244]
	TIME [epoch: 5.73 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01167447851699135		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.01167447851699135 | validation: 0.016036077401591686]
	TIME [epoch: 5.72 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011239390569227051		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.011239390569227051 | validation: 0.017971023994771896]
	TIME [epoch: 5.73 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011667439336547956		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.011667439336547956 | validation: 0.016790798003522324]
	TIME [epoch: 5.73 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012069332344564318		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.012069332344564318 | validation: 0.01908552301171218]
	TIME [epoch: 5.73 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011810874745921619		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.011810874745921619 | validation: 0.018796070884568528]
	TIME [epoch: 5.73 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011722179671424042		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.011722179671424042 | validation: 0.0176622358188818]
	TIME [epoch: 5.73 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011909649901249828		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.011909649901249828 | validation: 0.01653732381777503]
	TIME [epoch: 5.75 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012288275486681656		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.012288275486681656 | validation: 0.013035506994959823]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1068.pth
	Model improved!!!
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013563333658258632		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.013563333658258632 | validation: 0.02095572378801154]
	TIME [epoch: 5.73 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012364825236092803		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.012364825236092803 | validation: 0.019480646867515307]
	TIME [epoch: 5.74 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012293615528539057		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.012293615528539057 | validation: 0.01675775008685798]
	TIME [epoch: 5.74 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012077296442326014		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.012077296442326014 | validation: 0.01399982371465578]
	TIME [epoch: 5.74 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011621440591836793		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.011621440591836793 | validation: 0.01806374466349242]
	TIME [epoch: 5.73 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01173814661458257		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.01173814661458257 | validation: 0.014500092380229269]
	TIME [epoch: 5.75 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012215436720601965		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.012215436720601965 | validation: 0.019471403825102054]
	TIME [epoch: 5.73 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012354432559180962		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.012354432559180962 | validation: 0.01677856905566234]
	TIME [epoch: 5.75 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011752280371869131		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.011752280371869131 | validation: 0.014835050513147886]
	TIME [epoch: 5.74 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011673681283832065		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.011673681283832065 | validation: 0.012352563295330132]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1078.pth
	Model improved!!!
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011823006219477112		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.011823006219477112 | validation: 0.017016168053517123]
	TIME [epoch: 5.73 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013244734853705404		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.013244734853705404 | validation: 0.015344247695941116]
	TIME [epoch: 5.73 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012039336848333429		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.012039336848333429 | validation: 0.017982984300190987]
	TIME [epoch: 5.74 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011607863812033315		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.011607863812033315 | validation: 0.015650396047252137]
	TIME [epoch: 5.73 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011284766258721669		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.011284766258721669 | validation: 0.018882472461808165]
	TIME [epoch: 5.75 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011422294735358914		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.011422294735358914 | validation: 0.015069820592652572]
	TIME [epoch: 5.73 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011523661439395292		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.011523661439395292 | validation: 0.01384782238257566]
	TIME [epoch: 5.74 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011013979305921812		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.011013979305921812 | validation: 0.015890988280964646]
	TIME [epoch: 5.76 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011084619373180385		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.011084619373180385 | validation: 0.015468669020959613]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01142177392463524		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.01142177392463524 | validation: 0.02303036464861783]
	TIME [epoch: 5.74 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010622959952848583		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.010622959952848583 | validation: 0.016552172134766886]
	TIME [epoch: 5.74 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0112025099299879		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0112025099299879 | validation: 0.016747864001312254]
	TIME [epoch: 5.73 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01140175694671558		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.01140175694671558 | validation: 0.015660662616311185]
	TIME [epoch: 5.74 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011127509448130653		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.011127509448130653 | validation: 0.019613663623081923]
	TIME [epoch: 5.73 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011807589941785226		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.011807589941785226 | validation: 0.01620674664632904]
	TIME [epoch: 5.73 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01161543985048969		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.01161543985048969 | validation: 0.01440920622486589]
	TIME [epoch: 5.74 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010343749350937397		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.010343749350937397 | validation: 0.016225325799465683]
	TIME [epoch: 5.73 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010619123076945923		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.010619123076945923 | validation: 0.015688479901056873]
	TIME [epoch: 5.74 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011647942196552314		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.011647942196552314 | validation: 0.015553103796474422]
	TIME [epoch: 5.73 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010901758819957887		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.010901758819957887 | validation: 0.019239353186186817]
	TIME [epoch: 5.73 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01058507130173761		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.01058507130173761 | validation: 0.016562954643885363]
	TIME [epoch: 5.74 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010711427598167176		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.010711427598167176 | validation: 0.018578604998310655]
	TIME [epoch: 5.74 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011292385289304102		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.011292385289304102 | validation: 0.018354616031830286]
	TIME [epoch: 5.73 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011504363038509768		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.011504363038509768 | validation: 0.015947459761310057]
	TIME [epoch: 5.73 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01060677568935556		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.01060677568935556 | validation: 0.018813536535334487]
	TIME [epoch: 5.76 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01077965273571024		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.01077965273571024 | validation: 0.01839976947213995]
	TIME [epoch: 5.73 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010860644199336642		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.010860644199336642 | validation: 0.015655908835578127]
	TIME [epoch: 5.73 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011343623946380173		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.011343623946380173 | validation: 0.01480488441940252]
	TIME [epoch: 5.74 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011938612397117321		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.011938612397117321 | validation: 0.013998928642903963]
	TIME [epoch: 5.74 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010368706945120057		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.010368706945120057 | validation: 0.01532590613234931]
	TIME [epoch: 5.73 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01052760139297573		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.01052760139297573 | validation: 0.014196043087011723]
	TIME [epoch: 5.73 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010819570018033534		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.010819570018033534 | validation: 0.014569956656030215]
	TIME [epoch: 5.74 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012109062175680956		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.012109062175680956 | validation: 0.016215903974586766]
	TIME [epoch: 5.72 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012596790864251925		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.012596790864251925 | validation: 0.015160353257173177]
	TIME [epoch: 5.74 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010775918891601542		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.010775918891601542 | validation: 0.01655922623967654]
	TIME [epoch: 5.74 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010944569504733352		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.010944569504733352 | validation: 0.016168486006773643]
	TIME [epoch: 5.73 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012069642872104205		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.012069642872104205 | validation: 0.018436635396373902]
	TIME [epoch: 5.73 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01130199753414132		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.01130199753414132 | validation: 0.014829510149297198]
	TIME [epoch: 5.74 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01115126545476434		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.01115126545476434 | validation: 0.01987417600866046]
	TIME [epoch: 5.73 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011143565469323548		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.011143565469323548 | validation: 0.015286501794409026]
	TIME [epoch: 5.73 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011049140898632853		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.011049140898632853 | validation: 0.01562323088988965]
	TIME [epoch: 5.74 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011016746333958353		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.011016746333958353 | validation: 0.01540780255457469]
	TIME [epoch: 5.72 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011430900601447712		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.011430900601447712 | validation: 0.01416454971249963]
	TIME [epoch: 5.75 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010914007379063687		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.010914007379063687 | validation: 0.014025731057820057]
	TIME [epoch: 5.74 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011463831211258314		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.011463831211258314 | validation: 0.01759339008478067]
	TIME [epoch: 5.76 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011550786601695088		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.011550786601695088 | validation: 0.013622361704449493]
	TIME [epoch: 5.73 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01040281317001507		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.01040281317001507 | validation: 0.013957340883227243]
	TIME [epoch: 5.76 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011975100003701902		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.011975100003701902 | validation: 0.015362790503332335]
	TIME [epoch: 5.74 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01081244248952762		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.01081244248952762 | validation: 0.01599590679482219]
	TIME [epoch: 5.75 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01171421610946121		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.01171421610946121 | validation: 0.016723642580046973]
	TIME [epoch: 5.75 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01155898680907506		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.01155898680907506 | validation: 0.01440638920980144]
	TIME [epoch: 5.74 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011435287628853787		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.011435287628853787 | validation: 0.01640811189516298]
	TIME [epoch: 5.73 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010911858508174284		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.010911858508174284 | validation: 0.015113533745828356]
	TIME [epoch: 5.75 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011071643299097265		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.011071643299097265 | validation: 0.015285545723160876]
	TIME [epoch: 5.73 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011787584537604938		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.011787584537604938 | validation: 0.013445097762047066]
	TIME [epoch: 5.73 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011865516670953938		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.011865516670953938 | validation: 0.018104527652015312]
	TIME [epoch: 5.73 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01077572476400897		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.01077572476400897 | validation: 0.01851781305794873]
	TIME [epoch: 5.73 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011317267707950825		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.011317267707950825 | validation: 0.01687880186195321]
	TIME [epoch: 5.72 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010740072675102274		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.010740072675102274 | validation: 0.01551988299714755]
	TIME [epoch: 5.73 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011923843696042234		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.011923843696042234 | validation: 0.01762732168343897]
	TIME [epoch: 5.73 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011001669493223898		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.011001669493223898 | validation: 0.01637604502523521]
	TIME [epoch: 5.73 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011563645241909249		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.011563645241909249 | validation: 0.016747774753011736]
	TIME [epoch: 5.74 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01067436659961489		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.01067436659961489 | validation: 0.012698923850059319]
	TIME [epoch: 5.75 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010179486228144899		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.010179486228144899 | validation: 0.014122788652288821]
	TIME [epoch: 5.74 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011993648946027384		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.011993648946027384 | validation: 0.01991081252512147]
	TIME [epoch: 5.74 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010839698038794752		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.010839698038794752 | validation: 0.014766976505486973]
	TIME [epoch: 5.74 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010165903925806687		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.010165903925806687 | validation: 0.01565791357731209]
	TIME [epoch: 5.74 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010953426799644598		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.010953426799644598 | validation: 0.01589382717861835]
	TIME [epoch: 5.74 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010924551396819547		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.010924551396819547 | validation: 0.014334474422410183]
	TIME [epoch: 5.74 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011232694992180358		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.011232694992180358 | validation: 0.016268633512050146]
	TIME [epoch: 5.73 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011192240516073237		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.011192240516073237 | validation: 0.013752594953457166]
	TIME [epoch: 5.74 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011460865149920467		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.011460865149920467 | validation: 0.01660056874676713]
	TIME [epoch: 5.74 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011237007081833129		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.011237007081833129 | validation: 0.015673033540769876]
	TIME [epoch: 5.75 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010536439127470765		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.010536439127470765 | validation: 0.014020644304562957]
	TIME [epoch: 5.74 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011144989086941164		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.011144989086941164 | validation: 0.014261759290278021]
	TIME [epoch: 5.74 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009949403525568378		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.009949403525568378 | validation: 0.017797300921110405]
	TIME [epoch: 5.74 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010071580789090696		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.010071580789090696 | validation: 0.012675985439495097]
	TIME [epoch: 5.74 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010725172783155617		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.010725172783155617 | validation: 0.01609516831920852]
	TIME [epoch: 5.73 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010448695956548175		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.010448695956548175 | validation: 0.01664998833973702]
	TIME [epoch: 5.74 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010246387091910813		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.010246387091910813 | validation: 0.016437175025611317]
	TIME [epoch: 5.73 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010931969800332095		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.010931969800332095 | validation: 0.016116018556721123]
	TIME [epoch: 5.74 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010451241165359778		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.010451241165359778 | validation: 0.01716554097477816]
	TIME [epoch: 5.74 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010553951414725277		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.010553951414725277 | validation: 0.016811879071875813]
	TIME [epoch: 5.73 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011307953405977605		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.011307953405977605 | validation: 0.014344247732523064]
	TIME [epoch: 5.74 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010386752183295668		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.010386752183295668 | validation: 0.016220992665208312]
	TIME [epoch: 5.73 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010575140181670321		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.010575140181670321 | validation: 0.012306656721515485]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1164.pth
	Model improved!!!
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009385870262815958		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.009385870262815958 | validation: 0.013712484062667297]
	TIME [epoch: 5.73 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00991752239831149		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.00991752239831149 | validation: 0.018045782978720337]
	TIME [epoch: 5.74 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010212931995728298		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.010212931995728298 | validation: 0.015447635961537165]
	TIME [epoch: 5.73 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011180327330179863		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.011180327330179863 | validation: 0.015590439895792963]
	TIME [epoch: 5.74 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010222421529125914		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.010222421529125914 | validation: 0.014638610352668502]
	TIME [epoch: 5.74 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011182891965873694		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.011182891965873694 | validation: 0.015992044421345022]
	TIME [epoch: 5.74 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010131309114262686		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.010131309114262686 | validation: 0.016980622956234048]
	TIME [epoch: 5.74 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010538918900844565		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.010538918900844565 | validation: 0.01747532096170983]
	TIME [epoch: 5.72 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01120411767211881		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.01120411767211881 | validation: 0.01451997474868162]
	TIME [epoch: 5.74 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010483441736613473		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.010483441736613473 | validation: 0.014294209164228844]
	TIME [epoch: 5.72 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01127295501023485		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.01127295501023485 | validation: 0.014264571284135663]
	TIME [epoch: 5.74 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011091270449788557		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.011091270449788557 | validation: 0.015546005260683582]
	TIME [epoch: 5.74 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009933454027913208		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.009933454027913208 | validation: 0.013969392173403728]
	TIME [epoch: 5.75 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011072011928259391		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.011072011928259391 | validation: 0.011596427672245514]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1178.pth
	Model improved!!!
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010903027351549075		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.010903027351549075 | validation: 0.01671557770380072]
	TIME [epoch: 5.74 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011334658376409075		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.011334658376409075 | validation: 0.013751238903548381]
	TIME [epoch: 5.74 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010245676056786581		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.010245676056786581 | validation: 0.015015019858572854]
	TIME [epoch: 5.75 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011028690906485004		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.011028690906485004 | validation: 0.01632077301386389]
	TIME [epoch: 5.74 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010253273266702003		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.010253273266702003 | validation: 0.014810855790499223]
	TIME [epoch: 5.74 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011141731087346824		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.011141731087346824 | validation: 0.015037272726546404]
	TIME [epoch: 5.74 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01094750137347012		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.01094750137347012 | validation: 0.014015110571727364]
	TIME [epoch: 5.74 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01090688281693372		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.01090688281693372 | validation: 0.01600821345913053]
	TIME [epoch: 5.73 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010889903272995832		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.010889903272995832 | validation: 0.014201347248140594]
	TIME [epoch: 5.74 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00965593115505701		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.00965593115505701 | validation: 0.017287866679305097]
	TIME [epoch: 5.74 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00978690979723267		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.00978690979723267 | validation: 0.013670883999822171]
	TIME [epoch: 5.73 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010634350588604029		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.010634350588604029 | validation: 0.014124135555379625]
	TIME [epoch: 5.74 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010316824856527516		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.010316824856527516 | validation: 0.015614729540813511]
	TIME [epoch: 5.74 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010430511012770611		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.010430511012770611 | validation: 0.01477873517749706]
	TIME [epoch: 5.74 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010375978812649915		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.010375978812649915 | validation: 0.015107004150061543]
	TIME [epoch: 5.74 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009880098312451315		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.009880098312451315 | validation: 0.014912875185592523]
	TIME [epoch: 5.74 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009981458204441653		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.009981458204441653 | validation: 0.015083041825590948]
	TIME [epoch: 5.74 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009903488496280128		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.009903488496280128 | validation: 0.012383588461196493]
	TIME [epoch: 5.74 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009515958932253914		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.009515958932253914 | validation: 0.014396794859601492]
	TIME [epoch: 5.73 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010065019340510442		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.010065019340510442 | validation: 0.013680873906477875]
	TIME [epoch: 5.74 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009606935522747525		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.009606935522747525 | validation: 0.01448860486657485]
	TIME [epoch: 5.74 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010220201383203493		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.010220201383203493 | validation: 0.015159309925619669]
	TIME [epoch: 5.73 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010958199288284252		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.010958199288284252 | validation: 0.016220996931457465]
	TIME [epoch: 5.73 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010030185005155961		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.010030185005155961 | validation: 0.01373984592547688]
	TIME [epoch: 5.73 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010623507795746905		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.010623507795746905 | validation: 0.014129906618480504]
	TIME [epoch: 5.74 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01045114637067823		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.01045114637067823 | validation: 0.015127812484571235]
	TIME [epoch: 5.73 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010097266065553247		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.010097266065553247 | validation: 0.01576120675345939]
	TIME [epoch: 5.73 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00971537678619631		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.00971537678619631 | validation: 0.013221837182655793]
	TIME [epoch: 5.73 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010592872273150662		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.010592872273150662 | validation: 0.014611715974316165]
	TIME [epoch: 5.74 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009971071110827465		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.009971071110827465 | validation: 0.014205864104850542]
	TIME [epoch: 5.74 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01079861963527144		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.01079861963527144 | validation: 0.013261046779141628]
	TIME [epoch: 5.73 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009723876602889973		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.009723876602889973 | validation: 0.015884450472559652]
	TIME [epoch: 5.75 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009558440937514328		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.009558440937514328 | validation: 0.012474631357800782]
	TIME [epoch: 5.74 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010312361795813714		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.010312361795813714 | validation: 0.018435722356069818]
	TIME [epoch: 5.72 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010081694408041025		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.010081694408041025 | validation: 0.0141237156671162]
	TIME [epoch: 5.74 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00909728079045468		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.00909728079045468 | validation: 0.013435933294078085]
	TIME [epoch: 5.72 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010177030075295797		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.010177030075295797 | validation: 0.015179462849339754]
	TIME [epoch: 5.74 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010130798977183454		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.010130798977183454 | validation: 0.014601301475054418]
	TIME [epoch: 5.74 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010262629280342392		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.010262629280342392 | validation: 0.013745079587576148]
	TIME [epoch: 5.75 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011146395286050166		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.011146395286050166 | validation: 0.01406558557762283]
	TIME [epoch: 5.73 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010354216098169356		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.010354216098169356 | validation: 0.012461289584319003]
	TIME [epoch: 5.74 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009510956929236968		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.009510956929236968 | validation: 0.01663380442796757]
	TIME [epoch: 5.72 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01063910285608842		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.01063910285608842 | validation: 0.01292950524233767]
	TIME [epoch: 5.74 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010055986754557123		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.010055986754557123 | validation: 0.013877691139864756]
	TIME [epoch: 5.73 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00991795921018655		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.00991795921018655 | validation: 0.01414590376442626]
	TIME [epoch: 5.74 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009885533998817218		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.009885533998817218 | validation: 0.012790153046483344]
	TIME [epoch: 5.73 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00981871626865706		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.00981871626865706 | validation: 0.017982095359207608]
	TIME [epoch: 5.74 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010570741398759607		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.010570741398759607 | validation: 0.013624811364355994]
	TIME [epoch: 5.73 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010311122306210481		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.010311122306210481 | validation: 0.013706691973984686]
	TIME [epoch: 5.73 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009030051168001482		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.009030051168001482 | validation: 0.015579368557038087]
	TIME [epoch: 5.74 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01043459924099259		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.01043459924099259 | validation: 0.013233802253027683]
	TIME [epoch: 5.74 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009374630480825149		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.009374630480825149 | validation: 0.01141656041856588]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1230.pth
	Model improved!!!
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01042387952929157		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.01042387952929157 | validation: 0.015090956808082224]
	TIME [epoch: 5.74 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010322682471180573		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.010322682471180573 | validation: 0.016096448063315394]
	TIME [epoch: 5.74 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009876361701088014		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.009876361701088014 | validation: 0.01578755497741774]
	TIME [epoch: 5.72 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009875383647463811		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.009875383647463811 | validation: 0.016639636215833833]
	TIME [epoch: 5.73 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009668756147539126		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.009668756147539126 | validation: 0.01277105410511058]
	TIME [epoch: 5.73 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009446779060924941		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.009446779060924941 | validation: 0.012551983755787234]
	TIME [epoch: 5.74 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009859755636906399		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.009859755636906399 | validation: 0.01363155976820174]
	TIME [epoch: 5.73 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010256237441445269		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.010256237441445269 | validation: 0.015854113377114266]
	TIME [epoch: 5.73 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009977649876262481		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.009977649876262481 | validation: 0.016010235204762958]
	TIME [epoch: 5.74 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010539813511647914		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.010539813511647914 | validation: 0.013153528936992499]
	TIME [epoch: 5.74 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009784649469552957		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.009784649469552957 | validation: 0.013808569662093163]
	TIME [epoch: 5.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009883103330052325		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.009883103330052325 | validation: 0.012901169242661038]
	TIME [epoch: 5.75 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008982391357794411		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.008982391357794411 | validation: 0.01273405152153191]
	TIME [epoch: 5.74 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00985881006673356		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.00985881006673356 | validation: 0.016516374605313234]
	TIME [epoch: 5.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01070597660446766		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.01070597660446766 | validation: 0.013294183940404204]
	TIME [epoch: 5.73 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00985265472431356		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.00985265472431356 | validation: 0.014410771280612822]
	TIME [epoch: 5.73 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009681398586353142		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.009681398586353142 | validation: 0.014733866401374208]
	TIME [epoch: 5.73 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009748740539530358		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.009748740539530358 | validation: 0.013093393860017144]
	TIME [epoch: 5.72 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00984025907812954		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.00984025907812954 | validation: 0.012647223011708243]
	TIME [epoch: 5.73 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009544135973772542		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.009544135973772542 | validation: 0.01435530949584053]
	TIME [epoch: 5.72 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009422812493536593		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.009422812493536593 | validation: 0.010716499126788749]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1251.pth
	Model improved!!!
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009882507109837804		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.009882507109837804 | validation: 0.01606571586042135]
	TIME [epoch: 5.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010320069546609424		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.010320069546609424 | validation: 0.0151636599272305]
	TIME [epoch: 5.72 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010028692696743904		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.010028692696743904 | validation: 0.011431416885975033]
	TIME [epoch: 5.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010149014762338489		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.010149014762338489 | validation: 0.016090220362044504]
	TIME [epoch: 5.73 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00949120959306444		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.00949120959306444 | validation: 0.014584495989656776]
	TIME [epoch: 5.71 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009697437302325078		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.009697437302325078 | validation: 0.01711729405095074]
	TIME [epoch: 5.74 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009895532165946512		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.009895532165946512 | validation: 0.012528328616370122]
	TIME [epoch: 5.71 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01036452978801468		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.01036452978801468 | validation: 0.013532298471547289]
	TIME [epoch: 5.74 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008682487184656244		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.008682487184656244 | validation: 0.014491917227482977]
	TIME [epoch: 5.73 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010709953244891225		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.010709953244891225 | validation: 0.016317918209842807]
	TIME [epoch: 5.73 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009849813332600049		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.009849813332600049 | validation: 0.013174539238230122]
	TIME [epoch: 5.72 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010245060978414938		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.010245060978414938 | validation: 0.013141748291825739]
	TIME [epoch: 5.73 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009117898740107235		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.009117898740107235 | validation: 0.017356024468879405]
	TIME [epoch: 5.71 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008807738221595364		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.008807738221595364 | validation: 0.013122084600823425]
	TIME [epoch: 5.73 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010326551320313073		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.010326551320313073 | validation: 0.01388002929534813]
	TIME [epoch: 5.73 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01061380048866432		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.01061380048866432 | validation: 0.013273376687492667]
	TIME [epoch: 5.74 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009194250801940446		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.009194250801940446 | validation: 0.014322486344287866]
	TIME [epoch: 5.72 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010039168771564015		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.010039168771564015 | validation: 0.011962282125416158]
	TIME [epoch: 5.72 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009499353149552248		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.009499353149552248 | validation: 0.013518734223057105]
	TIME [epoch: 5.71 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009726889869364425		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.009726889869364425 | validation: 0.011106600146086477]
	TIME [epoch: 5.72 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009808132742688798		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.009808132742688798 | validation: 0.013795976505816954]
	TIME [epoch: 5.7 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009708707454175838		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.009708707454175838 | validation: 0.014497106251946834]
	TIME [epoch: 5.74 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010226294463314078		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.010226294463314078 | validation: 0.012101083051758534]
	TIME [epoch: 5.72 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008601782250139349		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.008601782250139349 | validation: 0.011339240952021913]
	TIME [epoch: 5.73 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009812097801373761		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.009812097801373761 | validation: 0.01195709937988283]
	TIME [epoch: 5.72 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009353969138583568		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.009353969138583568 | validation: 0.016215955756378686]
	TIME [epoch: 5.73 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00951255693009948		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.00951255693009948 | validation: 0.016020441027160437]
	TIME [epoch: 5.71 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009552870069260032		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.009552870069260032 | validation: 0.01241384148109993]
	TIME [epoch: 5.73 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009877423099996044		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.009877423099996044 | validation: 0.013143781549343382]
	TIME [epoch: 5.72 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009715497594519187		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.009715497594519187 | validation: 0.014727838174003017]
	TIME [epoch: 5.71 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009879789549257657		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.009879789549257657 | validation: 0.01677902239246094]
	TIME [epoch: 5.73 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009482138275589292		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.009482138275589292 | validation: 0.01359523530175515]
	TIME [epoch: 5.72 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010029998840360243		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.010029998840360243 | validation: 0.01624940598299921]
	TIME [epoch: 5.72 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010289617923739764		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.010289617923739764 | validation: 0.01447722362301812]
	TIME [epoch: 5.74 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01019633101517458		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.01019633101517458 | validation: 0.013539052739572933]
	TIME [epoch: 5.72 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00878690959008737		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.00878690959008737 | validation: 0.015043646036751714]
	TIME [epoch: 5.72 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0094365932411915		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.0094365932411915 | validation: 0.013432615220694222]
	TIME [epoch: 5.73 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0097892591840541		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.0097892591840541 | validation: 0.015200981142576642]
	TIME [epoch: 5.73 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010220355506663455		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.010220355506663455 | validation: 0.013313498691764493]
	TIME [epoch: 5.72 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009595020578132562		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.009595020578132562 | validation: 0.012869515925384913]
	TIME [epoch: 5.73 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010045043762099436		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.010045043762099436 | validation: 0.014631547652983868]
	TIME [epoch: 5.74 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009742532219405858		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.009742532219405858 | validation: 0.012731400248201963]
	TIME [epoch: 5.74 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009709936080568167		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.009709936080568167 | validation: 0.014991812506585801]
	TIME [epoch: 5.73 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009937372758648939		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.009937372758648939 | validation: 0.013308362064513458]
	TIME [epoch: 5.74 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008944651738027658		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.008944651738027658 | validation: 0.012286529634531052]
	TIME [epoch: 5.73 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009431516562573279		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.009431516562573279 | validation: 0.012305340761353168]
	TIME [epoch: 5.74 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00902056818209623		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.00902056818209623 | validation: 0.01441197087003887]
	TIME [epoch: 5.72 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009547877416962539		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.009547877416962539 | validation: 0.013044854484493874]
	TIME [epoch: 5.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00959651280258819		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.00959651280258819 | validation: 0.01336821330574014]
	TIME [epoch: 5.73 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009344101213624193		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.009344101213624193 | validation: 0.011701313233646517]
	TIME [epoch: 5.69 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009991784833435982		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.009991784833435982 | validation: 0.01608024954386448]
	TIME [epoch: 5.69 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009189238918454787		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.009189238918454787 | validation: 0.011514048903098506]
	TIME [epoch: 5.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01003760340044062		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.01003760340044062 | validation: 0.013912905055634018]
	TIME [epoch: 5.74 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010076509122391391		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.010076509122391391 | validation: 0.012890479921281783]
	TIME [epoch: 5.75 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00909864358741407		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.00909864358741407 | validation: 0.012607419761896133]
	TIME [epoch: 5.74 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009265309954913942		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.009265309954913942 | validation: 0.015304163036158515]
	TIME [epoch: 5.76 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00943324139833654		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.00943324139833654 | validation: 0.01064585929058849]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1308.pth
	Model improved!!!
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008018979614075564		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.008018979614075564 | validation: 0.01265719421325452]
	TIME [epoch: 5.71 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009458647314112209		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.009458647314112209 | validation: 0.01505533056844427]
	TIME [epoch: 5.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008587654633505134		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.008587654633505134 | validation: 0.01171280894678045]
	TIME [epoch: 5.71 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009840631494702386		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.009840631494702386 | validation: 0.01333287551168818]
	TIME [epoch: 5.72 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008703651360814796		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.008703651360814796 | validation: 0.014761970530307023]
	TIME [epoch: 5.7 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009887155501453774		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.009887155501453774 | validation: 0.011231906864368535]
	TIME [epoch: 5.72 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009340912756687384		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.009340912756687384 | validation: 0.012611527761833864]
	TIME [epoch: 5.72 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009563271574456857		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.009563271574456857 | validation: 0.012265927127263987]
	TIME [epoch: 5.72 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009977201343808232		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.009977201343808232 | validation: 0.0161253942830198]
	TIME [epoch: 5.75 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009379042849932857		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.009379042849932857 | validation: 0.013555013547103811]
	TIME [epoch: 5.74 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009049697194790275		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.009049697194790275 | validation: 0.01528026417217885]
	TIME [epoch: 5.74 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00912739314678987		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.00912739314678987 | validation: 0.015339532217397657]
	TIME [epoch: 5.72 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009339861551335732		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.009339861551335732 | validation: 0.01255815142108292]
	TIME [epoch: 5.73 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010451747984092577		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.010451747984092577 | validation: 0.012509736636437542]
	TIME [epoch: 5.73 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00826859325285306		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.00826859325285306 | validation: 0.010940000828448815]
	TIME [epoch: 5.73 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009513375184704475		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.009513375184704475 | validation: 0.013451016453272048]
	TIME [epoch: 5.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010355614959566651		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.010355614959566651 | validation: 0.013604511234330353]
	TIME [epoch: 5.74 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0090540054945901		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.0090540054945901 | validation: 0.013600171248002203]
	TIME [epoch: 5.72 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010426445842483508		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.010426445842483508 | validation: 0.010789953313520851]
	TIME [epoch: 5.76 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009078831690931443		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.009078831690931443 | validation: 0.012033334813992547]
	TIME [epoch: 5.72 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009600898916356822		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.009600898916356822 | validation: 0.011950936278505731]
	TIME [epoch: 5.76 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009037144526185292		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.009037144526185292 | validation: 0.012260994839475958]
	TIME [epoch: 5.74 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009903457399743139		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.009903457399743139 | validation: 0.014284901660828242]
	TIME [epoch: 5.74 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008684173349503107		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.008684173349503107 | validation: 0.01598631087277638]
	TIME [epoch: 5.72 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008907347090121929		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.008907347090121929 | validation: 0.010721236819807124]
	TIME [epoch: 5.74 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008946122029637054		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.008946122029637054 | validation: 0.01376502642922225]
	TIME [epoch: 5.72 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008926953111524843		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.008926953111524843 | validation: 0.01569089960267076]
	TIME [epoch: 5.72 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009376025726347569		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.009376025726347569 | validation: 0.012832625682121401]
	TIME [epoch: 5.73 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008718095307416029		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.008718095307416029 | validation: 0.013193459481680471]
	TIME [epoch: 5.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009321001660402648		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.009321001660402648 | validation: 0.013856183209669927]
	TIME [epoch: 5.72 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009236466662716969		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.009236466662716969 | validation: 0.01204101174412673]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009316006076341813		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.009316006076341813 | validation: 0.01291605733691763]
	TIME [epoch: 5.72 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008903713752762884		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.008903713752762884 | validation: 0.011167112604695596]
	TIME [epoch: 5.71 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009217657397078844		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.009217657397078844 | validation: 0.011569303915138008]
	TIME [epoch: 5.7 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009531529175593575		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.009531529175593575 | validation: 0.01204804678888859]
	TIME [epoch: 5.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008946467258544111		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.008946467258544111 | validation: 0.01432507126077467]
	TIME [epoch: 5.71 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008553770141956104		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.008553770141956104 | validation: 0.011991660874766864]
	TIME [epoch: 5.69 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009287469031824168		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.009287469031824168 | validation: 0.014561021689049092]
	TIME [epoch: 5.72 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00960947061312254		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.00960947061312254 | validation: 0.01293977188844866]
	TIME [epoch: 5.72 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008914382972062682		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.008914382972062682 | validation: 0.01702263956678265]
	TIME [epoch: 5.72 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008944322883566431		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.008944322883566431 | validation: 0.012029790740028669]
	TIME [epoch: 5.71 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009120416867911893		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.009120416867911893 | validation: 0.013226685368821113]
	TIME [epoch: 5.72 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009261571862949325		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.009261571862949325 | validation: 0.010964746573320472]
	TIME [epoch: 5.7 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008851106707834719		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.008851106707834719 | validation: 0.013459891776306722]
	TIME [epoch: 5.72 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008934934351553767		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.008934934351553767 | validation: 0.013653420856581212]
	TIME [epoch: 5.7 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009405477166067409		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.009405477166067409 | validation: 0.011441218960380285]
	TIME [epoch: 5.73 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008609615719820628		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.008609615719820628 | validation: 0.012574393700514608]
	TIME [epoch: 5.72 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00945693484983255		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.00945693484983255 | validation: 0.013303311920523432]
	TIME [epoch: 5.72 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008598516246388542		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.008598516246388542 | validation: 0.014280841259278277]
	TIME [epoch: 5.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009450020213176178		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.009450020213176178 | validation: 0.01227622195257362]
	TIME [epoch: 5.72 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009250489323658713		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.009250489323658713 | validation: 0.013025624220634635]
	TIME [epoch: 5.7 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00936747382753386		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.00936747382753386 | validation: 0.017354296691073745]
	TIME [epoch: 5.73 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008749657713218922		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.008749657713218922 | validation: 0.013343715039895088]
	TIME [epoch: 5.71 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008420939594731752		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.008420939594731752 | validation: 0.010660098392302942]
	TIME [epoch: 5.71 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008423312568473503		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.008423312568473503 | validation: 0.014631060379075756]
	TIME [epoch: 5.71 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009069175288025668		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.009069175288025668 | validation: 0.013566302744746107]
	TIME [epoch: 5.72 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008859063887482636		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.008859063887482636 | validation: 0.014717609195908832]
	TIME [epoch: 5.7 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009397299127625838		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.009397299127625838 | validation: 0.013196771597551461]
	TIME [epoch: 5.72 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009240490060762005		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.009240490060762005 | validation: 0.010997955216573187]
	TIME [epoch: 5.73 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009018119054096628		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.009018119054096628 | validation: 0.01307922354811658]
	TIME [epoch: 5.72 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009032829115203004		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.009032829115203004 | validation: 0.014271599911126642]
	TIME [epoch: 5.71 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008820675758788333		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.008820675758788333 | validation: 0.014548406313569308]
	TIME [epoch: 5.71 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008640416637094655		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.008640416637094655 | validation: 0.00958489801022876]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1371.pth
	Model improved!!!
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008528289314584698		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.008528289314584698 | validation: 0.012780841873311178]
	TIME [epoch: 5.73 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009202244895380806		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.009202244895380806 | validation: 0.011817327693502877]
	TIME [epoch: 5.75 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008424782402826472		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.008424782402826472 | validation: 0.015007007617817248]
	TIME [epoch: 5.73 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008533066071460586		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.008533066071460586 | validation: 0.011011202083870842]
	TIME [epoch: 5.74 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008567794803912104		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.008567794803912104 | validation: 0.011446096226585002]
	TIME [epoch: 5.75 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009611784732705516		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.009611784732705516 | validation: 0.012583198882190227]
	TIME [epoch: 5.74 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008487089245440855		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.008487089245440855 | validation: 0.012941534395022403]
	TIME [epoch: 5.74 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00855212215901706		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.00855212215901706 | validation: 0.01366685211840618]
	TIME [epoch: 5.75 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008902877958387284		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.008902877958387284 | validation: 0.010745394737982151]
	TIME [epoch: 5.71 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008376555759913907		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.008376555759913907 | validation: 0.014505160753027324]
	TIME [epoch: 5.74 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008868009892594236		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.008868009892594236 | validation: 0.01204252448479083]
	TIME [epoch: 5.73 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00838894820020554		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.00838894820020554 | validation: 0.014005014841767971]
	TIME [epoch: 5.72 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008640240502667224		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.008640240502667224 | validation: 0.012350817288196914]
	TIME [epoch: 5.73 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008811695344227585		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.008811695344227585 | validation: 0.014390377262548904]
	TIME [epoch: 5.74 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009527374252156025		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.009527374252156025 | validation: 0.011385319622853319]
	TIME [epoch: 5.73 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009597065594873201		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.009597065594873201 | validation: 0.012475041430131208]
	TIME [epoch: 5.73 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00807316124934308		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.00807316124934308 | validation: 0.01354355493086399]
	TIME [epoch: 5.71 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00911610648725152		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.00911610648725152 | validation: 0.011335175975143164]
	TIME [epoch: 5.71 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009013629465212539		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.009013629465212539 | validation: 0.012360214738727249]
	TIME [epoch: 5.71 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008222965445140572		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.008222965445140572 | validation: 0.013833895952811027]
	TIME [epoch: 5.74 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008681771658959277		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.008681771658959277 | validation: 0.009380496113140725]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1392.pth
	Model improved!!!
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00848857787075949		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.00848857787075949 | validation: 0.01238097684086993]
	TIME [epoch: 5.72 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008451720943806034		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.008451720943806034 | validation: 0.013512924980397502]
	TIME [epoch: 5.72 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00885357492177991		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.00885357492177991 | validation: 0.010137948585786361]
	TIME [epoch: 5.72 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009001178103945563		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.009001178103945563 | validation: 0.013373289060327122]
	TIME [epoch: 5.73 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008680597340390813		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.008680597340390813 | validation: 0.01273648841120727]
	TIME [epoch: 5.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008915962902488386		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.008915962902488386 | validation: 0.016890912967874185]
	TIME [epoch: 5.73 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008793561990248873		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.008793561990248873 | validation: 0.011337593983149152]
	TIME [epoch: 5.73 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008420523403076632		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.008420523403076632 | validation: 0.014917103313418745]
	TIME [epoch: 5.74 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008068582592430026		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.008068582592430026 | validation: 0.012208498608210862]
	TIME [epoch: 5.74 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009140183431402193		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.009140183431402193 | validation: 0.013210984061656994]
	TIME [epoch: 5.75 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008595362374462222		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.008595362374462222 | validation: 0.012561641779613542]
	TIME [epoch: 5.74 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008759993359595297		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.008759993359595297 | validation: 0.013810954066657355]
	TIME [epoch: 5.75 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008888826233882555		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.008888826233882555 | validation: 0.012084242427665749]
	TIME [epoch: 5.75 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00857853693462003		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.00857853693462003 | validation: 0.010850769666716331]
	TIME [epoch: 5.74 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00825981847764836		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.00825981847764836 | validation: 0.012936331735122087]
	TIME [epoch: 5.77 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008504087261365277		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.008504087261365277 | validation: 0.017477814261342095]
	TIME [epoch: 5.74 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009181102957080522		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.009181102957080522 | validation: 0.011586600946450165]
	TIME [epoch: 5.76 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009068337086577297		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.009068337086577297 | validation: 0.014066751207419648]
	TIME [epoch: 5.72 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008881801791143019		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.008881801791143019 | validation: 0.013986822240559572]
	TIME [epoch: 5.72 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008629412621295096		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.008629412621295096 | validation: 0.013938868105115]
	TIME [epoch: 5.73 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008934053796262487		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.008934053796262487 | validation: 0.01195707826091156]
	TIME [epoch: 5.74 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008976105311693287		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.008976105311693287 | validation: 0.011949979309118675]
	TIME [epoch: 5.73 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008647376410642298		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.008647376410642298 | validation: 0.012127631493187074]
	TIME [epoch: 5.74 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008808010102581358		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.008808010102581358 | validation: 0.014171216652402008]
	TIME [epoch: 5.74 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009062232685784596		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.009062232685784596 | validation: 0.011167740222152434]
	TIME [epoch: 5.74 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008099862850856433		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.008099862850856433 | validation: 0.0148444750742462]
	TIME [epoch: 5.74 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008119440384539182		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.008119440384539182 | validation: 0.01237603831932287]
	TIME [epoch: 5.75 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008704434240053498		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.008704434240053498 | validation: 0.014981986476001264]
	TIME [epoch: 5.74 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008251072141154012		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.008251072141154012 | validation: 0.014196729687503141]
	TIME [epoch: 5.75 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008602502071981209		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.008602502071981209 | validation: 0.012432940062635667]
	TIME [epoch: 5.74 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009638148860587		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.009638148860587 | validation: 0.01428597378134433]
	TIME [epoch: 5.74 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00885842564729626		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.00885842564729626 | validation: 0.012637509664087566]
	TIME [epoch: 5.72 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00877160072318798		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.00877160072318798 | validation: 0.011127915792046285]
	TIME [epoch: 5.74 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008898864124923892		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.008898864124923892 | validation: 0.012321663859379451]
	TIME [epoch: 5.74 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008023843439104863		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.008023843439104863 | validation: 0.012322857403524769]
	TIME [epoch: 5.72 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00829775555536831		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.00829775555536831 | validation: 0.01280620356711809]
	TIME [epoch: 5.73 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008323611129101698		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.008323611129101698 | validation: 0.011052985953891915]
	TIME [epoch: 5.74 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008424067461635147		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.008424067461635147 | validation: 0.011531031420529116]
	TIME [epoch: 5.72 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00801847517284452		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.00801847517284452 | validation: 0.013445047950350987]
	TIME [epoch: 5.72 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009116747046848105		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.009116747046848105 | validation: 0.010731963121316569]
	TIME [epoch: 5.73 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008930022770104816		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.008930022770104816 | validation: 0.011239904674490209]
	TIME [epoch: 5.74 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009256815872406761		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.009256815872406761 | validation: 0.013772925406007908]
	TIME [epoch: 5.73 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009222061823262814		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.009222061823262814 | validation: 0.013869562117102975]
	TIME [epoch: 5.74 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008204710278616209		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.008204710278616209 | validation: 0.014020476447258857]
	TIME [epoch: 5.75 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009217998003797585		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.009217998003797585 | validation: 0.010874255465037896]
	TIME [epoch: 5.74 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00791431599683639		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.00791431599683639 | validation: 0.013016134802403868]
	TIME [epoch: 5.75 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008783442016160992		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.008783442016160992 | validation: 0.010368756237864896]
	TIME [epoch: 5.71 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00829216579781827		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.00829216579781827 | validation: 0.011663228092584955]
	TIME [epoch: 5.75 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008539516106794591		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.008539516106794591 | validation: 0.01263048430756979]
	TIME [epoch: 5.72 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008675341817643366		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.008675341817643366 | validation: 0.01375099493335702]
	TIME [epoch: 5.75 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009489753955684684		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.009489753955684684 | validation: 0.013215938334147538]
	TIME [epoch: 5.72 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007885095422147752		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.007885095422147752 | validation: 0.010375070538859221]
	TIME [epoch: 5.75 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008283369355611504		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.008283369355611504 | validation: 0.010760484040883644]
	TIME [epoch: 5.73 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009021059722393785		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.009021059722393785 | validation: 0.010602210135178348]
	TIME [epoch: 5.75 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008413741562274948		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.008413741562274948 | validation: 0.011863504799938585]
	TIME [epoch: 5.74 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008840997381444374		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.008840997381444374 | validation: 0.014218265494781846]
	TIME [epoch: 5.74 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008431496221667521		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.008431496221667521 | validation: 0.010958977076404964]
	TIME [epoch: 5.72 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007816926618378205		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.007816926618378205 | validation: 0.011883176985890044]
	TIME [epoch: 5.74 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008381319197481711		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.008381319197481711 | validation: 0.01015486130458766]
	TIME [epoch: 5.73 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008174328250834082		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.008174328250834082 | validation: 0.012632281657458989]
	TIME [epoch: 5.74 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008720512744343174		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.008720512744343174 | validation: 0.013420341570624795]
	TIME [epoch: 5.72 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008555267367516975		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.008555267367516975 | validation: 0.011907202383673222]
	TIME [epoch: 5.75 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008219743797951094		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.008219743797951094 | validation: 0.012052191542032666]
	TIME [epoch: 5.75 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00872878243561181		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.00872878243561181 | validation: 0.011931840597843614]
	TIME [epoch: 5.74 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008084867365616664		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.008084867365616664 | validation: 0.010491219719406098]
	TIME [epoch: 5.74 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008874528490627689		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.008874528490627689 | validation: 0.013030149567646]
	TIME [epoch: 5.75 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008484677575359395		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.008484677575359395 | validation: 0.013728948176696754]
	TIME [epoch: 5.75 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008177235382740235		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.008177235382740235 | validation: 0.011018731227396439]
	TIME [epoch: 5.74 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008182645022014668		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.008182645022014668 | validation: 0.00973493867262535]
	TIME [epoch: 5.76 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008261531647186068		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.008261531647186068 | validation: 0.011137131036801263]
	TIME [epoch: 5.75 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008258026466128485		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.008258026466128485 | validation: 0.013474047314741978]
	TIME [epoch: 5.74 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0077701961561748665		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.0077701961561748665 | validation: 0.013543123416923175]
	TIME [epoch: 5.72 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008369552727425832		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.008369552727425832 | validation: 0.012743870930543322]
	TIME [epoch: 5.71 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008161406624796795		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.008161406624796795 | validation: 0.013594447123029485]
	TIME [epoch: 5.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00858484725414378		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.00858484725414378 | validation: 0.015386133180667795]
	TIME [epoch: 5.73 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008872094822128583		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.008872094822128583 | validation: 0.013048794387574015]
	TIME [epoch: 5.71 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008369159002275216		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.008369159002275216 | validation: 0.01134465785156541]
	TIME [epoch: 5.71 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008882839980273581		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.008882839980273581 | validation: 0.015143131177594617]
	TIME [epoch: 5.73 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008539829393732068		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.008539829393732068 | validation: 0.011763040508964996]
	TIME [epoch: 5.73 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008293611193981426		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.008293611193981426 | validation: 0.010157160256072929]
	TIME [epoch: 5.73 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008486672060490428		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.008486672060490428 | validation: 0.012716725380471194]
	TIME [epoch: 5.73 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007957444034108465		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.007957444034108465 | validation: 0.014040508588429335]
	TIME [epoch: 5.74 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008583524840255654		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.008583524840255654 | validation: 0.01075239182325093]
	TIME [epoch: 5.75 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008281964511738378		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.008281964511738378 | validation: 0.014396132299511888]
	TIME [epoch: 5.74 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008648541161304356		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.008648541161304356 | validation: 0.009875760233206021]
	TIME [epoch: 5.75 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008608652865833595		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.008608652865833595 | validation: 0.00891662389804997]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1478.pth
	Model improved!!!
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008812867451360139		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.008812867451360139 | validation: 0.011341517997116402]
	TIME [epoch: 5.72 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008908631391787243		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.008908631391787243 | validation: 0.011562581712121834]
	TIME [epoch: 5.72 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008413885118434578		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.008413885118434578 | validation: 0.012063634441951167]
	TIME [epoch: 5.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008693174353988354		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.008693174353988354 | validation: 0.010038955691686469]
	TIME [epoch: 5.72 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008059090479177453		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.008059090479177453 | validation: 0.014718459687687625]
	TIME [epoch: 5.74 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009008412865629145		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.009008412865629145 | validation: 0.013467498222840359]
	TIME [epoch: 5.75 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00907365718475207		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.00907365718475207 | validation: 0.01420202934407444]
	TIME [epoch: 5.75 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008962711329210562		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.008962711329210562 | validation: 0.011615345548795432]
	TIME [epoch: 5.74 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008020870987179218		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.008020870987179218 | validation: 0.01196343800064912]
	TIME [epoch: 5.74 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009081237074983147		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.009081237074983147 | validation: 0.011307496563343645]
	TIME [epoch: 5.74 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009037626152313136		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.009037626152313136 | validation: 0.011901383902700647]
	TIME [epoch: 5.74 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008128121244233228		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.008128121244233228 | validation: 0.011995411697998115]
	TIME [epoch: 5.74 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007618878347202516		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.007618878347202516 | validation: 0.011639386075000348]
	TIME [epoch: 5.72 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008047925468306403		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.008047925468306403 | validation: 0.013947072790444694]
	TIME [epoch: 5.74 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00788711983061067		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.00788711983061067 | validation: 0.010575889556339636]
	TIME [epoch: 5.73 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008436047953737353		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.008436047953737353 | validation: 0.011936002548877911]
	TIME [epoch: 5.75 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008428696672062103		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.008428696672062103 | validation: 0.010210274439621081]
	TIME [epoch: 5.74 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007973282890730968		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.007973282890730968 | validation: 0.012285775475917493]
	TIME [epoch: 5.74 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007792514046784343		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.007792514046784343 | validation: 0.010978314622772767]
	TIME [epoch: 5.72 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00898396331788328		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.00898396331788328 | validation: 0.012614846998758578]
	TIME [epoch: 5.74 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008481628522562255		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.008481628522562255 | validation: 0.011284621899008564]
	TIME [epoch: 5.72 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007929146270663677		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.007929146270663677 | validation: 0.010836953128826255]
	TIME [epoch: 5.73 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0076915048773615055		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.0076915048773615055 | validation: 0.01105966150341603]
	TIME [epoch: 5.73 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008008352266273232		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.008008352266273232 | validation: 0.012798264767970625]
	TIME [epoch: 5.73 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008670856168772782		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.008670856168772782 | validation: 0.01193920898832952]
	TIME [epoch: 5.73 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008940140203153679		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.008940140203153679 | validation: 0.01138958029284597]
	TIME [epoch: 5.73 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00837026612258293		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.00837026612258293 | validation: 0.010709017971284885]
	TIME [epoch: 5.74 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008163580046726461		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.008163580046726461 | validation: 0.011155231668969945]
	TIME [epoch: 5.73 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008058639931896604		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.008058639931896604 | validation: 0.009766616539258643]
	TIME [epoch: 5.73 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008443624145858418		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.008443624145858418 | validation: 0.01014857983396943]
	TIME [epoch: 5.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008339883350447806		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.008339883350447806 | validation: 0.009021121634850504]
	TIME [epoch: 5.72 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008834672952225548		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.008834672952225548 | validation: 0.012784444481734282]
	TIME [epoch: 5.73 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008031217703529213		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.008031217703529213 | validation: 0.00957215370518656]
	TIME [epoch: 5.75 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009105771799204812		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.009105771799204812 | validation: 0.01051269090278908]
	TIME [epoch: 5.74 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007986407765602532		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.007986407765602532 | validation: 0.009297336608787421]
	TIME [epoch: 5.72 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00882195628134124		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.00882195628134124 | validation: 0.010506405107398031]
	TIME [epoch: 5.73 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007605559796269859		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.007605559796269859 | validation: 0.01112834149647919]
	TIME [epoch: 5.72 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008588003495782674		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.008588003495782674 | validation: 0.014729786058470218]
	TIME [epoch: 5.74 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008287189574871064		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.008287189574871064 | validation: 0.01395381029454218]
	TIME [epoch: 5.72 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008428659972644244		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.008428659972644244 | validation: 0.011955326593947447]
	TIME [epoch: 5.74 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007644617712571863		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.007644617712571863 | validation: 0.013746089693341002]
	TIME [epoch: 5.72 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007701839706638636		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.007701839706638636 | validation: 0.010775805677748475]
	TIME [epoch: 5.73 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00857537238518902		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.00857537238518902 | validation: 0.011926847342663683]
	TIME [epoch: 5.75 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007890978347979777		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.007890978347979777 | validation: 0.01215839842733859]
	TIME [epoch: 5.73 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007908011912874944		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.007908011912874944 | validation: 0.011791156069323839]
	TIME [epoch: 5.74 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0075091834260788215		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.0075091834260788215 | validation: 0.010961117087971251]
	TIME [epoch: 5.72 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0076082432127601895		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.0076082432127601895 | validation: 0.010626168217817911]
	TIME [epoch: 5.71 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008648595721696328		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.008648595721696328 | validation: 0.013937688006553173]
	TIME [epoch: 5.72 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007938810609768667		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.007938810609768667 | validation: 0.010227402594125735]
	TIME [epoch: 5.72 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008217744130317468		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.008217744130317468 | validation: 0.011904866734669829]
	TIME [epoch: 5.72 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008357681823953765		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.008357681823953765 | validation: 0.012171883596215738]
	TIME [epoch: 5.73 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00796513007904191		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.00796513007904191 | validation: 0.01130666061777258]
	TIME [epoch: 5.74 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008279218632767983		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.008279218632767983 | validation: 0.010528448010643378]
	TIME [epoch: 5.72 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009184111175232365		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.009184111175232365 | validation: 0.00981451887529823]
	TIME [epoch: 5.74 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00763410188658934		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.00763410188658934 | validation: 0.01436152422482151]
	TIME [epoch: 5.72 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008166594089586247		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.008166594089586247 | validation: 0.010587109737906964]
	TIME [epoch: 5.72 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008546954620970538		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.008546954620970538 | validation: 0.009949342503802406]
	TIME [epoch: 5.74 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008472711632709391		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.008472711632709391 | validation: 0.010497198792788431]
	TIME [epoch: 5.74 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008151259357130531		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.008151259357130531 | validation: 0.010588531536649216]
	TIME [epoch: 5.74 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007886597889106032		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.007886597889106032 | validation: 0.012798345925927502]
	TIME [epoch: 5.75 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008550558303726915		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.008550558303726915 | validation: 0.01020765631386379]
	TIME [epoch: 5.75 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008325891045873217		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.008325891045873217 | validation: 0.012049098251945833]
	TIME [epoch: 5.76 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008067060020311417		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.008067060020311417 | validation: 0.011401789733514456]
	TIME [epoch: 5.75 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008448902015263141		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.008448902015263141 | validation: 0.011373897298865888]
	TIME [epoch: 5.74 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008014739938407863		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.008014739938407863 | validation: 0.009671426908846271]
	TIME [epoch: 5.75 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008044260464842292		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.008044260464842292 | validation: 0.011831065991408608]
	TIME [epoch: 5.75 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00791713382235834		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.00791713382235834 | validation: 0.011369571068621876]
	TIME [epoch: 5.76 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007928830939888621		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.007928830939888621 | validation: 0.008079203332683272]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1546.pth
	Model improved!!!
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008188240407070403		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.008188240407070403 | validation: 0.012123450401556914]
	TIME [epoch: 5.91 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007590272384513239		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.007590272384513239 | validation: 0.011047930346089852]
	TIME [epoch: 5.72 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008307560736564494		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.008307560736564494 | validation: 0.010052236511012514]
	TIME [epoch: 5.73 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008167552040788345		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.008167552040788345 | validation: 0.009598503195693931]
	TIME [epoch: 6.81 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00874555911836768		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.00874555911836768 | validation: 0.013069289603135349]
	TIME [epoch: 5.72 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00818563964498339		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.00818563964498339 | validation: 0.008220115136974848]
	TIME [epoch: 5.75 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008140486834148255		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.008140486834148255 | validation: 0.013286253408156168]
	TIME [epoch: 5.76 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007905873709905146		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.007905873709905146 | validation: 0.01319847976042976]
	TIME [epoch: 5.76 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008058359873831833		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.008058359873831833 | validation: 0.010556466322272529]
	TIME [epoch: 5.76 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008238838318063706		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.008238838318063706 | validation: 0.009940573215098548]
	TIME [epoch: 5.76 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008254984259467933		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.008254984259467933 | validation: 0.013051151417669283]
	TIME [epoch: 5.76 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00820470152444426		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.00820470152444426 | validation: 0.01009939554595033]
	TIME [epoch: 5.76 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007556351918155622		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.007556351918155622 | validation: 0.012970937920976167]
	TIME [epoch: 5.74 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007952818643367252		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.007952818643367252 | validation: 0.01183904731804748]
	TIME [epoch: 5.76 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007872085139299446		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.007872085139299446 | validation: 0.01126213096028993]
	TIME [epoch: 5.73 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007380022309716402		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.007380022309716402 | validation: 0.012708240733412646]
	TIME [epoch: 5.75 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007659164620414413		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.007659164620414413 | validation: 0.010910231781048597]
	TIME [epoch: 5.74 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00903687600082023		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.00903687600082023 | validation: 0.010100904201192584]
	TIME [epoch: 5.76 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008308989078971952		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.008308989078971952 | validation: 0.013768511817512541]
	TIME [epoch: 5.76 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007965065164857635		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.007965065164857635 | validation: 0.010530214900166202]
	TIME [epoch: 5.75 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00818720522644574		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.00818720522644574 | validation: 0.012006599499715165]
	TIME [epoch: 5.73 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008232957359578552		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.008232957359578552 | validation: 0.009053116865420985]
	TIME [epoch: 5.75 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008192935929480482		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.008192935929480482 | validation: 0.012132416995928054]
	TIME [epoch: 5.73 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007753070034994104		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.007753070034994104 | validation: 0.010828923917396273]
	TIME [epoch: 5.75 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007345590662377992		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.007345590662377992 | validation: 0.012913789054346314]
	TIME [epoch: 5.73 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008021570995170101		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.008021570995170101 | validation: 0.010904193691026343]
	TIME [epoch: 5.75 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0072735177789178184		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.0072735177789178184 | validation: 0.014299287710695375]
	TIME [epoch: 5.77 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008168211102678716		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.008168211102678716 | validation: 0.012335415390689176]
	TIME [epoch: 5.75 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008815542380751818		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.008815542380751818 | validation: 0.010299337841638901]
	TIME [epoch: 5.77 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0075007102097713945		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.0075007102097713945 | validation: 0.011597534462010207]
	TIME [epoch: 5.75 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00844337410875628		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.00844337410875628 | validation: 0.009260970120333523]
	TIME [epoch: 5.76 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007618299683572052		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.007618299683572052 | validation: 0.01118188661087789]
	TIME [epoch: 5.76 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008022245170425038		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.008022245170425038 | validation: 0.012171944146810243]
	TIME [epoch: 5.76 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007438484128393706		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.007438484128393706 | validation: 0.012693870027217204]
	TIME [epoch: 5.75 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00800525816955477		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.00800525816955477 | validation: 0.012596602644790756]
	TIME [epoch: 5.74 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007948641933319258		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.007948641933319258 | validation: 0.012241467488020609]
	TIME [epoch: 5.75 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00833041104075995		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.00833041104075995 | validation: 0.011761958666454742]
	TIME [epoch: 5.74 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007876196462479076		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.007876196462479076 | validation: 0.011208059514491964]
	TIME [epoch: 5.76 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007288571714791537		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.007288571714791537 | validation: 0.012662996138805761]
	TIME [epoch: 5.73 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008876782398811987		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.008876782398811987 | validation: 0.011201720792340032]
	TIME [epoch: 5.74 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008204618796219285		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.008204618796219285 | validation: 0.01183924437395344]
	TIME [epoch: 5.74 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007747648866228493		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.007747648866228493 | validation: 0.012358004227313923]
	TIME [epoch: 5.76 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007655670326011537		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.007655670326011537 | validation: 0.010283190515509367]
	TIME [epoch: 5.74 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008003369987567425		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.008003369987567425 | validation: 0.011927432766415537]
	TIME [epoch: 5.76 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008118300102452429		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.008118300102452429 | validation: 0.01191989618633923]
	TIME [epoch: 5.74 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007918627995274794		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.007918627995274794 | validation: 0.011791732056534821]
	TIME [epoch: 5.75 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007647549651152894		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.007647549651152894 | validation: 0.012582896264820276]
	TIME [epoch: 5.76 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007831742170019008		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.007831742170019008 | validation: 0.013540195026000968]
	TIME [epoch: 5.75 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008302676584750996		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.008302676584750996 | validation: 0.010750222498826645]
	TIME [epoch: 5.75 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007418051147220186		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.007418051147220186 | validation: 0.01246297543822077]
	TIME [epoch: 5.76 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008789585676446634		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.008789585676446634 | validation: 0.010448690127754469]
	TIME [epoch: 5.75 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008004117424721208		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.008004117424721208 | validation: 0.011346835880091478]
	TIME [epoch: 5.77 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007910549372247421		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.007910549372247421 | validation: 0.008808325092745683]
	TIME [epoch: 5.74 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007585809266462907		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.007585809266462907 | validation: 0.010721422148863003]
	TIME [epoch: 5.76 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007646719069425337		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.007646719069425337 | validation: 0.012260087973926394]
	TIME [epoch: 5.73 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007592350229501477		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.007592350229501477 | validation: 0.013544381387010342]
	TIME [epoch: 5.74 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00872423457285035		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.00872423457285035 | validation: 0.009006450136532885]
	TIME [epoch: 5.74 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00874658064506219		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.00874658064506219 | validation: 0.012819665181649599]
	TIME [epoch: 5.73 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007781842370835306		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.007781842370835306 | validation: 0.010576343853446392]
	TIME [epoch: 5.74 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008175553304164206		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.008175553304164206 | validation: 0.010074879229191104]
	TIME [epoch: 5.75 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007668652820873203		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.007668652820873203 | validation: 0.011536667611866515]
	TIME [epoch: 5.74 sec]
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00856865186416655		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.00856865186416655 | validation: 0.013668561607149965]
	TIME [epoch: 5.75 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008361617633590912		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.008361617633590912 | validation: 0.012853124728062916]
	TIME [epoch: 5.75 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008737153404928897		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.008737153404928897 | validation: 0.009918190976182406]
	TIME [epoch: 5.75 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007821231628755145		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.007821231628755145 | validation: 0.009811045842587962]
	TIME [epoch: 5.74 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007707717388243567		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.007707717388243567 | validation: 0.013954612493387177]
	TIME [epoch: 5.75 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008543063509714871		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.008543063509714871 | validation: 0.010954327792208852]
	TIME [epoch: 5.74 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008136502107567471		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.008136502107567471 | validation: 0.012444419719733014]
	TIME [epoch: 5.76 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007244103998464808		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.007244103998464808 | validation: 0.011150011209282574]
	TIME [epoch: 5.76 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008617522841016417		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.008617522841016417 | validation: 0.010919972623245755]
	TIME [epoch: 5.76 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007760504310295469		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.007760504310295469 | validation: 0.009161807659090016]
	TIME [epoch: 5.74 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008256607290900718		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.008256607290900718 | validation: 0.010374890113239256]
	TIME [epoch: 5.77 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00831599968135921		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.00831599968135921 | validation: 0.010423185570204286]
	TIME [epoch: 5.76 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0076485669088917465		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.0076485669088917465 | validation: 0.010688313606181055]
	TIME [epoch: 5.75 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00830451864999401		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.00830451864999401 | validation: 0.01034264399139495]
	TIME [epoch: 5.72 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007857226154731032		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.007857226154731032 | validation: 0.010017259694074122]
	TIME [epoch: 5.74 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008438577470649982		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.008438577470649982 | validation: 0.012238466468939469]
	TIME [epoch: 5.73 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009192360048057341		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.009192360048057341 | validation: 0.011533624612492223]
	TIME [epoch: 5.74 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008189637189864412		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.008189637189864412 | validation: 0.013203940398430447]
	TIME [epoch: 5.73 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00756187808296336		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.00756187808296336 | validation: 0.010413552851784292]
	TIME [epoch: 5.74 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007376452896178649		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.007376452896178649 | validation: 0.011772331317492403]
	TIME [epoch: 5.74 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00835778950541295		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.00835778950541295 | validation: 0.00904458425018173]
	TIME [epoch: 5.75 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007294941816576497		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.007294941816576497 | validation: 0.010669476804221135]
	TIME [epoch: 5.75 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008040283425007231		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.008040283425007231 | validation: 0.013421812875795025]
	TIME [epoch: 5.75 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007847777341241993		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.007847777341241993 | validation: 0.012116314389592776]
	TIME [epoch: 5.76 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0073430821708214715		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.0073430821708214715 | validation: 0.010423742012943694]
	TIME [epoch: 5.76 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007436095822850374		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.007436095822850374 | validation: 0.009794402043055928]
	TIME [epoch: 5.74 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007396111623971089		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.007396111623971089 | validation: 0.01053649037139689]
	TIME [epoch: 5.75 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007966334440232384		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.007966334440232384 | validation: 0.008852908354925105]
	TIME [epoch: 5.74 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008593833807779989		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.008593833807779989 | validation: 0.012047893472579863]
	TIME [epoch: 5.75 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007713725348463349		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.007713725348463349 | validation: 0.012369494692210626]
	TIME [epoch: 5.74 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007572768977036094		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.007572768977036094 | validation: 0.013213885130806414]
	TIME [epoch: 5.76 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007799662889549276		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.007799662889549276 | validation: 0.011494988421640762]
	TIME [epoch: 5.73 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007269374195504184		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.007269374195504184 | validation: 0.010568332493036093]
	TIME [epoch: 5.75 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007334812726320492		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.007334812726320492 | validation: 0.010563564998061804]
	TIME [epoch: 5.75 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007746873611411287		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.007746873611411287 | validation: 0.010470085050020906]
	TIME [epoch: 5.76 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007422202762240188		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.007422202762240188 | validation: 0.015067606512111531]
	TIME [epoch: 5.74 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00713420652588432		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.00713420652588432 | validation: 0.011597767597332076]
	TIME [epoch: 5.76 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00744036237726051		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.00744036237726051 | validation: 0.012698360621928629]
	TIME [epoch: 5.72 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008594067321932361		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.008594067321932361 | validation: 0.010877624025920896]
	TIME [epoch: 5.76 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007581707237723089		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.007581707237723089 | validation: 0.01345390058000322]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_153219/states/model_phi1_4a_v_mmd1_1647.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6406.663 seconds.
