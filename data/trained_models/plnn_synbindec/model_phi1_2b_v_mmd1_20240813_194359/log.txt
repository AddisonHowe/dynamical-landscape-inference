Args:
Namespace(name='model_phi1_2b_v_mmd1', outdir='out/model_training/model_phi1_2b_v_mmd1', training_data='data/training_data/basic/data_phi1_2b/training', validation_data='data/training_data/basic/data_phi1_2b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4174059357

Training model...

Saving initial model state to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.328435412413621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.328435412413621 | validation: 5.14834688494514]
	TIME [epoch: 105 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.691054859965421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.691054859965421 | validation: 5.996532476239074]
	TIME [epoch: 3.35 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 2/2] avg loss: 5.220669332517626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.220669332517626 | validation: 4.934963701598214]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.30551047008184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.30551047008184 | validation: 4.238337959707161]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 2/2] avg loss: 4.184261963944728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.184261963944728 | validation: 4.42686034909169]
	TIME [epoch: 3.27 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.7847310348773147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7847310348773147 | validation: 3.9772215739037913]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.7551468714277902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7551468714277902 | validation: 4.358536562627552]
	TIME [epoch: 3.27 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.5683460458017073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5683460458017073 | validation: 4.010772013489074]
	TIME [epoch: 3.27 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.392969283844502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.392969283844502 | validation: 3.9365076103985923]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.4103365989044434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4103365989044434 | validation: 4.583717480031104]
	TIME [epoch: 3.27 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.406172522231187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.406172522231187 | validation: 3.7417590361772852]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.299566502509116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.299566502509116 | validation: 4.3378983930544575]
	TIME [epoch: 3.28 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.283841340145557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.283841340145557 | validation: 3.6944374166632246]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.223314826429225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223314826429225 | validation: 3.7853907747465536]
	TIME [epoch: 3.29 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.127988255921627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.127988255921627 | validation: 3.8510013133736245]
	TIME [epoch: 3.28 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.0164081116724635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0164081116724635 | validation: 3.6479513773632366]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.963341918671146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.963341918671146 | validation: 4.593128542930202]
	TIME [epoch: 3.27 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.43875463382808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.43875463382808 | validation: 3.4237892211050993]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 2/2] avg loss: 3.0549804422977807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0549804422977807 | validation: 3.3973233687145274]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.9249619352826204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9249619352826204 | validation: 3.6143006418125823]
	TIME [epoch: 3.27 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.7868049143600566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7868049143600566 | validation: 3.1583923687821844]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.70205486582251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.70205486582251 | validation: 2.7743354036239314]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.7592754691507206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7592754691507206 | validation: 3.063857113460026]
	TIME [epoch: 3.3 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.54361209857787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.54361209857787 | validation: 3.679709380356822]
	TIME [epoch: 3.27 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.7860842604108482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7860842604108482 | validation: 2.8136445185247583]
	TIME [epoch: 3.27 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3733731212154576		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 2.3733731212154576 | validation: 2.6101115914185664]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2719812796141774		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 2.2719812796141774 | validation: 3.278584651596452]
	TIME [epoch: 3.27 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3819171257158103		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 2.3819171257158103 | validation: 2.4738473645026255]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.585414449081242		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 2.585414449081242 | validation: 2.420450514900041]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2879985110132472		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 2.2879985110132472 | validation: 2.7802255590123135]
	TIME [epoch: 3.27 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1943255652073193		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.1943255652073193 | validation: 2.283703631887032]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.2505766981140676		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.2505766981140676 | validation: 2.227576056947193]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0828617694747447		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.0828617694747447 | validation: 2.3386341141928213]
	TIME [epoch: 3.27 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.3766635411590835		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.3766635411590835 | validation: 2.3554110500203094]
	TIME [epoch: 3.3 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1985947319348575		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.1985947319348575 | validation: 2.793053586847294]
	TIME [epoch: 3.28 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1355892856276952		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.1355892856276952 | validation: 2.2078973673072086]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0265513051588213		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.0265513051588213 | validation: 2.1684276249089414]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9991327733438655		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.9991327733438655 | validation: 2.361765215180455]
	TIME [epoch: 3.27 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.0381846375173516		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.0381846375173516 | validation: 2.1577240637944506]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9249762216517445		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.9249762216517445 | validation: 2.260787965333384]
	TIME [epoch: 3.27 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.198609064498299		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.198609064498299 | validation: 3.078602451146054]
	TIME [epoch: 3.27 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.111217600535489		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 2.111217600535489 | validation: 2.307030476887183]
	TIME [epoch: 3.27 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.065468867132407		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.065468867132407 | validation: 2.166530796714439]
	TIME [epoch: 3.27 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8617823554600599		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.8617823554600599 | validation: 2.8163460577764803]
	TIME [epoch: 3.28 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.004018004784967		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.004018004784967 | validation: 2.155852262598356]
	TIME [epoch: 3.29 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.137417255520431		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.137417255520431 | validation: 2.0711723173519156]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8487878771095985		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 1.8487878771095985 | validation: 2.2150196705006384]
	TIME [epoch: 3.27 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.837510409015031		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 1.837510409015031 | validation: 2.151300204401664]
	TIME [epoch: 3.27 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8188726591062956		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 1.8188726591062956 | validation: 2.399936879151904]
	TIME [epoch: 3.27 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9803386578285842		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.9803386578285842 | validation: 2.056698706244449]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7866583254561492		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.7866583254561492 | validation: 2.0219953636227426]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.1099946435254777		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.1099946435254777 | validation: 2.09293191322875]
	TIME [epoch: 3.27 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.9575879516866124		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.9575879516866124 | validation: 1.9775576574134157]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8166457519487675		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.8166457519487675 | validation: 1.9684910246054095]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7415607249585456		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.7415607249585456 | validation: 1.9304615972696384]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6592671178792315		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.6592671178792315 | validation: 1.9678122583223279]
	TIME [epoch: 3.28 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6900659396396818		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.6900659396396818 | validation: 2.2132819981753733]
	TIME [epoch: 3.27 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7947843231228817		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.7947843231228817 | validation: 1.941368133228017]
	TIME [epoch: 3.27 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.6421207101384825		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.6421207101384825 | validation: 2.0625075494017535]
	TIME [epoch: 3.27 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.8814219233052343		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.8814219233052343 | validation: 1.8131877972022665]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.580235617254565		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.580235617254565 | validation: 1.8511381946188188]
	TIME [epoch: 3.28 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5522885639169417		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.5522885639169417 | validation: 1.747288968068168]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4965854007908832		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.4965854007908832 | validation: 1.9277570693717572]
	TIME [epoch: 3.27 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7076267674332009		[learning rate: 0.0076127]
	Learning Rate: 0.00761269
	LOSS [training: 1.7076267674332009 | validation: 2.0052167283041444]
	TIME [epoch: 3.27 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.5307631471078564		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.5307631471078564 | validation: 1.5381583709252393]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3105967645082364		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 1.3105967645082364 | validation: 1.51033353294175]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3582421882408786		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 1.3582421882408786 | validation: 1.491021266750542]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.4525956892847858		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 1.4525956892847858 | validation: 1.346786629206522]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3573467340219714		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.3573467340219714 | validation: 1.692266038084911]
	TIME [epoch: 3.27 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.338061503976249		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.338061503976249 | validation: 1.4055297329523506]
	TIME [epoch: 3.27 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1651073960026956		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 1.1651073960026956 | validation: 1.5854550318273954]
	TIME [epoch: 3.27 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.258710755534319		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 1.258710755534319 | validation: 2.128688630850355]
	TIME [epoch: 3.27 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.530256518799885		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 1.530256518799885 | validation: 1.6678494809464603]
	TIME [epoch: 3.27 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3687326410662202		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 1.3687326410662202 | validation: 1.3458872962833315]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1788394861203324		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 1.1788394861203324 | validation: 1.3024113250987668]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.095078034043163		[learning rate: 0.0069922]
	Learning Rate: 0.00699222
	LOSS [training: 1.095078034043163 | validation: 1.3113884021572622]
	TIME [epoch: 3.3 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.062093323312909		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 1.062093323312909 | validation: 1.3811001319111922]
	TIME [epoch: 3.29 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3633075994233492		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 1.3633075994233492 | validation: 1.4584730958257517]
	TIME [epoch: 3.27 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2819629051387222		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 1.2819629051387222 | validation: 1.6911251988370357]
	TIME [epoch: 3.27 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.236987622826643		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 1.236987622826643 | validation: 1.4371307209390594]
	TIME [epoch: 3.27 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.191990454693416		[learning rate: 0.0067489]
	Learning Rate: 0.00674887
	LOSS [training: 1.191990454693416 | validation: 1.302653579071914]
	TIME [epoch: 3.27 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1064053509124459		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 1.1064053509124459 | validation: 1.3255011151972624]
	TIME [epoch: 3.27 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1306330345451485		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 1.1306330345451485 | validation: 1.221386848727082]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0319227035966323		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.0319227035966323 | validation: 1.2949831651685852]
	TIME [epoch: 3.27 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1763229924307943		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 1.1763229924307943 | validation: 1.4547302280007663]
	TIME [epoch: 3.28 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3076474431431158		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 1.3076474431431158 | validation: 1.1946795890180444]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.120538712145833		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 1.120538712145833 | validation: 1.1828728707960332]
	TIME [epoch: 3.31 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9983889159423951		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.9983889159423951 | validation: 1.5764230939847272]
	TIME [epoch: 3.29 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2007198238591994		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 1.2007198238591994 | validation: 1.8490144588889024]
	TIME [epoch: 3.28 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.3990813594085147		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 1.3990813594085147 | validation: 3.0031515212317643]
	TIME [epoch: 3.28 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 2/2] avg loss: 2.118985649942518		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 2.118985649942518 | validation: 1.6035753847833356]
	TIME [epoch: 3.28 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2242649843251512		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 1.2242649843251512 | validation: 1.3171282857611284]
	TIME [epoch: 3.28 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1902881213154677		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 1.1902881213154677 | validation: 1.2540146129306162]
	TIME [epoch: 3.27 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.011614918875142		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 1.011614918875142 | validation: 1.2601989413194343]
	TIME [epoch: 3.27 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0200713328805644		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 1.0200713328805644 | validation: 1.243767470335022]
	TIME [epoch: 3.28 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0236277161581384		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 1.0236277161581384 | validation: 1.1662380931808418]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9707523019804263		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.9707523019804263 | validation: 1.3437865679093517]
	TIME [epoch: 3.27 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9803091767301357		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.9803091767301357 | validation: 1.1504358868348132]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.035130205981826		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 1.035130205981826 | validation: 1.2104691189896901]
	TIME [epoch: 3.28 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0080044669447568		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 1.0080044669447568 | validation: 1.2105574182071004]
	TIME [epoch: 3.27 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9485341463534334		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.9485341463534334 | validation: 1.1309501970648197]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.945223140442855		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.945223140442855 | validation: 1.2143645214988092]
	TIME [epoch: 3.28 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9784070228783025		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.9784070228783025 | validation: 1.2563541799619538]
	TIME [epoch: 3.27 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9443440159461779		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.9443440159461779 | validation: 1.1898857472918014]
	TIME [epoch: 3.27 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.900351648230757		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.900351648230757 | validation: 1.1492002166168491]
	TIME [epoch: 3.27 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9016820890727666		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.9016820890727666 | validation: 1.3734625767607627]
	TIME [epoch: 3.28 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0976138368237076		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 1.0976138368237076 | validation: 1.5888742101206135]
	TIME [epoch: 3.28 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0928946348752828		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 1.0928946348752828 | validation: 1.083318206582395]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.855426208106248		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.855426208106248 | validation: 1.1495462341489773]
	TIME [epoch: 3.31 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0336410792525637		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 1.0336410792525637 | validation: 1.8682609146326428]
	TIME [epoch: 3.28 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2047939905135352		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 1.2047939905135352 | validation: 1.1267507950444933]
	TIME [epoch: 3.28 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8569860987440636		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.8569860987440636 | validation: 1.2695674008329816]
	TIME [epoch: 3.27 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9923855745933756		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.9923855745933756 | validation: 1.05935956030771]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8361483140048945		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.8361483140048945 | validation: 1.0641675520833]
	TIME [epoch: 3.27 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8447522905755472		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.8447522905755472 | validation: 1.1236647069061276]
	TIME [epoch: 3.27 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8645328956759808		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.8645328956759808 | validation: 1.3083501379173212]
	TIME [epoch: 3.27 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.1668810453352485		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 1.1668810453352485 | validation: 1.1717715321749222]
	TIME [epoch: 3.27 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9130100132883282		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.9130100132883282 | validation: 1.0405838505674943]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8606025364465246		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.8606025364465246 | validation: 1.1130837973645449]
	TIME [epoch: 3.28 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.976076654346042		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.976076654346042 | validation: 1.2011501845246144]
	TIME [epoch: 3.31 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8768120316068347		[learning rate: 0.0050834]
	Learning Rate: 0.00508339
	LOSS [training: 0.8768120316068347 | validation: 1.068372251506062]
	TIME [epoch: 3.28 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8514023202248666		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.8514023202248666 | validation: 1.032050963717993]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8065264666232883		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.8065264666232883 | validation: 1.0675263387555487]
	TIME [epoch: 3.28 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8080585551484285		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8080585551484285 | validation: 1.0371177382757064]
	TIME [epoch: 3.28 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.808106444067953		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.808106444067953 | validation: 1.1213628514070153]
	TIME [epoch: 3.27 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8403916287613697		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.8403916287613697 | validation: 1.0115623349835159]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7966445406328871		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7966445406328871 | validation: 0.9879203983431047]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8867337495215772		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.8867337495215772 | validation: 1.237646391962701]
	TIME [epoch: 3.26 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.147142809107686		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 1.147142809107686 | validation: 1.3094578127788898]
	TIME [epoch: 3.26 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0425737352011346		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 1.0425737352011346 | validation: 0.9782562320054929]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.809810186497266		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.809810186497266 | validation: 0.971837202798041]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7986040501257536		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7986040501257536 | validation: 1.0596955893489126]
	TIME [epoch: 3.27 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7914969949418129		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7914969949418129 | validation: 1.169986383395127]
	TIME [epoch: 3.27 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8511888808693882		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.8511888808693882 | validation: 0.9692432993636164]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.737691686284222		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.737691686284222 | validation: 0.9639138588551923]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8000730184993914		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.8000730184993914 | validation: 0.9880560685301308]
	TIME [epoch: 3.28 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7389574022955196		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.7389574022955196 | validation: 0.9872772637175754]
	TIME [epoch: 3.27 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7523317970868074		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.7523317970868074 | validation: 1.083135005280833]
	TIME [epoch: 3.27 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8032608862332673		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.8032608862332673 | validation: 0.9728000162368347]
	TIME [epoch: 3.27 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7880688414531919		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7880688414531919 | validation: 1.0499034127895106]
	TIME [epoch: 3.27 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7572163324249446		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7572163324249446 | validation: 1.1442265081933407]
	TIME [epoch: 3.3 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7568613402825217		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.7568613402825217 | validation: 0.9801606042500997]
	TIME [epoch: 3.29 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.770717099140847		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.770717099140847 | validation: 0.8888747438445187]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7638709827008585		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.7638709827008585 | validation: 0.9789614119058752]
	TIME [epoch: 3.28 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8030482074101387		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.8030482074101387 | validation: 0.9903549632674953]
	TIME [epoch: 3.28 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7988312366682402		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7988312366682402 | validation: 0.9665609070896937]
	TIME [epoch: 3.28 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7035431901698188		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.7035431901698188 | validation: 0.9268380751485559]
	TIME [epoch: 3.27 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.7175779496395687		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.7175779496395687 | validation: 0.9106912221359537]
	TIME [epoch: 3.27 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6516947546488007		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6516947546488007 | validation: 0.9256185605856571]
	TIME [epoch: 3.27 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6876606337873681		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6876606337873681 | validation: 1.263884604632847]
	TIME [epoch: 3.27 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8069743428744851		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.8069743428744851 | validation: 0.8565733620922513]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.661258861371807		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.661258861371807 | validation: 0.8363771036685986]
	TIME [epoch: 3.31 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6675289069823163		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6675289069823163 | validation: 1.0840454037369003]
	TIME [epoch: 3.29 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.657540097783754		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.657540097783754 | validation: 0.8046846763315968]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6363379767054348		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6363379767054348 | validation: 0.795858737983931]
	TIME [epoch: 3.26 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6127567184237039		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6127567184237039 | validation: 1.2887349854205923]
	TIME [epoch: 3.26 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.845018416636194		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.845018416636194 | validation: 0.8898385913504221]
	TIME [epoch: 3.26 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6600638924412878		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6600638924412878 | validation: 0.9590414363519795]
	TIME [epoch: 3.26 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.639279849026288		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.639279849026288 | validation: 0.8666417124726714]
	TIME [epoch: 3.27 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6655742720229981		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6655742720229981 | validation: 0.8200524097498536]
	TIME [epoch: 3.26 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5425071343206065		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.5425071343206065 | validation: 0.8702397836984025]
	TIME [epoch: 3.26 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6679195893908823		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6679195893908823 | validation: 0.8784460275885911]
	TIME [epoch: 3.26 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6054808021150246		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6054808021150246 | validation: 0.7669933201841019]
	TIME [epoch: 3.3 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5616325853924647		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.5616325853924647 | validation: 0.7982380466928524]
	TIME [epoch: 3.27 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5354457886537505		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.5354457886537505 | validation: 0.7181305334801134]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5741893419165884		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5741893419165884 | validation: 0.7455145516214794]
	TIME [epoch: 3.27 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5423136848429122		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.5423136848429122 | validation: 0.74693618090209]
	TIME [epoch: 3.27 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5228367956170631		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.5228367956170631 | validation: 0.7333168183448624]
	TIME [epoch: 3.27 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4947815408727483		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.4947815408727483 | validation: 0.7813671462821956]
	TIME [epoch: 3.27 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6203362266846507		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6203362266846507 | validation: 0.7944266497066421]
	TIME [epoch: 3.27 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.0191925333289484		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 1.0191925333289484 | validation: 2.1376204122757727]
	TIME [epoch: 3.27 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2367292030799748		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 1.2367292030799748 | validation: 0.8439972959997383]
	TIME [epoch: 3.27 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.677655460949542		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.677655460949542 | validation: 0.7989797194657368]
	TIME [epoch: 3.28 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5173512937720253		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5173512937720253 | validation: 0.6594319047092697]
	TIME [epoch: 3.31 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46793112158351036		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.46793112158351036 | validation: 0.8139755000747986]
	TIME [epoch: 3.28 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48713330129654964		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.48713330129654964 | validation: 0.6891427238906839]
	TIME [epoch: 3.28 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.48652538333412354		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.48652538333412354 | validation: 0.8342869963571982]
	TIME [epoch: 3.28 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4658905166051634		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.4658905166051634 | validation: 0.6175789357840487]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4264800256032215		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.4264800256032215 | validation: 0.7295952106554133]
	TIME [epoch: 3.28 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43797791689402166		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.43797791689402166 | validation: 0.5842821355475678]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.40470831870137314		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.40470831870137314 | validation: 0.6583444079562195]
	TIME [epoch: 3.28 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.38185185276385547		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.38185185276385547 | validation: 0.566362606599727]
	TIME [epoch: 3.28 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5151946137369023		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5151946137369023 | validation: 0.5822444071487919]
	TIME [epoch: 3.27 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4848571068747757		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.4848571068747757 | validation: 0.6676465902142827]
	TIME [epoch: 3.29 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4597185305331839		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.4597185305331839 | validation: 0.7707448689324558]
	TIME [epoch: 3.31 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.43986593759365644		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.43986593759365644 | validation: 0.5950314350179531]
	TIME [epoch: 3.28 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3535082149550127		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.3535082149550127 | validation: 0.5830083007895376]
	TIME [epoch: 3.27 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41464540766889607		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.41464540766889607 | validation: 0.6252435940221188]
	TIME [epoch: 3.28 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46609526423674313		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.46609526423674313 | validation: 0.57610550070143]
	TIME [epoch: 3.28 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.41940930541412913		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.41940930541412913 | validation: 0.6231225830400512]
	TIME [epoch: 3.28 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.39785311322264216		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.39785311322264216 | validation: 0.5644156223489042]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3561117879929235		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.3561117879929235 | validation: 0.6739604061488113]
	TIME [epoch: 3.27 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4354061834813704		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.4354061834813704 | validation: 0.5041843200986106]
	TIME [epoch: 3.27 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.9685275054105511		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.9685275054105511 | validation: 2.6560617127771025]
	TIME [epoch: 3.27 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.7251261052919675		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 1.7251261052919675 | validation: 1.448327141947713]
	TIME [epoch: 3.28 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 2/2] avg loss: 1.2412688363473006		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 1.2412688363473006 | validation: 0.845029228357941]
	TIME [epoch: 3.29 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5616064770796854		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5616064770796854 | validation: 0.6720874774912045]
	TIME [epoch: 3.27 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5859880147129037		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.5859880147129037 | validation: 0.6396085435481933]
	TIME [epoch: 3.26 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.46130664874833177		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.46130664874833177 | validation: 0.565584503599574]
	TIME [epoch: 3.26 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.369427144946359		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.369427144946359 | validation: 0.5363831280949396]
	TIME [epoch: 3.26 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.33191148016365146		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.33191148016365146 | validation: 0.475354388908941]
	TIME [epoch: 108 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30534651706101146		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.30534651706101146 | validation: 0.44437400854088366]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3090836697509688		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.3090836697509688 | validation: 0.5020373654410223]
	TIME [epoch: 6.42 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.313134891562706		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.313134891562706 | validation: 0.5451171522701393]
	TIME [epoch: 6.44 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31890723911271746		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.31890723911271746 | validation: 0.5235336859782384]
	TIME [epoch: 6.43 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2859947101958964		[learning rate: 0.0027837]
	Learning Rate: 0.00278366
	LOSS [training: 0.2859947101958964 | validation: 0.4470891060581087]
	TIME [epoch: 6.42 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3468592031165669		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.3468592031165669 | validation: 0.44984724729181835]
	TIME [epoch: 6.42 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29628442892506507		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.29628442892506507 | validation: 0.44794734264903485]
	TIME [epoch: 6.43 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.32369497758163934		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.32369497758163934 | validation: 0.46414687399779225]
	TIME [epoch: 6.42 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2753870002203066		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.2753870002203066 | validation: 0.44503535946526]
	TIME [epoch: 6.45 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25386436345095764		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.25386436345095764 | validation: 0.3935477993378325]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25927851689059045		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.25927851689059045 | validation: 0.521965894862127]
	TIME [epoch: 6.43 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3447666931286431		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.3447666931286431 | validation: 0.6705584973452821]
	TIME [epoch: 6.41 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3701706195354061		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.3701706195354061 | validation: 0.43847381692188026]
	TIME [epoch: 6.43 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.336725281508966		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.336725281508966 | validation: 0.4335581743341479]
	TIME [epoch: 6.43 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.304912096453042		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.304912096453042 | validation: 0.47263620476919277]
	TIME [epoch: 6.46 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.27189269664835913		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.27189269664835913 | validation: 0.5737702631066671]
	TIME [epoch: 6.48 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.29409630240459383		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.29409630240459383 | validation: 0.48869292577617807]
	TIME [epoch: 6.43 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25054554465193757		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.25054554465193757 | validation: 0.41857842404193923]
	TIME [epoch: 6.43 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24437449254996782		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.24437449254996782 | validation: 0.4984385514867585]
	TIME [epoch: 6.43 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.30493224818030445		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.30493224818030445 | validation: 0.4129248510177835]
	TIME [epoch: 6.43 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24182332154200645		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.24182332154200645 | validation: 0.3906509798927302]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5913801000739864		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.5913801000739864 | validation: 0.7515281054661548]
	TIME [epoch: 6.46 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3780437588095289		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.3780437588095289 | validation: 0.49100451004038903]
	TIME [epoch: 6.43 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25761486483439694		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.25761486483439694 | validation: 0.4380980021809013]
	TIME [epoch: 6.44 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.25660812994652205		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.25660812994652205 | validation: 0.6003401737747963]
	TIME [epoch: 6.44 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.31189194235850126		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.31189194235850126 | validation: 0.38997572893926036]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23008565494219707		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.23008565494219707 | validation: 0.36646498943034633]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2907840732536733		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.2907840732536733 | validation: 0.48096473851245625]
	TIME [epoch: 6.46 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2878248059475763		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.2878248059475763 | validation: 0.39090494397294295]
	TIME [epoch: 6.42 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23428889162964728		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.23428889162964728 | validation: 0.3808498959814337]
	TIME [epoch: 6.43 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2859099714041252		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2859099714041252 | validation: 0.7479425129623505]
	TIME [epoch: 6.42 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.3170739739945408		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.3170739739945408 | validation: 0.32715709556965333]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24786184549290552		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.24786184549290552 | validation: 0.3310134717024576]
	TIME [epoch: 6.46 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21821913697424836		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.21821913697424836 | validation: 0.31793650770273346]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2125079709481108		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2125079709481108 | validation: 0.3506522629586278]
	TIME [epoch: 6.43 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2242242266303668		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.2242242266303668 | validation: 0.32843358368927805]
	TIME [epoch: 6.43 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18706852669286128		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.18706852669286128 | validation: 0.339380059088803]
	TIME [epoch: 6.43 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2370126199814892		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.2370126199814892 | validation: 0.41559414471761563]
	TIME [epoch: 6.45 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22649736075478494		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.22649736075478494 | validation: 0.3213146410667731]
	TIME [epoch: 6.46 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19601608957479588		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.19601608957479588 | validation: 0.4214982472639583]
	TIME [epoch: 6.43 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.24647057832411626		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.24647057832411626 | validation: 0.2984729150456789]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2384671815841753		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.2384671815841753 | validation: 0.7771939830028377]
	TIME [epoch: 6.44 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6182850748679771		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.6182850748679771 | validation: 0.3527646969453157]
	TIME [epoch: 6.41 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.22316432767519623		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.22316432767519623 | validation: 0.3588625138153764]
	TIME [epoch: 6.44 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2582171455201292		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.2582171455201292 | validation: 0.2767892167517954]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.330715798789161		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.330715798789161 | validation: 0.3393265741541184]
	TIME [epoch: 6.45 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.21261740182691524		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.21261740182691524 | validation: 0.6405321435584569]
	TIME [epoch: 6.45 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2978008740901057		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.2978008740901057 | validation: 0.3047996496342976]
	TIME [epoch: 6.45 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20320375306903835		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.20320375306903835 | validation: 0.4082121973338235]
	TIME [epoch: 6.45 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18943408824087557		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.18943408824087557 | validation: 0.2821323730893868]
	TIME [epoch: 6.46 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18700580720276277		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.18700580720276277 | validation: 0.2935242803276754]
	TIME [epoch: 6.47 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16601820513019577		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.16601820513019577 | validation: 0.3023380311154593]
	TIME [epoch: 6.42 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.20216278819155498		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.20216278819155498 | validation: 0.27180683800415517]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19190601955856273		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.19190601955856273 | validation: 0.2829900973931955]
	TIME [epoch: 6.43 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16867258477815855		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.16867258477815855 | validation: 0.3178571990722112]
	TIME [epoch: 6.43 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18478673515047794		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.18478673515047794 | validation: 0.31325978349737016]
	TIME [epoch: 6.47 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2122902528393199		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.2122902528393199 | validation: 0.2702329508481973]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1646853698843191		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1646853698843191 | validation: 0.2835132896136952]
	TIME [epoch: 6.43 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15909742296185386		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.15909742296185386 | validation: 0.33503845617147904]
	TIME [epoch: 6.42 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.2362035815928099		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.2362035815928099 | validation: 0.3114984612145149]
	TIME [epoch: 6.43 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1573837363370299		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1573837363370299 | validation: 0.2505838624805733]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15231982634434832		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.15231982634434832 | validation: 0.2471529727423276]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26777715548395103		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.26777715548395103 | validation: 0.2601514988548653]
	TIME [epoch: 6.46 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16790970459622132		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.16790970459622132 | validation: 0.29301340250550617]
	TIME [epoch: 6.44 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14239077680304696		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.14239077680304696 | validation: 0.24288311656012246]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1647687389343488		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1647687389343488 | validation: 0.22549498433470264]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1527616305949985		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1527616305949985 | validation: 0.24795483658761702]
	TIME [epoch: 6.47 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14059488634085365		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.14059488634085365 | validation: 0.3010968556544198]
	TIME [epoch: 6.51 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.19949589768292353		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.19949589768292353 | validation: 0.2854738278138487]
	TIME [epoch: 6.46 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17117207196522846		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.17117207196522846 | validation: 0.41200071805546856]
	TIME [epoch: 6.47 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1991538878411165		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.1991538878411165 | validation: 0.3691294814321889]
	TIME [epoch: 6.46 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1729331677175489		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.1729331677175489 | validation: 0.270129369255928]
	TIME [epoch: 6.45 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16482823706641855		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.16482823706641855 | validation: 0.491963237671429]
	TIME [epoch: 6.47 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.26609663011725243		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.26609663011725243 | validation: 0.3876969114099923]
	TIME [epoch: 6.5 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16821563288264582		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.16821563288264582 | validation: 0.21902121781660613]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15594589305919218		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.15594589305919218 | validation: 0.26354482996108475]
	TIME [epoch: 6.45 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15723529878737233		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.15723529878737233 | validation: 0.2832616005174473]
	TIME [epoch: 6.44 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1565354710129198		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.1565354710129198 | validation: 0.41283968738662946]
	TIME [epoch: 6.45 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.23183487500433697		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.23183487500433697 | validation: 0.2991950945569033]
	TIME [epoch: 6.46 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15041824735277864		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.15041824735277864 | validation: 0.22290366311001109]
	TIME [epoch: 6.48 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16419458622051092		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.16419458622051092 | validation: 0.23048819857160352]
	TIME [epoch: 6.45 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12409431824116235		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.12409431824116235 | validation: 0.20082416608815984]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12374864471609874		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.12374864471609874 | validation: 0.7504496848404609]
	TIME [epoch: 6.45 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.8677860986277275		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.8677860986277275 | validation: 0.5798555678049316]
	TIME [epoch: 6.47 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.5241095378991958		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.5241095378991958 | validation: 0.26553910826968763]
	TIME [epoch: 6.47 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.18078584587720747		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.18078584587720747 | validation: 0.2542825706426669]
	TIME [epoch: 6.48 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13963282477063962		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.13963282477063962 | validation: 0.24854171654074364]
	TIME [epoch: 6.45 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13791060116652354		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.13791060116652354 | validation: 0.2181966885405908]
	TIME [epoch: 6.46 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12398575726523023		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.12398575726523023 | validation: 0.2092499546963961]
	TIME [epoch: 6.45 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12184381274146246		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.12184381274146246 | validation: 0.23220059623485778]
	TIME [epoch: 6.46 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13103018331821495		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.13103018331821495 | validation: 0.34208761197464677]
	TIME [epoch: 6.46 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1843158278819561		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.1843158278819561 | validation: 0.4176401441935367]
	TIME [epoch: 6.47 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1690339178286133		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.1690339178286133 | validation: 0.21988235901769126]
	TIME [epoch: 6.45 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13505910681137		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.13505910681137 | validation: 0.2559887868372305]
	TIME [epoch: 6.45 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13752338056185548		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.13752338056185548 | validation: 0.220797371312255]
	TIME [epoch: 6.45 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12779692583942906		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.12779692583942906 | validation: 0.2666389916450651]
	TIME [epoch: 6.45 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13383717429018363		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.13383717429018363 | validation: 0.27697338138289657]
	TIME [epoch: 6.47 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13081175553907945		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.13081175553907945 | validation: 0.1968117594711133]
	TIME [epoch: 6.49 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12094326188560367		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.12094326188560367 | validation: 0.21076988419039833]
	TIME [epoch: 6.45 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14208854499579288		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.14208854499579288 | validation: 0.25448528537781306]
	TIME [epoch: 6.44 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13550217363204087		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.13550217363204087 | validation: 0.18984453546863167]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10729019316667829		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.10729019316667829 | validation: 0.2638109425030108]
	TIME [epoch: 6.47 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15042336433015874		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.15042336433015874 | validation: 0.23240700055157731]
	TIME [epoch: 6.49 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.14338607114957352		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.14338607114957352 | validation: 0.21385114375227043]
	TIME [epoch: 6.44 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1337690419624612		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.1337690419624612 | validation: 0.17903921115021915]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10717797552954045		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.10717797552954045 | validation: 0.18721633939545057]
	TIME [epoch: 6.45 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11001311901015208		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.11001311901015208 | validation: 0.1923541633370689]
	TIME [epoch: 6.44 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11432703024072854		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.11432703024072854 | validation: 0.19441065311300212]
	TIME [epoch: 6.47 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10845504696521151		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10845504696521151 | validation: 0.2343790148979598]
	TIME [epoch: 6.49 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1632125821170907		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.1632125821170907 | validation: 0.16856532941660662]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09988722489422475		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.09988722489422475 | validation: 0.1899763997951196]
	TIME [epoch: 6.44 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11128678132572101		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.11128678132572101 | validation: 0.28143515703908123]
	TIME [epoch: 6.44 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16638652784314795		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.16638652784314795 | validation: 0.27022481298207357]
	TIME [epoch: 6.44 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15132706157948145		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.15132706157948145 | validation: 0.23083704518721612]
	TIME [epoch: 6.44 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12368881158338908		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.12368881158338908 | validation: 0.24989030492738096]
	TIME [epoch: 6.48 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1128922217051766		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.1128922217051766 | validation: 0.18853929779612388]
	TIME [epoch: 6.49 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10650980410217299		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.10650980410217299 | validation: 0.18997568814092122]
	TIME [epoch: 6.44 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1044038831817522		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.1044038831817522 | validation: 0.16908833045688898]
	TIME [epoch: 6.46 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0971105305511714		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.0971105305511714 | validation: 0.22279481657256586]
	TIME [epoch: 6.45 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12576481269353323		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.12576481269353323 | validation: 0.2779819168529839]
	TIME [epoch: 6.47 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16290730782780138		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.16290730782780138 | validation: 0.1713393916252886]
	TIME [epoch: 6.49 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12472205491728194		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.12472205491728194 | validation: 0.15882907714269343]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09571279981692975		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.09571279981692975 | validation: 0.17538148982065882]
	TIME [epoch: 6.46 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09299762605570701		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.09299762605570701 | validation: 0.17456136244526968]
	TIME [epoch: 6.45 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.6464510429446805		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.6464510429446805 | validation: 1.0801165935885553]
	TIME [epoch: 6.46 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.4592325778645274		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.4592325778645274 | validation: 0.22380486743817835]
	TIME [epoch: 6.46 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1593721025644842		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.1593721025644842 | validation: 0.17424583250990366]
	TIME [epoch: 6.51 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11327892170025812		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.11327892170025812 | validation: 0.14739549406072947]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1186063566772417		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.1186063566772417 | validation: 0.15241189846406203]
	TIME [epoch: 6.46 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09533715369462731		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.09533715369462731 | validation: 0.1429953642858631]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09861039274965472		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.09861039274965472 | validation: 0.16740524372464033]
	TIME [epoch: 6.43 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1029688169244447		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.1029688169244447 | validation: 0.16774786220654928]
	TIME [epoch: 6.45 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09206229635958274		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.09206229635958274 | validation: 0.13880474273598875]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1032837573593893		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.1032837573593893 | validation: 0.12860992466100313]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09497039076701055		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.09497039076701055 | validation: 0.16725965006063143]
	TIME [epoch: 6.45 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08800682228312887		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.08800682228312887 | validation: 0.1547588766269891]
	TIME [epoch: 6.44 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09280728114196779		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.09280728114196779 | validation: 0.16422243365125846]
	TIME [epoch: 6.45 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10206364608934125		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.10206364608934125 | validation: 0.13848877194729686]
	TIME [epoch: 6.47 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09950205978074078		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.09950205978074078 | validation: 0.15916167351854255]
	TIME [epoch: 6.5 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09725558475387894		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.09725558475387894 | validation: 0.180055776704936]
	TIME [epoch: 6.46 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11621089467758437		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.11621089467758437 | validation: 0.3168209507410964]
	TIME [epoch: 6.46 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.12620769949657984		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.12620769949657984 | validation: 0.16632913255450268]
	TIME [epoch: 6.45 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10373768315445515		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.10373768315445515 | validation: 0.17675565452080957]
	TIME [epoch: 6.45 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09384700791366116		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.09384700791366116 | validation: 0.15974302407667695]
	TIME [epoch: 6.47 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11862673861076395		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.11862673861076395 | validation: 0.13231423318756683]
	TIME [epoch: 6.48 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0927961794660333		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0927961794660333 | validation: 0.16846071761330916]
	TIME [epoch: 6.46 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08883327051269542		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08883327051269542 | validation: 0.14434366769226278]
	TIME [epoch: 6.46 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08726364664539402		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.08726364664539402 | validation: 0.13624969200494894]
	TIME [epoch: 6.47 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08264970348868472		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.08264970348868472 | validation: 0.162953260754304]
	TIME [epoch: 6.45 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08565943010507276		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.08565943010507276 | validation: 0.15749332374590783]
	TIME [epoch: 6.47 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09701906571314405		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.09701906571314405 | validation: 0.24018072201587348]
	TIME [epoch: 6.48 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.13189084231844175		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.13189084231844175 | validation: 0.1280474539499196]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0856300353968221		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.0856300353968221 | validation: 0.1364326357303399]
	TIME [epoch: 6.45 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0881445497686166		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.0881445497686166 | validation: 0.14726051354497546]
	TIME [epoch: 6.47 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11354924312203034		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.11354924312203034 | validation: 0.1818918085919441]
	TIME [epoch: 6.45 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09825007986709024		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.09825007986709024 | validation: 0.18335612359045608]
	TIME [epoch: 6.48 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10092146461083573		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.10092146461083573 | validation: 0.12700257391016226]
	TIME [epoch: 6.47 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08151356561766196		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.08151356561766196 | validation: 0.14295079588131618]
	TIME [epoch: 6.43 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07931158131313458		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.07931158131313458 | validation: 0.14161159261116038]
	TIME [epoch: 6.45 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08939627121475995		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.08939627121475995 | validation: 0.14001580978851189]
	TIME [epoch: 6.45 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09102052661480871		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.09102052661480871 | validation: 0.11792247488454902]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08048758029602526		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.08048758029602526 | validation: 0.14289834277865185]
	TIME [epoch: 6.49 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07554992856220707		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.07554992856220707 | validation: 0.13702423318734755]
	TIME [epoch: 6.46 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08120848762268326		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.08120848762268326 | validation: 0.1213990195014828]
	TIME [epoch: 6.46 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07828385964032925		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.07828385964032925 | validation: 0.17524895119641778]
	TIME [epoch: 6.45 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11315530119586212		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.11315530119586212 | validation: 0.11634941049987946]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08048400412356166		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.08048400412356166 | validation: 0.10923488623718902]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07913480864060018		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.07913480864060018 | validation: 0.1574173267933482]
	TIME [epoch: 6.45 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.16167996673643276		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.16167996673643276 | validation: 0.19813851221478812]
	TIME [epoch: 6.44 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10555519738234916		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.10555519738234916 | validation: 0.11637527527760719]
	TIME [epoch: 6.44 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0790793213886795		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.0790793213886795 | validation: 0.11929516491968528]
	TIME [epoch: 6.43 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0727118357837451		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.0727118357837451 | validation: 0.11330806077503852]
	TIME [epoch: 6.44 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06819049680514377		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.06819049680514377 | validation: 0.12508430592035397]
	TIME [epoch: 6.46 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07415055250749984		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.07415055250749984 | validation: 0.1148282782559965]
	TIME [epoch: 6.47 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07938695359008494		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.07938695359008494 | validation: 0.11862275994153118]
	TIME [epoch: 6.45 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08587677280566822		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.08587677280566822 | validation: 0.15015459442009763]
	TIME [epoch: 6.44 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10052766291435103		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.10052766291435103 | validation: 0.15025621713432621]
	TIME [epoch: 6.45 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07874092235071642		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.07874092235071642 | validation: 0.11346444830052088]
	TIME [epoch: 6.46 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07951682604039166		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.07951682604039166 | validation: 0.1638726954820141]
	TIME [epoch: 6.46 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08676461086126874		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.08676461086126874 | validation: 0.1261777285847114]
	TIME [epoch: 6.48 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08002419732563326		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.08002419732563326 | validation: 0.10737280191949215]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07171843020933838		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.07171843020933838 | validation: 0.1295747551899906]
	TIME [epoch: 6.45 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.069557453118212		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.069557453118212 | validation: 0.1347967380584918]
	TIME [epoch: 6.46 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06904468383727852		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.06904468383727852 | validation: 0.10635950664873306]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07044438745912468		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.07044438745912468 | validation: 0.10225458998210123]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07030671934848698		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.07030671934848698 | validation: 0.17151370975537292]
	TIME [epoch: 6.49 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10980516778293971		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.10980516778293971 | validation: 0.10052408428791514]
	TIME [epoch: 6.43 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06921291515372341		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.06921291515372341 | validation: 0.11196252595364956]
	TIME [epoch: 6.45 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07302555678474827		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.07302555678474827 | validation: 0.10415870878963931]
	TIME [epoch: 6.45 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06423727066073817		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.06423727066073817 | validation: 0.11509786153789477]
	TIME [epoch: 6.46 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06949212847766348		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.06949212847766348 | validation: 0.10832445891313991]
	TIME [epoch: 6.46 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08456764309374204		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.08456764309374204 | validation: 0.09877531249881966]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07159059713039705		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.07159059713039705 | validation: 0.11248768475796174]
	TIME [epoch: 6.44 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06997450471139167		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.06997450471139167 | validation: 0.12379682565282919]
	TIME [epoch: 6.47 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06741902442992909		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06741902442992909 | validation: 0.10753874467926812]
	TIME [epoch: 6.45 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06990029127574278		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.06990029127574278 | validation: 0.1226499406733094]
	TIME [epoch: 6.46 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07911257136489022		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.07911257136489022 | validation: 0.1476408244408165]
	TIME [epoch: 6.47 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09500831635178927		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.09500831635178927 | validation: 0.11931080691470886]
	TIME [epoch: 6.51 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06758846322429968		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.06758846322429968 | validation: 0.2229314011416193]
	TIME [epoch: 6.44 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.11254423120875393		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.11254423120875393 | validation: 0.13295476020001187]
	TIME [epoch: 6.45 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0899214361147074		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.0899214361147074 | validation: 0.15292387054011283]
	TIME [epoch: 6.45 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09443147428949844		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.09443147428949844 | validation: 0.14315935764232587]
	TIME [epoch: 6.46 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07787089476676226		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.07787089476676226 | validation: 0.10045776873061939]
	TIME [epoch: 6.46 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07155730163676209		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.07155730163676209 | validation: 0.09773228608106406]
	TIME [epoch: 6.48 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_405.pth
	Model improved!!!
EPOCH 406/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06380804121672942		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.06380804121672942 | validation: 0.10512175241776296]
	TIME [epoch: 6.46 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0638517891688295		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.0638517891688295 | validation: 0.09390317494044681]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_407.pth
	Model improved!!!
EPOCH 408/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.061853898339705525		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.061853898339705525 | validation: 0.09827067655970546]
	TIME [epoch: 6.45 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06210561381576285		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.06210561381576285 | validation: 0.1136493482774874]
	TIME [epoch: 6.45 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06698959211812819		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.06698959211812819 | validation: 0.10322379070888169]
	TIME [epoch: 6.46 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07062482966734068		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.07062482966734068 | validation: 0.11716026504797833]
	TIME [epoch: 6.51 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07194298795292961		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.07194298795292961 | validation: 0.09371342402472356]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06255988262295006		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.06255988262295006 | validation: 0.0911328828456937]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06913399395322893		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.06913399395322893 | validation: 0.1315733175704427]
	TIME [epoch: 6.46 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06576287620533858		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.06576287620533858 | validation: 0.11101477954936526]
	TIME [epoch: 6.46 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0702606633995557		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.0702606633995557 | validation: 0.11829725473714077]
	TIME [epoch: 6.48 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08058019922254649		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.08058019922254649 | validation: 0.09836846179603592]
	TIME [epoch: 6.48 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08352941049426076		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.08352941049426076 | validation: 0.10551336045071938]
	TIME [epoch: 6.47 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05925854487856804		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.05925854487856804 | validation: 0.0959154220453719]
	TIME [epoch: 6.46 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06207724060577435		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.06207724060577435 | validation: 0.13221582046263278]
	TIME [epoch: 6.46 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06793119645137947		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.06793119645137947 | validation: 0.09850821721447715]
	TIME [epoch: 6.47 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05961590415856538		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.05961590415856538 | validation: 0.09451542675633813]
	TIME [epoch: 6.5 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07111382881895185		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.07111382881895185 | validation: 0.08982884757292925]
	TIME [epoch: 6.5 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_423.pth
	Model improved!!!
EPOCH 424/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07776711307845124		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.07776711307845124 | validation: 0.12317268356933693]
	TIME [epoch: 6.45 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0684660572043572		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.0684660572043572 | validation: 0.09137522332899227]
	TIME [epoch: 6.46 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06941932253159105		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.06941932253159105 | validation: 0.10623386450462186]
	TIME [epoch: 6.46 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0641778960480266		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.0641778960480266 | validation: 0.08942387405376148]
	TIME [epoch: 6.46 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06803534091267968		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.06803534091267968 | validation: 0.07949727878458906]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06603027260231659		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06603027260231659 | validation: 0.09884888175419054]
	TIME [epoch: 6.44 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06818863722182669		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.06818863722182669 | validation: 0.1218843390275673]
	TIME [epoch: 6.44 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06941972258002818		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.06941972258002818 | validation: 0.12093679110589922]
	TIME [epoch: 6.44 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06664919248601303		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.06664919248601303 | validation: 0.11201713740245554]
	TIME [epoch: 6.43 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07334164656300332		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.07334164656300332 | validation: 0.10398308618294312]
	TIME [epoch: 6.44 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059833699672875426		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.059833699672875426 | validation: 0.08484066270861917]
	TIME [epoch: 6.48 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15419628394310309		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.15419628394310309 | validation: 0.32473382550831364]
	TIME [epoch: 6.45 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.17130119015647322		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.17130119015647322 | validation: 0.10087305340954894]
	TIME [epoch: 6.43 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0642625976577236		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0642625976577236 | validation: 0.11278758083025636]
	TIME [epoch: 6.43 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06970355481339469		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.06970355481339469 | validation: 0.08358138873927186]
	TIME [epoch: 6.44 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05781861944713235		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.05781861944713235 | validation: 0.08872951040826398]
	TIME [epoch: 6.44 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05948423573072868		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.05948423573072868 | validation: 0.10026386087074508]
	TIME [epoch: 6.45 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.058423071022706774		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.058423071022706774 | validation: 0.08568886411684638]
	TIME [epoch: 6.46 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060889118254534476		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.060889118254534476 | validation: 0.09020240813413015]
	TIME [epoch: 6.43 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05805164063741633		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.05805164063741633 | validation: 0.09271444237107601]
	TIME [epoch: 6.43 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.057462057419914364		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.057462057419914364 | validation: 0.09686829756310669]
	TIME [epoch: 6.42 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054867194926486056		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.054867194926486056 | validation: 0.09122018907584871]
	TIME [epoch: 6.43 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06141558315130469		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.06141558315130469 | validation: 0.08330639211285884]
	TIME [epoch: 6.45 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.062405148484487694		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.062405148484487694 | validation: 0.12017694445150831]
	TIME [epoch: 6.47 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06855519281967452		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.06855519281967452 | validation: 0.08816607290767636]
	TIME [epoch: 6.44 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05600080204838513		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.05600080204838513 | validation: 0.09855504348555329]
	TIME [epoch: 6.42 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05896181527653723		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.05896181527653723 | validation: 0.08734735360637003]
	TIME [epoch: 6.41 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05900450026997818		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.05900450026997818 | validation: 0.10751755353504484]
	TIME [epoch: 6.41 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05918977326331715		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.05918977326331715 | validation: 0.10101640750644428]
	TIME [epoch: 6.47 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06712725503933067		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.06712725503933067 | validation: 0.10109214545635044]
	TIME [epoch: 6.47 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06294109665231012		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.06294109665231012 | validation: 0.08280840285800424]
	TIME [epoch: 6.43 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059367369981892446		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.059367369981892446 | validation: 0.09689599174815165]
	TIME [epoch: 6.43 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059933366649841904		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.059933366649841904 | validation: 0.0821959486112994]
	TIME [epoch: 6.41 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06060163351319956		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.06060163351319956 | validation: 0.091611305936582]
	TIME [epoch: 6.43 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05661635460559363		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.05661635460559363 | validation: 0.08122671823928221]
	TIME [epoch: 6.46 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05801522959483954		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.05801522959483954 | validation: 0.08494147894243963]
	TIME [epoch: 6.46 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05692271122222922		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.05692271122222922 | validation: 0.08997133701392135]
	TIME [epoch: 6.44 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054765468136674586		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.054765468136674586 | validation: 0.10133721804106054]
	TIME [epoch: 6.44 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06141012531763843		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.06141012531763843 | validation: 0.10306491304237951]
	TIME [epoch: 6.43 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05991904310205004		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.05991904310205004 | validation: 0.07487214268469003]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060167319188831406		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.060167319188831406 | validation: 0.08788090538910372]
	TIME [epoch: 6.48 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05447815028083388		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.05447815028083388 | validation: 0.08125859867963087]
	TIME [epoch: 6.44 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05854323256209269		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.05854323256209269 | validation: 0.07450722658162302]
	TIME [epoch: 6.45 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056572396488688684		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.056572396488688684 | validation: 0.08356478823057087]
	TIME [epoch: 6.42 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05853463775615784		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.05853463775615784 | validation: 0.08833548721476353]
	TIME [epoch: 6.44 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07203343060832434		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.07203343060832434 | validation: 0.08816805837289991]
	TIME [epoch: 6.44 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06093884280079925		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.06093884280079925 | validation: 0.07871179053883198]
	TIME [epoch: 6.47 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05593706402110299		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.05593706402110299 | validation: 0.07937411104171775]
	TIME [epoch: 6.45 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.053748905505417326		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.053748905505417326 | validation: 0.08060603096545463]
	TIME [epoch: 6.44 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.059461276727110886		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.059461276727110886 | validation: 0.08511876008385824]
	TIME [epoch: 6.43 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052567822981610314		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.052567822981610314 | validation: 0.10077393317822414]
	TIME [epoch: 6.43 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0610626255212811		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.0610626255212811 | validation: 0.10521033314818054]
	TIME [epoch: 6.43 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06596742224391902		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.06596742224391902 | validation: 0.08917785922881308]
	TIME [epoch: 6.45 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06170876909958804		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.06170876909958804 | validation: 0.07813825794264505]
	TIME [epoch: 6.46 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055712530972560896		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.055712530972560896 | validation: 0.10045168253248282]
	TIME [epoch: 6.44 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05546958974901122		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.05546958974901122 | validation: 0.09606018261484287]
	TIME [epoch: 6.43 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05661245141824588		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.05661245141824588 | validation: 0.08091168349739493]
	TIME [epoch: 6.44 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05174599317142363		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.05174599317142363 | validation: 0.09341640048900704]
	TIME [epoch: 6.43 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056855863807592256		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.056855863807592256 | validation: 0.08261936053492207]
	TIME [epoch: 6.47 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05598745721372453		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.05598745721372453 | validation: 0.07704652906293695]
	TIME [epoch: 6.46 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05644321912197809		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.05644321912197809 | validation: 0.07493476208309184]
	TIME [epoch: 6.44 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05379222028918465		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.05379222028918465 | validation: 0.07084906206732351]
	TIME [epoch: 6.44 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_485.pth
	Model improved!!!
EPOCH 486/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05467049621644634		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.05467049621644634 | validation: 0.09586672489745476]
	TIME [epoch: 6.44 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05554546798193331		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.05554546798193331 | validation: 0.07175277860127097]
	TIME [epoch: 6.44 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05035405306910455		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.05035405306910455 | validation: 0.08615332811398062]
	TIME [epoch: 6.46 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04855286323129082		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.04855286323129082 | validation: 0.10372221864571374]
	TIME [epoch: 6.48 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06636819832408808		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.06636819832408808 | validation: 0.0727035853395251]
	TIME [epoch: 6.45 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.10984603086390762		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.10984603086390762 | validation: 0.2087598701236912]
	TIME [epoch: 6.44 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.15052064895070494		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.15052064895070494 | validation: 0.09887879855750477]
	TIME [epoch: 6.44 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06667193474987118		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.06667193474987118 | validation: 0.08202866613989877]
	TIME [epoch: 6.44 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06787585632128146		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.06787585632128146 | validation: 0.08562762621933767]
	TIME [epoch: 6.46 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05376926019328611		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.05376926019328611 | validation: 0.08155349000425278]
	TIME [epoch: 6.49 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05007530642291745		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.05007530642291745 | validation: 0.0778090612710418]
	TIME [epoch: 6.43 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05457504656331843		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.05457504656331843 | validation: 0.09102835872107062]
	TIME [epoch: 6.45 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05513314659500334		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.05513314659500334 | validation: 0.08142462682868037]
	TIME [epoch: 6.44 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.056609769788737455		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.056609769788737455 | validation: 0.07978641425875488]
	TIME [epoch: 6.43 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04856268367689967		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.04856268367689967 | validation: 0.07314317938348425]
	TIME [epoch: 6.49 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052922131979139364		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.052922131979139364 | validation: 0.07678754129623003]
	TIME [epoch: 114 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05208076911370029		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.05208076911370029 | validation: 0.06791773602970708]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.055395256737077965		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.055395256737077965 | validation: 0.07655262875165487]
	TIME [epoch: 14.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052359413000304805		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.052359413000304805 | validation: 0.08131382747427586]
	TIME [epoch: 14.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05345263566250648		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.05345263566250648 | validation: 0.07717569431900445]
	TIME [epoch: 14.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052903696179066345		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.052903696179066345 | validation: 0.0717175276267102]
	TIME [epoch: 14.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060899697208005504		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.060899697208005504 | validation: 0.08579102532513312]
	TIME [epoch: 14.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0598351772217785		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.0598351772217785 | validation: 0.0664627824418129]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06653461338896145		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.06653461338896145 | validation: 0.08153480923075408]
	TIME [epoch: 14.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05139457060828627		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.05139457060828627 | validation: 0.07859375930603263]
	TIME [epoch: 14.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05164680427891313		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.05164680427891313 | validation: 0.08168427135340779]
	TIME [epoch: 14 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05206706762291931		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.05206706762291931 | validation: 0.08235277787314087]
	TIME [epoch: 14.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05036560076557767		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.05036560076557767 | validation: 0.07140151832132012]
	TIME [epoch: 14.1 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05291584700996513		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.05291584700996513 | validation: 0.07772675087205805]
	TIME [epoch: 14 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05192550431673423		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.05192550431673423 | validation: 0.06924384660743324]
	TIME [epoch: 14.1 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048170423242354335		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.048170423242354335 | validation: 0.08095040082870475]
	TIME [epoch: 14.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.052521472142477856		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.052521472142477856 | validation: 0.06967899334895403]
	TIME [epoch: 14.1 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05364901910252609		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.05364901910252609 | validation: 0.07326938982692592]
	TIME [epoch: 14.1 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.051127754021010996		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.051127754021010996 | validation: 0.19083733761626945]
	TIME [epoch: 14.1 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1409575137809827		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.1409575137809827 | validation: 0.17204235305576332]
	TIME [epoch: 14 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.08721719489386137		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.08721719489386137 | validation: 0.08418423516110388]
	TIME [epoch: 14.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05164001355724841		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.05164001355724841 | validation: 0.07450750308251326]
	TIME [epoch: 14.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050947820435447		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.050947820435447 | validation: 0.07385341860512938]
	TIME [epoch: 14 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04928493021162525		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.04928493021162525 | validation: 0.07727280487561146]
	TIME [epoch: 14.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05015214190684956		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.05015214190684956 | validation: 0.07713265372622716]
	TIME [epoch: 14 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04955406951629699		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.04955406951629699 | validation: 0.07213193087099351]
	TIME [epoch: 14 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049728754692570806		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.049728754692570806 | validation: 0.07333667695544377]
	TIME [epoch: 14.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04651064691026463		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.04651064691026463 | validation: 0.07882206504150312]
	TIME [epoch: 14.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048109880674741735		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.048109880674741735 | validation: 0.07076318350618331]
	TIME [epoch: 14 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04799195404262081		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.04799195404262081 | validation: 0.08492727967428744]
	TIME [epoch: 14.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047861793138135614		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.047861793138135614 | validation: 0.07322810671075945]
	TIME [epoch: 14.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05074762114421643		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.05074762114421643 | validation: 0.08557351144224211]
	TIME [epoch: 14.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049000654264863226		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.049000654264863226 | validation: 0.0711685590214544]
	TIME [epoch: 14.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09059193520872935		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.09059193520872935 | validation: 0.26072133682482995]
	TIME [epoch: 14 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.1619626613827257		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.1619626613827257 | validation: 0.13576453285396295]
	TIME [epoch: 14.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.09569666971925908		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.09569666971925908 | validation: 0.11032589203147065]
	TIME [epoch: 14.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07024312548612091		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.07024312548612091 | validation: 0.08647389053902338]
	TIME [epoch: 14 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054753570728538176		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.054753570728538176 | validation: 0.08582361343479626]
	TIME [epoch: 14.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05002972314385813		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.05002972314385813 | validation: 0.07431262476462488]
	TIME [epoch: 14.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05081701936123842		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.05081701936123842 | validation: 0.0700425826581856]
	TIME [epoch: 14.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04812564434934674		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.04812564434934674 | validation: 0.0713746780376454]
	TIME [epoch: 14.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04974636685185476		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.04974636685185476 | validation: 0.07075709419946186]
	TIME [epoch: 14.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05006223669003385		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.05006223669003385 | validation: 0.07654756175258419]
	TIME [epoch: 14.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05116934501996049		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.05116934501996049 | validation: 0.07182941820846349]
	TIME [epoch: 14.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04953464982726016		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.04953464982726016 | validation: 0.07281448654183438]
	TIME [epoch: 14.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04666510855452848		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.04666510855452848 | validation: 0.07295594713083567]
	TIME [epoch: 14.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04566281235356043		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.04566281235356043 | validation: 0.08099714432567323]
	TIME [epoch: 14.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05135048499796653		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.05135048499796653 | validation: 0.07712660110848656]
	TIME [epoch: 14 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05078640482246484		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.05078640482246484 | validation: 0.06912170812790955]
	TIME [epoch: 14 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04821417333260737		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.04821417333260737 | validation: 0.07741632355156239]
	TIME [epoch: 14.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050465578695877714		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.050465578695877714 | validation: 0.07687777916592124]
	TIME [epoch: 14.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050576408282841726		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.050576408282841726 | validation: 0.07092238210093976]
	TIME [epoch: 14.1 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05177353596696682		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.05177353596696682 | validation: 0.06870125924824026]
	TIME [epoch: 14.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05290556946190517		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.05290556946190517 | validation: 0.07886413690323109]
	TIME [epoch: 14.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04945490955751318		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.04945490955751318 | validation: 0.07093776503930871]
	TIME [epoch: 14.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06013572285634066		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.06013572285634066 | validation: 0.06760523908507077]
	TIME [epoch: 14.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04626622076100445		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.04626622076100445 | validation: 0.080901921549002]
	TIME [epoch: 14.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04881835826318015		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.04881835826318015 | validation: 0.0697167164055004]
	TIME [epoch: 14.1 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04536864431443134		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.04536864431443134 | validation: 0.07853843040634861]
	TIME [epoch: 14.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04506907129166879		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.04506907129166879 | validation: 0.07372484756719178]
	TIME [epoch: 14.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050791148924892345		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.050791148924892345 | validation: 0.06951723159903385]
	TIME [epoch: 14.1 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04878990330571673		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.04878990330571673 | validation: 0.07555858123491675]
	TIME [epoch: 14 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04931769831054244		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.04931769831054244 | validation: 0.07306776407953415]
	TIME [epoch: 14.1 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05107521807854575		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.05107521807854575 | validation: 0.06471284381860785]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04575575307991442		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.04575575307991442 | validation: 0.08142435349354914]
	TIME [epoch: 14.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05017016054841317		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.05017016054841317 | validation: 0.07033924322438577]
	TIME [epoch: 14 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04975295397767035		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.04975295397767035 | validation: 0.07347841143088173]
	TIME [epoch: 14.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048911849474716745		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.048911849474716745 | validation: 0.11085743050845805]
	TIME [epoch: 14.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06460077234651408		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.06460077234651408 | validation: 0.06846606130363313]
	TIME [epoch: 14 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05007596308245091		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.05007596308245091 | validation: 0.06168128566689627]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04903034765557841		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.04903034765557841 | validation: 0.07555135419186235]
	TIME [epoch: 14.1 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05014194541574829		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.05014194541574829 | validation: 0.07761644308320917]
	TIME [epoch: 14.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05109182264829692		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.05109182264829692 | validation: 0.07441250532207137]
	TIME [epoch: 14.1 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04607270105557089		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.04607270105557089 | validation: 0.09974453204378236]
	TIME [epoch: 14.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.054703062661524596		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.054703062661524596 | validation: 0.06855447316076571]
	TIME [epoch: 14.1 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04982769559225768		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.04982769559225768 | validation: 0.0719267315155098]
	TIME [epoch: 14.1 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046460734040443354		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.046460734040443354 | validation: 0.06561484103966615]
	TIME [epoch: 14.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04948193363218589		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.04948193363218589 | validation: 0.06817978584209701]
	TIME [epoch: 14.1 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04569029699151578		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.04569029699151578 | validation: 0.0716166063145273]
	TIME [epoch: 14.1 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04876210651810853		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.04876210651810853 | validation: 0.07457573172628032]
	TIME [epoch: 14.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04738382574834047		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.04738382574834047 | validation: 0.061960599534408516]
	TIME [epoch: 14 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04827067399074385		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.04827067399074385 | validation: 0.07454141150236253]
	TIME [epoch: 14.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050590921441509625		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.050590921441509625 | validation: 0.09712244204759402]
	TIME [epoch: 14.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0650223381247706		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.0650223381247706 | validation: 0.07199540590612712]
	TIME [epoch: 14.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05073600002431686		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.05073600002431686 | validation: 0.07043686713686174]
	TIME [epoch: 14.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05160031607602135		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.05160031607602135 | validation: 0.06577770389768489]
	TIME [epoch: 14.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04911701438719434		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.04911701438719434 | validation: 0.07213980486100663]
	TIME [epoch: 14.1 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05206569125897523		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.05206569125897523 | validation: 0.06877939982624627]
	TIME [epoch: 14.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04750194022334591		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.04750194022334591 | validation: 0.07091558213476976]
	TIME [epoch: 14.1 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04748200233783252		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.04748200233783252 | validation: 0.07357090180291533]
	TIME [epoch: 14.1 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046941926782537245		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.046941926782537245 | validation: 0.07508225901648613]
	TIME [epoch: 14.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.060048165409957784		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.060048165409957784 | validation: 0.06502821699224408]
	TIME [epoch: 14.1 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04523837835595572		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.04523837835595572 | validation: 0.0732627495796465]
	TIME [epoch: 14.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04987827250466738		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.04987827250466738 | validation: 0.08854394065975431]
	TIME [epoch: 14.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04868239541010522		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.04868239541010522 | validation: 0.07146355053870856]
	TIME [epoch: 14.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049581135889342966		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.049581135889342966 | validation: 0.06666817918798533]
	TIME [epoch: 14.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047713928557389866		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.047713928557389866 | validation: 0.06468903179356843]
	TIME [epoch: 14 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04403663407366325		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.04403663407366325 | validation: 0.07106009257727204]
	TIME [epoch: 14.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04833490643957391		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.04833490643957391 | validation: 0.060514467753963]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04521157225699478		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.04521157225699478 | validation: 0.07069247049331502]
	TIME [epoch: 14.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04714675107311281		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.04714675107311281 | validation: 0.06530683869059634]
	TIME [epoch: 14.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04745922274391407		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.04745922274391407 | validation: 0.07613062921716612]
	TIME [epoch: 14.2 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045694422724924105		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.045694422724924105 | validation: 0.06150458449811087]
	TIME [epoch: 14.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044274645827660365		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.044274645827660365 | validation: 0.06630389105534494]
	TIME [epoch: 14.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04763071940585355		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.04763071940585355 | validation: 0.06816057989434957]
	TIME [epoch: 14.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04671192918964512		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.04671192918964512 | validation: 0.06384244904414983]
	TIME [epoch: 14.1 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0493647437409637		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.0493647437409637 | validation: 0.11348644853949703]
	TIME [epoch: 14.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06737359666890785		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.06737359666890785 | validation: 0.07957207386510551]
	TIME [epoch: 14.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05046388832426342		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.05046388832426342 | validation: 0.061605764069559935]
	TIME [epoch: 14.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04605713855029399		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.04605713855029399 | validation: 0.07551501431492168]
	TIME [epoch: 14.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044398828702912174		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.044398828702912174 | validation: 0.07064411518610658]
	TIME [epoch: 14.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04826175960233707		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.04826175960233707 | validation: 0.0635862123266344]
	TIME [epoch: 14.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046402057160431484		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.046402057160431484 | validation: 0.06332174558523708]
	TIME [epoch: 14.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04964957857908768		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.04964957857908768 | validation: 0.06145343733162706]
	TIME [epoch: 14.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04861104047215779		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.04861104047215779 | validation: 0.06965207052862742]
	TIME [epoch: 14.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.049098144983367106		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.049098144983367106 | validation: 0.0664776573087417]
	TIME [epoch: 14.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04720559382375		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.04720559382375 | validation: 0.06913069594882051]
	TIME [epoch: 14.1 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04830432176752216		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.04830432176752216 | validation: 0.061737692557819884]
	TIME [epoch: 14.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046399514548591245		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.046399514548591245 | validation: 0.07581254044995918]
	TIME [epoch: 14.1 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.06141404333929491		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.06141404333929491 | validation: 0.07465890861015514]
	TIME [epoch: 14.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05144802919154406		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.05144802919154406 | validation: 0.06452551518616291]
	TIME [epoch: 14.1 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04496664374888554		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.04496664374888554 | validation: 0.10248820560595912]
	TIME [epoch: 14.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07187685405403228		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.07187685405403228 | validation: 0.12896039806864068]
	TIME [epoch: 14.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.07473178159677935		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.07473178159677935 | validation: 0.1020410598735775]
	TIME [epoch: 14.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05702587002735024		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.05702587002735024 | validation: 0.07684317337681329]
	TIME [epoch: 14.1 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05303356540426428		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.05303356540426428 | validation: 0.07315936524028979]
	TIME [epoch: 14.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047993761048482506		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.047993761048482506 | validation: 0.06623013177929964]
	TIME [epoch: 14.2 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045541121365858445		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.045541121365858445 | validation: 0.0690178513026028]
	TIME [epoch: 14.1 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0472376382636608		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.0472376382636608 | validation: 0.06587800671419611]
	TIME [epoch: 14.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04449489220225815		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.04449489220225815 | validation: 0.06689862470358461]
	TIME [epoch: 14.2 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047895073807549024		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.047895073807549024 | validation: 0.06873904248770961]
	TIME [epoch: 14.1 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04580884631712909		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.04580884631712909 | validation: 0.06826614831856392]
	TIME [epoch: 14.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04543360417059172		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.04543360417059172 | validation: 0.06798745493916247]
	TIME [epoch: 14.2 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04356852665531698		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.04356852665531698 | validation: 0.06514027372597342]
	TIME [epoch: 14.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04855869488084867		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.04855869488084867 | validation: 0.06641507330723807]
	TIME [epoch: 14.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04080144001460018		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.04080144001460018 | validation: 0.06925072484963243]
	TIME [epoch: 14.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04589403668206453		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.04589403668206453 | validation: 0.06825728957765956]
	TIME [epoch: 14.1 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04738523061820267		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.04738523061820267 | validation: 0.08427546708269218]
	TIME [epoch: 14.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048091914838931694		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.048091914838931694 | validation: 0.06369006666007326]
	TIME [epoch: 14.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.050119795544755257		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.050119795544755257 | validation: 0.06556655311454218]
	TIME [epoch: 14.1 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04657630290646819		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.04657630290646819 | validation: 0.06097295033442756]
	TIME [epoch: 14.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0470598437843793		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.0470598437843793 | validation: 0.07268666191664928]
	TIME [epoch: 14.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043716063116977816		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.043716063116977816 | validation: 0.0710598151726081]
	TIME [epoch: 14.1 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04461394051004103		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.04461394051004103 | validation: 0.0679302220817342]
	TIME [epoch: 14.1 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044949664400120916		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.044949664400120916 | validation: 0.06007266145723394]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04401959032759069		[learning rate: 0.00012324]
	Learning Rate: 0.000123245
	LOSS [training: 0.04401959032759069 | validation: 0.06598623262271756]
	TIME [epoch: 14.1 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0445914360628204		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.0445914360628204 | validation: 0.06828332595442495]
	TIME [epoch: 14.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04734015878067557		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.04734015878067557 | validation: 0.07306772165941527]
	TIME [epoch: 14.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04651639486674301		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.04651639486674301 | validation: 0.06982019674890927]
	TIME [epoch: 14.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04319034367050956		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.04319034367050956 | validation: 0.05864839890374705]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04697228062241948		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.04697228062241948 | validation: 0.06465148756233337]
	TIME [epoch: 14.1 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04859796187726041		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.04859796187726041 | validation: 0.07563983779983817]
	TIME [epoch: 14.1 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04775209202872624		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.04775209202872624 | validation: 0.06340773277538662]
	TIME [epoch: 14.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046151644860150005		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.046151644860150005 | validation: 0.06764244081353078]
	TIME [epoch: 14.1 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0442025889202482		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.0442025889202482 | validation: 0.06698254136645114]
	TIME [epoch: 14.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04501663464496587		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.04501663464496587 | validation: 0.0657713367921748]
	TIME [epoch: 14.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0476122269294924		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.0476122269294924 | validation: 0.06202822857033921]
	TIME [epoch: 14.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04869589603793632		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.04869589603793632 | validation: 0.06661790350488615]
	TIME [epoch: 14.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04643005409873954		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.04643005409873954 | validation: 0.05953859066932846]
	TIME [epoch: 14.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04597317805649259		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.04597317805649259 | validation: 0.06390355699465411]
	TIME [epoch: 14.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04628870817024333		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.04628870817024333 | validation: 0.06019792182033204]
	TIME [epoch: 14.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04674696691109869		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.04674696691109869 | validation: 0.07241316218158515]
	TIME [epoch: 14.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042820763978023896		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.042820763978023896 | validation: 0.06250349955026813]
	TIME [epoch: 14.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043463969174940824		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.043463969174940824 | validation: 0.07010412123634567]
	TIME [epoch: 14.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04925786356759865		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.04925786356759865 | validation: 0.06666777739150165]
	TIME [epoch: 14.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04464059618587021		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.04464059618587021 | validation: 0.06525138063560597]
	TIME [epoch: 14.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04716709784882768		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.04716709784882768 | validation: 0.06045165423490959]
	TIME [epoch: 14.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046838616653207626		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.046838616653207626 | validation: 0.06417746527998301]
	TIME [epoch: 14.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045542317921196646		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.045542317921196646 | validation: 0.06878268863430358]
	TIME [epoch: 14.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04622839861248831		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.04622839861248831 | validation: 0.06805098496981021]
	TIME [epoch: 14.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04706622001873746		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.04706622001873746 | validation: 0.0746477339853844]
	TIME [epoch: 14.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.048151440735439296		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.048151440735439296 | validation: 0.0657102556026509]
	TIME [epoch: 14.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046854809226046		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.046854809226046 | validation: 0.05744313540940582]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_673.pth
	Model improved!!!
EPOCH 674/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046349084360125636		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.046349084360125636 | validation: 0.06792452741946235]
	TIME [epoch: 14.1 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044494630074380946		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.044494630074380946 | validation: 0.05869217305451527]
	TIME [epoch: 14.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045588460754446725		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.045588460754446725 | validation: 0.06480411601072471]
	TIME [epoch: 14.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0474447288003588		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.0474447288003588 | validation: 0.06365203121756287]
	TIME [epoch: 14.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044225253345729736		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.044225253345729736 | validation: 0.061324834712779476]
	TIME [epoch: 14.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043604744663976305		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.043604744663976305 | validation: 0.060063418274070274]
	TIME [epoch: 14.1 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044670510188197385		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.044670510188197385 | validation: 0.0644557850071452]
	TIME [epoch: 14.1 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04523967740174366		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.04523967740174366 | validation: 0.06502565396688173]
	TIME [epoch: 14.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04439547524130136		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.04439547524130136 | validation: 0.068071468137771]
	TIME [epoch: 14.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045148264334843244		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.045148264334843244 | validation: 0.06516390586324938]
	TIME [epoch: 14.1 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046872973302701175		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.046872973302701175 | validation: 0.060029521279714816]
	TIME [epoch: 14.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043253444482678205		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.043253444482678205 | validation: 0.06306241802794957]
	TIME [epoch: 14.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04552477701734771		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.04552477701734771 | validation: 0.05997777216889221]
	TIME [epoch: 14.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045802888124311596		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.045802888124311596 | validation: 0.06717682372144777]
	TIME [epoch: 14.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04387844846924496		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.04387844846924496 | validation: 0.05451486172657121]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_688.pth
	Model improved!!!
EPOCH 689/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04882119741190012		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.04882119741190012 | validation: 0.05400475661934308]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04408472324826891		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.04408472324826891 | validation: 0.06022485094469009]
	TIME [epoch: 14.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042133730499288535		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.042133730499288535 | validation: 0.06617808871341187]
	TIME [epoch: 14.1 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0432352569217759		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.0432352569217759 | validation: 0.06797005512723725]
	TIME [epoch: 14.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04408971547153345		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.04408971547153345 | validation: 0.059312092687295585]
	TIME [epoch: 14.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05103796040072979		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.05103796040072979 | validation: 0.09030469013676715]
	TIME [epoch: 14.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05116401745023975		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.05116401745023975 | validation: 0.07396279381390114]
	TIME [epoch: 14.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04546985470230504		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.04546985470230504 | validation: 0.06600852340836966]
	TIME [epoch: 14.2 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040160109509739854		[learning rate: 8.5871e-05]
	Learning Rate: 8.58709e-05
	LOSS [training: 0.040160109509739854 | validation: 0.0644359973146084]
	TIME [epoch: 14.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041484995334329935		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.041484995334329935 | validation: 0.06259610976387645]
	TIME [epoch: 14.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04531938632061723		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.04531938632061723 | validation: 0.06281756921231503]
	TIME [epoch: 14.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041698134759683404		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.041698134759683404 | validation: 0.06532498423569821]
	TIME [epoch: 14.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04459880158512521		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.04459880158512521 | validation: 0.07316355713427052]
	TIME [epoch: 14.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04535734229522873		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.04535734229522873 | validation: 0.06707712529343948]
	TIME [epoch: 14.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045727276632092986		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.045727276632092986 | validation: 0.06732641387887676]
	TIME [epoch: 14.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04282264285886707		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.04282264285886707 | validation: 0.06511632460983316]
	TIME [epoch: 14.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044249417350214204		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.044249417350214204 | validation: 0.06026452763534369]
	TIME [epoch: 14.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045182177420899416		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.045182177420899416 | validation: 0.05913949902159548]
	TIME [epoch: 14.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04482207996948323		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.04482207996948323 | validation: 0.059129000688135196]
	TIME [epoch: 14.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04764683787424201		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.04764683787424201 | validation: 0.0622701831564828]
	TIME [epoch: 14.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04290053151798928		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.04290053151798928 | validation: 0.06658667660609448]
	TIME [epoch: 14.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04436328348404335		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.04436328348404335 | validation: 0.059131789046884146]
	TIME [epoch: 14.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04488834888756464		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.04488834888756464 | validation: 0.06173441798113186]
	TIME [epoch: 14.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04270084801344086		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.04270084801344086 | validation: 0.06240993494803915]
	TIME [epoch: 14.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046176087114027306		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.046176087114027306 | validation: 0.05736287264754373]
	TIME [epoch: 14.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044193706385547024		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.044193706385547024 | validation: 0.05380588706012053]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04247285534287898		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.04247285534287898 | validation: 0.05954760626866509]
	TIME [epoch: 14.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041522010287364186		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.041522010287364186 | validation: 0.06402367226950123]
	TIME [epoch: 14.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04373639002506382		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.04373639002506382 | validation: 0.06536957009382081]
	TIME [epoch: 14.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047429147114689624		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.047429147114689624 | validation: 0.0639586359121736]
	TIME [epoch: 14.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045291843292300585		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.045291843292300585 | validation: 0.0594395469013297]
	TIME [epoch: 14.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043628132198829286		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.043628132198829286 | validation: 0.0637950564851414]
	TIME [epoch: 14.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04291369829304179		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.04291369829304179 | validation: 0.06166873704811391]
	TIME [epoch: 14.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04254414308302737		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.04254414308302737 | validation: 0.06578068253288656]
	TIME [epoch: 14.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04522059784768498		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.04522059784768498 | validation: 0.06038898407275656]
	TIME [epoch: 14.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04409306632722928		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.04409306632722928 | validation: 0.0665346878466349]
	TIME [epoch: 14.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0454337745779904		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.0454337745779904 | validation: 0.05910557958449954]
	TIME [epoch: 14.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04234376286779443		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.04234376286779443 | validation: 0.05865018107954367]
	TIME [epoch: 14.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04303559406781965		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.04303559406781965 | validation: 0.060158026057562666]
	TIME [epoch: 14.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04069019943618535		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.04069019943618535 | validation: 0.058977239337207665]
	TIME [epoch: 14.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041570002771635084		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.041570002771635084 | validation: 0.06441933482569005]
	TIME [epoch: 14.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041612575999432105		[learning rate: 6.7968e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.041612575999432105 | validation: 0.07905226091323453]
	TIME [epoch: 14.2 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044788083685596336		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.044788083685596336 | validation: 0.06215031702696488]
	TIME [epoch: 14.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04640325518729187		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.04640325518729187 | validation: 0.06208743210538166]
	TIME [epoch: 14.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04493120999594627		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.04493120999594627 | validation: 0.058362594804159756]
	TIME [epoch: 14.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042532777085794726		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.042532777085794726 | validation: 0.061422707822800286]
	TIME [epoch: 14.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04383071563023642		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.04383071563023642 | validation: 0.06095333019627639]
	TIME [epoch: 14.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043363936429181474		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.043363936429181474 | validation: 0.06507755903235239]
	TIME [epoch: 14.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04317242215485265		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.04317242215485265 | validation: 0.06485201553733931]
	TIME [epoch: 14.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0413090087153324		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.0413090087153324 | validation: 0.05931029103200819]
	TIME [epoch: 14.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04423430606969081		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.04423430606969081 | validation: 0.060899513911776074]
	TIME [epoch: 14.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04534049825534145		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.04534049825534145 | validation: 0.05379823891479462]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_740.pth
	Model improved!!!
EPOCH 741/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046319014920499946		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.046319014920499946 | validation: 0.06054389288201707]
	TIME [epoch: 14.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042373748154800744		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.042373748154800744 | validation: 0.06415087255345446]
	TIME [epoch: 14.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04429790956502484		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.04429790956502484 | validation: 0.06771786058241447]
	TIME [epoch: 14.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045101527932986635		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.045101527932986635 | validation: 0.05729023704926073]
	TIME [epoch: 14.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044084705088061886		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.044084705088061886 | validation: 0.06135324056602265]
	TIME [epoch: 14.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04590008728368773		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.04590008728368773 | validation: 0.05478973247142685]
	TIME [epoch: 14.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044591215496334546		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.044591215496334546 | validation: 0.05632555073303585]
	TIME [epoch: 14.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04219602694138716		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.04219602694138716 | validation: 0.06860427056725908]
	TIME [epoch: 14.2 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0464593570863953		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.0464593570863953 | validation: 0.058866327439098566]
	TIME [epoch: 14.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047205223123345755		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.047205223123345755 | validation: 0.057224113172594085]
	TIME [epoch: 14.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04195906841083778		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.04195906841083778 | validation: 0.05565499965850244]
	TIME [epoch: 14.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044095244410633824		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.044095244410633824 | validation: 0.059646171970681484]
	TIME [epoch: 14.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04303690165377971		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.04303690165377971 | validation: 0.06526023076970593]
	TIME [epoch: 14.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04569776316556077		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.04569776316556077 | validation: 0.06304214082934408]
	TIME [epoch: 14.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04153818205958252		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.04153818205958252 | validation: 0.06063061669939989]
	TIME [epoch: 14.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04495401117163711		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.04495401117163711 | validation: 0.061946013865040596]
	TIME [epoch: 14.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04434382302582385		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.04434382302582385 | validation: 0.0627152360998866]
	TIME [epoch: 14.2 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04585813061379622		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.04585813061379622 | validation: 0.061164283801360256]
	TIME [epoch: 14.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0431918043198605		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.0431918043198605 | validation: 0.05619151849515887]
	TIME [epoch: 14.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04428650993340888		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.04428650993340888 | validation: 0.05792845721966389]
	TIME [epoch: 14.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04276488389478811		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.04276488389478811 | validation: 0.062143139455518207]
	TIME [epoch: 14.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04528325198049925		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.04528325198049925 | validation: 0.06874299772006535]
	TIME [epoch: 14.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043704625377905065		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.043704625377905065 | validation: 0.06249301917816337]
	TIME [epoch: 14.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0433368058514533		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.0433368058514533 | validation: 0.063295180992614]
	TIME [epoch: 14.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043933740351710354		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.043933740351710354 | validation: 0.05523569352028375]
	TIME [epoch: 14.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04148553485390303		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.04148553485390303 | validation: 0.05737984092559323]
	TIME [epoch: 14.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04646207258300557		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.04646207258300557 | validation: 0.06392769319862715]
	TIME [epoch: 14.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045847280720984915		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.045847280720984915 | validation: 0.0624990346789101]
	TIME [epoch: 14.1 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042113498514698616		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.042113498514698616 | validation: 0.061479662773142874]
	TIME [epoch: 14.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03994021723493704		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.03994021723493704 | validation: 0.06459702750617212]
	TIME [epoch: 14.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045589563176707815		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.045589563176707815 | validation: 0.06285589288899185]
	TIME [epoch: 14.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04508445913436221		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.04508445913436221 | validation: 0.0614513842296797]
	TIME [epoch: 14.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.047263292487338474		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.047263292487338474 | validation: 0.060666597656331936]
	TIME [epoch: 14.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043899452818734475		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.043899452818734475 | validation: 0.056592684203633725]
	TIME [epoch: 14.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04310974302744219		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.04310974302744219 | validation: 0.060062231633869236]
	TIME [epoch: 14.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04358890909696472		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.04358890909696472 | validation: 0.06276416093067931]
	TIME [epoch: 14.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04447861580902369		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.04447861580902369 | validation: 0.060225576662203475]
	TIME [epoch: 14.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04440795700180725		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.04440795700180725 | validation: 0.06334198512657763]
	TIME [epoch: 14.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04209851590651663		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.04209851590651663 | validation: 0.07781800415830081]
	TIME [epoch: 14.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04862720583167943		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.04862720583167943 | validation: 0.08155748857291355]
	TIME [epoch: 14.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.05360745606559675		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.05360745606559675 | validation: 0.07046980791838203]
	TIME [epoch: 14.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0476213969441005		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.0476213969441005 | validation: 0.06574526528946929]
	TIME [epoch: 14.2 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042684593304869026		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.042684593304869026 | validation: 0.06219621559724234]
	TIME [epoch: 14.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041320329687724575		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.041320329687724575 | validation: 0.05576680395677516]
	TIME [epoch: 14.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041034741091130464		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.041034741091130464 | validation: 0.0589955388091284]
	TIME [epoch: 14.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04378854028647933		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.04378854028647933 | validation: 0.06441565586283436]
	TIME [epoch: 14.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04492622444078971		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.04492622444078971 | validation: 0.06168829129720226]
	TIME [epoch: 14.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04177305227768159		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.04177305227768159 | validation: 0.06265124276474711]
	TIME [epoch: 14.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03997917889127576		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.03997917889127576 | validation: 0.05843272432117605]
	TIME [epoch: 14.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04484113680741203		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.04484113680741203 | validation: 0.05820403863719869]
	TIME [epoch: 14.2 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0441260074332174		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.0441260074332174 | validation: 0.05885740045957247]
	TIME [epoch: 14.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04388340578745474		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.04388340578745474 | validation: 0.06416123030390301]
	TIME [epoch: 14.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04368037504654669		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.04368037504654669 | validation: 0.06393912715090902]
	TIME [epoch: 14.2 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04347758935820797		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.04347758935820797 | validation: 0.05796953936619738]
	TIME [epoch: 14.2 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041215084157863686		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.041215084157863686 | validation: 0.0672364403800986]
	TIME [epoch: 14.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04380090016802679		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.04380090016802679 | validation: 0.06149034441185103]
	TIME [epoch: 14.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04293627590455541		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.04293627590455541 | validation: 0.06072631944362792]
	TIME [epoch: 14.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04242217081323876		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.04242217081323876 | validation: 0.05758449598164154]
	TIME [epoch: 14.2 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04287913854371017		[learning rate: 4.1687e-05]
	Learning Rate: 4.16869e-05
	LOSS [training: 0.04287913854371017 | validation: 0.06212014777100825]
	TIME [epoch: 14.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041274992184343044		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.041274992184343044 | validation: 0.05738461375740533]
	TIME [epoch: 14.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04384043960207609		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.04384043960207609 | validation: 0.05828575785206674]
	TIME [epoch: 14.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04001360439362424		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.04001360439362424 | validation: 0.05773772052600043]
	TIME [epoch: 14.2 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044135606577491465		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.044135606577491465 | validation: 0.05557112523933533]
	TIME [epoch: 14.2 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03994131639019773		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.03994131639019773 | validation: 0.06176362703798639]
	TIME [epoch: 14.2 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04639816170650271		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.04639816170650271 | validation: 0.06186024383322774]
	TIME [epoch: 14.2 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040754439560740985		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.040754439560740985 | validation: 0.05707662547680473]
	TIME [epoch: 14.2 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042765115295025524		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.042765115295025524 | validation: 0.05710588898549651]
	TIME [epoch: 14.2 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042184019411643986		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.042184019411643986 | validation: 0.06428292270710792]
	TIME [epoch: 14.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04410825842934861		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.04410825842934861 | validation: 0.06272099595082888]
	TIME [epoch: 14.2 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04430184215438805		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.04430184215438805 | validation: 0.0588149411880258]
	TIME [epoch: 14.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04417778718597601		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.04417778718597601 | validation: 0.05997242950743192]
	TIME [epoch: 14.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04358447276786431		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.04358447276786431 | validation: 0.061242168150359794]
	TIME [epoch: 14.2 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04550236981571563		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.04550236981571563 | validation: 0.05926045006780705]
	TIME [epoch: 14.2 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03920984228839765		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.03920984228839765 | validation: 0.06327521144867665]
	TIME [epoch: 14.2 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043036968558443814		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.043036968558443814 | validation: 0.0650640473287993]
	TIME [epoch: 14.2 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043119140608389776		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.043119140608389776 | validation: 0.06096958665480026]
	TIME [epoch: 14.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045097385588014016		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.045097385588014016 | validation: 0.056514371623756214]
	TIME [epoch: 14.2 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040633744373513705		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.040633744373513705 | validation: 0.05877244326660909]
	TIME [epoch: 14.2 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04290849423924791		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.04290849423924791 | validation: 0.05712419329045393]
	TIME [epoch: 14.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043621170930579545		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.043621170930579545 | validation: 0.056500392208874754]
	TIME [epoch: 14.2 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0444784440532964		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.0444784440532964 | validation: 0.06291703989193653]
	TIME [epoch: 14.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046660224794705246		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.046660224794705246 | validation: 0.06432169615690213]
	TIME [epoch: 14.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0412515000383981		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.0412515000383981 | validation: 0.06239134883024289]
	TIME [epoch: 14.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043672660672521554		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.043672660672521554 | validation: 0.0634097743750643]
	TIME [epoch: 14.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045731090562631374		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.045731090562631374 | validation: 0.06493723416904622]
	TIME [epoch: 14.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043806999381247144		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.043806999381247144 | validation: 0.06063288327232408]
	TIME [epoch: 14.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04377609046289567		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.04377609046289567 | validation: 0.06742811345719901]
	TIME [epoch: 14.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042985365314704455		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.042985365314704455 | validation: 0.061821554072053635]
	TIME [epoch: 14.2 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044645346868992686		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.044645346868992686 | validation: 0.06725722582159398]
	TIME [epoch: 14.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043641165839047646		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.043641165839047646 | validation: 0.05429027247210031]
	TIME [epoch: 14.2 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04066042689880914		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.04066042689880914 | validation: 0.05256896255034658]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045118248949607066		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.045118248949607066 | validation: 0.06183565198708592]
	TIME [epoch: 14.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04550131010271262		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.04550131010271262 | validation: 0.06437525366773746]
	TIME [epoch: 14.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04658681853947774		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.04658681853947774 | validation: 0.05935996979327462]
	TIME [epoch: 14.2 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04351350512370892		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.04351350512370892 | validation: 0.05492034291820337]
	TIME [epoch: 14.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04342444576053651		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.04342444576053651 | validation: 0.06116544231887777]
	TIME [epoch: 14.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042308444643564175		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.042308444643564175 | validation: 0.055986561531086776]
	TIME [epoch: 14.2 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04513617659912524		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.04513617659912524 | validation: 0.06318864175956998]
	TIME [epoch: 14.2 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04427792953526874		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.04427792953526874 | validation: 0.05527427517444339]
	TIME [epoch: 14.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04212040049406107		[learning rate: 3.1178e-05]
	Learning Rate: 3.11778e-05
	LOSS [training: 0.04212040049406107 | validation: 0.05903529958360916]
	TIME [epoch: 14.2 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03949990580522557		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.03949990580522557 | validation: 0.05688576053600574]
	TIME [epoch: 14.2 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042918156162438784		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.042918156162438784 | validation: 0.05892410413068747]
	TIME [epoch: 14.2 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04259753706638438		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.04259753706638438 | validation: 0.056950265840849223]
	TIME [epoch: 14.2 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0418666938983757		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.0418666938983757 | validation: 0.05454197236301768]
	TIME [epoch: 14.2 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042772795323258284		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.042772795323258284 | validation: 0.05207772258035818]
	TIME [epoch: 14.2 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04274995783487791		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.04274995783487791 | validation: 0.058900864964588684]
	TIME [epoch: 14.2 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04194440409273529		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.04194440409273529 | validation: 0.05588840845486142]
	TIME [epoch: 14.2 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0424787804010221		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.0424787804010221 | validation: 0.06444606435956184]
	TIME [epoch: 14.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04873138219847789		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.04873138219847789 | validation: 0.06041617756996762]
	TIME [epoch: 14.2 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04415498597968455		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.04415498597968455 | validation: 0.05663138580448306]
	TIME [epoch: 14.2 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044666537269913716		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.044666537269913716 | validation: 0.06719969224358051]
	TIME [epoch: 14.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04149963517370314		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.04149963517370314 | validation: 0.06349544352330769]
	TIME [epoch: 14.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.037470498591672445		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.037470498591672445 | validation: 0.0652545066930301]
	TIME [epoch: 14.1 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03933762713724093		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.03933762713724093 | validation: 0.06359324148348047]
	TIME [epoch: 14.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043722384814316846		[learning rate: 2.8034e-05]
	Learning Rate: 2.80345e-05
	LOSS [training: 0.043722384814316846 | validation: 0.06197082468723436]
	TIME [epoch: 14.2 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04158731761978938		[learning rate: 2.7837e-05]
	Learning Rate: 2.78366e-05
	LOSS [training: 0.04158731761978938 | validation: 0.05995897852501941]
	TIME [epoch: 14.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04100875461939263		[learning rate: 2.764e-05]
	Learning Rate: 2.764e-05
	LOSS [training: 0.04100875461939263 | validation: 0.058954722574849884]
	TIME [epoch: 14.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044492191299965914		[learning rate: 2.7445e-05]
	Learning Rate: 2.74449e-05
	LOSS [training: 0.044492191299965914 | validation: 0.05775216754666675]
	TIME [epoch: 14.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04461806194140717		[learning rate: 2.7251e-05]
	Learning Rate: 2.72511e-05
	LOSS [training: 0.04461806194140717 | validation: 0.05940852516777802]
	TIME [epoch: 14.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04260790314594669		[learning rate: 2.7059e-05]
	Learning Rate: 2.70587e-05
	LOSS [training: 0.04260790314594669 | validation: 0.052665865840540386]
	TIME [epoch: 14.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04257151637546895		[learning rate: 2.6868e-05]
	Learning Rate: 2.68677e-05
	LOSS [training: 0.04257151637546895 | validation: 0.060876452509913394]
	TIME [epoch: 14.2 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04386158510548582		[learning rate: 2.6678e-05]
	Learning Rate: 2.6678e-05
	LOSS [training: 0.04386158510548582 | validation: 0.06074002289888637]
	TIME [epoch: 14.2 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041370938180933695		[learning rate: 2.649e-05]
	Learning Rate: 2.64897e-05
	LOSS [training: 0.041370938180933695 | validation: 0.0636877879713706]
	TIME [epoch: 14.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043353339239615485		[learning rate: 2.6303e-05]
	Learning Rate: 2.63027e-05
	LOSS [training: 0.043353339239615485 | validation: 0.06037061675358646]
	TIME [epoch: 14.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042897052022722953		[learning rate: 2.6117e-05]
	Learning Rate: 2.6117e-05
	LOSS [training: 0.042897052022722953 | validation: 0.05624506986516964]
	TIME [epoch: 14.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04401817391509831		[learning rate: 2.5933e-05]
	Learning Rate: 2.59326e-05
	LOSS [training: 0.04401817391509831 | validation: 0.059766971416718495]
	TIME [epoch: 14.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04310854077168057		[learning rate: 2.575e-05]
	Learning Rate: 2.57495e-05
	LOSS [training: 0.04310854077168057 | validation: 0.05470792600739312]
	TIME [epoch: 14.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04266557949127815		[learning rate: 2.5568e-05]
	Learning Rate: 2.55677e-05
	LOSS [training: 0.04266557949127815 | validation: 0.056036027789956235]
	TIME [epoch: 14.2 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04366691990417878		[learning rate: 2.5387e-05]
	Learning Rate: 2.53872e-05
	LOSS [training: 0.04366691990417878 | validation: 0.06074452120039589]
	TIME [epoch: 14.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042642856144245886		[learning rate: 2.5208e-05]
	Learning Rate: 2.5208e-05
	LOSS [training: 0.042642856144245886 | validation: 0.056308677761529105]
	TIME [epoch: 14.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03793332435301181		[learning rate: 2.503e-05]
	Learning Rate: 2.503e-05
	LOSS [training: 0.03793332435301181 | validation: 0.05572163182390044]
	TIME [epoch: 14.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04251064204371073		[learning rate: 2.4853e-05]
	Learning Rate: 2.48533e-05
	LOSS [training: 0.04251064204371073 | validation: 0.05679836757555726]
	TIME [epoch: 14.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045414654572667676		[learning rate: 2.4678e-05]
	Learning Rate: 2.46779e-05
	LOSS [training: 0.045414654572667676 | validation: 0.05976362791608876]
	TIME [epoch: 14.2 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044430895833938555		[learning rate: 2.4504e-05]
	Learning Rate: 2.45037e-05
	LOSS [training: 0.044430895833938555 | validation: 0.057179516517383504]
	TIME [epoch: 14.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04111692125258719		[learning rate: 2.4331e-05]
	Learning Rate: 2.43307e-05
	LOSS [training: 0.04111692125258719 | validation: 0.057783292923833884]
	TIME [epoch: 14.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03923027946106915		[learning rate: 2.4159e-05]
	Learning Rate: 2.41589e-05
	LOSS [training: 0.03923027946106915 | validation: 0.056700916827503]
	TIME [epoch: 14.2 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043593592182823576		[learning rate: 2.3988e-05]
	Learning Rate: 2.39883e-05
	LOSS [training: 0.043593592182823576 | validation: 0.06857105545306301]
	TIME [epoch: 14.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044565599091634434		[learning rate: 2.3819e-05]
	Learning Rate: 2.3819e-05
	LOSS [training: 0.044565599091634434 | validation: 0.07619743843709771]
	TIME [epoch: 14.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04600725839759727		[learning rate: 2.3651e-05]
	Learning Rate: 2.36508e-05
	LOSS [training: 0.04600725839759727 | validation: 0.07737421055325876]
	TIME [epoch: 14.2 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04813663897676507		[learning rate: 2.3484e-05]
	Learning Rate: 2.34838e-05
	LOSS [training: 0.04813663897676507 | validation: 0.06898335643544305]
	TIME [epoch: 14.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04551632721943236		[learning rate: 2.3318e-05]
	Learning Rate: 2.33181e-05
	LOSS [training: 0.04551632721943236 | validation: 0.06686677710962947]
	TIME [epoch: 14.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04505677237988888		[learning rate: 2.3153e-05]
	Learning Rate: 2.31534e-05
	LOSS [training: 0.04505677237988888 | validation: 0.06451293331532588]
	TIME [epoch: 14.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0437978252005959		[learning rate: 2.299e-05]
	Learning Rate: 2.299e-05
	LOSS [training: 0.0437978252005959 | validation: 0.06042985082356096]
	TIME [epoch: 14.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043685241027747276		[learning rate: 2.2828e-05]
	Learning Rate: 2.28277e-05
	LOSS [training: 0.043685241027747276 | validation: 0.057359738718420906]
	TIME [epoch: 14.1 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04312489379918992		[learning rate: 2.2667e-05]
	Learning Rate: 2.26665e-05
	LOSS [training: 0.04312489379918992 | validation: 0.05788748184681225]
	TIME [epoch: 14.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04251931549274014		[learning rate: 2.2506e-05]
	Learning Rate: 2.25065e-05
	LOSS [training: 0.04251931549274014 | validation: 0.06472973868999685]
	TIME [epoch: 14.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044090207902814704		[learning rate: 2.2348e-05]
	Learning Rate: 2.23476e-05
	LOSS [training: 0.044090207902814704 | validation: 0.06185312256247828]
	TIME [epoch: 14.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04399831354267335		[learning rate: 2.219e-05]
	Learning Rate: 2.21898e-05
	LOSS [training: 0.04399831354267335 | validation: 0.06058373890840155]
	TIME [epoch: 14.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04250289994941056		[learning rate: 2.2033e-05]
	Learning Rate: 2.20332e-05
	LOSS [training: 0.04250289994941056 | validation: 0.05667945454798813]
	TIME [epoch: 14.2 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039496448426139016		[learning rate: 2.1878e-05]
	Learning Rate: 2.18776e-05
	LOSS [training: 0.039496448426139016 | validation: 0.061781322956541054]
	TIME [epoch: 14.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039136051006436454		[learning rate: 2.1723e-05]
	Learning Rate: 2.17232e-05
	LOSS [training: 0.039136051006436454 | validation: 0.05491375694531641]
	TIME [epoch: 14.2 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04188225402392198		[learning rate: 2.157e-05]
	Learning Rate: 2.15698e-05
	LOSS [training: 0.04188225402392198 | validation: 0.05773562390816949]
	TIME [epoch: 14.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04167146049941887		[learning rate: 2.1418e-05]
	Learning Rate: 2.14175e-05
	LOSS [training: 0.04167146049941887 | validation: 0.057464896544398195]
	TIME [epoch: 14.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040927121538003215		[learning rate: 2.1266e-05]
	Learning Rate: 2.12663e-05
	LOSS [training: 0.040927121538003215 | validation: 0.05982676070777582]
	TIME [epoch: 14.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04155640077603584		[learning rate: 2.1116e-05]
	Learning Rate: 2.11162e-05
	LOSS [training: 0.04155640077603584 | validation: 0.05850211371415329]
	TIME [epoch: 14.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04002292000589534		[learning rate: 2.0967e-05]
	Learning Rate: 2.09671e-05
	LOSS [training: 0.04002292000589534 | validation: 0.057244669082704536]
	TIME [epoch: 14.2 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04192109231432904		[learning rate: 2.0819e-05]
	Learning Rate: 2.08191e-05
	LOSS [training: 0.04192109231432904 | validation: 0.06037762689316231]
	TIME [epoch: 14.2 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0434061071613244		[learning rate: 2.0672e-05]
	Learning Rate: 2.06721e-05
	LOSS [training: 0.0434061071613244 | validation: 0.05299072324419518]
	TIME [epoch: 14.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04149414714738805		[learning rate: 2.0526e-05]
	Learning Rate: 2.05262e-05
	LOSS [training: 0.04149414714738805 | validation: 0.055907834799666246]
	TIME [epoch: 14.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03884002231824968		[learning rate: 2.0381e-05]
	Learning Rate: 2.03813e-05
	LOSS [training: 0.03884002231824968 | validation: 0.05780108842325293]
	TIME [epoch: 14.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03902345876901067		[learning rate: 2.0237e-05]
	Learning Rate: 2.02374e-05
	LOSS [training: 0.03902345876901067 | validation: 0.05879386542855634]
	TIME [epoch: 14.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04232717489222097		[learning rate: 2.0094e-05]
	Learning Rate: 2.00945e-05
	LOSS [training: 0.04232717489222097 | validation: 0.06092877499654728]
	TIME [epoch: 14.1 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04478505757778493		[learning rate: 1.9953e-05]
	Learning Rate: 1.99526e-05
	LOSS [training: 0.04478505757778493 | validation: 0.06496651740034907]
	TIME [epoch: 14.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041023841669893146		[learning rate: 1.9812e-05]
	Learning Rate: 1.98118e-05
	LOSS [training: 0.041023841669893146 | validation: 0.06361168465842497]
	TIME [epoch: 14.1 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04535159438195335		[learning rate: 1.9672e-05]
	Learning Rate: 1.96719e-05
	LOSS [training: 0.04535159438195335 | validation: 0.05623442782301355]
	TIME [epoch: 14.1 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041450924061378		[learning rate: 1.9533e-05]
	Learning Rate: 1.9533e-05
	LOSS [training: 0.041450924061378 | validation: 0.0633730600976019]
	TIME [epoch: 14.1 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04280460295590895		[learning rate: 1.9395e-05]
	Learning Rate: 1.93951e-05
	LOSS [training: 0.04280460295590895 | validation: 0.05738994354779453]
	TIME [epoch: 14.1 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041025098516655686		[learning rate: 1.9258e-05]
	Learning Rate: 1.92582e-05
	LOSS [training: 0.041025098516655686 | validation: 0.06025872772631753]
	TIME [epoch: 14.1 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04302016388101941		[learning rate: 1.9122e-05]
	Learning Rate: 1.91222e-05
	LOSS [training: 0.04302016388101941 | validation: 0.05795231214788801]
	TIME [epoch: 14.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039989663915554544		[learning rate: 1.8987e-05]
	Learning Rate: 1.89872e-05
	LOSS [training: 0.039989663915554544 | validation: 0.058920329930808694]
	TIME [epoch: 14.1 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04141178827191119		[learning rate: 1.8853e-05]
	Learning Rate: 1.88532e-05
	LOSS [training: 0.04141178827191119 | validation: 0.059336214557707705]
	TIME [epoch: 14.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04125021328020109		[learning rate: 1.872e-05]
	Learning Rate: 1.87201e-05
	LOSS [training: 0.04125021328020109 | validation: 0.06004720311688916]
	TIME [epoch: 14.1 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042057003294816334		[learning rate: 1.8588e-05]
	Learning Rate: 1.85879e-05
	LOSS [training: 0.042057003294816334 | validation: 0.057581971598938755]
	TIME [epoch: 14.1 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04118595696283142		[learning rate: 1.8457e-05]
	Learning Rate: 1.84567e-05
	LOSS [training: 0.04118595696283142 | validation: 0.05557710117979447]
	TIME [epoch: 14.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04225149618339134		[learning rate: 1.8326e-05]
	Learning Rate: 1.83264e-05
	LOSS [training: 0.04225149618339134 | validation: 0.054838316590707425]
	TIME [epoch: 14.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04092663091239172		[learning rate: 1.8197e-05]
	Learning Rate: 1.8197e-05
	LOSS [training: 0.04092663091239172 | validation: 0.05930434727732081]
	TIME [epoch: 14.1 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03893824907794863		[learning rate: 1.8069e-05]
	Learning Rate: 1.80685e-05
	LOSS [training: 0.03893824907794863 | validation: 0.05833445687886259]
	TIME [epoch: 14.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040928094052689865		[learning rate: 1.7941e-05]
	Learning Rate: 1.7941e-05
	LOSS [training: 0.040928094052689865 | validation: 0.05662335081005046]
	TIME [epoch: 14.1 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.046329944215624475		[learning rate: 1.7814e-05]
	Learning Rate: 1.78143e-05
	LOSS [training: 0.046329944215624475 | validation: 0.051136453319334474]
	TIME [epoch: 14.1 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04196354921885667		[learning rate: 1.7689e-05]
	Learning Rate: 1.76886e-05
	LOSS [training: 0.04196354921885667 | validation: 0.05781318843084937]
	TIME [epoch: 14.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04721642923571484		[learning rate: 1.7564e-05]
	Learning Rate: 1.75637e-05
	LOSS [training: 0.04721642923571484 | validation: 0.055878698592169845]
	TIME [epoch: 14.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041658872436473836		[learning rate: 1.744e-05]
	Learning Rate: 1.74397e-05
	LOSS [training: 0.041658872436473836 | validation: 0.05591906268616984]
	TIME [epoch: 14.1 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04005830251021537		[learning rate: 1.7317e-05]
	Learning Rate: 1.73166e-05
	LOSS [training: 0.04005830251021537 | validation: 0.061796388961690756]
	TIME [epoch: 14.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04577700724466789		[learning rate: 1.7194e-05]
	Learning Rate: 1.71943e-05
	LOSS [training: 0.04577700724466789 | validation: 0.060777106540855025]
	TIME [epoch: 14.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03998178380843437		[learning rate: 1.7073e-05]
	Learning Rate: 1.70729e-05
	LOSS [training: 0.03998178380843437 | validation: 0.05668486881118813]
	TIME [epoch: 14.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0406897754328268		[learning rate: 1.6952e-05]
	Learning Rate: 1.69524e-05
	LOSS [training: 0.0406897754328268 | validation: 0.059359492281067544]
	TIME [epoch: 14.1 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04181288693469519		[learning rate: 1.6833e-05]
	Learning Rate: 1.68327e-05
	LOSS [training: 0.04181288693469519 | validation: 0.05728702334655982]
	TIME [epoch: 14.1 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041411624187973214		[learning rate: 1.6714e-05]
	Learning Rate: 1.67139e-05
	LOSS [training: 0.041411624187973214 | validation: 0.060140490096192226]
	TIME [epoch: 14.1 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042802984375088685		[learning rate: 1.6596e-05]
	Learning Rate: 1.65959e-05
	LOSS [training: 0.042802984375088685 | validation: 0.05771122538220977]
	TIME [epoch: 14.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04015259124782135		[learning rate: 1.6479e-05]
	Learning Rate: 1.64787e-05
	LOSS [training: 0.04015259124782135 | validation: 0.06050004656825439]
	TIME [epoch: 14.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0423431614629234		[learning rate: 1.6362e-05]
	Learning Rate: 1.63624e-05
	LOSS [training: 0.0423431614629234 | validation: 0.05777022741332419]
	TIME [epoch: 14.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040202290677415264		[learning rate: 1.6247e-05]
	Learning Rate: 1.62469e-05
	LOSS [training: 0.040202290677415264 | validation: 0.051894385476728026]
	TIME [epoch: 14.1 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043958617908499587		[learning rate: 1.6132e-05]
	Learning Rate: 1.61322e-05
	LOSS [training: 0.043958617908499587 | validation: 0.05948133173590591]
	TIME [epoch: 14.1 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042751597094786674		[learning rate: 1.6018e-05]
	Learning Rate: 1.60183e-05
	LOSS [training: 0.042751597094786674 | validation: 0.06351143198393926]
	TIME [epoch: 14.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041920677999735624		[learning rate: 1.5905e-05]
	Learning Rate: 1.59052e-05
	LOSS [training: 0.041920677999735624 | validation: 0.05924683860765676]
	TIME [epoch: 14.1 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043191060620388744		[learning rate: 1.5793e-05]
	Learning Rate: 1.57929e-05
	LOSS [training: 0.043191060620388744 | validation: 0.0643749522664899]
	TIME [epoch: 14.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043816313433447295		[learning rate: 1.5681e-05]
	Learning Rate: 1.56814e-05
	LOSS [training: 0.043816313433447295 | validation: 0.05555569943492953]
	TIME [epoch: 14.1 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04298590697532192		[learning rate: 1.5571e-05]
	Learning Rate: 1.55707e-05
	LOSS [training: 0.04298590697532192 | validation: 0.05463486473203556]
	TIME [epoch: 14.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04052023341571835		[learning rate: 1.5461e-05]
	Learning Rate: 1.54608e-05
	LOSS [training: 0.04052023341571835 | validation: 0.056773109244398635]
	TIME [epoch: 14.1 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04192871966520671		[learning rate: 1.5352e-05]
	Learning Rate: 1.53516e-05
	LOSS [training: 0.04192871966520671 | validation: 0.058399048216998956]
	TIME [epoch: 14.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0382217603970956		[learning rate: 1.5243e-05]
	Learning Rate: 1.52432e-05
	LOSS [training: 0.0382217603970956 | validation: 0.0533713369870798]
	TIME [epoch: 14.1 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04005288223252449		[learning rate: 1.5136e-05]
	Learning Rate: 1.51356e-05
	LOSS [training: 0.04005288223252449 | validation: 0.05732606668923991]
	TIME [epoch: 14.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04047782229206577		[learning rate: 1.5029e-05]
	Learning Rate: 1.50288e-05
	LOSS [training: 0.04047782229206577 | validation: 0.0575914416525571]
	TIME [epoch: 14.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042961147028983473		[learning rate: 1.4923e-05]
	Learning Rate: 1.49227e-05
	LOSS [training: 0.042961147028983473 | validation: 0.057534688067660225]
	TIME [epoch: 14.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04305589630499773		[learning rate: 1.4817e-05]
	Learning Rate: 1.48173e-05
	LOSS [training: 0.04305589630499773 | validation: 0.06312922200758414]
	TIME [epoch: 14.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04375341285620829		[learning rate: 1.4713e-05]
	Learning Rate: 1.47127e-05
	LOSS [training: 0.04375341285620829 | validation: 0.05869682546872399]
	TIME [epoch: 14.1 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04274704906740121		[learning rate: 1.4609e-05]
	Learning Rate: 1.46088e-05
	LOSS [training: 0.04274704906740121 | validation: 0.05814335026609511]
	TIME [epoch: 14.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04307336513025052		[learning rate: 1.4506e-05]
	Learning Rate: 1.45057e-05
	LOSS [training: 0.04307336513025052 | validation: 0.061904913360781015]
	TIME [epoch: 14.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040496183540723533		[learning rate: 1.4403e-05]
	Learning Rate: 1.44033e-05
	LOSS [training: 0.040496183540723533 | validation: 0.05909428251234181]
	TIME [epoch: 14.1 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040649349819233335		[learning rate: 1.4302e-05]
	Learning Rate: 1.43016e-05
	LOSS [training: 0.040649349819233335 | validation: 0.06159390070760884]
	TIME [epoch: 14.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04304422633925621		[learning rate: 1.4201e-05]
	Learning Rate: 1.42006e-05
	LOSS [training: 0.04304422633925621 | validation: 0.05633369746497218]
	TIME [epoch: 14.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04199987526423648		[learning rate: 1.41e-05]
	Learning Rate: 1.41004e-05
	LOSS [training: 0.04199987526423648 | validation: 0.06001401773307265]
	TIME [epoch: 14.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039003889465974016		[learning rate: 1.4001e-05]
	Learning Rate: 1.40008e-05
	LOSS [training: 0.039003889465974016 | validation: 0.06447266865345808]
	TIME [epoch: 14.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04164326699914507		[learning rate: 1.3902e-05]
	Learning Rate: 1.3902e-05
	LOSS [training: 0.04164326699914507 | validation: 0.05703046495458383]
	TIME [epoch: 14.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04095520290433688		[learning rate: 1.3804e-05]
	Learning Rate: 1.38038e-05
	LOSS [training: 0.04095520290433688 | validation: 0.05993729437578321]
	TIME [epoch: 14.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0414500544812936		[learning rate: 1.3706e-05]
	Learning Rate: 1.37064e-05
	LOSS [training: 0.0414500544812936 | validation: 0.06365617310233095]
	TIME [epoch: 14.1 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04531458000486119		[learning rate: 1.361e-05]
	Learning Rate: 1.36096e-05
	LOSS [training: 0.04531458000486119 | validation: 0.05921295723380287]
	TIME [epoch: 14.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04082574088756122		[learning rate: 1.3514e-05]
	Learning Rate: 1.35135e-05
	LOSS [training: 0.04082574088756122 | validation: 0.05300440697039972]
	TIME [epoch: 14.1 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.044599396712350345		[learning rate: 1.3418e-05]
	Learning Rate: 1.34181e-05
	LOSS [training: 0.044599396712350345 | validation: 0.05457579752356876]
	TIME [epoch: 14.1 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04338494136694807		[learning rate: 1.3323e-05]
	Learning Rate: 1.33234e-05
	LOSS [training: 0.04338494136694807 | validation: 0.058867772324989624]
	TIME [epoch: 14.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04406733729204178		[learning rate: 1.3229e-05]
	Learning Rate: 1.32294e-05
	LOSS [training: 0.04406733729204178 | validation: 0.056556027723857885]
	TIME [epoch: 14.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041838986474517656		[learning rate: 1.3136e-05]
	Learning Rate: 1.3136e-05
	LOSS [training: 0.041838986474517656 | validation: 0.05953149525323112]
	TIME [epoch: 14.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04261401664111636		[learning rate: 1.3043e-05]
	Learning Rate: 1.30432e-05
	LOSS [training: 0.04261401664111636 | validation: 0.05299368976347554]
	TIME [epoch: 14.1 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04209496572398265		[learning rate: 1.2951e-05]
	Learning Rate: 1.29511e-05
	LOSS [training: 0.04209496572398265 | validation: 0.06614206304664316]
	TIME [epoch: 14.1 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03956604992322996		[learning rate: 1.286e-05]
	Learning Rate: 1.28597e-05
	LOSS [training: 0.03956604992322996 | validation: 0.057544831913813745]
	TIME [epoch: 14.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040567756902340255		[learning rate: 1.2769e-05]
	Learning Rate: 1.27689e-05
	LOSS [training: 0.040567756902340255 | validation: 0.06023133429313537]
	TIME [epoch: 14.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04135914830030305		[learning rate: 1.2679e-05]
	Learning Rate: 1.26788e-05
	LOSS [training: 0.04135914830030305 | validation: 0.058738040693669905]
	TIME [epoch: 14.1 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040178449183481596		[learning rate: 1.2589e-05]
	Learning Rate: 1.25893e-05
	LOSS [training: 0.040178449183481596 | validation: 0.05822366641251099]
	TIME [epoch: 14.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04203003681323474		[learning rate: 1.25e-05]
	Learning Rate: 1.25004e-05
	LOSS [training: 0.04203003681323474 | validation: 0.05531677917687692]
	TIME [epoch: 14.1 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04360065553798157		[learning rate: 1.2412e-05]
	Learning Rate: 1.24121e-05
	LOSS [training: 0.04360065553798157 | validation: 0.0580686103210079]
	TIME [epoch: 14.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041375945167534525		[learning rate: 1.2325e-05]
	Learning Rate: 1.23245e-05
	LOSS [training: 0.041375945167534525 | validation: 0.05836577402756043]
	TIME [epoch: 14.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04220216425716124		[learning rate: 1.2237e-05]
	Learning Rate: 1.22375e-05
	LOSS [training: 0.04220216425716124 | validation: 0.06175453223349045]
	TIME [epoch: 14.1 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040933866981202605		[learning rate: 1.2151e-05]
	Learning Rate: 1.21511e-05
	LOSS [training: 0.040933866981202605 | validation: 0.06045850060545837]
	TIME [epoch: 14.1 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04317453500805328		[learning rate: 1.2065e-05]
	Learning Rate: 1.20653e-05
	LOSS [training: 0.04317453500805328 | validation: 0.05574882332819109]
	TIME [epoch: 14.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045264576735690495		[learning rate: 1.198e-05]
	Learning Rate: 1.19801e-05
	LOSS [training: 0.045264576735690495 | validation: 0.05379747304401368]
	TIME [epoch: 14.1 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04019640711813152		[learning rate: 1.1896e-05]
	Learning Rate: 1.18956e-05
	LOSS [training: 0.04019640711813152 | validation: 0.057534061170288635]
	TIME [epoch: 14.1 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043291050476843895		[learning rate: 1.1812e-05]
	Learning Rate: 1.18116e-05
	LOSS [training: 0.043291050476843895 | validation: 0.05721818709495984]
	TIME [epoch: 14.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040317798013907724		[learning rate: 1.1728e-05]
	Learning Rate: 1.17282e-05
	LOSS [training: 0.040317798013907724 | validation: 0.057381249966033776]
	TIME [epoch: 14.1 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04094573674308212		[learning rate: 1.1645e-05]
	Learning Rate: 1.16454e-05
	LOSS [training: 0.04094573674308212 | validation: 0.06364553779156405]
	TIME [epoch: 14.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0404245477286202		[learning rate: 1.1563e-05]
	Learning Rate: 1.15632e-05
	LOSS [training: 0.0404245477286202 | validation: 0.05922114012693578]
	TIME [epoch: 14.1 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04264192077515429		[learning rate: 1.1482e-05]
	Learning Rate: 1.14815e-05
	LOSS [training: 0.04264192077515429 | validation: 0.060747068864545395]
	TIME [epoch: 14.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04116975417006784		[learning rate: 1.14e-05]
	Learning Rate: 1.14005e-05
	LOSS [training: 0.04116975417006784 | validation: 0.06187993264691638]
	TIME [epoch: 14.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04206509814215395		[learning rate: 1.132e-05]
	Learning Rate: 1.132e-05
	LOSS [training: 0.04206509814215395 | validation: 0.05487209456745105]
	TIME [epoch: 14.1 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04204645385550127		[learning rate: 1.124e-05]
	Learning Rate: 1.12401e-05
	LOSS [training: 0.04204645385550127 | validation: 0.05632032784160821]
	TIME [epoch: 14.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03996385640295217		[learning rate: 1.1161e-05]
	Learning Rate: 1.11607e-05
	LOSS [training: 0.03996385640295217 | validation: 0.05806059056362172]
	TIME [epoch: 14.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03844873763279583		[learning rate: 1.1082e-05]
	Learning Rate: 1.10819e-05
	LOSS [training: 0.03844873763279583 | validation: 0.057788011063208994]
	TIME [epoch: 14.1 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04339272146776532		[learning rate: 1.1004e-05]
	Learning Rate: 1.10037e-05
	LOSS [training: 0.04339272146776532 | validation: 0.06262598994140067]
	TIME [epoch: 14.1 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041906654397497786		[learning rate: 1.0926e-05]
	Learning Rate: 1.0926e-05
	LOSS [training: 0.041906654397497786 | validation: 0.05883409682249028]
	TIME [epoch: 14.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04164821314650002		[learning rate: 1.0849e-05]
	Learning Rate: 1.08489e-05
	LOSS [training: 0.04164821314650002 | validation: 0.05635092188262078]
	TIME [epoch: 14.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04249615195020691		[learning rate: 1.0772e-05]
	Learning Rate: 1.07723e-05
	LOSS [training: 0.04249615195020691 | validation: 0.06340193606731058]
	TIME [epoch: 14.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.038481206927209		[learning rate: 1.0696e-05]
	Learning Rate: 1.06962e-05
	LOSS [training: 0.038481206927209 | validation: 0.05878399547388641]
	TIME [epoch: 14.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04141340922114796		[learning rate: 1.0621e-05]
	Learning Rate: 1.06207e-05
	LOSS [training: 0.04141340922114796 | validation: 0.057497542180651465]
	TIME [epoch: 14.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04176221315954734		[learning rate: 1.0546e-05]
	Learning Rate: 1.05457e-05
	LOSS [training: 0.04176221315954734 | validation: 0.056427674267098875]
	TIME [epoch: 14.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040997551566442844		[learning rate: 1.0471e-05]
	Learning Rate: 1.04713e-05
	LOSS [training: 0.040997551566442844 | validation: 0.05425490136755038]
	TIME [epoch: 14.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040681220615597605		[learning rate: 1.0397e-05]
	Learning Rate: 1.03974e-05
	LOSS [training: 0.040681220615597605 | validation: 0.05461745344547591]
	TIME [epoch: 14.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04456865883354773		[learning rate: 1.0324e-05]
	Learning Rate: 1.0324e-05
	LOSS [training: 0.04456865883354773 | validation: 0.05852779079346825]
	TIME [epoch: 14.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041878989741362976		[learning rate: 1.0251e-05]
	Learning Rate: 1.02511e-05
	LOSS [training: 0.041878989741362976 | validation: 0.06018258771374903]
	TIME [epoch: 14.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.043322567403977774		[learning rate: 1.0179e-05]
	Learning Rate: 1.01787e-05
	LOSS [training: 0.043322567403977774 | validation: 0.05593481650036634]
	TIME [epoch: 14.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.03801973415969204		[learning rate: 1.0107e-05]
	Learning Rate: 1.01068e-05
	LOSS [training: 0.03801973415969204 | validation: 0.05389024698669762]
	TIME [epoch: 14.1 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0428645426526746		[learning rate: 1.0035e-05]
	Learning Rate: 1.00355e-05
	LOSS [training: 0.0428645426526746 | validation: 0.05741289953975035]
	TIME [epoch: 14.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04316296819698383		[learning rate: 9.9646e-06]
	Learning Rate: 9.96464e-06
	LOSS [training: 0.04316296819698383 | validation: 0.05408868388155977]
	TIME [epoch: 130 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04035174138593267		[learning rate: 9.8943e-06]
	Learning Rate: 9.89429e-06
	LOSS [training: 0.04035174138593267 | validation: 0.059328565671284667]
	TIME [epoch: 30.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041624037389632454		[learning rate: 9.8244e-06]
	Learning Rate: 9.82444e-06
	LOSS [training: 0.041624037389632454 | validation: 0.054664803492514624]
	TIME [epoch: 30.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04035566408180455		[learning rate: 9.7551e-06]
	Learning Rate: 9.75508e-06
	LOSS [training: 0.04035566408180455 | validation: 0.0542927515131355]
	TIME [epoch: 30.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04037731726627466		[learning rate: 9.6862e-06]
	Learning Rate: 9.68621e-06
	LOSS [training: 0.04037731726627466 | validation: 0.05444180042421337]
	TIME [epoch: 30.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0403573177810226		[learning rate: 9.6178e-06]
	Learning Rate: 9.61783e-06
	LOSS [training: 0.0403573177810226 | validation: 0.05372527664993331]
	TIME [epoch: 30.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04018017181313552		[learning rate: 9.5499e-06]
	Learning Rate: 9.54993e-06
	LOSS [training: 0.04018017181313552 | validation: 0.06447999780384249]
	TIME [epoch: 30.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.045369503489452526		[learning rate: 9.4825e-06]
	Learning Rate: 9.48251e-06
	LOSS [training: 0.045369503489452526 | validation: 0.06368277186630499]
	TIME [epoch: 30.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04424842421500304		[learning rate: 9.4156e-06]
	Learning Rate: 9.41556e-06
	LOSS [training: 0.04424842421500304 | validation: 0.06555280912817885]
	TIME [epoch: 30.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04192215719189095		[learning rate: 9.3491e-06]
	Learning Rate: 9.34909e-06
	LOSS [training: 0.04192215719189095 | validation: 0.0642341569866136]
	TIME [epoch: 30.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04156913962905559		[learning rate: 9.2831e-06]
	Learning Rate: 9.28308e-06
	LOSS [training: 0.04156913962905559 | validation: 0.062252648274951854]
	TIME [epoch: 30.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040466613573180425		[learning rate: 9.2176e-06]
	Learning Rate: 9.21755e-06
	LOSS [training: 0.040466613573180425 | validation: 0.06110749791818565]
	TIME [epoch: 30.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.041319534171980116		[learning rate: 9.1525e-06]
	Learning Rate: 9.15248e-06
	LOSS [training: 0.041319534171980116 | validation: 0.06409845390786055]
	TIME [epoch: 30.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.039983524730594286		[learning rate: 9.0879e-06]
	Learning Rate: 9.08786e-06
	LOSS [training: 0.039983524730594286 | validation: 0.06208074483605125]
	TIME [epoch: 30.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04087393290515699		[learning rate: 9.0237e-06]
	Learning Rate: 9.0237e-06
	LOSS [training: 0.04087393290515699 | validation: 0.06253239150739158]
	TIME [epoch: 30.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.040416162400914085		[learning rate: 8.96e-06]
	Learning Rate: 8.96e-06
	LOSS [training: 0.040416162400914085 | validation: 0.05980811502474775]
	TIME [epoch: 30.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04482322388525764		[learning rate: 8.8967e-06]
	Learning Rate: 8.89674e-06
	LOSS [training: 0.04482322388525764 | validation: 0.06266426914266905]
	TIME [epoch: 30.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.0439714521768754		[learning rate: 8.8339e-06]
	Learning Rate: 8.83393e-06
	LOSS [training: 0.0439714521768754 | validation: 0.05814050767100028]
	TIME [epoch: 30.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.042539601375882974		[learning rate: 8.7716e-06]
	Learning Rate: 8.77157e-06
	LOSS [training: 0.042539601375882974 | validation: 0.058647057038443244]
	TIME [epoch: 30.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 2/2] avg loss: 0.04386895383763699		[learning rate: 8.7096e-06]
	Learning Rate: 8.70964e-06
	LOSS [training: 0.04386895383763699 | validation: 0.05973524665302413]
	TIME [epoch: 30.4 sec]
	Saving model to: out/model_training/model_phi1_2b_v_mmd1_20240813_194359/states/model_phi1_2b_v_mmd1_1020.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 10805.217 seconds.
