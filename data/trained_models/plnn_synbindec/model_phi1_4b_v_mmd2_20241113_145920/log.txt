Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/basic/data_phi1_4b/training', validation_data='data/training_data/basic/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1457973096

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.929559173378662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.929559173378662 | validation: 4.384097645349718]
	TIME [epoch: 185 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.549554305823278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.549554305823278 | validation: 3.8562556555906498]
	TIME [epoch: 1.45 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.136246179099756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.136246179099756 | validation: 3.5100280001191937]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.813406260376972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.813406260376972 | validation: 6.685018301960326]
	TIME [epoch: 1.38 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.915893412923025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.915893412923025 | validation: 5.104633302209511]
	TIME [epoch: 1.39 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.296342796649035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.296342796649035 | validation: 3.8495008260033035]
	TIME [epoch: 1.39 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.10387981867334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.10387981867334 | validation: 3.4900100245200223]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7722586900350676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7722586900350676 | validation: 3.291673885283302]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2403372614830266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2403372614830266 | validation: 3.1875897491021203]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9196522595345744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9196522595345744 | validation: 2.886416168671065]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.620369011265188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.620369011265188 | validation: 2.591301735971363]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3511174719512304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3511174719512304 | validation: 2.4109883728961985]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.050866862718616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.050866862718616 | validation: 1.9412900930654757]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.619307987592911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.619307987592911 | validation: 1.666502566567719]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4160711656119553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4160711656119553 | validation: 3.576385000722654]
	TIME [epoch: 1.39 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.728479860915626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.728479860915626 | validation: 1.539616163016049]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6203734623951398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6203734623951398 | validation: 1.8428571579927266]
	TIME [epoch: 1.39 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7707009957623223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7707009957623223 | validation: 1.5037542148016811]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.396572861480642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.396572861480642 | validation: 1.4627649272250638]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3373530063248853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3373530063248853 | validation: 1.4239002385596413]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2130662447769678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2130662447769678 | validation: 1.2550574108991073]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0627639196310803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0627639196310803 | validation: 1.1831792956008826]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1065010949500622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1065010949500622 | validation: 1.076597840634853]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9766618080338342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9766618080338342 | validation: 1.0492005293567424]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9411063843855665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9411063843855665 | validation: 1.0646573524687193]
	TIME [epoch: 1.39 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9551533096746135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9551533096746135 | validation: 1.077889350380465]
	TIME [epoch: 1.39 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9661532746922175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9661532746922175 | validation: 1.034167661435368]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152074550783845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152074550783845 | validation: 1.010561494435573]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983825037795569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983825037795569 | validation: 0.9909235893626301]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8874198162669988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8874198162669988 | validation: 0.9757913128656087]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839966185366812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8839966185366812 | validation: 0.9811332389306568]
	TIME [epoch: 1.39 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8770973326697415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8770973326697415 | validation: 0.9678749540576222]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723382424617242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723382424617242 | validation: 0.9878565441333885]
	TIME [epoch: 1.39 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712234589203132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712234589203132 | validation: 0.9736923674659193]
	TIME [epoch: 1.39 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634339400627892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634339400627892 | validation: 0.9985725051684051]
	TIME [epoch: 1.39 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659001302017751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659001302017751 | validation: 0.9650911755585695]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738844642605011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738844642605011 | validation: 1.0431438229632455]
	TIME [epoch: 1.39 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934320481079556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934320481079556 | validation: 1.070076460145019]
	TIME [epoch: 1.39 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0382883441413047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0382883441413047 | validation: 1.0647478911344617]
	TIME [epoch: 1.39 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0506374558443126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0506374558443126 | validation: 1.2050300253774262]
	TIME [epoch: 1.39 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9926080909933939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9926080909933939 | validation: 1.0042378038039832]
	TIME [epoch: 1.39 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013137859785029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9013137859785029 | validation: 0.9749762242222044]
	TIME [epoch: 1.39 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137268344122015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9137268344122015 | validation: 1.02211421629335]
	TIME [epoch: 1.39 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862307987600662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.862307987600662 | validation: 0.9896393257395815]
	TIME [epoch: 1.39 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434728196174581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8434728196174581 | validation: 0.9230238070827937]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394616232908066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394616232908066 | validation: 0.942553314792864]
	TIME [epoch: 1.39 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235279825289001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8235279825289001 | validation: 0.9924398182476516]
	TIME [epoch: 1.39 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.821715338903315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.821715338903315 | validation: 0.9305196060214723]
	TIME [epoch: 1.39 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130123231238785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8130123231238785 | validation: 0.9570587893524176]
	TIME [epoch: 1.39 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125650107216865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8125650107216865 | validation: 0.960656661696855]
	TIME [epoch: 1.39 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083995875793928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083995875793928 | validation: 0.947483171508654]
	TIME [epoch: 1.39 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385806167948205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8385806167948205 | validation: 1.0698497555718582]
	TIME [epoch: 1.39 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986552456779279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8986552456779279 | validation: 1.0313274773476113]
	TIME [epoch: 1.39 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9592457723733913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9592457723733913 | validation: 0.9165931671773708]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204032789841699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8204032789841699 | validation: 1.1502905574846054]
	TIME [epoch: 1.39 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746895693475361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8746895693475361 | validation: 0.9450874546158504]
	TIME [epoch: 1.39 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713486432471484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8713486432471484 | validation: 0.8854436119852492]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869932582495065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869932582495065 | validation: 1.0176870862779441]
	TIME [epoch: 1.39 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982336682541913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982336682541913 | validation: 0.8752554523469002]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7817272449374619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7817272449374619 | validation: 0.9013700423014468]
	TIME [epoch: 1.39 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707153024122511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7707153024122511 | validation: 0.9348024185928226]
	TIME [epoch: 1.39 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690203985849852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690203985849852 | validation: 0.9067999349190512]
	TIME [epoch: 1.39 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690734521296078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690734521296078 | validation: 0.8924472741799149]
	TIME [epoch: 1.39 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651175025253075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651175025253075 | validation: 0.9458741308093432]
	TIME [epoch: 1.39 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645997643282881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645997643282881 | validation: 0.8895435614715952]
	TIME [epoch: 1.39 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720794883254558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720794883254558 | validation: 0.9514724926075813]
	TIME [epoch: 1.39 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768584403753725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7768584403753725 | validation: 1.0531247760646483]
	TIME [epoch: 1.39 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966222293904155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966222293904155 | validation: 1.098204332081486]
	TIME [epoch: 1.39 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9974665735611754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9974665735611754 | validation: 1.019430358522986]
	TIME [epoch: 1.39 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731380997664669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731380997664669 | validation: 0.9045183496795662]
	TIME [epoch: 1.39 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872746145604133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872746145604133 | validation: 1.0097159360171026]
	TIME [epoch: 1.39 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057507648989143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057507648989143 | validation: 0.8762440550534683]
	TIME [epoch: 1.39 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629920517779907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629920517779907 | validation: 0.9267037145940265]
	TIME [epoch: 1.39 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7614386756932692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7614386756932692 | validation: 0.8943497239176212]
	TIME [epoch: 1.39 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638562498639493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638562498639493 | validation: 1.0973320085303075]
	TIME [epoch: 1.43 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167917937925252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167917937925252 | validation: 0.9568730678750745]
	TIME [epoch: 1.39 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908745611683745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908745611683745 | validation: 1.1114608788403926]
	TIME [epoch: 1.39 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047525015710175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8047525015710175 | validation: 0.912963944788892]
	TIME [epoch: 1.39 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613367457392615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7613367457392615 | validation: 0.8679477684299718]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754252118100311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7754252118100311 | validation: 0.9278615882327856]
	TIME [epoch: 1.39 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657644764229328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657644764229328 | validation: 0.9013517223039273]
	TIME [epoch: 1.39 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532940620734128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532940620734128 | validation: 0.8708677381988186]
	TIME [epoch: 1.39 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586769571271856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586769571271856 | validation: 0.9871340061005376]
	TIME [epoch: 1.38 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700025997669663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700025997669663 | validation: 0.8433437283550593]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769912698320377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769912698320377 | validation: 1.068968200745536]
	TIME [epoch: 1.39 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037173368124428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8037173368124428 | validation: 0.8788232173082614]
	TIME [epoch: 1.39 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642709038729102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642709038729102 | validation: 0.9455881372580528]
	TIME [epoch: 1.39 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584259255088587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584259255088587 | validation: 0.8692036156880388]
	TIME [epoch: 1.39 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7652930717412056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7652930717412056 | validation: 0.910446819346268]
	TIME [epoch: 1.39 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8005684051501277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8005684051501277 | validation: 1.2515578584162221]
	TIME [epoch: 1.39 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9306869797074921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9306869797074921 | validation: 1.0483396361365267]
	TIME [epoch: 1.39 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9964484704049384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9964484704049384 | validation: 0.9341737509033075]
	TIME [epoch: 1.39 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713709527578194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713709527578194 | validation: 1.0814895752665723]
	TIME [epoch: 1.39 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467819042114839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467819042114839 | validation: 0.887384997614277]
	TIME [epoch: 1.39 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757908023006934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757908023006934 | validation: 0.861113119559019]
	TIME [epoch: 1.39 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553385724269057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553385724269057 | validation: 0.9429305554764396]
	TIME [epoch: 1.39 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584421174741909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584421174741909 | validation: 0.896516824546933]
	TIME [epoch: 1.39 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483136180386794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7483136180386794 | validation: 0.9052434979588843]
	TIME [epoch: 1.39 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435324805062896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7435324805062896 | validation: 0.8834928646228515]
	TIME [epoch: 1.39 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514104440146429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514104440146429 | validation: 0.9063936932155111]
	TIME [epoch: 1.39 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459378003957139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7459378003957139 | validation: 0.8640100618175279]
	TIME [epoch: 1.38 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.74580386769884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.74580386769884 | validation: 0.9401553371800642]
	TIME [epoch: 1.38 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536312375452952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536312375452952 | validation: 0.8772331661860749]
	TIME [epoch: 1.39 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7560380004966949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7560380004966949 | validation: 0.9709280959325164]
	TIME [epoch: 1.39 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896233996172112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896233996172112 | validation: 0.9564294823765951]
	TIME [epoch: 1.38 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502653634921756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502653634921756 | validation: 1.0188556463937395]
	TIME [epoch: 1.39 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8378098320671765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8378098320671765 | validation: 0.9355776006256626]
	TIME [epoch: 1.39 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575187757433852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7575187757433852 | validation: 0.8686821993609641]
	TIME [epoch: 1.38 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7420263473550867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7420263473550867 | validation: 0.9756093006808172]
	TIME [epoch: 1.39 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572119066548385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572119066548385 | validation: 0.8502064370474989]
	TIME [epoch: 1.39 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7513475992834799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7513475992834799 | validation: 0.921565201999909]
	TIME [epoch: 1.39 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417573854796016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7417573854796016 | validation: 0.8841754223420065]
	TIME [epoch: 1.39 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7404641176457689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7404641176457689 | validation: 0.9410512964708583]
	TIME [epoch: 1.39 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484359217057696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484359217057696 | validation: 0.8539045261971053]
	TIME [epoch: 1.39 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523421154658742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523421154658742 | validation: 1.0965910336521194]
	TIME [epoch: 1.39 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7910440006812838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7910440006812838 | validation: 0.9461504952735909]
	TIME [epoch: 1.39 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873810517085164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.873810517085164 | validation: 1.197476479899962]
	TIME [epoch: 1.39 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8849782344920447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8849782344920447 | validation: 0.9687631475436744]
	TIME [epoch: 1.39 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286908603172002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286908603172002 | validation: 0.8422350240284193]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507823549959838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507823549959838 | validation: 1.0035034977412034]
	TIME [epoch: 1.39 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773654845151115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773654845151115 | validation: 0.87192113556081]
	TIME [epoch: 1.39 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749280750315565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749280750315565 | validation: 0.8576420842316446]
	TIME [epoch: 1.39 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548175354417004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548175354417004 | validation: 0.9797138883617287]
	TIME [epoch: 1.39 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563388709551422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7563388709551422 | validation: 0.8721670393678131]
	TIME [epoch: 1.39 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494382606073078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7494382606073078 | validation: 0.8837408337513453]
	TIME [epoch: 1.39 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511389505546109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7511389505546109 | validation: 0.9394739399731746]
	TIME [epoch: 1.39 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626413297284131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7626413297284131 | validation: 0.8923553334843666]
	TIME [epoch: 1.39 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701903671855703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7701903671855703 | validation: 0.9980052243606498]
	TIME [epoch: 1.39 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775109768844028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775109768844028 | validation: 0.9089615255302639]
	TIME [epoch: 1.39 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7724820635119682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7724820635119682 | validation: 0.8937299637351123]
	TIME [epoch: 1.39 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538952174194811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7538952174194811 | validation: 1.0235719048557574]
	TIME [epoch: 1.39 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858440986192122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858440986192122 | validation: 0.8478162466536313]
	TIME [epoch: 1.39 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7644160870790003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7644160870790003 | validation: 0.9780413612112696]
	TIME [epoch: 1.39 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642437347496557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642437347496557 | validation: 0.8533210447816819]
	TIME [epoch: 1.39 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7391902655608281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7391902655608281 | validation: 0.8753354248891352]
	TIME [epoch: 1.39 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332757199257828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7332757199257828 | validation: 0.903365631380353]
	TIME [epoch: 1.39 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317902011808519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317902011808519 | validation: 0.8551980328223601]
	TIME [epoch: 1.39 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481113739478543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7481113739478543 | validation: 1.0542255538254015]
	TIME [epoch: 1.39 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954840123466838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954840123466838 | validation: 1.0066085085155527]
	TIME [epoch: 1.39 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747537530898873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8747537530898873 | validation: 0.8841031277229195]
	TIME [epoch: 1.39 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8027531703671185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8027531703671185 | validation: 1.0555247523009192]
	TIME [epoch: 1.39 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799187396663818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7799187396663818 | validation: 0.8567310773084209]
	TIME [epoch: 1.39 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423356016287054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7423356016287054 | validation: 0.859863011746985]
	TIME [epoch: 1.39 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460049902388962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7460049902388962 | validation: 0.9538809052919279]
	TIME [epoch: 1.39 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755521801163553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755521801163553 | validation: 0.8609127157084204]
	TIME [epoch: 1.39 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337772370795271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7337772370795271 | validation: 0.9157221382733834]
	TIME [epoch: 1.39 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7306017951878997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7306017951878997 | validation: 0.872308732105963]
	TIME [epoch: 1.39 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182778083482327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7182778083482327 | validation: 0.8967954689485271]
	TIME [epoch: 1.39 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254086759495105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254086759495105 | validation: 0.9047009337278269]
	TIME [epoch: 1.39 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903074862549656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7903074862549656 | validation: 1.1329522069550186]
	TIME [epoch: 1.39 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9820096645961905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9820096645961905 | validation: 0.971834620713879]
	TIME [epoch: 1.39 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915198412556084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7915198412556084 | validation: 0.8295040461387023]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317392375531341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317392375531341 | validation: 0.96026171254913]
	TIME [epoch: 1.39 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73817204609507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73817204609507 | validation: 0.8714114692066688]
	TIME [epoch: 1.39 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310347263634553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310347263634553 | validation: 0.9096312884082016]
	TIME [epoch: 1.39 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275059060987513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7275059060987513 | validation: 0.8443919658415965]
	TIME [epoch: 1.39 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722031566420272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722031566420272 | validation: 0.8805487594863962]
	TIME [epoch: 1.39 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177445001733854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7177445001733854 | validation: 0.8859191856224439]
	TIME [epoch: 1.39 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231222515596298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7231222515596298 | validation: 0.8888018691519532]
	TIME [epoch: 1.39 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7389339462799223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7389339462799223 | validation: 0.9056896973834588]
	TIME [epoch: 1.39 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126238899495418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8126238899495418 | validation: 1.042250693681261]
	TIME [epoch: 1.39 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8012538253746389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8012538253746389 | validation: 0.970455427535525]
	TIME [epoch: 1.39 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586839767637443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586839767637443 | validation: 0.7847624654091585]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882396493460246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7882396493460246 | validation: 0.9209974613806282]
	TIME [epoch: 1.39 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216219584747847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7216219584747847 | validation: 0.7658499583536897]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911757516162355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911757516162355 | validation: 2.0158483380264243]
	TIME [epoch: 1.41 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2481300464824794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2481300464824794 | validation: 1.137389879546889]
	TIME [epoch: 1.39 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0877006432622878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0877006432622878 | validation: 0.9902328413382337]
	TIME [epoch: 1.39 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.885386226499453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.885386226499453 | validation: 0.950050880867001]
	TIME [epoch: 1.39 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484549721840436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8484549721840436 | validation: 0.9659262402898201]
	TIME [epoch: 1.39 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806239173097259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806239173097259 | validation: 0.9361304171770497]
	TIME [epoch: 1.39 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374804416804385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7374804416804385 | validation: 0.9039960799316606]
	TIME [epoch: 1.39 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7417556306913369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7417556306913369 | validation: 0.9280197516780048]
	TIME [epoch: 1.39 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7324301441321339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7324301441321339 | validation: 0.9095594464090937]
	TIME [epoch: 1.39 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7314735034796829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7314735034796829 | validation: 0.9114221882167399]
	TIME [epoch: 1.39 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301930834549484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7301930834549484 | validation: 0.9526081757344816]
	TIME [epoch: 1.39 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7197433747465644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7197433747465644 | validation: 0.9157396672367574]
	TIME [epoch: 1.39 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7230306409188464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7230306409188464 | validation: 0.9025063702787979]
	TIME [epoch: 1.39 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199621637812361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199621637812361 | validation: 0.8841169948546229]
	TIME [epoch: 1.39 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7225506048285502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7225506048285502 | validation: 0.8859838642878558]
	TIME [epoch: 1.39 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232725815636641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7232725815636641 | validation: 0.9074971917435466]
	TIME [epoch: 1.39 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714388311392186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.714388311392186 | validation: 0.8688314997929247]
	TIME [epoch: 1.39 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7148683576743652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7148683576743652 | validation: 0.8376722192695258]
	TIME [epoch: 1.39 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710904659865485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.710904659865485 | validation: 0.8568410683396706]
	TIME [epoch: 1.39 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7140796596377887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7140796596377887 | validation: 0.8298648468040553]
	TIME [epoch: 1.39 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715992448925737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715992448925737 | validation: 0.8940992877623644]
	TIME [epoch: 1.39 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065833624431489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8065833624431489 | validation: 1.181893844039569]
	TIME [epoch: 1.39 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9652697437601491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9652697437601491 | validation: 0.8608918779342511]
	TIME [epoch: 1.39 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954353836566651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6954353836566651 | validation: 0.796655264036408]
	TIME [epoch: 1.39 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982094627645637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6982094627645637 | validation: 0.8085215721923925]
	TIME [epoch: 1.39 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7084499577210466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7084499577210466 | validation: 0.8766755168570342]
	TIME [epoch: 1.39 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253835925985868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8253835925985868 | validation: 1.26723541692432]
	TIME [epoch: 1.39 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.067791045397704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.067791045397704 | validation: 0.9177831064522897]
	TIME [epoch: 1.39 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7354776039547091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7354776039547091 | validation: 1.029937690846239]
	TIME [epoch: 1.39 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177028748514724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177028748514724 | validation: 0.9861570973126167]
	TIME [epoch: 1.39 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254954017182763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254954017182763 | validation: 0.9257558125673833]
	TIME [epoch: 1.39 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490183982409429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7490183982409429 | validation: 0.936478157441197]
	TIME [epoch: 1.39 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7112228590836256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7112228590836256 | validation: 0.9395661155810893]
	TIME [epoch: 1.39 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7188323209252095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7188323209252095 | validation: 0.9384583500703245]
	TIME [epoch: 1.39 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707158584539658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.707158584539658 | validation: 0.8287488032155839]
	TIME [epoch: 1.39 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066659418822229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066659418822229 | validation: 0.8875405625144577]
	TIME [epoch: 183 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7034207414717365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7034207414717365 | validation: 0.8250551248242645]
	TIME [epoch: 2.77 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870377608462992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6870377608462992 | validation: 0.8441292356979775]
	TIME [epoch: 2.75 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6865295570693573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6865295570693573 | validation: 0.798320146721289]
	TIME [epoch: 2.75 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6754027406207308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6754027406207308 | validation: 0.7631108133710696]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654413194074484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.654413194074484 | validation: 0.7441806971510734]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6472299265615381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6472299265615381 | validation: 1.0658728247331961]
	TIME [epoch: 2.75 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9455705989121248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9455705989121248 | validation: 1.7350601221590516]
	TIME [epoch: 2.75 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8342844028667953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8342844028667953 | validation: 0.7668582670607087]
	TIME [epoch: 2.75 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965701545778118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965701545778118 | validation: 0.906923823238974]
	TIME [epoch: 2.76 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684151227742453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684151227742453 | validation: 0.9886727991489654]
	TIME [epoch: 2.75 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786749723126525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7786749723126525 | validation: 0.9443536225806682]
	TIME [epoch: 2.75 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7185566719511244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7185566719511244 | validation: 0.9158587078478956]
	TIME [epoch: 2.75 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265789915835213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7265789915835213 | validation: 0.9029079254731412]
	TIME [epoch: 2.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098168678196529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7098168678196529 | validation: 0.9013954265368398]
	TIME [epoch: 2.75 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.704248485702011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.704248485702011 | validation: 0.894370606601587]
	TIME [epoch: 2.75 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053563005449743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053563005449743 | validation: 0.8882565711846162]
	TIME [epoch: 2.75 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993808365615487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993808365615487 | validation: 0.8812239009412148]
	TIME [epoch: 2.75 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949662498349588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6949662498349588 | validation: 0.8562833752782283]
	TIME [epoch: 2.75 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922968409166907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6922968409166907 | validation: 0.8465618810326678]
	TIME [epoch: 2.75 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6905939681232433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6905939681232433 | validation: 0.8787267037168338]
	TIME [epoch: 2.75 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684605546365889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684605546365889 | validation: 0.8430149383004686]
	TIME [epoch: 2.75 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.685335797313561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.685335797313561 | validation: 0.8648518142477581]
	TIME [epoch: 2.75 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988897825955869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6988897825955869 | validation: 0.8927541448981785]
	TIME [epoch: 2.75 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7320334229935773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7320334229935773 | validation: 0.9098358035883927]
	TIME [epoch: 2.75 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7470132238769556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470132238769556 | validation: 0.8201867968264707]
	TIME [epoch: 2.75 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186321800161187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186321800161187 | validation: 0.8875679942087574]
	TIME [epoch: 2.75 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6997915849326709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6997915849326709 | validation: 0.8001922011318587]
	TIME [epoch: 2.75 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6956810599674036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6956810599674036 | validation: 0.8380707555127884]
	TIME [epoch: 2.75 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6777722273475767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6777722273475767 | validation: 0.8202032144164595]
	TIME [epoch: 2.75 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710920747740773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.710920747740773 | validation: 0.8867073041605338]
	TIME [epoch: 2.76 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751887600240878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751887600240878 | validation: 0.8918588014479101]
	TIME [epoch: 2.75 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786771664023337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786771664023337 | validation: 0.7994574002455699]
	TIME [epoch: 2.75 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405278540132424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6405278540132424 | validation: 0.7486732423750531]
	TIME [epoch: 2.75 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6125289476744181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6125289476744181 | validation: 0.7730550506986384]
	TIME [epoch: 2.75 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6063810170152608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6063810170152608 | validation: 0.7060070244502582]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6195377584909751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6195377584909751 | validation: 0.9080180092991782]
	TIME [epoch: 2.76 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573877449674254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573877449674254 | validation: 0.9267763387058171]
	TIME [epoch: 2.75 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085847501994985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8085847501994985 | validation: 0.7430195987413825]
	TIME [epoch: 2.76 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.693406585502437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.693406585502437 | validation: 0.7653813932828332]
	TIME [epoch: 2.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.642450081477989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.642450081477989 | validation: 0.8709923054180587]
	TIME [epoch: 2.75 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579410846693916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579410846693916 | validation: 0.7921063508988739]
	TIME [epoch: 2.76 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6305295603460879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6305295603460879 | validation: 0.7665487805925039]
	TIME [epoch: 2.75 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069940276307241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6069940276307241 | validation: 0.7657330621367913]
	TIME [epoch: 2.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973426942818548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5973426942818548 | validation: 0.826365256126691]
	TIME [epoch: 2.78 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402097286522533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7402097286522533 | validation: 1.0796255866639999]
	TIME [epoch: 2.76 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.061053278043509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.061053278043509 | validation: 0.7341433381323218]
	TIME [epoch: 2.75 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762823138696357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6762823138696357 | validation: 0.948398502411024]
	TIME [epoch: 2.76 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78966788138448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.78966788138448 | validation: 0.831373792430167]
	TIME [epoch: 2.76 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359761396744462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359761396744462 | validation: 0.8237193759016865]
	TIME [epoch: 2.76 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6706540380930662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6706540380930662 | validation: 0.7861317895655672]
	TIME [epoch: 2.76 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233075117643918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6233075117643918 | validation: 0.7638136544825525]
	TIME [epoch: 2.76 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5865330861781385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5865330861781385 | validation: 0.6589378148885221]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5536046769035161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5536046769035161 | validation: 0.7019908516988365]
	TIME [epoch: 2.76 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593547088596933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5593547088596933 | validation: 0.624798016596756]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7026087815316985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7026087815316985 | validation: 0.8900304292399982]
	TIME [epoch: 2.76 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8188998601081353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8188998601081353 | validation: 0.9897755703877982]
	TIME [epoch: 2.76 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237714345737027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8237714345737027 | validation: 0.6878352065988835]
	TIME [epoch: 2.75 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6535132362094371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6535132362094371 | validation: 0.7486021385442087]
	TIME [epoch: 2.75 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381383578288028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6381383578288028 | validation: 0.8459714136119905]
	TIME [epoch: 2.75 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320183319413091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6320183319413091 | validation: 0.7390773288767323]
	TIME [epoch: 2.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5891975426380854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5891975426380854 | validation: 0.6779963968334192]
	TIME [epoch: 2.75 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5583718871287939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5583718871287939 | validation: 0.7089230324197566]
	TIME [epoch: 2.76 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5769149912408738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5769149912408738 | validation: 0.779280493011682]
	TIME [epoch: 2.75 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610658688564016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610658688564016 | validation: 0.7753892659537274]
	TIME [epoch: 2.75 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6874786963097096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6874786963097096 | validation: 0.7384733016682306]
	TIME [epoch: 2.75 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6338762684554267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6338762684554267 | validation: 0.7374741189407802]
	TIME [epoch: 2.76 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5591021149611851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5591021149611851 | validation: 0.6377165739903351]
	TIME [epoch: 2.75 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530544338929509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.530544338929509 | validation: 0.6860279880575704]
	TIME [epoch: 2.76 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252382341645343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5252382341645343 | validation: 0.5832456636061851]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5596509814038001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5596509814038001 | validation: 0.8428235428608094]
	TIME [epoch: 2.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999021499111632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999021499111632 | validation: 0.9264454819086275]
	TIME [epoch: 2.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456983318315378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456983318315378 | validation: 0.6853732385771208]
	TIME [epoch: 2.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6195675074381678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6195675074381678 | validation: 0.8218057711210079]
	TIME [epoch: 2.74 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6701977195649313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6701977195649313 | validation: 0.8672490753437944]
	TIME [epoch: 2.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6040187795974096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6040187795974096 | validation: 0.702359996768184]
	TIME [epoch: 2.75 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635147463994452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5635147463994452 | validation: 0.6935873602412235]
	TIME [epoch: 2.75 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308174722965566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5308174722965566 | validation: 0.6847858177558309]
	TIME [epoch: 2.75 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154502130181052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5154502130181052 | validation: 0.6654866173184919]
	TIME [epoch: 2.75 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6136534002605056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6136534002605056 | validation: 0.8594535875231509]
	TIME [epoch: 2.75 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554039209209563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554039209209563 | validation: 0.7248323913090229]
	TIME [epoch: 2.75 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578103106071595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.578103106071595 | validation: 0.587796526757199]
	TIME [epoch: 2.75 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5322410136204639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5322410136204639 | validation: 0.6753870806133944]
	TIME [epoch: 2.75 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132524734395321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5132524734395321 | validation: 0.6044587457056072]
	TIME [epoch: 2.75 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019545358950633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5019545358950633 | validation: 0.6974655865687099]
	TIME [epoch: 2.75 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5984989201798431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5984989201798431 | validation: 0.7756629606694007]
	TIME [epoch: 2.76 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650527984953017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650527984953017 | validation: 0.6640851888295027]
	TIME [epoch: 2.75 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5227503801500114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5227503801500114 | validation: 0.7553059123371112]
	TIME [epoch: 2.76 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5399886800644803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5399886800644803 | validation: 0.6473438163043234]
	TIME [epoch: 2.75 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5172108244421421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5172108244421421 | validation: 0.6502553681594817]
	TIME [epoch: 2.75 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5368229244533553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5368229244533553 | validation: 0.6882228511755712]
	TIME [epoch: 2.75 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6373564899428644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6373564899428644 | validation: 0.6590681461987951]
	TIME [epoch: 2.75 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000988256137809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000988256137809 | validation: 0.6066243979780772]
	TIME [epoch: 2.75 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44440175918480096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44440175918480096 | validation: 0.5026047704381335]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4504811687680929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4504811687680929 | validation: 0.6978530146496765]
	TIME [epoch: 2.74 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5079252697189547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5079252697189547 | validation: 0.5441794676624675]
	TIME [epoch: 2.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6187464879954194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6187464879954194 | validation: 0.6614363698475725]
	TIME [epoch: 2.74 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185311606919725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5185311606919725 | validation: 0.7144441708258449]
	TIME [epoch: 2.74 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5848877570444146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5848877570444146 | validation: 0.63343691588116]
	TIME [epoch: 2.74 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4831542070163485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4831542070163485 | validation: 0.6030119536778993]
	TIME [epoch: 2.73 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42352364216119126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42352364216119126 | validation: 0.619403201842117]
	TIME [epoch: 2.76 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5514541052457498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5514541052457498 | validation: 0.8291451180300604]
	TIME [epoch: 2.76 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706428845377099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706428845377099 | validation: 0.6681012699380502]
	TIME [epoch: 2.75 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6284848273991865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6284848273991865 | validation: 0.8708701833679069]
	TIME [epoch: 2.75 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6245846895280585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6245846895280585 | validation: 0.8556513650313881]
	TIME [epoch: 2.75 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5552605565829696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5552605565829696 | validation: 0.5871380423298399]
	TIME [epoch: 2.75 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48224009306604293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48224009306604293 | validation: 0.56045941043003]
	TIME [epoch: 2.75 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4270055836482826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4270055836482826 | validation: 0.5819078840859814]
	TIME [epoch: 2.77 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47514105868597817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47514105868597817 | validation: 0.6679234977185079]
	TIME [epoch: 2.75 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6411413055437051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6411413055437051 | validation: 0.6990876855665683]
	TIME [epoch: 2.75 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832409473327268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6832409473327268 | validation: 0.6527356056537004]
	TIME [epoch: 2.75 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47819692784362267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47819692784362267 | validation: 0.7456048101724906]
	TIME [epoch: 2.75 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5034461248163423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5034461248163423 | validation: 0.5560438190269809]
	TIME [epoch: 2.75 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4422575382256029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4422575382256029 | validation: 0.5513934685605587]
	TIME [epoch: 2.75 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44228024597549953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44228024597549953 | validation: 0.4982601637188414]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5172463119759592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5172463119759592 | validation: 0.612453703418522]
	TIME [epoch: 2.74 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728910479633403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4728910479633403 | validation: 0.5053709580676227]
	TIME [epoch: 2.74 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44669307781070383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44669307781070383 | validation: 0.5942834032147051]
	TIME [epoch: 2.74 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40605537558840016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40605537558840016 | validation: 0.46811895380611185]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.379215979127243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.379215979127243 | validation: 0.5517221183087412]
	TIME [epoch: 2.74 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.458575583811331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.458575583811331 | validation: 0.6179633413511951]
	TIME [epoch: 2.74 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61861709329049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61861709329049 | validation: 0.6272414351707929]
	TIME [epoch: 2.74 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4516665910283219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4516665910283219 | validation: 0.7877128797563666]
	TIME [epoch: 2.75 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4859303026199788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4859303026199788 | validation: 0.4822724112635892]
	TIME [epoch: 2.75 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5593237393890703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5593237393890703 | validation: 0.5243417338394812]
	TIME [epoch: 2.75 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42074614267062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42074614267062 | validation: 0.5475126910633521]
	TIME [epoch: 2.75 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47083742021501623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47083742021501623 | validation: 0.6028533435079271]
	TIME [epoch: 2.75 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46571919198047457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46571919198047457 | validation: 0.5790976759130313]
	TIME [epoch: 2.75 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3869618597309754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3869618597309754 | validation: 0.462613768581081]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3432284559418679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3432284559418679 | validation: 0.4468939994339367]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3669884685746111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3669884685746111 | validation: 0.6060145914775767]
	TIME [epoch: 2.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5527918619299895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5527918619299895 | validation: 0.6289555006682435]
	TIME [epoch: 2.75 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163739335063941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6163739335063941 | validation: 0.6258908200268981]
	TIME [epoch: 2.76 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49368113656899654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49368113656899654 | validation: 0.7804322672521473]
	TIME [epoch: 2.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47780288661923387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47780288661923387 | validation: 0.46502091319839406]
	TIME [epoch: 2.74 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3606862951883316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3606862951883316 | validation: 0.4550724391862575]
	TIME [epoch: 2.74 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474975879353147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3474975879353147 | validation: 0.5239403915245159]
	TIME [epoch: 2.74 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43508644177924505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43508644177924505 | validation: 0.5902113301731623]
	TIME [epoch: 2.74 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49934856058453025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49934856058453025 | validation: 0.6091954418877177]
	TIME [epoch: 2.74 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4587916620092905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4587916620092905 | validation: 0.5727757732539461]
	TIME [epoch: 2.74 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34886696270244044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34886696270244044 | validation: 0.42966434144006016]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_341.pth
	Model improved!!!
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3021008296872551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3021008296872551 | validation: 0.4034447717529533]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3063698632771455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3063698632771455 | validation: 0.4161308896253437]
	TIME [epoch: 2.74 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3164749944944416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3164749944944416 | validation: 0.6376968683541191]
	TIME [epoch: 2.74 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44218421151643983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44218421151643983 | validation: 0.5941852893378526]
	TIME [epoch: 2.74 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41011733538964223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41011733538964223 | validation: 0.4235760057913643]
	TIME [epoch: 2.74 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.451584893089977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.451584893089977 | validation: 0.7443126652420713]
	TIME [epoch: 2.74 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5352727994068887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5352727994068887 | validation: 0.45432603930025267]
	TIME [epoch: 2.74 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.529305247785296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.529305247785296 | validation: 0.49877145647054155]
	TIME [epoch: 2.74 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32474704192470427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32474704192470427 | validation: 0.484960412922442]
	TIME [epoch: 2.74 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3426912770110381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3426912770110381 | validation: 0.47231649093611383]
	TIME [epoch: 2.74 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3957420256197051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3957420256197051 | validation: 0.5901058805674618]
	TIME [epoch: 2.74 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.484455186209759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.484455186209759 | validation: 0.5891190567965665]
	TIME [epoch: 2.74 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33253628103204064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33253628103204064 | validation: 0.4379437993262396]
	TIME [epoch: 2.74 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2809275610827135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2809275610827135 | validation: 0.38505382641593405]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35601383529103603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35601383529103603 | validation: 0.5803411935226751]
	TIME [epoch: 2.74 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5505221076533774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5505221076533774 | validation: 0.6014066660533498]
	TIME [epoch: 2.74 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4915108585661899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4915108585661899 | validation: 0.6873085514850428]
	TIME [epoch: 2.74 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42743937642705737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42743937642705737 | validation: 0.6854665934505138]
	TIME [epoch: 2.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3942818151820724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3942818151820724 | validation: 0.385232112372258]
	TIME [epoch: 2.74 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3427539998911244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3427539998911244 | validation: 0.5351234077112361]
	TIME [epoch: 2.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3923004178592322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3923004178592322 | validation: 0.39362361412956165]
	TIME [epoch: 2.74 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3709272276540115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3709272276540115 | validation: 0.5616916673667819]
	TIME [epoch: 2.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3399306207158682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3399306207158682 | validation: 0.38429389945290904]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28742636092919155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28742636092919155 | validation: 0.43784960003374157]
	TIME [epoch: 2.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29389911267965196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29389911267965196 | validation: 0.4329147513070848]
	TIME [epoch: 2.74 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33432639002992587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33432639002992587 | validation: 0.5088912083244581]
	TIME [epoch: 2.74 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3790635230080954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3790635230080954 | validation: 0.5916095461209467]
	TIME [epoch: 2.74 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40209999124124357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40209999124124357 | validation: 0.4332706915654567]
	TIME [epoch: 2.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2717770562987708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2717770562987708 | validation: 0.41588085470979586]
	TIME [epoch: 2.74 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23607201864908306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23607201864908306 | validation: 0.32534949788965506]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2800515512746476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2800515512746476 | validation: 0.7050838753429766]
	TIME [epoch: 2.74 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035900964227561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5035900964227561 | validation: 0.4011019576999981]
	TIME [epoch: 2.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3522736681938944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3522736681938944 | validation: 0.4646684279521505]
	TIME [epoch: 2.74 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30761879046315893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30761879046315893 | validation: 0.525957104378231]
	TIME [epoch: 2.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3773148386243677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3773148386243677 | validation: 0.47815018087386857]
	TIME [epoch: 2.74 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28996292252295564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28996292252295564 | validation: 0.3714328389410857]
	TIME [epoch: 2.74 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23348924278094324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23348924278094324 | validation: 0.41459447108045977]
	TIME [epoch: 2.74 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3034675528301719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3034675528301719 | validation: 0.6261467038589852]
	TIME [epoch: 2.74 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4742577457949766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4742577457949766 | validation: 0.6790639768734605]
	TIME [epoch: 2.74 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.338327928023785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.338327928023785 | validation: 0.3464770746313369]
	TIME [epoch: 2.74 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21839081524632334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21839081524632334 | validation: 0.39591882628899816]
	TIME [epoch: 2.74 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27409276902984503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27409276902984503 | validation: 0.38379651330449177]
	TIME [epoch: 2.73 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23836295720206543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23836295720206543 | validation: 0.542303072539959]
	TIME [epoch: 2.75 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26600544511931384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26600544511931384 | validation: 0.3810295367396829]
	TIME [epoch: 2.74 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2577230441017819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2577230441017819 | validation: 0.480162759374623]
	TIME [epoch: 2.74 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37512953466959276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37512953466959276 | validation: 0.695285076357564]
	TIME [epoch: 2.74 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5183177831786584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5183177831786584 | validation: 0.3746943556953638]
	TIME [epoch: 2.74 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34455071344050964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34455071344050964 | validation: 0.4739701908471907]
	TIME [epoch: 2.73 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2577421605666402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2577421605666402 | validation: 0.35992518228107695]
	TIME [epoch: 2.74 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562872276947956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2562872276947956 | validation: 0.4810623399017813]
	TIME [epoch: 2.74 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3137335670883291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3137335670883291 | validation: 0.5252195855266955]
	TIME [epoch: 2.74 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34750582727675794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34750582727675794 | validation: 0.44382068158367755]
	TIME [epoch: 2.74 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20410954210816543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20410954210816543 | validation: 0.325691111193407]
	TIME [epoch: 2.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20733333649522742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20733333649522742 | validation: 0.522615014990567]
	TIME [epoch: 2.73 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23237472814973603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23237472814973603 | validation: 0.41063545280580255]
	TIME [epoch: 2.74 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2701639915473421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2701639915473421 | validation: 0.5300694548765551]
	TIME [epoch: 2.74 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25006496234388786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25006496234388786 | validation: 0.38644921376129515]
	TIME [epoch: 2.74 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2206344759453902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2206344759453902 | validation: 0.41385269730813107]
	TIME [epoch: 2.74 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41375682747958775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41375682747958775 | validation: 0.705795920177227]
	TIME [epoch: 2.74 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39655439983608587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39655439983608587 | validation: 0.314183496518003]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22284156997417445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22284156997417445 | validation: 0.3941514044693336]
	TIME [epoch: 2.75 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15853745680420822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15853745680420822 | validation: 0.36982026163612214]
	TIME [epoch: 2.75 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1554365587298395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1554365587298395 | validation: 0.3358242360092737]
	TIME [epoch: 2.75 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15047451435670262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15047451435670262 | validation: 0.4231051994173924]
	TIME [epoch: 2.75 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1871852426217741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1871852426217741 | validation: 0.6834724378148405]
	TIME [epoch: 2.75 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5870834148355846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5870834148355846 | validation: 0.6993617728076451]
	TIME [epoch: 2.75 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5597730406838011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5597730406838011 | validation: 0.6492591358885805]
	TIME [epoch: 2.75 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5942845986417006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5942845986417006 | validation: 0.4901007933860072]
	TIME [epoch: 2.74 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5205102141138775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5205102141138775 | validation: 0.4307484425783519]
	TIME [epoch: 2.73 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26562813377063016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26562813377063016 | validation: 0.42942355481412114]
	TIME [epoch: 2.73 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24377027187794986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24377027187794986 | validation: 0.48586554605723375]
	TIME [epoch: 2.74 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37480370374596184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37480370374596184 | validation: 0.4963300633336177]
	TIME [epoch: 2.73 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26787382938825255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26787382938825255 | validation: 0.32992782293614076]
	TIME [epoch: 2.73 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19127088985045293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19127088985045293 | validation: 0.4746883914508834]
	TIME [epoch: 2.73 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2096521802924147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2096521802924147 | validation: 0.3317796635564618]
	TIME [epoch: 2.74 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2561520280385713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2561520280385713 | validation: 0.60287954119575]
	TIME [epoch: 2.74 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3070704592534374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3070704592534374 | validation: 0.3341055380482152]
	TIME [epoch: 2.74 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.222849651396403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.222849651396403 | validation: 0.4014102351481601]
	TIME [epoch: 2.74 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19354685215167713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19354685215167713 | validation: 0.4430570350635414]
	TIME [epoch: 2.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24360859512309682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24360859512309682 | validation: 0.45284230041596657]
	TIME [epoch: 2.74 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2953350630606699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2953350630606699 | validation: 0.6081279373811945]
	TIME [epoch: 2.73 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30545852173157945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30545852173157945 | validation: 0.42200544673369955]
	TIME [epoch: 2.74 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1628840896290251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1628840896290251 | validation: 0.30517040386354133]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18020469685523588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18020469685523588 | validation: 0.5177586855644637]
	TIME [epoch: 2.76 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1921723142604119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1921723142604119 | validation: 0.3118418735280305]
	TIME [epoch: 2.75 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19420443194094372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19420443194094372 | validation: 0.5039157083668103]
	TIME [epoch: 2.75 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25956660926344166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25956660926344166 | validation: 0.37382459767798687]
	TIME [epoch: 2.75 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32622696323771266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32622696323771266 | validation: 0.4848413432122538]
	TIME [epoch: 2.75 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3173801975497547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3173801975497547 | validation: 0.5646243384537412]
	TIME [epoch: 2.75 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24674433592601452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24674433592601452 | validation: 0.3138412122840803]
	TIME [epoch: 2.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17228100456698292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17228100456698292 | validation: 0.38990636592694977]
	TIME [epoch: 2.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16501330803285527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16501330803285527 | validation: 0.38201101772232776]
	TIME [epoch: 2.75 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21942303952034564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21942303952034564 | validation: 0.5338368181273999]
	TIME [epoch: 2.75 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36212301111963086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36212301111963086 | validation: 0.5694676143943872]
	TIME [epoch: 2.75 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780768960241487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2780768960241487 | validation: 0.3414472344524893]
	TIME [epoch: 2.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15157705427825477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15157705427825477 | validation: 0.34258859351397364]
	TIME [epoch: 2.75 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14347621915657072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14347621915657072 | validation: 0.3479319038753243]
	TIME [epoch: 2.75 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14375380534685828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14375380534685828 | validation: 0.385612140027825]
	TIME [epoch: 2.75 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16867575279306984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16867575279306984 | validation: 0.36996676739252643]
	TIME [epoch: 2.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2631091046237384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2631091046237384 | validation: 0.5080545844956733]
	TIME [epoch: 2.75 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30213786352870703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30213786352870703 | validation: 0.3400547007008346]
	TIME [epoch: 2.75 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15359852306885707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15359852306885707 | validation: 0.3311265642769865]
	TIME [epoch: 2.75 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12988157312438334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12988157312438334 | validation: 0.3822677501904745]
	TIME [epoch: 2.75 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345391327704736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13345391327704736 | validation: 0.34435522149090403]
	TIME [epoch: 2.75 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825505325761218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825505325761218 | validation: 0.7943257042485267]
	TIME [epoch: 2.75 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3951584585830641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3951584585830641 | validation: 0.43521885640633795]
	TIME [epoch: 2.75 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40142331502670014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40142331502670014 | validation: 0.348254036627643]
	TIME [epoch: 2.75 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17059888985527719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17059888985527719 | validation: 0.6434721438815603]
	TIME [epoch: 2.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3340933574038026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3340933574038026 | validation: 0.5104800509994997]
	TIME [epoch: 2.75 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19993392337142454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19993392337142454 | validation: 0.3105408782834269]
	TIME [epoch: 2.75 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1889890921389593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1889890921389593 | validation: 0.40848704159182336]
	TIME [epoch: 2.75 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1898111710680556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1898111710680556 | validation: 0.41988375867553257]
	TIME [epoch: 2.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435066944961789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2435066944961789 | validation: 0.45158933123503103]
	TIME [epoch: 2.75 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22275424598943702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22275424598943702 | validation: 0.3377984945532455]
	TIME [epoch: 2.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16610835266776142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16610835266776142 | validation: 0.370989000049157]
	TIME [epoch: 2.75 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14229231831483571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14229231831483571 | validation: 0.34059827375720547]
	TIME [epoch: 2.75 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12236174994908434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12236174994908434 | validation: 0.3493803817537365]
	TIME [epoch: 2.75 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313841772095875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1313841772095875 | validation: 0.3994964391151911]
	TIME [epoch: 2.75 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551585784848164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551585784848164 | validation: 0.4571931172414811]
	TIME [epoch: 2.75 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305635804644081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2305635804644081 | validation: 0.42963469100493107]
	TIME [epoch: 2.76 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22087004813444902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22087004813444902 | validation: 0.45199558755381464]
	TIME [epoch: 2.75 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21817030921351158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21817030921351158 | validation: 0.33886555002655316]
	TIME [epoch: 2.75 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15583230141610274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15583230141610274 | validation: 0.3809792255752068]
	TIME [epoch: 2.75 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1217524870727223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1217524870727223 | validation: 0.30022204202335073]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1183004866613093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1183004866613093 | validation: 0.5263130999648008]
	TIME [epoch: 2.74 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1607426199555153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1607426199555153 | validation: 0.32076062759513135]
	TIME [epoch: 2.73 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20418066061593504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20418066061593504 | validation: 0.5808771981605786]
	TIME [epoch: 2.73 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18998618986489565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18998618986489565 | validation: 0.3374392144836125]
	TIME [epoch: 2.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14435296001053188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14435296001053188 | validation: 0.3685392508724016]
	TIME [epoch: 2.74 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2196535952400681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2196535952400681 | validation: 0.813468470639006]
	TIME [epoch: 2.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935211722763521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7935211722763521 | validation: 0.8639312135804036]
	TIME [epoch: 2.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3977564488621454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3977564488621454 | validation: 0.4558844043513335]
	TIME [epoch: 2.73 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30296861456008467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30296861456008467 | validation: 0.34501220528845]
	TIME [epoch: 2.74 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18996636973452685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18996636973452685 | validation: 0.3784440319320759]
	TIME [epoch: 2.73 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13472183885354114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13472183885354114 | validation: 0.38246461564562795]
	TIME [epoch: 2.73 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13915697915682892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13915697915682892 | validation: 0.3357746519953464]
	TIME [epoch: 2.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336441684077737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336441684077737 | validation: 0.3595841800423437]
	TIME [epoch: 2.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11690095089181556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11690095089181556 | validation: 0.3253043980627565]
	TIME [epoch: 2.74 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12646515098774364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12646515098774364 | validation: 0.41297303907414923]
	TIME [epoch: 2.73 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14621700155719516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14621700155719516 | validation: 0.33898345444889866]
	TIME [epoch: 2.73 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13799190605756642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13799190605756642 | validation: 0.3997799730542707]
	TIME [epoch: 2.74 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13385077113468144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13385077113468144 | validation: 0.34824432152345985]
	TIME [epoch: 2.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15776802193756292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15776802193756292 | validation: 0.3887331795574114]
	TIME [epoch: 2.73 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19817864727792567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19817864727792567 | validation: 0.44890071035444623]
	TIME [epoch: 2.73 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23183253133927514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23183253133927514 | validation: 0.40917707986308277]
	TIME [epoch: 2.73 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1630772768389243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1630772768389243 | validation: 0.33148703564958804]
	TIME [epoch: 2.73 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12805058893387206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12805058893387206 | validation: 0.3955077103339797]
	TIME [epoch: 2.74 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11663417883816259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11663417883816259 | validation: 0.3138155431595886]
	TIME [epoch: 2.73 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11837846947451112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11837846947451112 | validation: 0.42890704296816456]
	TIME [epoch: 2.74 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14496717936182882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14496717936182882 | validation: 0.39042444011174343]
	TIME [epoch: 2.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20819254740892143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20819254740892143 | validation: 0.5272330375367382]
	TIME [epoch: 2.74 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20286343685274616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20286343685274616 | validation: 0.3780366767334618]
	TIME [epoch: 2.73 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14560684643793856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14560684643793856 | validation: 0.28423636055762486]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15114948274912152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15114948274912152 | validation: 0.5668930817590226]
	TIME [epoch: 2.75 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22279693367033887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22279693367033887 | validation: 0.31697658267034967]
	TIME [epoch: 2.75 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16699550813766084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16699550813766084 | validation: 0.46887809055422747]
	TIME [epoch: 2.74 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12206247155546301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12206247155546301 | validation: 0.32331409199870803]
	TIME [epoch: 2.75 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09058388231858006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09058388231858006 | validation: 0.2910995143373481]
	TIME [epoch: 2.75 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09533492923326531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09533492923326531 | validation: 0.3695615230697946]
	TIME [epoch: 2.75 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11371232872927468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11371232872927468 | validation: 0.3102254038070825]
	TIME [epoch: 185 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11515981637855373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11515981637855373 | validation: 0.45449398497268717]
	TIME [epoch: 5.87 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2473828660822348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2473828660822348 | validation: 0.5998211363170561]
	TIME [epoch: 5.86 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3724694069926623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3724694069926623 | validation: 0.35083718087654897]
	TIME [epoch: 5.86 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17739208350095795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17739208350095795 | validation: 0.39416812848360966]
	TIME [epoch: 5.86 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10086376101667828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10086376101667828 | validation: 0.2988479274337083]
	TIME [epoch: 5.86 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10185071160768118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10185071160768118 | validation: 0.3331205811891369]
	TIME [epoch: 5.85 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10714094136831541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10714094136831541 | validation: 0.3296576286265026]
	TIME [epoch: 5.86 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197627715010712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197627715010712 | validation: 0.30491947436010547]
	TIME [epoch: 5.86 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129719434791297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08129719434791297 | validation: 0.3435115992133553]
	TIME [epoch: 5.86 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0854795885657159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0854795885657159 | validation: 0.304095606858509]
	TIME [epoch: 5.86 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732277359247931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10732277359247931 | validation: 0.5653180637431531]
	TIME [epoch: 5.86 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20940221522234623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20940221522234623 | validation: 0.472856504429306]
	TIME [epoch: 5.86 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2946049151713435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2946049151713435 | validation: 0.4524374561200792]
	TIME [epoch: 5.86 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738843016262193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1738843016262193 | validation: 0.3553451635110762]
	TIME [epoch: 5.86 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11339828113147275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11339828113147275 | validation: 0.29062526751039675]
	TIME [epoch: 5.86 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13682657131738407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13682657131738407 | validation: 0.4720164843275563]
	TIME [epoch: 5.86 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14181443539471977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14181443539471977 | validation: 0.27938110826370005]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11279433990700717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11279433990700717 | validation: 0.3079150408125621]
	TIME [epoch: 5.86 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.096989533553837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.096989533553837 | validation: 0.31230377175952995]
	TIME [epoch: 5.86 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09720952309213637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09720952309213637 | validation: 0.3146767311805856]
	TIME [epoch: 5.86 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669635197562866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10669635197562866 | validation: 0.30515323099022795]
	TIME [epoch: 5.86 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12021441426792312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12021441426792312 | validation: 0.3770191176186633]
	TIME [epoch: 5.86 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17753340609338786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17753340609338786 | validation: 0.3748676800095844]
	TIME [epoch: 5.86 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17377222580836893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17377222580836893 | validation: 0.37985622012218406]
	TIME [epoch: 5.87 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2039768208237673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2039768208237673 | validation: 0.5046844793482385]
	TIME [epoch: 5.86 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471483673692368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1471483673692368 | validation: 0.29067724186548816]
	TIME [epoch: 5.86 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551372911246204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10551372911246204 | validation: 0.43829100780848773]
	TIME [epoch: 5.86 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09467725199450805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09467725199450805 | validation: 0.2747291471672801]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08248866368431265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08248866368431265 | validation: 0.43212115810119517]
	TIME [epoch: 5.86 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947364634520152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0947364634520152 | validation: 0.27338204587280884]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08374884357179091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08374884357179091 | validation: 0.3968753437744815]
	TIME [epoch: 5.86 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09574661824159918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09574661824159918 | validation: 0.2763718216382184]
	TIME [epoch: 5.86 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13000122485836732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13000122485836732 | validation: 0.5699559068008063]
	TIME [epoch: 5.85 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18310657193436725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18310657193436725 | validation: 0.26561865114670996]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479893624131776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1479893624131776 | validation: 0.7515449498510036]
	TIME [epoch: 5.85 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6947014474083585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6947014474083585 | validation: 0.6780494192315175]
	TIME [epoch: 5.86 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2243110344057568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2243110344057568 | validation: 0.40289075206365815]
	TIME [epoch: 5.86 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18095141896017305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18095141896017305 | validation: 0.27899432168345084]
	TIME [epoch: 5.86 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17931022487077783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17931022487077783 | validation: 0.2959284937147289]
	TIME [epoch: 5.86 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1083381626645308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1083381626645308 | validation: 0.3289293826939963]
	TIME [epoch: 5.86 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684443545340113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11684443545340113 | validation: 0.34327124462993713]
	TIME [epoch: 5.86 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10024286412839795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10024286412839795 | validation: 0.28086768697838477]
	TIME [epoch: 5.86 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840556833487258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0840556833487258 | validation: 0.30181882127102533]
	TIME [epoch: 5.86 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08550396707947566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08550396707947566 | validation: 0.3103336330651718]
	TIME [epoch: 5.86 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11204892481752264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11204892481752264 | validation: 0.34043672059150965]
	TIME [epoch: 5.86 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11574779592650652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11574779592650652 | validation: 0.31710616245441353]
	TIME [epoch: 5.86 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12036363036095686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12036363036095686 | validation: 0.34769054704673485]
	TIME [epoch: 5.86 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016753766584029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09016753766584029 | validation: 0.29383901733467177]
	TIME [epoch: 5.86 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835632128092638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0835632128092638 | validation: 0.3213980650683112]
	TIME [epoch: 5.86 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10235017022375051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10235017022375051 | validation: 0.3492798839963503]
	TIME [epoch: 5.86 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16381178110569813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16381178110569813 | validation: 0.35024221399981753]
	TIME [epoch: 5.86 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17636336279007267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17636336279007267 | validation: 0.3264283939551767]
	TIME [epoch: 5.86 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12924549629185583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12924549629185583 | validation: 0.291881363813108]
	TIME [epoch: 5.86 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348951314624885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07348951314624885 | validation: 0.28718983822187294]
	TIME [epoch: 5.86 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06625773413711268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06625773413711268 | validation: 0.29972673062369365]
	TIME [epoch: 5.86 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07355010846125284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07355010846125284 | validation: 0.27406471639406893]
	TIME [epoch: 5.86 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07448886931954064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07448886931954064 | validation: 0.2752586989447669]
	TIME [epoch: 5.86 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06781565891167136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06781565891167136 | validation: 0.30770373875118073]
	TIME [epoch: 5.86 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825009045556364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825009045556364 | validation: 0.2943688777763704]
	TIME [epoch: 5.86 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12527365763875192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12527365763875192 | validation: 0.782664385147846]
	TIME [epoch: 5.86 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3228896622451153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3228896622451153 | validation: 0.2776598655883889]
	TIME [epoch: 5.87 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13840168392284383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13840168392284383 | validation: 0.24160945731598835]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06049415203384973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06049415203384973 | validation: 0.310600272891144]
	TIME [epoch: 5.86 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09463608860075638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09463608860075638 | validation: 0.29533803861004765]
	TIME [epoch: 5.86 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10814941000169107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10814941000169107 | validation: 0.27531032639944114]
	TIME [epoch: 5.86 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10497881277921398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10497881277921398 | validation: 0.4423277131637197]
	TIME [epoch: 5.86 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17784042115265097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17784042115265097 | validation: 0.2901683644755791]
	TIME [epoch: 5.86 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1427282989259361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1427282989259361 | validation: 0.3289940235760145]
	TIME [epoch: 5.86 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1743283349803274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1743283349803274 | validation: 0.3096119605742369]
	TIME [epoch: 5.86 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14521274509269816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14521274509269816 | validation: 0.26628992412174196]
	TIME [epoch: 5.86 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764678409311073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0764678409311073 | validation: 0.2695688347999273]
	TIME [epoch: 5.86 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056372964047506725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056372964047506725 | validation: 0.27408016972166815]
	TIME [epoch: 5.86 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05788468390557348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05788468390557348 | validation: 0.2741115048454896]
	TIME [epoch: 5.86 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853710930820605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05853710930820605 | validation: 0.2689748119696579]
	TIME [epoch: 5.86 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846122298842545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06846122298842545 | validation: 0.31235329477195517]
	TIME [epoch: 5.86 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0700002870936233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0700002870936233 | validation: 0.28016813028887316]
	TIME [epoch: 5.86 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740961160119437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09740961160119437 | validation: 0.5451010643102289]
	TIME [epoch: 5.85 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19832856927860454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19832856927860454 | validation: 0.35497009618982445]
	TIME [epoch: 5.86 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23237916541906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23237916541906 | validation: 0.4888534740735489]
	TIME [epoch: 5.86 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11607682667588791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11607682667588791 | validation: 0.24101889211812375]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07151078071024039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07151078071024039 | validation: 0.2614995162453924]
	TIME [epoch: 5.85 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180457886115788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09180457886115788 | validation: 0.2825996104002457]
	TIME [epoch: 5.86 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493197788651596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08493197788651596 | validation: 0.3025459555479086]
	TIME [epoch: 5.85 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721815581695816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721815581695816 | validation: 0.2441697534095571]
	TIME [epoch: 5.86 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09655358578882287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09655358578882287 | validation: 0.40023253612953535]
	TIME [epoch: 5.85 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13761800558090834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13761800558090834 | validation: 0.2843397334919292]
	TIME [epoch: 5.86 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1089320573123929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1089320573123929 | validation: 0.34460742042818654]
	TIME [epoch: 5.85 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0820740194516029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0820740194516029 | validation: 0.2683599948435792]
	TIME [epoch: 5.86 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07536295430123385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07536295430123385 | validation: 0.27298595404381354]
	TIME [epoch: 5.86 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651819833798635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0651819833798635 | validation: 0.260877172589569]
	TIME [epoch: 5.86 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06737404029048294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06737404029048294 | validation: 0.5160937054572804]
	TIME [epoch: 5.86 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128922733337249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128922733337249 | validation: 0.31148939039755286]
	TIME [epoch: 5.86 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1546982134816727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1546982134816727 | validation: 0.4990182186855947]
	TIME [epoch: 5.86 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15511381126209609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15511381126209609 | validation: 0.2634924332834707]
	TIME [epoch: 5.86 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08474729880287506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08474729880287506 | validation: 0.23273999839114412]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_596.pth
	Model improved!!!
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12066165896388238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12066165896388238 | validation: 0.3683841529129946]
	TIME [epoch: 5.89 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1998111262659392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1998111262659392 | validation: 0.2785050388432954]
	TIME [epoch: 5.89 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06732101535273907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06732101535273907 | validation: 0.28291149203301436]
	TIME [epoch: 5.89 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519201440244848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06519201440244848 | validation: 0.27390641187205916]
	TIME [epoch: 5.89 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060916615637404824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060916615637404824 | validation: 0.2666605713163887]
	TIME [epoch: 5.86 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044893108524867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06044893108524867 | validation: 0.2974843496065149]
	TIME [epoch: 5.87 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998813478492868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04998813478492868 | validation: 0.31332751706504136]
	TIME [epoch: 5.86 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12084180936408309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12084180936408309 | validation: 0.7120853162638185]
	TIME [epoch: 5.87 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23793083368721873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23793083368721873 | validation: 0.31715563364269256]
	TIME [epoch: 5.86 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18344485084736228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18344485084736228 | validation: 0.25521613556282213]
	TIME [epoch: 5.86 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07365488648237871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07365488648237871 | validation: 0.2819851303344879]
	TIME [epoch: 5.86 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348150917132625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07348150917132625 | validation: 0.23160864701787875]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06447201031403002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06447201031403002 | validation: 0.28313974167918626]
	TIME [epoch: 5.85 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0640968320928209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0640968320928209 | validation: 0.2708634985542724]
	TIME [epoch: 5.89 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06854471541945693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06854471541945693 | validation: 0.28309332147121263]
	TIME [epoch: 5.88 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720645700569328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08720645700569328 | validation: 0.24983596262411356]
	TIME [epoch: 5.89 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09843524705319855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09843524705319855 | validation: 0.3154793867737126]
	TIME [epoch: 5.89 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11939866270442746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11939866270442746 | validation: 0.2170057423192911]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913693307836894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10913693307836894 | validation: 0.25672714957414927]
	TIME [epoch: 5.88 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10617733075864898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10617733075864898 | validation: 0.452874694497352]
	TIME [epoch: 5.89 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12117947794086957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12117947794086957 | validation: 0.28217142391039224]
	TIME [epoch: 5.89 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13721212681569409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13721212681569409 | validation: 0.31162512455505925]
	TIME [epoch: 5.89 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788345377531797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06788345377531797 | validation: 0.23694629349878307]
	TIME [epoch: 5.89 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052230707274396056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052230707274396056 | validation: 0.24476092408399963]
	TIME [epoch: 5.89 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056280868041919056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056280868041919056 | validation: 0.2714945596265895]
	TIME [epoch: 5.88 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056608172896808674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056608172896808674 | validation: 0.22635081617492286]
	TIME [epoch: 5.88 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05530369180626305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05530369180626305 | validation: 0.2867537275621154]
	TIME [epoch: 5.88 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06093302622012172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06093302622012172 | validation: 0.2427166946512177]
	TIME [epoch: 5.88 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748356278552376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0748356278552376 | validation: 0.2733804778340537]
	TIME [epoch: 5.88 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11454979649159899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11454979649159899 | validation: 0.34493285691279774]
	TIME [epoch: 5.88 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23771245871116217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23771245871116217 | validation: 0.4204084036062116]
	TIME [epoch: 5.88 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17543632311179508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17543632311179508 | validation: 0.24461674072835235]
	TIME [epoch: 5.88 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05835013938755158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05835013938755158 | validation: 0.22951140761784422]
	TIME [epoch: 5.88 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635987053284807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0635987053284807 | validation: 0.3431877572642204]
	TIME [epoch: 5.89 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670809105027481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06670809105027481 | validation: 0.25909344484233443]
	TIME [epoch: 5.89 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056747684191396974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056747684191396974 | validation: 0.2734246196771896]
	TIME [epoch: 5.89 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04977624381518398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04977624381518398 | validation: 0.22320738051025102]
	TIME [epoch: 5.88 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05585203692623301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05585203692623301 | validation: 0.39328978133395415]
	TIME [epoch: 5.89 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09211534957855942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09211534957855942 | validation: 0.30210962803100827]
	TIME [epoch: 5.88 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14536900011069906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14536900011069906 | validation: 0.4991043166093457]
	TIME [epoch: 5.89 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1492333517366192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1492333517366192 | validation: 0.2657491462658261]
	TIME [epoch: 5.89 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09817911993212242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09817911993212242 | validation: 0.2184899123249367]
	TIME [epoch: 5.89 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058540935230962175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058540935230962175 | validation: 0.2302950598665815]
	TIME [epoch: 5.89 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04938063610572591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04938063610572591 | validation: 0.28157566138631185]
	TIME [epoch: 5.89 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04896673469008698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04896673469008698 | validation: 0.23134465173782714]
	TIME [epoch: 5.89 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05917751191401809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05917751191401809 | validation: 0.24276876564214814]
	TIME [epoch: 5.88 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495985307839186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07495985307839186 | validation: 0.2517691292174899]
	TIME [epoch: 5.88 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09354837775558514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09354837775558514 | validation: 0.33848177634859145]
	TIME [epoch: 5.88 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17551017629988422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17551017629988422 | validation: 0.26344320352596823]
	TIME [epoch: 5.88 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12550180576116185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12550180576116185 | validation: 0.27679207018814767]
	TIME [epoch: 5.88 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061563099046275396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061563099046275396 | validation: 0.20855051168107358]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05953437205228063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05953437205228063 | validation: 0.31194043584239667]
	TIME [epoch: 5.86 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875649646388377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07875649646388377 | validation: 0.22625867942136627]
	TIME [epoch: 5.86 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938917064519727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06938917064519727 | validation: 0.36692615321789046]
	TIME [epoch: 5.86 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340875382039565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07340875382039565 | validation: 0.2766215839824696]
	TIME [epoch: 5.85 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1047358505381634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1047358505381634 | validation: 0.5011664823108136]
	TIME [epoch: 5.86 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135222188826039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135222188826039 | validation: 0.22439354748102536]
	TIME [epoch: 5.84 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08076956561130215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08076956561130215 | validation: 0.24527420748423207]
	TIME [epoch: 5.86 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04366376653701425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04366376653701425 | validation: 0.22263467030113526]
	TIME [epoch: 5.84 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04468128568399907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04468128568399907 | validation: 0.2173045247378559]
	TIME [epoch: 5.85 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04995889973836734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04995889973836734 | validation: 0.2246237370335531]
	TIME [epoch: 5.85 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06297698611108885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06297698611108885 | validation: 0.22616862537590332]
	TIME [epoch: 5.85 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08923058912010191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08923058912010191 | validation: 0.2920550837913569]
	TIME [epoch: 5.84 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09459136914127907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09459136914127907 | validation: 0.23189976418331418]
	TIME [epoch: 5.84 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030566307350831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11030566307350831 | validation: 0.23853620502218376]
	TIME [epoch: 5.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565833065847053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565833065847053 | validation: 0.20117634722048217]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038106439762099216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038106439762099216 | validation: 0.23645214792549152]
	TIME [epoch: 5.86 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038129969507307265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038129969507307265 | validation: 0.20678837352263468]
	TIME [epoch: 5.87 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591817457004877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05591817457004877 | validation: 0.35583924429059555]
	TIME [epoch: 5.87 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11817180779824252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11817180779824252 | validation: 0.36469332732653503]
	TIME [epoch: 5.87 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20387916388197197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20387916388197197 | validation: 0.6017798914020815]
	TIME [epoch: 5.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800643656208213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1800643656208213 | validation: 0.23319679199108503]
	TIME [epoch: 5.85 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07780057308350426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07780057308350426 | validation: 0.1972610716222809]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_669.pth
	Model improved!!!
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05106042000651523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05106042000651523 | validation: 0.28835666316515707]
	TIME [epoch: 5.87 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06552946745981951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06552946745981951 | validation: 0.2010421054746825]
	TIME [epoch: 5.87 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05288606331223356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05288606331223356 | validation: 0.22422325936612508]
	TIME [epoch: 5.86 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0405319253566557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0405319253566557 | validation: 0.19934640126920683]
	TIME [epoch: 5.87 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037171287461550934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037171287461550934 | validation: 0.19883120272579452]
	TIME [epoch: 5.87 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05696995966213854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05696995966213854 | validation: 0.2143104954557339]
	TIME [epoch: 5.87 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158110842204039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10158110842204039 | validation: 0.27260453560194386]
	TIME [epoch: 5.87 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1939887078540883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1939887078540883 | validation: 0.30040979687358793]
	TIME [epoch: 5.88 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07105405447114936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07105405447114936 | validation: 0.19095919494062766]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_678.pth
	Model improved!!!
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04519037626349583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04519037626349583 | validation: 0.21072167058857702]
	TIME [epoch: 5.84 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04861610581870144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04861610581870144 | validation: 0.22272092831652093]
	TIME [epoch: 5.84 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046532850929114994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046532850929114994 | validation: 0.22697239862590324]
	TIME [epoch: 5.84 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04363228416514336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04363228416514336 | validation: 0.21243333657831776]
	TIME [epoch: 5.84 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04755883811807576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04755883811807576 | validation: 0.20706734023604456]
	TIME [epoch: 5.85 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048736797136690664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048736797136690664 | validation: 0.19925656643257741]
	TIME [epoch: 5.88 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043617507827743644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043617507827743644 | validation: 0.2512816166436788]
	TIME [epoch: 5.89 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05510298090253663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05510298090253663 | validation: 0.3020125293111681]
	TIME [epoch: 5.88 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13589411249329789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13589411249329789 | validation: 0.717948783356313]
	TIME [epoch: 5.88 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.264840337586553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.264840337586553 | validation: 0.23470511230091531]
	TIME [epoch: 5.88 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08941237449543848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08941237449543848 | validation: 0.16295692906343862]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0463239979020847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0463239979020847 | validation: 0.20153094390342952]
	TIME [epoch: 5.85 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0498102363641809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0498102363641809 | validation: 0.18102225367873492]
	TIME [epoch: 5.85 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03750638799145136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03750638799145136 | validation: 0.19112963134337277]
	TIME [epoch: 5.88 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039188552562456576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039188552562456576 | validation: 0.1816051154474284]
	TIME [epoch: 5.88 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04775563077117059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04775563077117059 | validation: 0.2112958899582547]
	TIME [epoch: 5.87 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003304402215238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09003304402215238 | validation: 0.2948462387301798]
	TIME [epoch: 5.89 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14307757290078468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14307757290078468 | validation: 0.2509424431125599]
	TIME [epoch: 5.88 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14165683370695006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14165683370695006 | validation: 0.24261142741012665]
	TIME [epoch: 5.89 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051388061623639326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051388061623639326 | validation: 0.18432047127471696]
	TIME [epoch: 5.87 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048389377831162726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048389377831162726 | validation: 0.19474317002530775]
	TIME [epoch: 5.88 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05034922322035711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05034922322035711 | validation: 0.19635364734886293]
	TIME [epoch: 5.88 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03934897998178553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03934897998178553 | validation: 0.1767587014177316]
	TIME [epoch: 5.85 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03081380966602334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03081380966602334 | validation: 0.17917065468689805]
	TIME [epoch: 5.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031434489644578345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031434489644578345 | validation: 0.2360632429901736]
	TIME [epoch: 5.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05265045936514439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05265045936514439 | validation: 0.25828853272405583]
	TIME [epoch: 5.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10203949909632791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10203949909632791 | validation: 0.5705405038127322]
	TIME [epoch: 5.85 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19903479992170384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19903479992170384 | validation: 0.25375346402679777]
	TIME [epoch: 5.86 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11406444792082723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11406444792082723 | validation: 0.18165049844331324]
	TIME [epoch: 5.84 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034262434045855564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034262434045855564 | validation: 0.19023293789893847]
	TIME [epoch: 5.85 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04147899179755136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04147899179755136 | validation: 0.17682658450702904]
	TIME [epoch: 5.84 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04854739843136829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04854739843136829 | validation: 0.20716001786926896]
	TIME [epoch: 5.84 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04621455994513166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04621455994513166 | validation: 0.1820697839410278]
	TIME [epoch: 5.84 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532234290361168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05532234290361168 | validation: 0.174453740173504]
	TIME [epoch: 5.83 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057455937702761584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057455937702761584 | validation: 0.2139173776376897]
	TIME [epoch: 5.84 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827183788909214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0827183788909214 | validation: 0.17403177561811903]
	TIME [epoch: 5.83 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10636942483049464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10636942483049464 | validation: 0.21926493456643656]
	TIME [epoch: 5.84 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053108352978696094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053108352978696094 | validation: 0.1922459930026663]
	TIME [epoch: 5.84 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217634878082822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03217634878082822 | validation: 0.1780875507396092]
	TIME [epoch: 5.84 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129804904680144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03129804904680144 | validation: 0.1829134900886229]
	TIME [epoch: 5.83 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03284594571723981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03284594571723981 | validation: 0.16760028740597135]
	TIME [epoch: 5.84 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029614424450105866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029614424450105866 | validation: 0.17780613442494014]
	TIME [epoch: 5.84 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027560296605706212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027560296605706212 | validation: 0.16053547013415384]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_721.pth
	Model improved!!!
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031572928011447364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031572928011447364 | validation: 0.3179222103513262]
	TIME [epoch: 5.84 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06951252492681978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06951252492681978 | validation: 0.3349898734705724]
	TIME [epoch: 5.84 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20300241291376078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20300241291376078 | validation: 0.5685962221348267]
	TIME [epoch: 5.85 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25342728770250333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25342728770250333 | validation: 0.25048899266063457]
	TIME [epoch: 5.84 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2073127875919143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2073127875919143 | validation: 0.24330261661014496]
	TIME [epoch: 5.85 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.101617575750326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.101617575750326 | validation: 0.30959903579818726]
	TIME [epoch: 5.84 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06493173975541248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06493173975541248 | validation: 0.20757646589414797]
	TIME [epoch: 5.84 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282779325583118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04282779325583118 | validation: 0.19447594920937014]
	TIME [epoch: 5.84 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023766231070503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04023766231070503 | validation: 0.20566473096615454]
	TIME [epoch: 5.85 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03356280878206396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03356280878206396 | validation: 0.18263708387545027]
	TIME [epoch: 5.84 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024674852128110287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024674852128110287 | validation: 0.1711479405913984]
	TIME [epoch: 5.84 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022914061682257545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022914061682257545 | validation: 0.16936962730167246]
	TIME [epoch: 5.84 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021628260880014107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021628260880014107 | validation: 0.17488766622019825]
	TIME [epoch: 5.86 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022094333361460983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022094333361460983 | validation: 0.15350131516373455]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_735.pth
	Model improved!!!
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020160992590718678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020160992590718678 | validation: 0.14993628980285706]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019653196311954137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019653196311954137 | validation: 0.13819002055988094]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019396889344215004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019396889344215004 | validation: 0.15532460950432672]
	TIME [epoch: 5.85 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023912124725101055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023912124725101055 | validation: 0.12331745533149596]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_739.pth
	Model improved!!!
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03718297055925058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03718297055925058 | validation: 0.22125105880570148]
	TIME [epoch: 5.87 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10986102280736354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10986102280736354 | validation: 0.2554969581081234]
	TIME [epoch: 5.87 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2252691565570771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2252691565570771 | validation: 0.2746417515121231]
	TIME [epoch: 5.87 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1316856855652314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1316856855652314 | validation: 0.5059817329220158]
	TIME [epoch: 5.86 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12073276674819408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12073276674819408 | validation: 0.3006786895827537]
	TIME [epoch: 5.87 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270692000156281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1270692000156281 | validation: 0.20216248651201238]
	TIME [epoch: 5.87 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048226029807806714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048226029807806714 | validation: 0.21118583628790577]
	TIME [epoch: 5.87 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038881636927411596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038881636927411596 | validation: 0.17408477417660714]
	TIME [epoch: 5.86 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023415954635112354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023415954635112354 | validation: 0.16818324328863812]
	TIME [epoch: 5.87 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0227782993183564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0227782993183564 | validation: 0.17298101422999426]
	TIME [epoch: 5.86 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025039054583477265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025039054583477265 | validation: 0.16415683476972553]
	TIME [epoch: 5.86 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019899724244820092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019899724244820092 | validation: 0.1533104798014978]
	TIME [epoch: 5.86 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02072137818859069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02072137818859069 | validation: 0.1669483523403518]
	TIME [epoch: 5.86 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02725665356844309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02725665356844309 | validation: 0.16405387445557204]
	TIME [epoch: 5.86 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03523654317598889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03523654317598889 | validation: 0.2099937588178812]
	TIME [epoch: 5.86 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362410946552163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06362410946552163 | validation: 0.24217423238793417]
	TIME [epoch: 5.86 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11249087139327302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11249087139327302 | validation: 0.38728210097782356]
	TIME [epoch: 5.87 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13580834328079683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13580834328079683 | validation: 0.21723695686196895]
	TIME [epoch: 5.87 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1070415355907595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1070415355907595 | validation: 0.20562340810843174]
	TIME [epoch: 5.86 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04970811427755404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04970811427755404 | validation: 0.15227795152584173]
	TIME [epoch: 5.86 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241223594724974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05241223594724974 | validation: 0.15192739650657117]
	TIME [epoch: 5.86 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04360784643634757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04360784643634757 | validation: 0.1728277299012705]
	TIME [epoch: 5.86 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677940779480813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03677940779480813 | validation: 0.13003625182918646]
	TIME [epoch: 5.86 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02366519443808446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02366519443808446 | validation: 0.1426463004605842]
	TIME [epoch: 5.87 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021025066054163568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021025066054163568 | validation: 0.11888385485673858]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_764.pth
	Model improved!!!
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024271036047656663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024271036047656663 | validation: 0.18118766972571435]
	TIME [epoch: 5.85 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03470667407139762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03470667407139762 | validation: 0.17583673035169584]
	TIME [epoch: 5.86 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07851927953162603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07851927953162603 | validation: 0.27050142646728775]
	TIME [epoch: 5.87 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11322624067205077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11322624067205077 | validation: 0.28433615400144235]
	TIME [epoch: 5.87 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1939208695817117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1939208695817117 | validation: 0.1916498561325223]
	TIME [epoch: 5.86 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07203673450145598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07203673450145598 | validation: 0.27257779146111094]
	TIME [epoch: 5.88 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04610928413751683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04610928413751683 | validation: 0.18577847976553474]
	TIME [epoch: 5.86 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042170750074782645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042170750074782645 | validation: 0.1774268335693156]
	TIME [epoch: 5.87 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034649248294523455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034649248294523455 | validation: 0.15988781497980506]
	TIME [epoch: 5.86 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022678681178049366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022678681178049366 | validation: 0.1573877372832298]
	TIME [epoch: 5.87 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017459354160464548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017459354160464548 | validation: 0.1500784282641373]
	TIME [epoch: 5.86 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01994006486389174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01994006486389174 | validation: 0.15846430146608662]
	TIME [epoch: 5.87 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023290242229647058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023290242229647058 | validation: 0.14198453420311516]
	TIME [epoch: 5.86 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03518179743672506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03518179743672506 | validation: 0.17899385573758583]
	TIME [epoch: 5.87 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06323725092104658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06323725092104658 | validation: 0.16662328493118728]
	TIME [epoch: 5.86 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076497834191321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06076497834191321 | validation: 0.15383095233527466]
	TIME [epoch: 5.87 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03708563660875529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03708563660875529 | validation: 0.14080984543413325]
	TIME [epoch: 5.86 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03529767231063755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03529767231063755 | validation: 0.25072533600291746]
	TIME [epoch: 5.87 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046810261364606504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046810261364606504 | validation: 0.2879912017803713]
	TIME [epoch: 5.86 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522999096498011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522999096498011 | validation: 0.4651235450627669]
	TIME [epoch: 5.87 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16460127166493435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16460127166493435 | validation: 0.16055068653121507]
	TIME [epoch: 5.86 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672237738811948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04672237738811948 | validation: 0.1280547507139957]
	TIME [epoch: 5.87 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015459288757509395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015459288757509395 | validation: 0.16060034315337873]
	TIME [epoch: 5.86 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03204272696362334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03204272696362334 | validation: 0.14459665722500983]
	TIME [epoch: 5.86 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03253804973135575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03253804973135575 | validation: 0.1628436492814696]
	TIME [epoch: 5.83 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026668272328769414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026668272328769414 | validation: 0.120144119376394]
	TIME [epoch: 5.84 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022112318420763253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022112318420763253 | validation: 0.11178921877163889]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02992470205356134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02992470205356134 | validation: 0.15913873402571144]
	TIME [epoch: 5.88 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715326664074935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06715326664074935 | validation: 0.15652443185215426]
	TIME [epoch: 5.87 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11918769220390463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11918769220390463 | validation: 0.17289826796260166]
	TIME [epoch: 5.87 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703588273297389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0703588273297389 | validation: 0.1666869068496237]
	TIME [epoch: 5.87 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038107950484298894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038107950484298894 | validation: 0.20893524591646295]
	TIME [epoch: 5.87 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05155717532318995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05155717532318995 | validation: 0.18588494814716225]
	TIME [epoch: 5.87 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062326750770206905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062326750770206905 | validation: 0.20966001125321512]
	TIME [epoch: 5.86 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07324744082345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07324744082345 | validation: 0.21607213153384644]
	TIME [epoch: 5.87 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08830661211345091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08830661211345091 | validation: 0.29516961136166703]
	TIME [epoch: 5.88 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052654281012428805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052654281012428805 | validation: 0.16756348663079365]
	TIME [epoch: 5.85 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02921840548930109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02921840548930109 | validation: 0.19080877203100113]
	TIME [epoch: 5.85 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06619134310522241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06619134310522241 | validation: 0.20676925561515988]
	TIME [epoch: 5.85 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394717234052631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05394717234052631 | validation: 0.15649990301356734]
	TIME [epoch: 5.86 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055303405094624176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055303405094624176 | validation: 0.17618192058293822]
	TIME [epoch: 5.85 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0340909167233412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0340909167233412 | validation: 0.1580007613693166]
	TIME [epoch: 5.85 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453879311321702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0453879311321702 | validation: 0.2818508071201074]
	TIME [epoch: 5.85 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03728066489942416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03728066489942416 | validation: 0.14344696598375548]
	TIME [epoch: 5.86 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03369101766152218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03369101766152218 | validation: 0.16237766951337038]
	TIME [epoch: 5.85 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042496879152945206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042496879152945206 | validation: 0.1781995715525937]
	TIME [epoch: 5.85 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07899979010766382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07899979010766382 | validation: 0.23962395861032687]
	TIME [epoch: 5.85 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09061112961383279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09061112961383279 | validation: 0.16908481661833244]
	TIME [epoch: 5.85 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626143710706127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626143710706127 | validation: 0.17163073932877007]
	TIME [epoch: 5.85 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601866089443757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05601866089443757 | validation: 0.1436846240803805]
	TIME [epoch: 5.85 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05790470105192392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05790470105192392 | validation: 0.12275034705837827]
	TIME [epoch: 5.85 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03652457610333368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03652457610333368 | validation: 0.13684887167731255]
	TIME [epoch: 5.86 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02631678249295996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02631678249295996 | validation: 0.11739979722748482]
	TIME [epoch: 5.85 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024288101414653537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024288101414653537 | validation: 0.1121538711407369]
	TIME [epoch: 5.87 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02244079632878412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02244079632878412 | validation: 0.10457889538468944]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_819.pth
	Model improved!!!
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023967883071603482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023967883071603482 | validation: 0.13216211827503144]
	TIME [epoch: 5.87 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03787460786480949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03787460786480949 | validation: 0.10734387340508307]
	TIME [epoch: 5.94 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06487265020242373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06487265020242373 | validation: 0.17610865412162263]
	TIME [epoch: 5.87 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08325089036814458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08325089036814458 | validation: 0.15596252529555477]
	TIME [epoch: 5.85 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765205046704618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04765205046704618 | validation: 0.16275768543784147]
	TIME [epoch: 5.85 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0421826040034417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0421826040034417 | validation: 0.29765239567822255]
	TIME [epoch: 5.86 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08335067782545785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08335067782545785 | validation: 0.2556899204814668]
	TIME [epoch: 5.85 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11892618711674845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11892618711674845 | validation: 0.322624330883217]
	TIME [epoch: 5.85 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07315442047319504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07315442047319504 | validation: 0.1729225272949786]
	TIME [epoch: 5.88 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03661892494972783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03661892494972783 | validation: 0.1294723302405026]
	TIME [epoch: 5.85 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026933627186372322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026933627186372322 | validation: 0.11940918370785902]
	TIME [epoch: 5.85 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025217754194012457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025217754194012457 | validation: 0.13944755475359402]
	TIME [epoch: 5.85 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047963551826464654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047963551826464654 | validation: 0.16845066274582723]
	TIME [epoch: 5.85 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692617118790791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03692617118790791 | validation: 0.12408176785040502]
	TIME [epoch: 5.85 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03330745449867115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03330745449867115 | validation: 0.1448891432228053]
	TIME [epoch: 5.85 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04761984565127518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04761984565127518 | validation: 0.11648649006916055]
	TIME [epoch: 5.85 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491478441974654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03491478441974654 | validation: 0.12832503698731612]
	TIME [epoch: 5.85 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03633715516030172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03633715516030172 | validation: 0.12356242850054716]
	TIME [epoch: 5.85 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811791371088928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02811791371088928 | validation: 0.12262183020924887]
	TIME [epoch: 5.86 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041424666949577985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041424666949577985 | validation: 0.19147763142281066]
	TIME [epoch: 5.85 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346315612845062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06346315612845062 | validation: 0.16330221948398163]
	TIME [epoch: 5.85 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07013625196984755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07013625196984755 | validation: 0.19546560753157993]
	TIME [epoch: 5.84 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0577225038854169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0577225038854169 | validation: 0.2041450754124889]
	TIME [epoch: 5.85 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09847299618731109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09847299618731109 | validation: 0.29098415068643596]
	TIME [epoch: 5.84 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09732050707624751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09732050707624751 | validation: 0.1478984856643359]
	TIME [epoch: 5.85 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06478499532767044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06478499532767044 | validation: 0.13244668147380315]
	TIME [epoch: 5.85 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016791816230716895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016791816230716895 | validation: 0.10638552345658657]
	TIME [epoch: 5.85 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01705249119535421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01705249119535421 | validation: 0.12320987826652748]
	TIME [epoch: 5.85 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022718814002218528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022718814002218528 | validation: 0.14700814899970746]
	TIME [epoch: 5.85 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029847399737981305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029847399737981305 | validation: 0.12221646853024648]
	TIME [epoch: 5.84 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346131338524664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0346131338524664 | validation: 0.12051995193117665]
	TIME [epoch: 5.85 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407211133361224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03407211133361224 | validation: 0.17724232889846522]
	TIME [epoch: 5.85 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07589137357145664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07589137357145664 | validation: 0.2504581801107552]
	TIME [epoch: 5.85 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06476062999344288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06476062999344288 | validation: 0.13647972556451102]
	TIME [epoch: 5.85 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038508118397487266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038508118397487266 | validation: 0.1296735915336574]
	TIME [epoch: 5.85 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023775034736146287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023775034736146287 | validation: 0.13727570479047682]
	TIME [epoch: 5.85 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030985603417758564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030985603417758564 | validation: 0.19423109738879735]
	TIME [epoch: 5.85 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07143674949969842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07143674949969842 | validation: 0.1728844610479634]
	TIME [epoch: 5.85 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0857198451465699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0857198451465699 | validation: 0.1608655018160505]
	TIME [epoch: 5.85 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05342728287012594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05342728287012594 | validation: 0.1098572963584108]
	TIME [epoch: 5.85 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02129616049320953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02129616049320953 | validation: 0.10182064721176354]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01601824680014976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01601824680014976 | validation: 0.12043533539087269]
	TIME [epoch: 5.88 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024015843133375906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024015843133375906 | validation: 0.18451094305386084]
	TIME [epoch: 5.87 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744324242936827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05744324242936827 | validation: 0.21430983473271936]
	TIME [epoch: 5.87 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10112547787741463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10112547787741463 | validation: 0.2452569944820478]
	TIME [epoch: 5.87 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771187927759375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08771187927759375 | validation: 0.1958187684435143]
	TIME [epoch: 5.87 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658424635626736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05658424635626736 | validation: 0.1612728790869888]
	TIME [epoch: 5.87 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032663351958395835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032663351958395835 | validation: 0.12805168592224772]
	TIME [epoch: 5.87 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579739160671664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03579739160671664 | validation: 0.12446827749976781]
	TIME [epoch: 5.88 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032568358919202185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032568358919202185 | validation: 0.10727128895161209]
	TIME [epoch: 5.87 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027913827965337694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027913827965337694 | validation: 0.10768463024291774]
	TIME [epoch: 5.87 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020407028017323966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020407028017323966 | validation: 0.11945706066698535]
	TIME [epoch: 5.87 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019527312347992133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019527312347992133 | validation: 0.11003589739637179]
	TIME [epoch: 5.87 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0239935988804736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0239935988804736 | validation: 0.10812451642032239]
	TIME [epoch: 5.87 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02734006846128255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02734006846128255 | validation: 0.11063968191678275]
	TIME [epoch: 5.87 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0275567218647263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0275567218647263 | validation: 0.1048306758443881]
	TIME [epoch: 5.87 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029743187020890006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029743187020890006 | validation: 0.12378334118917618]
	TIME [epoch: 5.87 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907483726283038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06907483726283038 | validation: 0.14973541450607822]
	TIME [epoch: 5.87 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155218375733081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07155218375733081 | validation: 0.16112065242050996]
	TIME [epoch: 5.88 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06391512578040998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06391512578040998 | validation: 0.11090110476096778]
	TIME [epoch: 5.87 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028510983920437286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028510983920437286 | validation: 0.11560867532143093]
	TIME [epoch: 5.87 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03133127309043568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03133127309043568 | validation: 0.26403079828183335]
	TIME [epoch: 5.87 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08227098277954738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08227098277954738 | validation: 0.18879079659038855]
	TIME [epoch: 5.87 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08561501988382887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08561501988382887 | validation: 0.2106101995473731]
	TIME [epoch: 5.88 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053590842870104995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053590842870104995 | validation: 0.12273219066789448]
	TIME [epoch: 5.88 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036270493443790956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036270493443790956 | validation: 0.10305299547778313]
	TIME [epoch: 5.87 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024786674764727682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024786674764727682 | validation: 0.10081014127259091]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_886.pth
	Model improved!!!
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014734690757869236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014734690757869236 | validation: 0.11029137825764455]
	TIME [epoch: 5.88 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01687164433785666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01687164433785666 | validation: 0.11841923601652292]
	TIME [epoch: 5.87 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03681072433160632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03681072433160632 | validation: 0.1642655983088732]
	TIME [epoch: 5.87 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061712780980296825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061712780980296825 | validation: 0.16891573987698213]
	TIME [epoch: 5.87 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743094206389466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0743094206389466 | validation: 0.1744511270770158]
	TIME [epoch: 5.87 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24748011790532082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24748011790532082 | validation: 0.13980659539412393]
	TIME [epoch: 5.87 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0350754817773884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0350754817773884 | validation: 0.1469278113826006]
	TIME [epoch: 5.87 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406337220721217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03406337220721217 | validation: 0.19029540760240826]
	TIME [epoch: 5.87 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0298162974831172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0298162974831172 | validation: 0.1362633397859297]
	TIME [epoch: 5.86 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311236327404798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0311236327404798 | validation: 0.1408380448336972]
	TIME [epoch: 5.87 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03619707334501402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03619707334501402 | validation: 0.1739017936439475]
	TIME [epoch: 5.87 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05149350807259685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05149350807259685 | validation: 0.21082524458616858]
	TIME [epoch: 5.87 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06909026322310587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06909026322310587 | validation: 0.14254466235941105]
	TIME [epoch: 5.87 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744311369857005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05744311369857005 | validation: 0.13274009613049073]
	TIME [epoch: 5.87 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036388818783516436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036388818783516436 | validation: 0.12176195997311177]
	TIME [epoch: 5.86 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021475916227354373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021475916227354373 | validation: 0.1768106737324744]
	TIME [epoch: 5.87 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027053126235474973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027053126235474973 | validation: 0.11085925407222069]
	TIME [epoch: 5.87 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02887111501426249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02887111501426249 | validation: 0.11501856737002969]
	TIME [epoch: 5.87 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032425710059353595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032425710059353595 | validation: 0.12182192190608641]
	TIME [epoch: 5.87 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03140770840014359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03140770840014359 | validation: 0.13714744568520085]
	TIME [epoch: 5.88 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03106399005624222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03106399005624222 | validation: 0.09817724491352291]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_907.pth
	Model improved!!!
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02493621241885192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02493621241885192 | validation: 0.161942312140397]
	TIME [epoch: 5.88 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046217008638740974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046217008638740974 | validation: 0.13473781994680145]
	TIME [epoch: 5.88 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050351891536853516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050351891536853516 | validation: 0.17557055752949718]
	TIME [epoch: 5.87 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07489426065282814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07489426065282814 | validation: 0.15284403680150338]
	TIME [epoch: 5.87 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06706764716540813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06706764716540813 | validation: 0.11090896274250257]
	TIME [epoch: 5.87 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03856826926152724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03856826926152724 | validation: 0.10727469404771953]
	TIME [epoch: 5.87 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01787898697651186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01787898697651186 | validation: 0.09586470687384596]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012384145279801672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012384145279801672 | validation: 0.08267735130612802]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_915.pth
	Model improved!!!
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015630608302741593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015630608302741593 | validation: 0.09307941775435835]
	TIME [epoch: 5.86 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029236840152685295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029236840152685295 | validation: 0.12946167218926669]
	TIME [epoch: 5.87 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08305488868368709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08305488868368709 | validation: 0.20352618511374523]
	TIME [epoch: 5.87 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1288859998306325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1288859998306325 | validation: 0.12941000468848965]
	TIME [epoch: 5.87 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04458041791448345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04458041791448345 | validation: 0.09593405712643789]
	TIME [epoch: 5.87 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02255322340098377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02255322340098377 | validation: 0.12166763449981494]
	TIME [epoch: 5.87 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030679222479220117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030679222479220117 | validation: 0.12404722681510263]
	TIME [epoch: 5.87 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886657468373312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886657468373312 | validation: 0.1642190520525616]
	TIME [epoch: 5.87 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03987900847628091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03987900847628091 | validation: 0.13649313460430926]
	TIME [epoch: 5.87 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04428950622577793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04428950622577793 | validation: 0.13140258868779972]
	TIME [epoch: 5.87 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05288168485044048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05288168485044048 | validation: 0.13203094012085392]
	TIME [epoch: 5.87 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04127443864140304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04127443864140304 | validation: 0.14381642163703554]
	TIME [epoch: 5.87 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03509109156368539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03509109156368539 | validation: 0.10258084330453912]
	TIME [epoch: 5.87 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023173354201959945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023173354201959945 | validation: 0.12703748319019362]
	TIME [epoch: 5.88 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02545980004697638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02545980004697638 | validation: 0.09079647355561321]
	TIME [epoch: 5.86 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022678497677028456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022678497677028456 | validation: 0.10008181214684397]
	TIME [epoch: 5.87 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01971647145184499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01971647145184499 | validation: 0.09652155812687793]
	TIME [epoch: 5.87 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018001670656044248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018001670656044248 | validation: 0.09973561931364153]
	TIME [epoch: 5.87 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02128856021572652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02128856021572652 | validation: 0.08508657271607034]
	TIME [epoch: 5.87 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022588010829903978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022588010829903978 | validation: 0.17636264579841698]
	TIME [epoch: 5.88 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04828685843088385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04828685843088385 | validation: 0.19968400643311046]
	TIME [epoch: 5.87 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09878020295027112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09878020295027112 | validation: 0.24435181418993634]
	TIME [epoch: 5.87 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12031427189732244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12031427189732244 | validation: 0.13866937966887544]
	TIME [epoch: 5.87 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059814144842561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059814144842561 | validation: 0.08153518461312266]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009443411822704615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009443411822704615 | validation: 0.0780421040889892]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_940.pth
	Model improved!!!
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01771008776911066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01771008776911066 | validation: 0.11167012313621852]
	TIME [epoch: 5.88 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025729591287509124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025729591287509124 | validation: 0.09671040203550624]
	TIME [epoch: 5.87 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02306429584589306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02306429584589306 | validation: 0.09814345261536844]
	TIME [epoch: 5.87 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020820941221922455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020820941221922455 | validation: 0.1014192054103133]
	TIME [epoch: 5.87 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022396656662609305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022396656662609305 | validation: 0.10050631785921427]
	TIME [epoch: 5.87 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04086290690554468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04086290690554468 | validation: 0.20637255388219333]
	TIME [epoch: 5.87 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08839163447728172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08839163447728172 | validation: 0.13476626656422308]
	TIME [epoch: 5.88 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439728605468245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06439728605468245 | validation: 0.0789809579454311]
	TIME [epoch: 5.87 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852073968808848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03852073968808848 | validation: 0.1260928195983891]
	TIME [epoch: 5.87 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030190257215673137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030190257215673137 | validation: 0.08672707582326905]
	TIME [epoch: 5.87 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026327105010669795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026327105010669795 | validation: 0.08803394770384079]
	TIME [epoch: 5.87 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019206771785772514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019206771785772514 | validation: 0.07644203013588657]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01650346228925924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01650346228925924 | validation: 0.08241139447124009]
	TIME [epoch: 5.87 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022966999017302977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022966999017302977 | validation: 0.14540146107539073]
	TIME [epoch: 5.87 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047107963102172475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047107963102172475 | validation: 0.3104128348161475]
	TIME [epoch: 5.86 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10457802071836905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10457802071836905 | validation: 0.18927030849228813]
	TIME [epoch: 5.87 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10924209179567512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10924209179567512 | validation: 0.10826095335838715]
	TIME [epoch: 5.86 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882264004819815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03882264004819815 | validation: 0.0888534423694527]
	TIME [epoch: 5.87 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012301787915307165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012301787915307165 | validation: 0.09826424665530498]
	TIME [epoch: 5.87 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019242876702398028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019242876702398028 | validation: 0.1060150599655798]
	TIME [epoch: 5.87 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02762577197424969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02762577197424969 | validation: 0.1020159149827637]
	TIME [epoch: 5.87 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027354621428441316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027354621428441316 | validation: 0.0953579105453947]
	TIME [epoch: 5.87 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02644715112890152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02644715112890152 | validation: 0.09586374061167252]
	TIME [epoch: 5.87 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027300604958135383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027300604958135383 | validation: 0.12279725984989193]
	TIME [epoch: 5.87 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030713964846181958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030713964846181958 | validation: 0.08058137663937646]
	TIME [epoch: 5.87 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0297251859397513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0297251859397513 | validation: 0.08986828298133162]
	TIME [epoch: 5.87 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028532007998621146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028532007998621146 | validation: 0.15603111154802787]
	TIME [epoch: 5.87 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048315152724626886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048315152724626886 | validation: 0.18558705567011968]
	TIME [epoch: 5.87 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12175809024829388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12175809024829388 | validation: 0.1506396899441779]
	TIME [epoch: 5.88 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409023212659617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07409023212659617 | validation: 0.11348142021269492]
	TIME [epoch: 5.87 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035753948521108594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035753948521108594 | validation: 0.08366293366992272]
	TIME [epoch: 5.87 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023086054927216822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023086054927216822 | validation: 0.09732274207714481]
	TIME [epoch: 5.87 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022628254391283235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022628254391283235 | validation: 0.08767342864982636]
	TIME [epoch: 5.87 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01523680232233515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01523680232233515 | validation: 0.07889695249017575]
	TIME [epoch: 5.87 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015494709335631763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015494709335631763 | validation: 0.07290204732267845]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015179884453201148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015179884453201148 | validation: 0.18748888429730648]
	TIME [epoch: 5.88 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048798800305064026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048798800305064026 | validation: 0.22584207765633002]
	TIME [epoch: 5.87 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06007217289453636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06007217289453636 | validation: 0.1175405467221383]
	TIME [epoch: 5.87 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04645956165934004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04645956165934004 | validation: 0.1075696164288059]
	TIME [epoch: 5.87 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663533009366103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03663533009366103 | validation: 0.16231028586198173]
	TIME [epoch: 5.87 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837840653494834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04837840653494834 | validation: 0.16428740911803014]
	TIME [epoch: 5.87 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058160918177523245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058160918177523245 | validation: 0.14035407541596473]
	TIME [epoch: 5.86 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05402584605069155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05402584605069155 | validation: 0.14129727586451885]
	TIME [epoch: 5.87 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033355085513297605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033355085513297605 | validation: 0.11626010340207188]
	TIME [epoch: 5.87 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02518424369824527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02518424369824527 | validation: 0.0899734664470209]
	TIME [epoch: 5.87 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0176644046919802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0176644046919802 | validation: 0.08810148862674057]
	TIME [epoch: 5.87 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020016391953932008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020016391953932008 | validation: 0.09587419928398634]
	TIME [epoch: 5.87 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029047018867153444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029047018867153444 | validation: 0.10486081724690544]
	TIME [epoch: 5.87 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820058397515653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03820058397515653 | validation: 0.11332111931101725]
	TIME [epoch: 5.87 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04095552848357329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04095552848357329 | validation: 0.12008136858053593]
	TIME [epoch: 5.87 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034389293368153426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034389293368153426 | validation: 0.09110648334769798]
	TIME [epoch: 5.87 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01910545329401623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01910545329401623 | validation: 0.07215946869092728]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018180013950382296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018180013950382296 | validation: 0.12536967321218656]
	TIME [epoch: 5.87 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050458896930866255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050458896930866255 | validation: 0.2608338635038972]
	TIME [epoch: 5.87 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11115086866104008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11115086866104008 | validation: 0.1888566946151701]
	TIME [epoch: 5.87 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11465787799349872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11465787799349872 | validation: 0.1241607105182043]
	TIME [epoch: 5.87 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02046491835442973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02046491835442973 | validation: 0.1028207010983036]
	TIME [epoch: 5.87 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01989465193007094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01989465193007094 | validation: 0.12906407739780953]
	TIME [epoch: 5.87 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882522947257824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03882522947257824 | validation: 0.12162135161678762]
	TIME [epoch: 5.86 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021012857206601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021012857206601 | validation: 0.09446931298762365]
	TIME [epoch: 5.86 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01129345212848619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01129345212848619 | validation: 0.09325233808975521]
	TIME [epoch: 189 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010782821435711446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010782821435711446 | validation: 0.08151873158276504]
	TIME [epoch: 12.4 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014464072706197584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014464072706197584 | validation: 0.09031251738992294]
	TIME [epoch: 12.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023407341124088075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023407341124088075 | validation: 0.11924777880010624]
	TIME [epoch: 12.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05913268660951884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05913268660951884 | validation: 0.1427451077799877]
	TIME [epoch: 12.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08700097451647075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08700097451647075 | validation: 0.3119631996768293]
	TIME [epoch: 12.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07180334730184013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07180334730184013 | validation: 0.11414982903912603]
	TIME [epoch: 12.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041810190217227365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041810190217227365 | validation: 0.10895736742797549]
	TIME [epoch: 12.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049620486224897904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049620486224897904 | validation: 0.08902805377997601]
	TIME [epoch: 12.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034380973059206864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034380973059206864 | validation: 0.08242368964700642]
	TIME [epoch: 12.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01779859654164807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01779859654164807 | validation: 0.07955730536321684]
	TIME [epoch: 12.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01223915728090302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01223915728090302 | validation: 0.06053806634078168]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_1012.pth
	Model improved!!!
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008326658657129192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008326658657129192 | validation: 0.06578550385780649]
	TIME [epoch: 12.4 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008930650756518364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008930650756518364 | validation: 0.06390739450603738]
	TIME [epoch: 12.4 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0095401648152724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0095401648152724 | validation: 0.07642637999652968]
	TIME [epoch: 12.4 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018616202420056377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018616202420056377 | validation: 0.11699485710028544]
	TIME [epoch: 12.4 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04136355549868223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04136355549868223 | validation: 0.11050947171279457]
	TIME [epoch: 12.4 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04529636859354856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04529636859354856 | validation: 0.1410009915187758]
	TIME [epoch: 12.4 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0897112175286041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0897112175286041 | validation: 0.20271612242483208]
	TIME [epoch: 12.4 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09119055682656643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09119055682656643 | validation: 0.17664947164413713]
	TIME [epoch: 12.4 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152704693979045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06152704693979045 | validation: 0.11250490311964384]
	TIME [epoch: 12.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04576277105211776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04576277105211776 | validation: 0.09021068746414496]
	TIME [epoch: 12.4 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780655882312732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03780655882312732 | validation: 0.14665176109785194]
	TIME [epoch: 12.4 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03148918918429013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03148918918429013 | validation: 0.07501866947520465]
	TIME [epoch: 12.4 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011742728181803299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011742728181803299 | validation: 0.07644819764877506]
	TIME [epoch: 12.4 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009568142454133139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009568142454133139 | validation: 0.07432306315276238]
	TIME [epoch: 12.4 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010083617486170595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010083617486170595 | validation: 0.08705137254190914]
	TIME [epoch: 12.4 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011762557296972469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011762557296972469 | validation: 0.07536714332816236]
	TIME [epoch: 12.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015579670997670127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015579670997670127 | validation: 0.09176026902156399]
	TIME [epoch: 12.4 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025734490000129295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025734490000129295 | validation: 0.1621305853754588]
	TIME [epoch: 12.4 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31772807892148053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31772807892148053 | validation: 0.2637902470205093]
	TIME [epoch: 12.4 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09951365910092116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09951365910092116 | validation: 0.17120235254672572]
	TIME [epoch: 12.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213607364884619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05213607364884619 | validation: 0.15663931655310256]
	TIME [epoch: 12.4 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07200583717081852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07200583717081852 | validation: 0.1759922736720903]
	TIME [epoch: 12.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058627036139435984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058627036139435984 | validation: 0.11891634097438018]
	TIME [epoch: 12.4 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04117509805893429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04117509805893429 | validation: 0.09638807644418523]
	TIME [epoch: 12.4 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01774492271483017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01774492271483017 | validation: 0.09078013008170628]
	TIME [epoch: 12.4 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017108067370089002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017108067370089002 | validation: 0.10653408111259055]
	TIME [epoch: 12.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013231409319903733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013231409319903733 | validation: 0.0826970811334421]
	TIME [epoch: 12.4 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012820340544183724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012820340544183724 | validation: 0.086024601086793]
	TIME [epoch: 12.4 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015324739072597085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015324739072597085 | validation: 0.12119631281947248]
	TIME [epoch: 12.4 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02726960004321237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02726960004321237 | validation: 0.11521968546608377]
	TIME [epoch: 12.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040808084967921474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040808084967921474 | validation: 0.1637878587834347]
	TIME [epoch: 12.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119276906063594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07119276906063594 | validation: 0.20185870148634388]
	TIME [epoch: 12.4 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08368752490503852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08368752490503852 | validation: 0.09517807766490105]
	TIME [epoch: 12.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03065417469471241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03065417469471241 | validation: 0.08774765207882267]
	TIME [epoch: 12.4 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012814705571746486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012814705571746486 | validation: 0.07921624058378382]
	TIME [epoch: 12.4 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011654744359236573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011654744359236573 | validation: 0.08138016656003369]
	TIME [epoch: 12.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015497207383325664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015497207383325664 | validation: 0.08382212798641178]
	TIME [epoch: 12.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018928936413658618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018928936413658618 | validation: 0.0848846021956687]
	TIME [epoch: 12.4 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029735182000968075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029735182000968075 | validation: 0.09965654202707441]
	TIME [epoch: 12.4 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474163773221605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04474163773221605 | validation: 0.09124461395670105]
	TIME [epoch: 12.4 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042932883405798344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042932883405798344 | validation: 0.07330389111277794]
	TIME [epoch: 12.4 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027984811681483448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027984811681483448 | validation: 0.0833624231203577]
	TIME [epoch: 12.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01718464460760678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01718464460760678 | validation: 0.13446913898881552]
	TIME [epoch: 12.4 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03522272160710804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03522272160710804 | validation: 0.12636300028712025]
	TIME [epoch: 12.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07033687688282757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07033687688282757 | validation: 0.19816643931106356]
	TIME [epoch: 12.4 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07793773135107915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07793773135107915 | validation: 0.15918897083835906]
	TIME [epoch: 12.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05281933540338386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05281933540338386 | validation: 0.08178188676256595]
	TIME [epoch: 12.4 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015064660923191728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015064660923191728 | validation: 0.07019529905685598]
	TIME [epoch: 12.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006750555658282065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.006750555658282065 | validation: 0.07187134187749622]
	TIME [epoch: 12.4 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00738289052942783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00738289052942783 | validation: 0.06374440924161337]
	TIME [epoch: 12.4 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009611122027489686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009611122027489686 | validation: 0.07775373931932156]
	TIME [epoch: 12.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012393325739446579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012393325739446579 | validation: 0.07807498361548389]
	TIME [epoch: 12.4 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020846329547505125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020846329547505125 | validation: 0.09234904533930989]
	TIME [epoch: 12.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887614273166904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04887614273166904 | validation: 0.13201472219899416]
	TIME [epoch: 12.4 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08032971464444325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08032971464444325 | validation: 0.10292656629061377]
	TIME [epoch: 12.4 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03712102881699224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03712102881699224 | validation: 0.061600518267608234]
	TIME [epoch: 12.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01735504626556301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01735504626556301 | validation: 0.11712383360177228]
	TIME [epoch: 12.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026940477369594137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026940477369594137 | validation: 0.14679114675841332]
	TIME [epoch: 12.4 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498085440514394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06498085440514394 | validation: 0.13779217405696292]
	TIME [epoch: 12.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07123482504013465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07123482504013465 | validation: 0.10960185540676524]
	TIME [epoch: 12.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027406484119895436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027406484119895436 | validation: 0.07748761990471315]
	TIME [epoch: 12.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012694605231345727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012694605231345727 | validation: 0.0804914104859785]
	TIME [epoch: 12.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014339388940814555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014339388940814555 | validation: 0.07733270736501285]
	TIME [epoch: 12.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016388904414863934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016388904414863934 | validation: 0.14112164248249767]
	TIME [epoch: 12.4 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029708981693556968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029708981693556968 | validation: 0.07747148166990017]
	TIME [epoch: 12.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018654673335541074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018654673335541074 | validation: 0.07925187387230935]
	TIME [epoch: 12.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02464953778340109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02464953778340109 | validation: 0.08041816726695555]
	TIME [epoch: 12.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020976786212422206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020976786212422206 | validation: 0.12622814923732187]
	TIME [epoch: 12.4 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857218132236552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03857218132236552 | validation: 0.19823925917093957]
	TIME [epoch: 12.4 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07586091087738998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07586091087738998 | validation: 0.16976772351185213]
	TIME [epoch: 12.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13654529596036136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13654529596036136 | validation: 0.09459112850464241]
	TIME [epoch: 12.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035248208420710005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035248208420710005 | validation: 0.0749395302244341]
	TIME [epoch: 12.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01754002055180622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01754002055180622 | validation: 0.13636846037924233]
	TIME [epoch: 12.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036009437701316885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036009437701316885 | validation: 0.08553844858516185]
	TIME [epoch: 12.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024189766468055485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024189766468055485 | validation: 0.0729373804173467]
	TIME [epoch: 12.4 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01449761632545008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01449761632545008 | validation: 0.0740383154717049]
	TIME [epoch: 12.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007980696483166159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007980696483166159 | validation: 0.0826938975837303]
	TIME [epoch: 12.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009035894406748885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009035894406748885 | validation: 0.06836695509196052]
	TIME [epoch: 12.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007890334700802575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.007890334700802575 | validation: 0.06212477748654545]
	TIME [epoch: 12.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012701370056423869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012701370056423869 | validation: 0.082099244213656]
	TIME [epoch: 12.4 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03895879973459568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03895879973459568 | validation: 0.0790349098929992]
	TIME [epoch: 12.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030627543973630395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030627543973630395 | validation: 0.0806420211648288]
	TIME [epoch: 12.4 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02456367018056555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02456367018056555 | validation: 0.07509877551485765]
	TIME [epoch: 12.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015184338868633858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015184338868633858 | validation: 0.07232941916585442]
	TIME [epoch: 12.4 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013465038192687495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013465038192687495 | validation: 0.11333603738014991]
	TIME [epoch: 12.4 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053908569382005694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053908569382005694 | validation: 0.21613384851799775]
	TIME [epoch: 12.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13646645578003125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13646645578003125 | validation: 0.11376403340790305]
	TIME [epoch: 12.4 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04738774370038277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04738774370038277 | validation: 0.07216621266717298]
	TIME [epoch: 12.4 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019861998679071787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019861998679071787 | validation: 0.07537594862918277]
	TIME [epoch: 12.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032165667564256706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032165667564256706 | validation: 0.10819211112346845]
	TIME [epoch: 12.4 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043460198813707204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043460198813707204 | validation: 0.15906527160553663]
	TIME [epoch: 12.4 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484312002927832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06484312002927832 | validation: 0.13024335609522852]
	TIME [epoch: 12.4 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05593815621264046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05593815621264046 | validation: 0.11308273092237653]
	TIME [epoch: 12.4 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029109362130531422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029109362130531422 | validation: 0.09067390111037048]
	TIME [epoch: 12.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019478760577449652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019478760577449652 | validation: 0.07171295467070131]
	TIME [epoch: 12.4 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01612008680990674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01612008680990674 | validation: 0.08391588923698894]
	TIME [epoch: 12.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01285970988903711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01285970988903711 | validation: 0.08317191265855836]
	TIME [epoch: 12.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01504142320217359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01504142320217359 | validation: 0.06797794802625921]
	TIME [epoch: 12.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0163543511735826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0163543511735826 | validation: 0.07277429472831469]
	TIME [epoch: 12.4 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02247220870236207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02247220870236207 | validation: 0.12488910730062064]
	TIME [epoch: 12.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03917998639655163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03917998639655163 | validation: 0.23219072786818123]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241113_145920/states/model_phi1_4b_v_mmd2_1113.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6244.549 seconds.
