Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3152685923

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.071949505901493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.071949505901493 | validation: 6.721533753333958]
	TIME [epoch: 184 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.856453828528893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.856453828528893 | validation: 6.931530274442718]
	TIME [epoch: 0.842 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2179000614262465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2179000614262465 | validation: 6.195022137236476]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.279143082303326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.279143082303326 | validation: 7.0616289290342085]
	TIME [epoch: 0.732 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.76996590036091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.76996590036091 | validation: 6.646155745252024]
	TIME [epoch: 0.732 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.480527276173037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.480527276173037 | validation: 6.028491166944531]
	TIME [epoch: 0.734 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.600735537236322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.600735537236322 | validation: 6.57310558680843]
	TIME [epoch: 0.733 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5487739048242295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5487739048242295 | validation: 6.421177140146293]
	TIME [epoch: 0.731 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.357793150987799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.357793150987799 | validation: 5.924930186059821]
	TIME [epoch: 0.731 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.348914760686993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.348914760686993 | validation: 6.475694520804675]
	TIME [epoch: 0.733 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.287854983065109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.287854983065109 | validation: 6.189593794315549]
	TIME [epoch: 0.733 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.124923790237071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.124923790237071 | validation: 5.951522898053878]
	TIME [epoch: 0.732 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.152721386758887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.152721386758887 | validation: 6.234596774099264]
	TIME [epoch: 0.734 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.114763979243888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.114763979243888 | validation: 6.04769802304983]
	TIME [epoch: 0.732 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.033991072916388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.033991072916388 | validation: 6.004380190903994]
	TIME [epoch: 0.733 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.011110094950535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.011110094950535 | validation: 6.113971310233632]
	TIME [epoch: 0.734 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.001010210478844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.001010210478844 | validation: 5.8975739181536655]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.996497635562527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.996497635562527 | validation: 6.153937533022091]
	TIME [epoch: 0.734 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025865940822267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025865940822267 | validation: 5.846333558204541]
	TIME [epoch: 0.732 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.990373468003903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.990373468003903 | validation: 6.091931978186783]
	TIME [epoch: 0.73 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9609670560869388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9609670560869388 | validation: 5.876714273877922]
	TIME [epoch: 0.73 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.901672798655078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.901672798655078 | validation: 5.988112548943473]
	TIME [epoch: 0.73 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8692181416620928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8692181416620928 | validation: 5.863818243898226]
	TIME [epoch: 0.729 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.853966446865386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.853966446865386 | validation: 5.994281973139934]
	TIME [epoch: 0.729 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.863705142986436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.863705142986436 | validation: 5.782537125351779]
	TIME [epoch: 0.731 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9508824066940917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9508824066940917 | validation: 6.047079872832456]
	TIME [epoch: 0.735 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9527791272155044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9527791272155044 | validation: 5.799737069514563]
	TIME [epoch: 0.733 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8174301849934684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8174301849934684 | validation: 5.866586705870139]
	TIME [epoch: 0.733 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.776230672506512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.776230672506512 | validation: 5.862422857362157]
	TIME [epoch: 0.728 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7653549702860905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7653549702860905 | validation: 5.791256752392247]
	TIME [epoch: 0.729 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.754099499759484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.754099499759484 | validation: 5.854673608789786]
	TIME [epoch: 0.731 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.749056960527936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.749056960527936 | validation: 5.748566223214418]
	TIME [epoch: 0.731 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.742343100732585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.742343100732585 | validation: 5.858411144707325]
	TIME [epoch: 0.736 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.760643153433286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.760643153433286 | validation: 5.7072056488664415]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.732287858970681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.732287858970681 | validation: 5.863830540373776]
	TIME [epoch: 0.731 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.773029584648378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.773029584648378 | validation: 5.760013534672206]
	TIME [epoch: 0.737 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9598644371235094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9598644371235094 | validation: 5.76695456328094]
	TIME [epoch: 0.735 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.665375374989813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.665375374989813 | validation: 5.8308562390211565]
	TIME [epoch: 0.734 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.782007054119493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.782007054119493 | validation: 5.7102055780248735]
	TIME [epoch: 0.737 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.819471065803841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.819471065803841 | validation: 5.7514574190143515]
	TIME [epoch: 0.736 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7110825774299983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7110825774299983 | validation: 5.744060297528893]
	TIME [epoch: 0.734 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6527117746691653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6527117746691653 | validation: 5.660962156488018]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6334166387695985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6334166387695985 | validation: 5.705849680874896]
	TIME [epoch: 0.73 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6242153732424374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6242153732424374 | validation: 5.647666397366924]
	TIME [epoch: 0.731 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.592186837789934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.592186837789934 | validation: 5.6548950411618035]
	TIME [epoch: 0.733 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5865382779674166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5865382779674166 | validation: 5.634845582972019]
	TIME [epoch: 0.734 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.572023697271835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.572023697271835 | validation: 5.621900572838137]
	TIME [epoch: 0.734 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.557767739912549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.557767739912549 | validation: 5.616120494576613]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.547453877073733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.547453877073733 | validation: 5.577880317763877]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.539193956288963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539193956288963 | validation: 5.623479107844709]
	TIME [epoch: 0.737 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.547876041400759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.547876041400759 | validation: 5.549900515896848]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6804974088192175		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.6804974088192175 | validation: 5.700739472583784]
	TIME [epoch: 0.736 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7789852644287176		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.7789852644287176 | validation: 5.54383245386391]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5205820060741986		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.5205820060741986 | validation: 5.499058019693159]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5585745271878317		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.5585745271878317 | validation: 5.59017315090882]
	TIME [epoch: 0.735 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.547872689204603		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.547872689204603 | validation: 5.5057679635877435]
	TIME [epoch: 0.734 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.481234529327252		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.481234529327252 | validation: 5.471593306314224]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4905304659776015		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.4905304659776015 | validation: 5.50759600618128]
	TIME [epoch: 0.736 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.474828911585779		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.474828911585779 | validation: 5.465359343734889]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.455371924209601		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.455371924209601 | validation: 5.448560955224127]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.445849287617022		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.445849287617022 | validation: 5.443944059294285]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4351675348176234		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.4351675348176234 | validation: 5.421102211394535]
	TIME [epoch: 0.734 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.422527440040822		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.422527440040822 | validation: 5.413950792923195]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4150644983937317		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.4150644983937317 | validation: 5.397994888337057]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.405177999517215		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.405177999517215 | validation: 5.390349216866284]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3997894716033774		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.3997894716033774 | validation: 5.360614905799378]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.393319900460475		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.393319900460475 | validation: 5.37287744932814]
	TIME [epoch: 0.735 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.389571186777876		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.389571186777876 | validation: 5.333112479092886]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.407626794392952		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.407626794392952 | validation: 5.370444772123554]
	TIME [epoch: 0.737 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4161827704221994		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.4161827704221994 | validation: 5.301171287086557]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.400424480820291		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.400424480820291 | validation: 5.310161494274329]
	TIME [epoch: 0.737 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.355701056680424		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.355701056680424 | validation: 5.280186921970916]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3380016940661323		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 3.3380016940661323 | validation: 5.247238766389133]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3287344418823968		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 3.3287344418823968 | validation: 5.252348429270774]
	TIME [epoch: 0.739 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3134318251879207		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 3.3134318251879207 | validation: 5.227067891340829]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.304533942875877		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 3.304533942875877 | validation: 5.21052488674466]
	TIME [epoch: 0.733 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.291768407669325		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 3.291768407669325 | validation: 5.17993073250859]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2822627888393163		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 3.2822627888393163 | validation: 5.176596146792777]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.270889763132816		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 3.270889763132816 | validation: 5.13810816843063]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.259151456863045		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 3.259151456863045 | validation: 5.125487733357839]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2539435187841703		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 3.2539435187841703 | validation: 5.116461447084117]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.236731454803709		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 3.236731454803709 | validation: 5.089482525793791]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.228219500639159		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 3.228219500639159 | validation: 5.048361725397286]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2164066801483315		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 3.2164066801483315 | validation: 5.044024261204739]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2013609923830444		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 3.2013609923830444 | validation: 5.004888565003771]
	TIME [epoch: 0.733 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1851089441519465		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 3.1851089441519465 | validation: 4.96700847051298]
	TIME [epoch: 0.731 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1688838262951133		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 3.1688838262951133 | validation: 4.9427972184990985]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.151212928661589		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 3.151212928661589 | validation: 4.888041085320281]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1300510427162784		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 3.1300510427162784 | validation: 4.789253915805154]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0683522234948146		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 3.0683522234948146 | validation: 4.122673632200015]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.605541615487031		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.605541615487031 | validation: 4.79591610888514]
	TIME [epoch: 0.735 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.944712954810453		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 3.944712954810453 | validation: 4.322929239372049]
	TIME [epoch: 0.735 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3295542547275225		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 3.3295542547275225 | validation: 4.654359295106288]
	TIME [epoch: 0.736 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1527638308332833		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 3.1527638308332833 | validation: 4.213226729024548]
	TIME [epoch: 0.735 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6914105073327232		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.6914105073327232 | validation: 3.8736116971190984]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4562237332608237		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.4562237332608237 | validation: 3.710297658381687]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.366506650375044		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.366506650375044 | validation: 3.6012621949073984]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.242462019968517		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.242462019968517 | validation: 3.499738087542039]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1765944863983213		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.1765944863983213 | validation: 3.2749008120853262]
	TIME [epoch: 0.733 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.060722267933383		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.060722267933383 | validation: 3.0913085313096116]
	TIME [epoch: 0.729 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9906918714287218		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.9906918714287218 | validation: 2.999097511127078]
	TIME [epoch: 0.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9137502080757358		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.9137502080757358 | validation: 2.77234140212303]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8178241437710403		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 1.8178241437710403 | validation: 2.6001345766276702]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.724605023434166		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 1.724605023434166 | validation: 2.3368309325341756]
	TIME [epoch: 0.739 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6337785168719534		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 1.6337785168719534 | validation: 2.144087889420488]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5460914135794412		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.5460914135794412 | validation: 1.7499504421699221]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5216534713832714		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 1.5216534713832714 | validation: 1.9060546279004307]
	TIME [epoch: 0.733 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.474934859409922		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 1.474934859409922 | validation: 1.1924759091577106]
	TIME [epoch: 0.733 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3298988157781506		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.3298988157781506 | validation: 1.1966867594453225]
	TIME [epoch: 0.733 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.233135766407857		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.233135766407857 | validation: 1.3922189431502965]
	TIME [epoch: 0.733 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2248012544735434		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 1.2248012544735434 | validation: 0.9845166057025475]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.305067525743597		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.305067525743597 | validation: 1.3636492111028482]
	TIME [epoch: 0.735 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1940999427298875		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 1.1940999427298875 | validation: 0.9891950286494451]
	TIME [epoch: 0.735 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1456164680581125		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 1.1456164680581125 | validation: 1.1992332372885992]
	TIME [epoch: 0.733 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1271788623956858		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 1.1271788623956858 | validation: 0.9149397585103789]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1445698521773957		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 1.1445698521773957 | validation: 1.455023636763698]
	TIME [epoch: 0.734 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1887488792626482		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 1.1887488792626482 | validation: 0.8874731163019896]
	TIME [epoch: 0.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2837292223622034		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.2837292223622034 | validation: 0.930314379233972]
	TIME [epoch: 0.735 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0705890643539027		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.0705890643539027 | validation: 1.5124960975563175]
	TIME [epoch: 0.733 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2015183381877967		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.2015183381877967 | validation: 0.9019925388501392]
	TIME [epoch: 0.736 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1625095356258066		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.1625095356258066 | validation: 0.8948567190812295]
	TIME [epoch: 0.732 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0496637592522566		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.0496637592522566 | validation: 1.1604779394265545]
	TIME [epoch: 0.735 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0519250678620045		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.0519250678620045 | validation: 0.8995557248887679]
	TIME [epoch: 0.735 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987449482344349		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.987449482344349 | validation: 0.864762167125559]
	TIME [epoch: 0.733 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956848923529722		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.956848923529722 | validation: 0.8979301666610159]
	TIME [epoch: 0.74 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420586681915541		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.9420586681915541 | validation: 0.7920315238567502]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9349435482374088		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.9349435482374088 | validation: 0.9274346787892199]
	TIME [epoch: 0.739 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232839100813441		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.9232839100813441 | validation: 0.7387844941854323]
	TIME [epoch: 0.745 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.893990299161634		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.893990299161634 | validation: 0.9646983547116632]
	TIME [epoch: 0.738 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9112108933516262		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.9112108933516262 | validation: 0.7064641782703004]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9687721119776436		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9687721119776436 | validation: 1.1156282833505486]
	TIME [epoch: 0.74 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9736695783088277		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9736695783088277 | validation: 0.69499197611375]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9081823507124221		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9081823507124221 | validation: 0.8306707051741168]
	TIME [epoch: 0.74 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341079368203898		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8341079368203898 | validation: 0.7095742642509029]
	TIME [epoch: 0.739 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045050287344678		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8045050287344678 | validation: 0.7885042808468511]
	TIME [epoch: 0.736 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8126533376735949		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8126533376735949 | validation: 0.7534757074282799]
	TIME [epoch: 0.738 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374794260084215		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8374794260084215 | validation: 0.9789200686607451]
	TIME [epoch: 0.738 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8567916068282653		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8567916068282653 | validation: 0.6325991695834765]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826377682717828		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7826377682717828 | validation: 0.8490565565888398]
	TIME [epoch: 0.732 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940319401360111		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.7940319401360111 | validation: 0.6685780512398534]
	TIME [epoch: 0.731 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470092770154713		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.9470092770154713 | validation: 0.9506536473116309]
	TIME [epoch: 0.732 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.845581109777313		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.845581109777313 | validation: 0.6359373294755921]
	TIME [epoch: 0.732 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317921246799395		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.7317921246799395 | validation: 0.7531738126915318]
	TIME [epoch: 0.737 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.738018037567058		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.738018037567058 | validation: 0.6982913455398503]
	TIME [epoch: 0.737 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669802957875882		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7669802957875882 | validation: 1.0380557428706705]
	TIME [epoch: 0.737 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297776609439959		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8297776609439959 | validation: 0.5746336987911892]
	TIME [epoch: 0.737 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7381245133833153		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7381245133833153 | validation: 0.736106094355203]
	TIME [epoch: 0.73 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282145278735281		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7282145278735281 | validation: 0.6602894374141632]
	TIME [epoch: 0.73 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769464404675368		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.769464404675368 | validation: 0.8652591284876288]
	TIME [epoch: 0.73 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659859530225789		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7659859530225789 | validation: 0.5968288164129003]
	TIME [epoch: 0.73 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873813140746118		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6873813140746118 | validation: 0.7055407222044832]
	TIME [epoch: 0.729 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6640222124391414		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6640222124391414 | validation: 0.6387424311386525]
	TIME [epoch: 0.729 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066633024047281		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7066633024047281 | validation: 0.9664950917658062]
	TIME [epoch: 0.729 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886010337679479		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7886010337679479 | validation: 0.5693353429267392]
	TIME [epoch: 0.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177017983773796		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7177017983773796 | validation: 0.685653633807053]
	TIME [epoch: 0.736 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6563659754532585		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6563659754532585 | validation: 0.5672378823104717]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6736795355966961		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6736795355966961 | validation: 0.7605192365432318]
	TIME [epoch: 0.736 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6857040856244251		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6857040856244251 | validation: 0.5673741394183445]
	TIME [epoch: 0.73 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6339278964097043		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6339278964097043 | validation: 0.6679011369356395]
	TIME [epoch: 0.732 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059313352929246		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6059313352929246 | validation: 0.5551935810868933]
	TIME [epoch: 0.728 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386105751559115		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6386105751559115 | validation: 0.9098598406044343]
	TIME [epoch: 0.738 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232654859233508		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7232654859233508 | validation: 0.5885451282747562]
	TIME [epoch: 0.735 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720438000250953		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.6720438000250953 | validation: 0.6751167982372682]
	TIME [epoch: 0.735 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5929032176317159		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.5929032176317159 | validation: 0.5153191692207609]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860288880260618		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.5860288880260618 | validation: 0.7405039663514605]
	TIME [epoch: 0.731 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260774943704448		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7260774943704448 | validation: 0.5661814143341836]
	TIME [epoch: 0.729 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6098692364164607		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.6098692364164607 | validation: 0.5998977984655864]
	TIME [epoch: 0.729 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7395597339024778		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7395597339024778 | validation: 0.7357851268060699]
	TIME [epoch: 0.731 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6026180089854942		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6026180089854942 | validation: 0.5270734583142855]
	TIME [epoch: 0.739 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5765259001068614		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.5765259001068614 | validation: 0.5883838621917162]
	TIME [epoch: 0.737 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798489624329739		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.798489624329739 | validation: 0.7088737114187715]
	TIME [epoch: 0.738 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5972068758044281		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.5972068758044281 | validation: 0.5972728542016401]
	TIME [epoch: 0.74 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5544954076697778		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.5544954076697778 | validation: 0.5365311619728366]
	TIME [epoch: 0.734 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276975305162983		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.5276975305162983 | validation: 0.5260324533986344]
	TIME [epoch: 0.737 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5339322591555117		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.5339322591555117 | validation: 0.6002849549673542]
	TIME [epoch: 0.737 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.622285730753445		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.622285730753445 | validation: 0.9543495779416468]
	TIME [epoch: 0.738 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733424378932878		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.8733424378932878 | validation: 0.8426580021042014]
	TIME [epoch: 0.738 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097542395425761		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8097542395425761 | validation: 0.8952501847059161]
	TIME [epoch: 0.738 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060202159903389		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8060202159903389 | validation: 0.4927147190979138]
	TIME [epoch: 0.735 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6132237977123056		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6132237977123056 | validation: 0.6925066794926923]
	TIME [epoch: 0.738 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673014327955805		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.5673014327955805 | validation: 0.6516773669586184]
	TIME [epoch: 0.737 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5681921527116927		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.5681921527116927 | validation: 0.49797449146887107]
	TIME [epoch: 0.739 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117184787914336		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.5117184787914336 | validation: 0.5158998338720872]
	TIME [epoch: 0.741 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49960797328677237		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.49960797328677237 | validation: 0.5023643749875278]
	TIME [epoch: 0.738 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4936876959874967		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.4936876959874967 | validation: 0.5908631543118037]
	TIME [epoch: 0.738 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5034860421333168		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5034860421333168 | validation: 0.6254532000902202]
	TIME [epoch: 0.738 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6979476690986408		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6979476690986408 | validation: 0.5195970271447411]
	TIME [epoch: 0.738 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49761893606509006		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.49761893606509006 | validation: 0.48162154021959025]
	TIME [epoch: 0.738 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4975486057317505		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.4975486057317505 | validation: 0.4962267954271415]
	TIME [epoch: 0.737 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4981447403272782		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.4981447403272782 | validation: 0.6830125577499362]
	TIME [epoch: 0.738 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5541175701972042		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5541175701972042 | validation: 0.6756062452288919]
	TIME [epoch: 0.737 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6756250214796603		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6756250214796603 | validation: 0.5618866615323567]
	TIME [epoch: 0.736 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5239928849350477		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.5239928849350477 | validation: 0.4622480798068866]
	TIME [epoch: 0.736 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140110486867753		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.5140110486867753 | validation: 0.6910771233490718]
	TIME [epoch: 0.738 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5738737073398589		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.5738737073398589 | validation: 0.493818428021523]
	TIME [epoch: 0.737 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5816013764392083		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.5816013764392083 | validation: 0.5473284376231827]
	TIME [epoch: 0.739 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4838055947412498		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.4838055947412498 | validation: 0.4820457580740616]
	TIME [epoch: 0.735 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4927075657880476		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.4927075657880476 | validation: 0.620097945040652]
	TIME [epoch: 0.738 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5066268326946937		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.5066268326946937 | validation: 0.5127474525440361]
	TIME [epoch: 0.738 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393757501502297		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6393757501502297 | validation: 0.5548502656520149]
	TIME [epoch: 0.737 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4708941070227096		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.4708941070227096 | validation: 0.5177174853536513]
	TIME [epoch: 192 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4879186151881349		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.4879186151881349 | validation: 0.4518218607833361]
	TIME [epoch: 1.46 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162525746287732		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.5162525746287732 | validation: 0.6245454028459415]
	TIME [epoch: 1.44 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510090658689205		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.510090658689205 | validation: 0.6058506593237007]
	TIME [epoch: 1.44 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5778048933039722		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5778048933039722 | validation: 0.42629310484810334]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5107904346006653		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.5107904346006653 | validation: 0.9099418531466924]
	TIME [epoch: 1.44 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796500233263776		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6796500233263776 | validation: 0.4526363746603277]
	TIME [epoch: 1.44 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5442783483723895		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5442783483723895 | validation: 0.5523838648556866]
	TIME [epoch: 1.44 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4750533062327154		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.4750533062327154 | validation: 0.5497852912301231]
	TIME [epoch: 1.44 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139553698940327		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.5139553698940327 | validation: 0.44571715396028716]
	TIME [epoch: 1.44 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114110329925549		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5114110329925549 | validation: 0.6012778503171777]
	TIME [epoch: 1.44 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46903519960895623		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.46903519960895623 | validation: 0.5359100862519459]
	TIME [epoch: 1.44 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4538313000878143		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.4538313000878143 | validation: 0.43763806479623724]
	TIME [epoch: 1.44 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4861122142124873		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.4861122142124873 | validation: 0.5866648057605731]
	TIME [epoch: 1.44 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5179811938264398		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.5179811938264398 | validation: 0.6040669884554667]
	TIME [epoch: 1.44 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5528463585723463		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5528463585723463 | validation: 0.6049312985335389]
	TIME [epoch: 1.44 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46508154701715443		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.46508154701715443 | validation: 0.5113275460789751]
	TIME [epoch: 1.44 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5872465778551823		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.5872465778551823 | validation: 0.5157662473558932]
	TIME [epoch: 1.44 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656288414991097		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4656288414991097 | validation: 0.543790814619883]
	TIME [epoch: 1.44 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46664507827481944		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.46664507827481944 | validation: 0.4816512112030319]
	TIME [epoch: 1.44 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45870070051990114		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.45870070051990114 | validation: 0.6161294483144321]
	TIME [epoch: 1.44 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4602301218617604		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.4602301218617604 | validation: 0.45856680802416955]
	TIME [epoch: 1.44 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46498286621651885		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.46498286621651885 | validation: 0.6691194790790266]
	TIME [epoch: 1.44 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49714653149100535		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.49714653149100535 | validation: 0.502515655300675]
	TIME [epoch: 1.44 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510496219603144		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.510496219603144 | validation: 0.39435011611139265]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49239942559026967		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.49239942559026967 | validation: 0.6864244567498077]
	TIME [epoch: 1.44 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48073692352077074		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.48073692352077074 | validation: 0.5258352020401781]
	TIME [epoch: 1.44 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4555640407200699		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.4555640407200699 | validation: 0.4771671999723971]
	TIME [epoch: 1.44 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44632931170528867		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.44632931170528867 | validation: 0.47829999972552434]
	TIME [epoch: 1.44 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4205904705089728		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.4205904705089728 | validation: 0.49400734798085805]
	TIME [epoch: 1.44 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4139104746776738		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.4139104746776738 | validation: 0.49209050146404976]
	TIME [epoch: 1.44 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4331619792967591		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.4331619792967591 | validation: 0.5007601578795369]
	TIME [epoch: 1.44 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5084421014277731		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5084421014277731 | validation: 0.5196559270791895]
	TIME [epoch: 1.44 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4710517426207039		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.4710517426207039 | validation: 0.4991755563067036]
	TIME [epoch: 1.44 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4519801344495901		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.4519801344495901 | validation: 0.5182074110152611]
	TIME [epoch: 1.44 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41609407416739996		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.41609407416739996 | validation: 0.4328920698470713]
	TIME [epoch: 1.44 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.414980550859346		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.414980550859346 | validation: 0.562169568924768]
	TIME [epoch: 1.44 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42185724383584655		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.42185724383584655 | validation: 0.428380835081676]
	TIME [epoch: 1.44 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46796102943109447		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.46796102943109447 | validation: 0.7324709146241863]
	TIME [epoch: 1.44 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.490285094909444		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.490285094909444 | validation: 0.46468432287119604]
	TIME [epoch: 1.44 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40309883235779015		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.40309883235779015 | validation: 0.42468880379292817]
	TIME [epoch: 1.44 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3862659272695595		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.3862659272695595 | validation: 0.5880170381286879]
	TIME [epoch: 1.44 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4957885397417631		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.4957885397417631 | validation: 0.42588784110335437]
	TIME [epoch: 1.44 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48611835302132217		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.48611835302132217 | validation: 0.49921449720287286]
	TIME [epoch: 1.44 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4047600978797554		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.4047600978797554 | validation: 0.5016978185766158]
	TIME [epoch: 1.44 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48162656198002796		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.48162656198002796 | validation: 0.44665066663225766]
	TIME [epoch: 1.44 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41070533617593585		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.41070533617593585 | validation: 0.5553907188373732]
	TIME [epoch: 1.44 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40820597924299423		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.40820597924299423 | validation: 0.44915798879059854]
	TIME [epoch: 1.44 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4452090095030104		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.4452090095030104 | validation: 0.49227859903655136]
	TIME [epoch: 1.44 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4180518769065094		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.4180518769065094 | validation: 0.4527310797562704]
	TIME [epoch: 1.44 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3867500275889198		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.3867500275889198 | validation: 0.4270949049326843]
	TIME [epoch: 1.44 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3796207723773633		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.3796207723773633 | validation: 0.4703268770352538]
	TIME [epoch: 1.44 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36157046889848404		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.36157046889848404 | validation: 0.3717989082221841]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701125824123633		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.3701125824123633 | validation: 0.5615095837250544]
	TIME [epoch: 1.44 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853485384135598		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3853485384135598 | validation: 0.3842473216531019]
	TIME [epoch: 1.43 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43710703359787434		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.43710703359787434 | validation: 0.47514198346004255]
	TIME [epoch: 1.43 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3530784460042351		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.3530784460042351 | validation: 0.417858350945021]
	TIME [epoch: 1.43 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3413582996416666		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.3413582996416666 | validation: 0.43098450266891186]
	TIME [epoch: 1.43 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35637745361502093		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.35637745361502093 | validation: 0.4222630066832713]
	TIME [epoch: 1.43 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4208329926712878		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4208329926712878 | validation: 0.4330968714376039]
	TIME [epoch: 1.43 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3355354621086488		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.3355354621086488 | validation: 0.39437221747524287]
	TIME [epoch: 1.43 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3025122608433623		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.3025122608433623 | validation: 0.3350542405575024]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122398287259673		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.3122398287259673 | validation: 0.5346715565952336]
	TIME [epoch: 1.42 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3906312707698434		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.3906312707698434 | validation: 0.3410037314764083]
	TIME [epoch: 1.42 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49686488146990854		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.49686488146990854 | validation: 0.6046983287440314]
	TIME [epoch: 1.43 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4228275875704347		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.4228275875704347 | validation: 0.4203873675182422]
	TIME [epoch: 1.42 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36810477659156293		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.36810477659156293 | validation: 0.3314523611738799]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4015345313869574		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.4015345313869574 | validation: 0.5874063532016754]
	TIME [epoch: 1.43 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37012824411552797		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.37012824411552797 | validation: 0.4034950697368487]
	TIME [epoch: 1.43 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2952529016321027		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.2952529016321027 | validation: 0.32982509994442155]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3702472341226296		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.3702472341226296 | validation: 0.4995768850550339]
	TIME [epoch: 1.43 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30134901178011875		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.30134901178011875 | validation: 0.294456294721094]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2435520271381138		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.2435520271381138 | validation: 0.30160086666134084]
	TIME [epoch: 1.43 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2457376076253554		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.2457376076253554 | validation: 0.3968203452473875]
	TIME [epoch: 1.43 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25008561463175905		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.25008561463175905 | validation: 0.24581101935326127]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2485517646769918		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.2485517646769918 | validation: 0.47766500188027455]
	TIME [epoch: 1.43 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27964306649764736		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.27964306649764736 | validation: 0.2679370660693103]
	TIME [epoch: 1.43 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3505765096921606		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.3505765096921606 | validation: 0.32935116798736747]
	TIME [epoch: 1.44 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613011387264391		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.2613011387264391 | validation: 0.31555175535428626]
	TIME [epoch: 1.43 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305099513815592		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.2305099513815592 | validation: 0.2847726048589204]
	TIME [epoch: 1.44 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19986366618590937		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.19986366618590937 | validation: 0.26259080849254673]
	TIME [epoch: 1.43 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2040170023285071		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.2040170023285071 | validation: 0.3282453559882374]
	TIME [epoch: 1.44 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26561696668288126		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.26561696668288126 | validation: 0.30910566683141383]
	TIME [epoch: 1.43 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3542644441466611		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.3542644441466611 | validation: 0.42605691593035644]
	TIME [epoch: 1.43 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27087580576603976		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.27087580576603976 | validation: 0.18450770406402436]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23946484980470176		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.23946484980470176 | validation: 0.5313560492752335]
	TIME [epoch: 1.43 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32715047734000285		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.32715047734000285 | validation: 0.19202476107065986]
	TIME [epoch: 1.43 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2273825093121997		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.2273825093121997 | validation: 0.3113520510543277]
	TIME [epoch: 1.43 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17935088612757688		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.17935088612757688 | validation: 0.20714095686882175]
	TIME [epoch: 1.43 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18334277664920223		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.18334277664920223 | validation: 0.3790876961328494]
	TIME [epoch: 1.43 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22490351744324955		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.22490351744324955 | validation: 0.27400695771051664]
	TIME [epoch: 1.43 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33936510940300274		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.33936510940300274 | validation: 0.26426506610710904]
	TIME [epoch: 1.43 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19092473866688373		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.19092473866688373 | validation: 0.22604338013389402]
	TIME [epoch: 1.43 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1801527284215026		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.1801527284215026 | validation: 0.32710175446677714]
	TIME [epoch: 1.43 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2332089073000948		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.2332089073000948 | validation: 0.21879471042097542]
	TIME [epoch: 1.43 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2366902500386452		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.2366902500386452 | validation: 0.37141862240301027]
	TIME [epoch: 1.43 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21583713914365832		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.21583713914365832 | validation: 0.17294003911364064]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21685766487424546		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.21685766487424546 | validation: 0.3311339625245073]
	TIME [epoch: 1.43 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1983203191477321		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.1983203191477321 | validation: 0.1643662502605007]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19212584922166565		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.19212584922166565 | validation: 0.3114000163609767]
	TIME [epoch: 1.43 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18257173644798086		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.18257173644798086 | validation: 0.1647453565970015]
	TIME [epoch: 1.44 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1811315293678427		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.1811315293678427 | validation: 0.3831790924956076]
	TIME [epoch: 1.43 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2240822473032047		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.2240822473032047 | validation: 0.22491765649925766]
	TIME [epoch: 1.43 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20886242145703726		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.20886242145703726 | validation: 0.19349741713379964]
	TIME [epoch: 1.43 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21472644682085582		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.21472644682085582 | validation: 0.201892328893152]
	TIME [epoch: 1.43 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1424067201400586		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.1424067201400586 | validation: 0.19234513902003694]
	TIME [epoch: 1.43 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13270477861576296		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.13270477861576296 | validation: 0.1648198543560395]
	TIME [epoch: 1.43 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16166144039183888		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.16166144039183888 | validation: 0.3471469549240329]
	TIME [epoch: 1.43 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24814551840287244		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.24814551840287244 | validation: 0.23019092451143452]
	TIME [epoch: 1.43 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26286757955459633		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.26286757955459633 | validation: 0.3305893244143517]
	TIME [epoch: 1.43 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1955151903363541		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.1955151903363541 | validation: 0.14581852984734403]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_311.pth
	Model improved!!!
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15274170378024207		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.15274170378024207 | validation: 0.25215126217421896]
	TIME [epoch: 1.43 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1580480580204575		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.1580480580204575 | validation: 0.14173336366108247]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16928259416280525		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.16928259416280525 | validation: 0.25316912516615797]
	TIME [epoch: 1.43 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15278686419576376		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.15278686419576376 | validation: 0.12741938208571932]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14304636354738898		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.14304636354738898 | validation: 0.2621869236101854]
	TIME [epoch: 1.43 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15180869561458585		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.15180869561458585 | validation: 0.14176682046872552]
	TIME [epoch: 1.43 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18302962020260807		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.18302962020260807 | validation: 0.46768574195998636]
	TIME [epoch: 1.43 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26205329516064635		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.26205329516064635 | validation: 0.2618893233941694]
	TIME [epoch: 1.43 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21399137900268797		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.21399137900268797 | validation: 0.15000114325245223]
	TIME [epoch: 1.43 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19362122885875946		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.19362122885875946 | validation: 0.24368847837061558]
	TIME [epoch: 1.43 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1392982126186183		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.1392982126186183 | validation: 0.18156852539987517]
	TIME [epoch: 1.43 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18263693699900446		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.18263693699900446 | validation: 0.3106181370788027]
	TIME [epoch: 1.43 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2814626910150534		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.2814626910150534 | validation: 0.17980568168150163]
	TIME [epoch: 1.43 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126785595475384		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.126785595475384 | validation: 0.19049168709493758]
	TIME [epoch: 1.43 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13180994481655312		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.13180994481655312 | validation: 0.1434187005399601]
	TIME [epoch: 1.43 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331106411409769		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.1331106411409769 | validation: 0.18845908922783283]
	TIME [epoch: 1.43 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11333955867144252		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.11333955867144252 | validation: 0.12575880235134554]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_328.pth
	Model improved!!!
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11259727706196924		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.11259727706196924 | validation: 0.23918067731048667]
	TIME [epoch: 1.43 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375795726147703		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.1375795726147703 | validation: 0.1709243520806336]
	TIME [epoch: 1.43 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15882085063929854		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.15882085063929854 | validation: 0.3522985290266047]
	TIME [epoch: 1.43 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2127419872042151		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.2127419872042151 | validation: 0.19046937577242914]
	TIME [epoch: 1.43 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419301717405085		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.1419301717405085 | validation: 0.13289769372363436]
	TIME [epoch: 1.43 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617894993835925		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.1617894993835925 | validation: 0.17796172044208142]
	TIME [epoch: 1.43 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10720682153309287		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.10720682153309287 | validation: 0.13483407168944914]
	TIME [epoch: 1.43 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09947185409012993		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.09947185409012993 | validation: 0.16199682919304265]
	TIME [epoch: 1.44 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10613062512297514		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.10613062512297514 | validation: 0.13242374834068538]
	TIME [epoch: 1.43 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453066570011061		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.1453066570011061 | validation: 0.2601278633036673]
	TIME [epoch: 1.43 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16109844151407426		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.16109844151407426 | validation: 0.1431171980251817]
	TIME [epoch: 1.43 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18104794589152395		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.18104794589152395 | validation: 0.3605011514441417]
	TIME [epoch: 1.43 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21139462376018273		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.21139462376018273 | validation: 0.19787674375450848]
	TIME [epoch: 1.43 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16024752782602697		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.16024752782602697 | validation: 0.14620673839879159]
	TIME [epoch: 1.44 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1708855610877655		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.1708855610877655 | validation: 0.15932245001263548]
	TIME [epoch: 1.43 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13416421410182025		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.13416421410182025 | validation: 0.26115514298778625]
	TIME [epoch: 1.43 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535903202304727		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.1535903202304727 | validation: 0.14505534360861802]
	TIME [epoch: 1.43 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11874439064789566		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.11874439064789566 | validation: 0.24983875059225907]
	TIME [epoch: 1.43 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15034056050202718		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.15034056050202718 | validation: 0.16500157590756562]
	TIME [epoch: 1.43 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11086429400805393		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.11086429400805393 | validation: 0.17451654779883174]
	TIME [epoch: 1.43 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336349309678936		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.1336349309678936 | validation: 0.18155773067028705]
	TIME [epoch: 1.43 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14305401392475517		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.14305401392475517 | validation: 0.11924533445999944]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095260383447755		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.1095260383447755 | validation: 0.15477249563457646]
	TIME [epoch: 1.43 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10145698835269479		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.10145698835269479 | validation: 0.12336226980404015]
	TIME [epoch: 1.43 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10791447626053674		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.10791447626053674 | validation: 0.21343947444150785]
	TIME [epoch: 1.43 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369662802673783		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.1369662802673783 | validation: 0.14801224727579218]
	TIME [epoch: 1.43 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.156130781336292		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.156130781336292 | validation: 0.26216056278584726]
	TIME [epoch: 1.43 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15862099720818104		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.15862099720818104 | validation: 0.1477194048350434]
	TIME [epoch: 1.43 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10456043636882072		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.10456043636882072 | validation: 0.15161019911075826]
	TIME [epoch: 1.43 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458195045709035		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.1458195045709035 | validation: 0.1643586719256383]
	TIME [epoch: 1.43 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13920308758941935		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.13920308758941935 | validation: 0.23705543741089682]
	TIME [epoch: 1.43 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16967447357293303		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.16967447357293303 | validation: 0.16576780437963923]
	TIME [epoch: 1.43 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1211895337374526		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.1211895337374526 | validation: 0.1494629981458324]
	TIME [epoch: 1.43 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213266089353968		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.11213266089353968 | validation: 0.15237579096108592]
	TIME [epoch: 1.43 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11308524689451589		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.11308524689451589 | validation: 0.2244821429861534]
	TIME [epoch: 1.43 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13688567660228496		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.13688567660228496 | validation: 0.14391153217241567]
	TIME [epoch: 1.43 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17614896788194487		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.17614896788194487 | validation: 0.20995071718504354]
	TIME [epoch: 1.43 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.121561096843766		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.121561096843766 | validation: 0.11421775046949922]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0912925612090165		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.0912925612090165 | validation: 0.15436291901013335]
	TIME [epoch: 1.43 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11696305445726562		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.11696305445726562 | validation: 0.18627783443708307]
	TIME [epoch: 1.43 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567386096102197		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.14567386096102197 | validation: 0.16885100013506357]
	TIME [epoch: 1.43 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13531155643192067		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.13531155643192067 | validation: 0.21076964600059803]
	TIME [epoch: 1.43 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295150470488532		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.1295150470488532 | validation: 0.12506625014104364]
	TIME [epoch: 1.43 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08453348863347826		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.08453348863347826 | validation: 0.15329130987223674]
	TIME [epoch: 1.44 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10451831362812908		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.10451831362812908 | validation: 0.10369712373349323]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_373.pth
	Model improved!!!
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419603054228404		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.1419603054228404 | validation: 0.1663792249105685]
	TIME [epoch: 1.43 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09526003860262412		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.09526003860262412 | validation: 0.10585566586729422]
	TIME [epoch: 1.43 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810110406713837		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.0810110406713837 | validation: 0.14882453363986056]
	TIME [epoch: 1.43 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925660456708801		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.09925660456708801 | validation: 0.1385965524280281]
	TIME [epoch: 1.43 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1250113602522687		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.1250113602522687 | validation: 0.23988463638211438]
	TIME [epoch: 1.43 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14009681253121337		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.14009681253121337 | validation: 0.12747683707147617]
	TIME [epoch: 1.43 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08257599263124696		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.08257599263124696 | validation: 0.1240892006540403]
	TIME [epoch: 1.43 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08515731102377906		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.08515731102377906 | validation: 0.11099636153191653]
	TIME [epoch: 1.43 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09313969005255489		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.09313969005255489 | validation: 0.21908535384089514]
	TIME [epoch: 1.43 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328942164325996		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.1328942164325996 | validation: 0.1314503421277304]
	TIME [epoch: 1.43 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09896425987979175		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.09896425987979175 | validation: 0.1439269274088446]
	TIME [epoch: 1.43 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10859544800862611		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.10859544800862611 | validation: 0.17311134074838863]
	TIME [epoch: 1.43 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1353058314788018		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1353058314788018 | validation: 0.1358552748371643]
	TIME [epoch: 1.43 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388891238724315		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.1388891238724315 | validation: 0.1452285113865909]
	TIME [epoch: 1.43 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051228549476516		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.1051228549476516 | validation: 0.1075229094784837]
	TIME [epoch: 1.43 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09546935303020813		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.09546935303020813 | validation: 0.11799943727920864]
	TIME [epoch: 1.43 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761633524486041		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.0761633524486041 | validation: 0.09956800449314188]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07633367096517576		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.07633367096517576 | validation: 0.13106387342177292]
	TIME [epoch: 1.43 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725968184258748		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.07725968184258748 | validation: 0.12845305780953478]
	TIME [epoch: 1.43 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887613496456649		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.09887613496456649 | validation: 0.16619615504596105]
	TIME [epoch: 1.43 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295718285498916		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.1295718285498916 | validation: 0.2103501499022069]
	TIME [epoch: 1.43 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1325255299732079		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.1325255299732079 | validation: 0.1097134332243952]
	TIME [epoch: 1.43 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780978558060787		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.0780978558060787 | validation: 0.15204863488897208]
	TIME [epoch: 1.44 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1241545913566673		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.1241545913566673 | validation: 0.09300572329814649]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11760764139556433		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.11760764139556433 | validation: 0.1439187505459483]
	TIME [epoch: 1.44 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08861188168729324		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.08861188168729324 | validation: 0.11586796303255037]
	TIME [epoch: 1.43 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112094877625285		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.1112094877625285 | validation: 0.29683687709755036]
	TIME [epoch: 1.43 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17321078105424711		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.17321078105424711 | validation: 0.13492190679824798]
	TIME [epoch: 1.43 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0867051878876935		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.0867051878876935 | validation: 0.12728350433026878]
	TIME [epoch: 1.43 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12382266433356534		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.12382266433356534 | validation: 0.21121097112803594]
	TIME [epoch: 1.43 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13288902247090886		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.13288902247090886 | validation: 0.13341966107920916]
	TIME [epoch: 1.43 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10129399263376918		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.10129399263376918 | validation: 0.1638830231148612]
	TIME [epoch: 1.43 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09816256854342509		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.09816256854342509 | validation: 0.09409216791598957]
	TIME [epoch: 1.43 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07096721465590118		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.07096721465590118 | validation: 0.11823432700768335]
	TIME [epoch: 1.43 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07003881182533742		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.07003881182533742 | validation: 0.08946918373355661]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893234844971733		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.0893234844971733 | validation: 0.14308086461897207]
	TIME [epoch: 1.43 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155802403188575		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.10155802403188575 | validation: 0.09539177618854544]
	TIME [epoch: 1.43 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09041938438460695		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.09041938438460695 | validation: 0.11030874965036236]
	TIME [epoch: 1.43 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07860949555389976		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.07860949555389976 | validation: 0.0990048323468836]
	TIME [epoch: 1.43 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07782359987807987		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.07782359987807987 | validation: 0.11212998414699077]
	TIME [epoch: 1.43 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768436887004097		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.0768436887004097 | validation: 0.10485195357212301]
	TIME [epoch: 1.43 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10034101918470763		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.10034101918470763 | validation: 0.17384502378051028]
	TIME [epoch: 1.43 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10958098586280195		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.10958098586280195 | validation: 0.12249927913529415]
	TIME [epoch: 1.42 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1037047607572319		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.1037047607572319 | validation: 0.2361145999188369]
	TIME [epoch: 1.43 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1497146941575533		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.1497146941575533 | validation: 0.1251415442148714]
	TIME [epoch: 1.42 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08437872330647941		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.08437872330647941 | validation: 0.10346558682702543]
	TIME [epoch: 1.42 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10747541600375343		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.10747541600375343 | validation: 0.1790293426838555]
	TIME [epoch: 1.43 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327078649515028		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.12327078649515028 | validation: 0.09655613270367044]
	TIME [epoch: 1.42 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093890566231968		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.07093890566231968 | validation: 0.08745109874456498]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07549802958382096		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.07549802958382096 | validation: 0.1440932356496818]
	TIME [epoch: 1.44 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09038095960044594		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.09038095960044594 | validation: 0.1759691222428703]
	TIME [epoch: 1.43 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22869034095144114		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.22869034095144114 | validation: 0.12098173030587095]
	TIME [epoch: 1.43 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647313715366924		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.07647313715366924 | validation: 0.13229249635077342]
	TIME [epoch: 1.43 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0915937509606221		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.0915937509606221 | validation: 0.1386192156318536]
	TIME [epoch: 1.43 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10010017299209548		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.10010017299209548 | validation: 0.11038773349723803]
	TIME [epoch: 1.43 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09085644668892591		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.09085644668892591 | validation: 0.1153181072050828]
	TIME [epoch: 1.43 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659067407386		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.0659067407386 | validation: 0.10111550502435585]
	TIME [epoch: 1.43 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739671697210109		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.05739671697210109 | validation: 0.09942371042315278]
	TIME [epoch: 1.43 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06194502685196298		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.06194502685196298 | validation: 0.11404354811425468]
	TIME [epoch: 1.43 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223252401917413		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.07223252401917413 | validation: 0.12595194852838773]
	TIME [epoch: 1.43 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08553002039390799		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.08553002039390799 | validation: 0.11489027438049582]
	TIME [epoch: 1.43 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07791645247599374		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.07791645247599374 | validation: 0.1533401204719689]
	TIME [epoch: 1.43 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10173294345921374		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.10173294345921374 | validation: 0.11032897202334141]
	TIME [epoch: 1.43 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09018691757611418		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.09018691757611418 | validation: 0.1757575091866605]
	TIME [epoch: 1.44 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1035134614825957		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.1035134614825957 | validation: 0.07967852926467822]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819954676806086		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.07819954676806086 | validation: 0.11899836316644195]
	TIME [epoch: 1.43 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1050564229997186		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1050564229997186 | validation: 0.08890629958502703]
	TIME [epoch: 1.43 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08355931929820862		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.08355931929820862 | validation: 0.11797829995827357]
	TIME [epoch: 1.43 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07383975465938834		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.07383975465938834 | validation: 0.09098188412371519]
	TIME [epoch: 1.43 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06673309595526752		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.06673309595526752 | validation: 0.10566584185734912]
	TIME [epoch: 1.43 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496733063037692		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.06496733063037692 | validation: 0.09021467735085376]
	TIME [epoch: 1.43 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559637468368063		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.08559637468368063 | validation: 0.11383939103662155]
	TIME [epoch: 1.43 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08048140639180204		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.08048140639180204 | validation: 0.06852035756109857]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05843600409816272		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.05843600409816272 | validation: 0.1032681882435659]
	TIME [epoch: 1.43 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05384424131868589		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.05384424131868589 | validation: 0.0803897551196013]
	TIME [epoch: 1.43 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055944489591875736		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.055944489591875736 | validation: 0.11455893799923554]
	TIME [epoch: 1.43 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06594972356831809		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.06594972356831809 | validation: 0.11932116111901392]
	TIME [epoch: 1.43 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1158209452591461		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1158209452591461 | validation: 0.27459123368810723]
	TIME [epoch: 1.43 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1632966815995996		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.1632966815995996 | validation: 0.12542992429228847]
	TIME [epoch: 1.43 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09524137680000977		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.09524137680000977 | validation: 0.11213397294859283]
	TIME [epoch: 1.43 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09883402512305703		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.09883402512305703 | validation: 0.16988847835672388]
	TIME [epoch: 1.43 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10042109712990432		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.10042109712990432 | validation: 0.09415765987897301]
	TIME [epoch: 1.43 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670542884855797		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.0670542884855797 | validation: 0.10875526636834243]
	TIME [epoch: 1.43 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298382382717624		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.06298382382717624 | validation: 0.07865092319095546]
	TIME [epoch: 1.43 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08025106537335944		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.08025106537335944 | validation: 0.12304462748216796]
	TIME [epoch: 1.43 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07521697470785008		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.07521697470785008 | validation: 0.08615420854132105]
	TIME [epoch: 1.43 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07511560992757765		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.07511560992757765 | validation: 0.12620727940025778]
	TIME [epoch: 1.43 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075151547082554		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.08075151547082554 | validation: 0.09674807305987154]
	TIME [epoch: 1.43 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07305004990479012		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.07305004990479012 | validation: 0.14231719788217148]
	TIME [epoch: 1.43 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08638269834375358		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.08638269834375358 | validation: 0.0946080372659405]
	TIME [epoch: 1.43 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05829994286856611		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.05829994286856611 | validation: 0.09337910485172479]
	TIME [epoch: 1.43 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05975698850659951		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.05975698850659951 | validation: 0.09255267341284797]
	TIME [epoch: 1.43 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06723158878437482		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.06723158878437482 | validation: 0.11061231197989004]
	TIME [epoch: 1.43 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08514087409651545		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.08514087409651545 | validation: 0.10652971733437805]
	TIME [epoch: 1.43 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07535131576514274		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.07535131576514274 | validation: 0.07670273503384309]
	TIME [epoch: 1.43 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05997774182661958		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.05997774182661958 | validation: 0.10313773854697]
	TIME [epoch: 1.43 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691015571500405		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.05691015571500405 | validation: 0.07150281552677623]
	TIME [epoch: 1.43 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06639230827208864		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.06639230827208864 | validation: 0.12754622480661132]
	TIME [epoch: 1.43 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08757149025054066		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.08757149025054066 | validation: 0.09464467218916255]
	TIME [epoch: 1.43 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08595025113912938		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.08595025113912938 | validation: 0.20685775167773662]
	TIME [epoch: 1.43 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11764359862722729		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.11764359862722729 | validation: 0.10726996585213229]
	TIME [epoch: 1.43 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05853051971529226		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.05853051971529226 | validation: 0.08281564048593654]
	TIME [epoch: 1.43 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0720296417253097		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.0720296417253097 | validation: 0.1959506008820375]
	TIME [epoch: 1.43 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11719066196585509		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.11719066196585509 | validation: 0.08276068634503375]
	TIME [epoch: 1.43 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532437837620236		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.0532437837620236 | validation: 0.08727558963398176]
	TIME [epoch: 1.43 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059974450201852963		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.059974450201852963 | validation: 0.09747182859516035]
	TIME [epoch: 1.43 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06900873457650522		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.06900873457650522 | validation: 0.10710627960273152]
	TIME [epoch: 1.43 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07041510037434107		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.07041510037434107 | validation: 0.109928315539166]
	TIME [epoch: 1.42 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06876262124644239		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.06876262124644239 | validation: 0.08299534058522863]
	TIME [epoch: 1.43 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056339964137062576		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.056339964137062576 | validation: 0.09487443484183809]
	TIME [epoch: 1.43 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056938097671260436		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.056938097671260436 | validation: 0.07372639726651652]
	TIME [epoch: 1.43 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286236978309448		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.06286236978309448 | validation: 0.12445968233800256]
	TIME [epoch: 1.43 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09005391216038575		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.09005391216038575 | validation: 0.0844183426703282]
	TIME [epoch: 1.44 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10213248163692754		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.10213248163692754 | validation: 0.10353774055596515]
	TIME [epoch: 1.43 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06327901663597371		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.06327901663597371 | validation: 0.09102626102586313]
	TIME [epoch: 1.43 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05840033107752138		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.05840033107752138 | validation: 0.10330703297140892]
	TIME [epoch: 1.43 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015465956721287		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.07015465956721287 | validation: 0.10899392704446642]
	TIME [epoch: 1.42 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258194983354249		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.07258194983354249 | validation: 0.08642414382271006]
	TIME [epoch: 1.43 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08560727444964845		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.08560727444964845 | validation: 0.08919699957046723]
	TIME [epoch: 1.43 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060623743798641654		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.060623743798641654 | validation: 0.09230293253998365]
	TIME [epoch: 1.43 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05391317414469804		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.05391317414469804 | validation: 0.07400510815282882]
	TIME [epoch: 1.43 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051698715640319116		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.051698715640319116 | validation: 0.11946503759189475]
	TIME [epoch: 1.43 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07523295650197398		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.07523295650197398 | validation: 0.08247203071677434]
	TIME [epoch: 1.43 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06927205432170151		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.06927205432170151 | validation: 0.08051690903503278]
	TIME [epoch: 1.43 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053135497333801976		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.053135497333801976 | validation: 0.06110773065040575]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_498.pth
	Model improved!!!
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046319754679363126		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.046319754679363126 | validation: 0.07612175374047489]
	TIME [epoch: 1.43 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044003078979836456		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.044003078979836456 | validation: 0.06995580677783368]
	TIME [epoch: 1.43 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324241163931872		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.04324241163931872 | validation: 0.07824713011957708]
	TIME [epoch: 194 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324077426062015		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.04324077426062015 | validation: 0.0744698115386687]
	TIME [epoch: 2.86 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04759282206471273		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.04759282206471273 | validation: 0.09021097909167625]
	TIME [epoch: 2.83 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061819111776779824		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.061819111776779824 | validation: 0.1267948448728553]
	TIME [epoch: 2.84 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710628370282666		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.08710628370282666 | validation: 0.1428503316689691]
	TIME [epoch: 2.83 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10532445331329761		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.10532445331329761 | validation: 0.08196463746547628]
	TIME [epoch: 2.84 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050247023940461384		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.050247023940461384 | validation: 0.07659766781530705]
	TIME [epoch: 2.84 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03960386990805672		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.03960386990805672 | validation: 0.06268864859840546]
	TIME [epoch: 2.83 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04447701235018726		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.04447701235018726 | validation: 0.10403086626286828]
	TIME [epoch: 2.84 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06364940000474453		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.06364940000474453 | validation: 0.08627772808104078]
	TIME [epoch: 2.83 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10832273300957751		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.10832273300957751 | validation: 0.13165809934905015]
	TIME [epoch: 2.83 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0731507213722105		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.0731507213722105 | validation: 0.08102768149571138]
	TIME [epoch: 2.84 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04538547641877987		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.04538547641877987 | validation: 0.068970306527872]
	TIME [epoch: 2.84 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0491535637320169		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.0491535637320169 | validation: 0.0888479314129892]
	TIME [epoch: 2.84 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057621936334548395		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.057621936334548395 | validation: 0.09352055802258546]
	TIME [epoch: 2.84 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267169353501156		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.07267169353501156 | validation: 0.09754342840751667]
	TIME [epoch: 2.84 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08039457568167603		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.08039457568167603 | validation: 0.14029070136377483]
	TIME [epoch: 2.84 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08908165964935875		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.08908165964935875 | validation: 0.07973634054644993]
	TIME [epoch: 2.84 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07494423568301124		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.07494423568301124 | validation: 0.07772339953676659]
	TIME [epoch: 2.84 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05171055415687011		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.05171055415687011 | validation: 0.10708515734355069]
	TIME [epoch: 2.84 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06098285011097297		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.06098285011097297 | validation: 0.07860556266179775]
	TIME [epoch: 2.84 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744461037868592		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.0744461037868592 | validation: 0.09653039202173602]
	TIME [epoch: 2.84 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059864226434880256		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.059864226434880256 | validation: 0.07126214363637143]
	TIME [epoch: 2.84 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04314907401143436		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.04314907401143436 | validation: 0.08243730370945812]
	TIME [epoch: 2.84 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04833822582993734		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.04833822582993734 | validation: 0.07547068808282117]
	TIME [epoch: 2.84 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04838114108194084		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.04838114108194084 | validation: 0.0792481742511455]
	TIME [epoch: 2.83 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046276003938180314		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.046276003938180314 | validation: 0.07649438885380075]
	TIME [epoch: 2.83 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04776791881301957		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.04776791881301957 | validation: 0.07838131436172165]
	TIME [epoch: 2.84 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05001285155093063		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.05001285155093063 | validation: 0.06824062200313845]
	TIME [epoch: 2.84 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04428482510389908		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.04428482510389908 | validation: 0.07267721710891441]
	TIME [epoch: 2.83 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042627346029628255		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.042627346029628255 | validation: 0.06434610000903061]
	TIME [epoch: 2.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055714691750916645		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.055714691750916645 | validation: 0.11188138135107986]
	TIME [epoch: 2.84 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735608293530873		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.0735608293530873 | validation: 0.06878991437777789]
	TIME [epoch: 2.83 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683350335016514		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.0683350335016514 | validation: 0.12428045463393006]
	TIME [epoch: 2.84 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215217059309072		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.07215217059309072 | validation: 0.08297357995273183]
	TIME [epoch: 2.84 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048302603456454214		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.048302603456454214 | validation: 0.09129198730923793]
	TIME [epoch: 2.84 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07376133375595754		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.07376133375595754 | validation: 0.08901662215065861]
	TIME [epoch: 2.84 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05374301754148423		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.05374301754148423 | validation: 0.05398115442164552]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_538.pth
	Model improved!!!
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05183757220728271		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.05183757220728271 | validation: 0.08123269321694829]
	TIME [epoch: 2.84 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048038479597250286		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.048038479597250286 | validation: 0.07163212229989808]
	TIME [epoch: 2.84 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04880771797739282		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.04880771797739282 | validation: 0.09947537449122655]
	TIME [epoch: 2.84 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059924590944621964		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.059924590944621964 | validation: 0.07613035144378438]
	TIME [epoch: 2.84 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05224256021565119		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.05224256021565119 | validation: 0.09871145779938499]
	TIME [epoch: 2.83 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05425336386585089		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.05425336386585089 | validation: 0.06736588013647982]
	TIME [epoch: 2.84 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04285179712043408		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.04285179712043408 | validation: 0.07170477308044379]
	TIME [epoch: 2.83 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042480286834513735		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.042480286834513735 | validation: 0.05642747606609653]
	TIME [epoch: 2.83 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04518631549741842		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.04518631549741842 | validation: 0.07783507492593585]
	TIME [epoch: 2.83 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04686854486199447		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.04686854486199447 | validation: 0.05774053325384809]
	TIME [epoch: 2.83 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05573449842106703		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.05573449842106703 | validation: 0.0961909792608995]
	TIME [epoch: 2.83 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05712308195100623		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.05712308195100623 | validation: 0.060583500041523256]
	TIME [epoch: 2.84 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05185463246543209		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.05185463246543209 | validation: 0.08439538506907263]
	TIME [epoch: 2.84 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05030558609472102		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.05030558609472102 | validation: 0.057877353511619725]
	TIME [epoch: 2.83 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04642164346633089		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.04642164346633089 | validation: 0.07843992203698599]
	TIME [epoch: 2.83 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04512970069233341		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.04512970069233341 | validation: 0.05984371911843339]
	TIME [epoch: 2.84 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530179255917974		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.04530179255917974 | validation: 0.0992771128167102]
	TIME [epoch: 2.84 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055520304634818905		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.055520304634818905 | validation: 0.07061257754663638]
	TIME [epoch: 2.84 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04817638115197681		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.04817638115197681 | validation: 0.0753690840895031]
	TIME [epoch: 2.92 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043008284986084784		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.043008284986084784 | validation: 0.07836921055592291]
	TIME [epoch: 2.83 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052532844228287225		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.052532844228287225 | validation: 0.14499864698708]
	TIME [epoch: 2.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0972131088672331		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.0972131088672331 | validation: 0.08101397028372555]
	TIME [epoch: 2.83 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04569733942696919		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.04569733942696919 | validation: 0.06332685141243846]
	TIME [epoch: 2.83 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06062937790555838		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.06062937790555838 | validation: 0.11554758906044156]
	TIME [epoch: 2.83 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06993315616059555		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.06993315616059555 | validation: 0.06259311455216891]
	TIME [epoch: 2.83 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346501217440305		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.04346501217440305 | validation: 0.06528976690955904]
	TIME [epoch: 2.83 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037587517266381425		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.037587517266381425 | validation: 0.05810674252579587]
	TIME [epoch: 2.83 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03566319605431818		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.03566319605431818 | validation: 0.06736072115998963]
	TIME [epoch: 2.83 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035086864995293114		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.035086864995293114 | validation: 0.05296733969970725]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0385503192704513		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.0385503192704513 | validation: 0.09019850519371717]
	TIME [epoch: 2.83 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05193234304314236		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.05193234304314236 | validation: 0.058645486379031345]
	TIME [epoch: 2.82 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543644678024658		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.04543644678024658 | validation: 0.07944933463554983]
	TIME [epoch: 2.83 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04451067806726915		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.04451067806726915 | validation: 0.05610518873522119]
	TIME [epoch: 2.83 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047532021074469716		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.047532021074469716 | validation: 0.0771604792346278]
	TIME [epoch: 2.83 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046868152449388925		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.046868152449388925 | validation: 0.060263020994855865]
	TIME [epoch: 3.48 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05615934290807706		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.05615934290807706 | validation: 0.15319343060988994]
	TIME [epoch: 2.84 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09065346692703444		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.09065346692703444 | validation: 0.08079585253801586]
	TIME [epoch: 2.83 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490450259221336		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.04490450259221336 | validation: 0.07310205543991025]
	TIME [epoch: 2.83 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07777421006338459		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.07777421006338459 | validation: 0.10437838517373565]
	TIME [epoch: 2.83 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0656512778257394		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.0656512778257394 | validation: 0.06741390149595357]
	TIME [epoch: 2.83 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040514589661706416		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.040514589661706416 | validation: 0.0763276128893288]
	TIME [epoch: 2.83 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522944670701764		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.05522944670701764 | validation: 0.06431271608838543]
	TIME [epoch: 2.83 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052183337531042415		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.052183337531042415 | validation: 0.07980528501674468]
	TIME [epoch: 2.83 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04060080257927503		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.04060080257927503 | validation: 0.05065215828419522]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03373427293840429		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.03373427293840429 | validation: 0.04735555075605507]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03561319749197473		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.03561319749197473 | validation: 0.06205405957735282]
	TIME [epoch: 2.83 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033619147964439954		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.033619147964439954 | validation: 0.0526778498600049]
	TIME [epoch: 2.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03236392647504947		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.03236392647504947 | validation: 0.059591248959205714]
	TIME [epoch: 2.83 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223699891056051		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.03223699891056051 | validation: 0.061515253275935]
	TIME [epoch: 2.83 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035300927481882245		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.035300927481882245 | validation: 0.06442016218510134]
	TIME [epoch: 2.83 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04969380788966788		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.04969380788966788 | validation: 0.1334451384517433]
	TIME [epoch: 2.83 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557325494489574		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.07557325494489574 | validation: 0.058291652235551376]
	TIME [epoch: 2.84 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05485937094279993		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.05485937094279993 | validation: 0.07739603255003565]
	TIME [epoch: 2.83 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04870277582474084		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.04870277582474084 | validation: 0.05244769704236022]
	TIME [epoch: 2.83 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048545215151514366		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.048545215151514366 | validation: 0.05950351084756361]
	TIME [epoch: 2.83 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03941550327310513		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.03941550327310513 | validation: 0.05219864709628186]
	TIME [epoch: 2.83 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03484562919106759		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.03484562919106759 | validation: 0.0828039227748707]
	TIME [epoch: 2.83 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05055423627644709		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.05055423627644709 | validation: 0.07224358622225231]
	TIME [epoch: 2.83 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05105859400493729		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.05105859400493729 | validation: 0.07644045946939619]
	TIME [epoch: 2.83 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0434686698741419		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.0434686698741419 | validation: 0.05176009062372519]
	TIME [epoch: 2.83 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037147458063155024		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.037147458063155024 | validation: 0.06729513213461862]
	TIME [epoch: 2.82 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04083417835780996		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.04083417835780996 | validation: 0.053127197557758625]
	TIME [epoch: 2.83 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020890193088647		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.04020890193088647 | validation: 0.06292417837077462]
	TIME [epoch: 2.83 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040849812670421475		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.040849812670421475 | validation: 0.04420302861890701]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054900616796868355		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.054900616796868355 | validation: 0.06828555351503876]
	TIME [epoch: 2.83 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04237739811286687		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.04237739811286687 | validation: 0.050916553334805076]
	TIME [epoch: 2.83 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038331716416920986		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.038331716416920986 | validation: 0.060123096485742406]
	TIME [epoch: 2.83 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03624524612985126		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.03624524612985126 | validation: 0.05783581080134531]
	TIME [epoch: 2.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019463590810416		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.04019463590810416 | validation: 0.08682106287973997]
	TIME [epoch: 2.83 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058382181983290024		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.058382181983290024 | validation: 0.06986577580082956]
	TIME [epoch: 2.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231491908269785		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.04231491908269785 | validation: 0.06092198152933146]
	TIME [epoch: 2.83 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03580479571182844		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.03580479571182844 | validation: 0.05390087434966065]
	TIME [epoch: 2.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722693231047162		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.03722693231047162 | validation: 0.0681982001553058]
	TIME [epoch: 2.83 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04233556316304687		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.04233556316304687 | validation: 0.04883563273544827]
	TIME [epoch: 2.83 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0376832626556614		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.0376832626556614 | validation: 0.07410937354749075]
	TIME [epoch: 2.83 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206966637214747		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.04206966637214747 | validation: 0.04602467340866844]
	TIME [epoch: 2.83 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05800928865899393		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.05800928865899393 | validation: 0.060321865797964384]
	TIME [epoch: 2.83 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03476870340204792		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.03476870340204792 | validation: 0.051554562215734935]
	TIME [epoch: 2.83 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03219955780106658		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.03219955780106658 | validation: 0.05194575948322527]
	TIME [epoch: 2.82 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401829388252252		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.03401829388252252 | validation: 0.059247676133510355]
	TIME [epoch: 2.82 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03339041445782152		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.03339041445782152 | validation: 0.057967043629225536]
	TIME [epoch: 2.83 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049475997716023186		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.049475997716023186 | validation: 0.10442138785795502]
	TIME [epoch: 2.82 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152344897656639		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.06152344897656639 | validation: 0.059067621281405074]
	TIME [epoch: 2.82 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03690298730192918		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.03690298730192918 | validation: 0.05976968932568]
	TIME [epoch: 2.83 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588947197146963		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.03588947197146963 | validation: 0.050256258590923364]
	TIME [epoch: 2.83 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030728401309127244		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.030728401309127244 | validation: 0.055381143690832794]
	TIME [epoch: 2.82 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03085868306789084		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.03085868306789084 | validation: 0.04390020000983335]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03840124312813452		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.03840124312813452 | validation: 0.08510914250014828]
	TIME [epoch: 2.84 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054322441701633195		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.054322441701633195 | validation: 0.05710917075044162]
	TIME [epoch: 2.84 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848257855151256		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.05848257855151256 | validation: 0.07380959689013503]
	TIME [epoch: 2.84 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04243487439169756		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.04243487439169756 | validation: 0.08317984520689742]
	TIME [epoch: 2.84 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044536454382380686		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.044536454382380686 | validation: 0.04961516099835398]
	TIME [epoch: 2.83 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04567651106288524		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.04567651106288524 | validation: 0.05882314115443369]
	TIME [epoch: 2.84 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031848619199433374		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.031848619199433374 | validation: 0.04824472137773678]
	TIME [epoch: 2.84 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028871309215801336		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.028871309215801336 | validation: 0.040366431714125996]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_633.pth
	Model improved!!!
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885043006820902		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.02885043006820902 | validation: 0.047587025228973706]
	TIME [epoch: 2.83 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02985686158123353		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.02985686158123353 | validation: 0.043672357308707624]
	TIME [epoch: 2.83 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03040914519658342		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.03040914519658342 | validation: 0.05510369833137915]
	TIME [epoch: 2.83 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03311649632451269		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.03311649632451269 | validation: 0.04220999589368593]
	TIME [epoch: 2.84 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983360941360226		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.03983360941360226 | validation: 0.07873661089090471]
	TIME [epoch: 2.83 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778967659966378		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.04778967659966378 | validation: 0.06310747191229749]
	TIME [epoch: 2.83 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058922210778391396		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.058922210778391396 | validation: 0.10521535560661358]
	TIME [epoch: 2.83 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06320485941854236		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.06320485941854236 | validation: 0.05324158507349396]
	TIME [epoch: 2.83 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029313521405271825		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.029313521405271825 | validation: 0.057475795365923635]
	TIME [epoch: 2.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034800708213740074		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.034800708213740074 | validation: 0.06406792593216439]
	TIME [epoch: 2.83 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037669323357978506		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.037669323357978506 | validation: 0.05508641138020431]
	TIME [epoch: 2.83 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03308000114825072		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.03308000114825072 | validation: 0.05523409407399509]
	TIME [epoch: 2.83 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033892944182307295		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.033892944182307295 | validation: 0.04234189756773881]
	TIME [epoch: 2.83 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033148064701933926		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.033148064701933926 | validation: 0.05544406271142428]
	TIME [epoch: 2.83 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031900456424150195		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.031900456424150195 | validation: 0.04892915274993492]
	TIME [epoch: 2.84 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03799469912050335		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.03799469912050335 | validation: 0.06642129783943317]
	TIME [epoch: 2.84 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04056215863295552		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.04056215863295552 | validation: 0.04745511417265962]
	TIME [epoch: 2.84 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0317575246058002		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.0317575246058002 | validation: 0.04782402456360199]
	TIME [epoch: 2.84 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031140088898175877		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.031140088898175877 | validation: 0.04094786346179594]
	TIME [epoch: 2.84 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032167065640886956		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.032167065640886956 | validation: 0.059514281832227955]
	TIME [epoch: 2.83 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033751477841409694		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.033751477841409694 | validation: 0.05184186994823681]
	TIME [epoch: 2.83 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03887978255741977		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.03887978255741977 | validation: 0.05562075051971607]
	TIME [epoch: 2.83 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03796600244834988		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.03796600244834988 | validation: 0.0746993718710263]
	TIME [epoch: 2.83 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04521128343598326		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.04521128343598326 | validation: 0.03729379886404382]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03790689557080006		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.03790689557080006 | validation: 0.057339096125185396]
	TIME [epoch: 2.81 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031055117915633597		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.031055117915633597 | validation: 0.05474147303501875]
	TIME [epoch: 2.81 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03222552112590062		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.03222552112590062 | validation: 0.05503989783674576]
	TIME [epoch: 2.81 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03892133903151947		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.03892133903151947 | validation: 0.06759538274873653]
	TIME [epoch: 2.84 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084838681279369		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.04084838681279369 | validation: 0.0555734502502573]
	TIME [epoch: 2.83 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037138793394664464		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.037138793394664464 | validation: 0.05142133492845361]
	TIME [epoch: 2.83 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029399122123919045		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.029399122123919045 | validation: 0.050166122328677065]
	TIME [epoch: 2.84 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030287410362440096		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.030287410362440096 | validation: 0.039226761993617854]
	TIME [epoch: 2.83 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929919974498927		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.03929919974498927 | validation: 0.07039348537684913]
	TIME [epoch: 2.86 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996365305081359		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.03996365305081359 | validation: 0.044021829687707736]
	TIME [epoch: 2.83 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04562759462642187		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.04562759462642187 | validation: 0.0797599620925572]
	TIME [epoch: 2.83 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04500229546343744		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.04500229546343744 | validation: 0.056425228096201874]
	TIME [epoch: 2.83 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030323899413883526		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.030323899413883526 | validation: 0.04244604897544725]
	TIME [epoch: 2.83 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026842765026670167		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.026842765026670167 | validation: 0.05026692189361024]
	TIME [epoch: 2.83 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025930998748607152		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.025930998748607152 | validation: 0.04922448723075691]
	TIME [epoch: 2.83 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02856492081040547		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.02856492081040547 | validation: 0.06490720839313459]
	TIME [epoch: 2.83 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03600248859554389		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.03600248859554389 | validation: 0.05089781102963972]
	TIME [epoch: 2.83 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03837637721513274		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.03837637721513274 | validation: 0.06784781899154867]
	TIME [epoch: 2.83 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881759124620436		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.03881759124620436 | validation: 0.04425939329473937]
	TIME [epoch: 2.83 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027026049214780856		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.027026049214780856 | validation: 0.04539038357138018]
	TIME [epoch: 2.83 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026704262607056432		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.026704262607056432 | validation: 0.04395122643199454]
	TIME [epoch: 2.83 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027413308642113332		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.027413308642113332 | validation: 0.055885555614723685]
	TIME [epoch: 2.83 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03467229135477972		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.03467229135477972 | validation: 0.09095723792041316]
	TIME [epoch: 2.83 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05609004985338789		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.05609004985338789 | validation: 0.053382771354130026]
	TIME [epoch: 2.83 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02709402647712241		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.02709402647712241 | validation: 0.05534933936476483]
	TIME [epoch: 2.81 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04223400347393008		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.04223400347393008 | validation: 0.06387681249184289]
	TIME [epoch: 2.81 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04318238848063177		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.04318238848063177 | validation: 0.050731024183191656]
	TIME [epoch: 2.81 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02897254014957511		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.02897254014957511 | validation: 0.04920151780385708]
	TIME [epoch: 2.81 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03688490194730684		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.03688490194730684 | validation: 0.07913420657918888]
	TIME [epoch: 2.81 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0517068324391037		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.0517068324391037 | validation: 0.046507560798624004]
	TIME [epoch: 2.81 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04123130012050209		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.04123130012050209 | validation: 0.058130255302771355]
	TIME [epoch: 2.81 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03243584510331151		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.03243584510331151 | validation: 0.055280405120159797]
	TIME [epoch: 2.81 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031298597857437355		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.031298597857437355 | validation: 0.045821392740623225]
	TIME [epoch: 2.81 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159160981811693		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.04159160981811693 | validation: 0.05736329917908957]
	TIME [epoch: 2.81 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03989658557976744		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.03989658557976744 | validation: 0.04041681326700889]
	TIME [epoch: 2.81 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026764255511656343		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.026764255511656343 | validation: 0.043847844490227654]
	TIME [epoch: 2.81 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0249893049969967		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0249893049969967 | validation: 0.059380611079271985]
	TIME [epoch: 2.81 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03490472789768154		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.03490472789768154 | validation: 0.042338518538458304]
	TIME [epoch: 2.81 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030373072005233172		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.030373072005233172 | validation: 0.05034834901228052]
	TIME [epoch: 2.81 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02648952387506284		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.02648952387506284 | validation: 0.04467697376106682]
	TIME [epoch: 2.81 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024776503253620935		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.024776503253620935 | validation: 0.044362185501019814]
	TIME [epoch: 2.81 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023919653953741866		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.023919653953741866 | validation: 0.0374069176749114]
	TIME [epoch: 2.81 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02475464480405038		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.02475464480405038 | validation: 0.04316612280592039]
	TIME [epoch: 2.81 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024706890504072238		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.024706890504072238 | validation: 0.04814433057652404]
	TIME [epoch: 2.83 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02977741801779299		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.02977741801779299 | validation: 0.04799421073016859]
	TIME [epoch: 2.84 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035847356280733475		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.035847356280733475 | validation: 0.0915007177463827]
	TIME [epoch: 2.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053755645195692314		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.053755645195692314 | validation: 0.04368575528390746]
	TIME [epoch: 2.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764751779604265		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.02764751779604265 | validation: 0.05372444008601479]
	TIME [epoch: 2.83 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03203061895855354		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.03203061895855354 | validation: 0.0477206373417817]
	TIME [epoch: 2.81 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03474323729521701		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.03474323729521701 | validation: 0.055778141754006465]
	TIME [epoch: 2.81 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029087615554595603		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.029087615554595603 | validation: 0.03498063737514712]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028661033529593087		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.028661033529593087 | validation: 0.04746813888902986]
	TIME [epoch: 2.83 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028489442070532892		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.028489442070532892 | validation: 0.04135950482619297]
	TIME [epoch: 2.83 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02831553360688359		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.02831553360688359 | validation: 0.05651506988390423]
	TIME [epoch: 2.83 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027423887095565078		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.027423887095565078 | validation: 0.03744239662349569]
	TIME [epoch: 2.83 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027074121337922162		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.027074121337922162 | validation: 0.04620636297722609]
	TIME [epoch: 2.83 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027320971958920196		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.027320971958920196 | validation: 0.04331746820690986]
	TIME [epoch: 2.83 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027855274795734124		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.027855274795734124 | validation: 0.060067771699232365]
	TIME [epoch: 2.83 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03454779229690831		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.03454779229690831 | validation: 0.038248670550044744]
	TIME [epoch: 2.83 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033172298774453865		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.033172298774453865 | validation: 0.05607510367351101]
	TIME [epoch: 2.83 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0320075588159589		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0320075588159589 | validation: 0.03359262829231176]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027407545197912372		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.027407545197912372 | validation: 0.0474252583836029]
	TIME [epoch: 2.81 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025657781896527558		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.025657781896527558 | validation: 0.03640093804114466]
	TIME [epoch: 2.81 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02550462122441436		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.02550462122441436 | validation: 0.043344753385543514]
	TIME [epoch: 2.81 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02428906965876762		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.02428906965876762 | validation: 0.04099242433086336]
	TIME [epoch: 2.81 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026563381540006623		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.026563381540006623 | validation: 0.04347274413476419]
	TIME [epoch: 2.81 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025359871812075264		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.025359871812075264 | validation: 0.04232873634640843]
	TIME [epoch: 2.81 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027340838667273742		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.027340838667273742 | validation: 0.05018129678760452]
	TIME [epoch: 2.81 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028430993474867404		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.028430993474867404 | validation: 0.04089810733518463]
	TIME [epoch: 2.81 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03583367973845753		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.03583367973845753 | validation: 0.1237873338470291]
	TIME [epoch: 2.81 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0692606188406198		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.0692606188406198 | validation: 0.046467636470337186]
	TIME [epoch: 2.83 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223958542361674		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.03223958542361674 | validation: 0.05821347393072839]
	TIME [epoch: 2.82 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039683062925770896		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.039683062925770896 | validation: 0.04871711101223787]
	TIME [epoch: 2.82 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026751437240329405		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.026751437240329405 | validation: 0.039430604988041046]
	TIME [epoch: 2.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028565824749642813		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.028565824749642813 | validation: 0.05128109271371156]
	TIME [epoch: 2.82 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027610139208210357		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.027610139208210357 | validation: 0.03991610531271135]
	TIME [epoch: 2.82 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026290029572143544		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.026290029572143544 | validation: 0.03548433831596463]
	TIME [epoch: 2.83 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024447437532688937		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.024447437532688937 | validation: 0.04163750155603229]
	TIME [epoch: 2.82 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023358541338314526		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.023358541338314526 | validation: 0.03685487221273838]
	TIME [epoch: 2.83 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023378884477156303		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.023378884477156303 | validation: 0.04649866162312184]
	TIME [epoch: 2.82 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023320748732576963		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.023320748732576963 | validation: 0.03675004239766292]
	TIME [epoch: 2.82 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02402010920372526		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.02402010920372526 | validation: 0.04568446973345924]
	TIME [epoch: 2.83 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02545600432525184		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.02545600432525184 | validation: 0.03788145016224757]
	TIME [epoch: 2.83 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030192450415609307		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.030192450415609307 | validation: 0.06406186413045374]
	TIME [epoch: 2.82 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0315208867756349		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.0315208867756349 | validation: 0.03910190998897585]
	TIME [epoch: 2.82 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031441701846351595		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.031441701846351595 | validation: 0.05035595663534707]
	TIME [epoch: 2.83 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028898032776539986		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.028898032776539986 | validation: 0.03943549402887825]
	TIME [epoch: 2.82 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025626015570986264		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.025626015570986264 | validation: 0.04595915020411517]
	TIME [epoch: 2.82 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024129983677490436		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.024129983677490436 | validation: 0.03738462251472796]
	TIME [epoch: 2.82 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026842068904533733		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.026842068904533733 | validation: 0.0426943021162736]
	TIME [epoch: 2.82 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025493565260600506		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.025493565260600506 | validation: 0.036804348027230226]
	TIME [epoch: 2.82 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023946599238950192		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.023946599238950192 | validation: 0.049539775355090666]
	TIME [epoch: 2.82 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025290546838207126		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.025290546838207126 | validation: 0.03125265679054632]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_750.pth
	Model improved!!!
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027189477061711466		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.027189477061711466 | validation: 0.05726454655920078]
	TIME [epoch: 2.82 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027912561879572768		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.027912561879572768 | validation: 0.037784483198556956]
	TIME [epoch: 2.82 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025772368309381646		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.025772368309381646 | validation: 0.054887923780423425]
	TIME [epoch: 2.82 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028694933349616952		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.028694933349616952 | validation: 0.04228775280519136]
	TIME [epoch: 2.82 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02745629905299796		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.02745629905299796 | validation: 0.04428592870538589]
	TIME [epoch: 2.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026704777061932198		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.026704777061932198 | validation: 0.038792831881837755]
	TIME [epoch: 2.82 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025497483613913942		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.025497483613913942 | validation: 0.040116340524166155]
	TIME [epoch: 2.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02498188103949234		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.02498188103949234 | validation: 0.041050131409467575]
	TIME [epoch: 2.83 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024618788619998527		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.024618788619998527 | validation: 0.046830329111525196]
	TIME [epoch: 2.83 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030616886035836687		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.030616886035836687 | validation: 0.04470374084964973]
	TIME [epoch: 2.82 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029649306965238712		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.029649306965238712 | validation: 0.03936508290044536]
	TIME [epoch: 2.83 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027038682110120088		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.027038682110120088 | validation: 0.03930715790561723]
	TIME [epoch: 2.82 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02299910204804399		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.02299910204804399 | validation: 0.04168143435152506]
	TIME [epoch: 2.83 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022765980765052155		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.022765980765052155 | validation: 0.030732540871710113]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_764.pth
	Model improved!!!
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029251902825644188		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.029251902825644188 | validation: 0.054196859616459585]
	TIME [epoch: 2.83 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029941659004504705		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.029941659004504705 | validation: 0.03443429136632408]
	TIME [epoch: 2.83 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02571252536564958		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.02571252536564958 | validation: 0.041390032026370205]
	TIME [epoch: 2.82 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023679930810601605		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.023679930810601605 | validation: 0.03747854781888689]
	TIME [epoch: 2.82 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023817388316620072		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.023817388316620072 | validation: 0.04509475897145535]
	TIME [epoch: 2.82 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02183312896734912		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.02183312896734912 | validation: 0.029553127029934834]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_770.pth
	Model improved!!!
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02393533740344854		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.02393533740344854 | validation: 0.04303926993731097]
	TIME [epoch: 2.82 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02245553568103288		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.02245553568103288 | validation: 0.0377125062609754]
	TIME [epoch: 2.82 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02603428244144017		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.02603428244144017 | validation: 0.06233953511627164]
	TIME [epoch: 2.82 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033044336746788916		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.033044336746788916 | validation: 0.04054008563945195]
	TIME [epoch: 2.82 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028930731527361923		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.028930731527361923 | validation: 0.04311720503310491]
	TIME [epoch: 2.82 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02683757939333287		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.02683757939333287 | validation: 0.03015480284124629]
	TIME [epoch: 2.82 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022329560207390964		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.022329560207390964 | validation: 0.03549056189718269]
	TIME [epoch: 2.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02267870347499315		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.02267870347499315 | validation: 0.038853456400447954]
	TIME [epoch: 2.82 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024574549234134376		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.024574549234134376 | validation: 0.03408013183456556]
	TIME [epoch: 2.83 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023512970779541145		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.023512970779541145 | validation: 0.043318081613115414]
	TIME [epoch: 2.83 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02325660394411874		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.02325660394411874 | validation: 0.029816284566922948]
	TIME [epoch: 2.82 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022382136983622198		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.022382136983622198 | validation: 0.04553355050606731]
	TIME [epoch: 2.83 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025278453351184026		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.025278453351184026 | validation: 0.035003910112858805]
	TIME [epoch: 2.82 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024262276832818588		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.024262276832818588 | validation: 0.0359073557681492]
	TIME [epoch: 2.82 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024086651462578473		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.024086651462578473 | validation: 0.043702082573137714]
	TIME [epoch: 2.82 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026507611354347916		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.026507611354347916 | validation: 0.04029902390321152]
	TIME [epoch: 2.82 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02438979073200583		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.02438979073200583 | validation: 0.05948462688854559]
	TIME [epoch: 2.82 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033529956942373165		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.033529956942373165 | validation: 0.03253794555782968]
	TIME [epoch: 2.82 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028782654322931256		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.028782654322931256 | validation: 0.04728233003069076]
	TIME [epoch: 2.82 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02736285984846653		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.02736285984846653 | validation: 0.028197786604023802]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02485962821501448		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.02485962821501448 | validation: 0.028105186229002568]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_791.pth
	Model improved!!!
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019961897207435582		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.019961897207435582 | validation: 0.039018994337598194]
	TIME [epoch: 2.83 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020087967522054474		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.020087967522054474 | validation: 0.03648253395613049]
	TIME [epoch: 2.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0204053126897695		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.0204053126897695 | validation: 0.033303014746153006]
	TIME [epoch: 2.83 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02090969179663852		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.02090969179663852 | validation: 0.0329169741919211]
	TIME [epoch: 2.83 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021723956751593124		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.021723956751593124 | validation: 0.04680647155937126]
	TIME [epoch: 2.83 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02476979581746276		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.02476979581746276 | validation: 0.02817375794503827]
	TIME [epoch: 2.83 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02951213944350336		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.02951213944350336 | validation: 0.04467726196715293]
	TIME [epoch: 2.83 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02428996492594675		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.02428996492594675 | validation: 0.037682755672869585]
	TIME [epoch: 2.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025234691821920265		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.025234691821920265 | validation: 0.051872893834487656]
	TIME [epoch: 2.82 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02913567474217693		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.02913567474217693 | validation: 0.030217760981875785]
	TIME [epoch: 2.82 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02326989661520294		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.02326989661520294 | validation: 0.04009296856016958]
	TIME [epoch: 2.83 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02269912087099275		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.02269912087099275 | validation: 0.029932117587360932]
	TIME [epoch: 2.82 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02132005287374382		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.02132005287374382 | validation: 0.03546942205180342]
	TIME [epoch: 2.82 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02083863305559782		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.02083863305559782 | validation: 0.04317174506045082]
	TIME [epoch: 2.82 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025143807703171595		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.025143807703171595 | validation: 0.03298284724291433]
	TIME [epoch: 2.83 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025007546738595635		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.025007546738595635 | validation: 0.040785680762943824]
	TIME [epoch: 2.82 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0218263906716549		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.0218263906716549 | validation: 0.03372398468722373]
	TIME [epoch: 2.82 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020989113812471812		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.020989113812471812 | validation: 0.0379071370272929]
	TIME [epoch: 2.83 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02215170049235682		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.02215170049235682 | validation: 0.0346472553032424]
	TIME [epoch: 2.82 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021237410985529368		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.021237410985529368 | validation: 0.028642090563974678]
	TIME [epoch: 2.82 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022135339927625557		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.022135339927625557 | validation: 0.04412316851732952]
	TIME [epoch: 2.82 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023770907366848868		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.023770907366848868 | validation: 0.031976002495922244]
	TIME [epoch: 2.82 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023775143055153755		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.023775143055153755 | validation: 0.044631645929915136]
	TIME [epoch: 2.82 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023447583400449716		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.023447583400449716 | validation: 0.029677435568672264]
	TIME [epoch: 2.83 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021749361752402923		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.021749361752402923 | validation: 0.035395022805677294]
	TIME [epoch: 2.83 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039317935946112		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.02039317935946112 | validation: 0.02899158581909488]
	TIME [epoch: 2.83 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024143245957972112		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.024143245957972112 | validation: 0.047362578297651586]
	TIME [epoch: 2.83 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028322775856772175		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.028322775856772175 | validation: 0.033852478352025664]
	TIME [epoch: 2.83 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024967963740040712		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.024967963740040712 | validation: 0.036731804292576886]
	TIME [epoch: 2.83 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02009039313257562		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.02009039313257562 | validation: 0.031148601518246014]
	TIME [epoch: 2.83 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020029140400222332		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.020029140400222332 | validation: 0.028948334304835288]
	TIME [epoch: 2.82 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02074811455528203		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.02074811455528203 | validation: 0.050346764011774764]
	TIME [epoch: 2.83 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027228536735715674		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.027228536735715674 | validation: 0.03175129143203078]
	TIME [epoch: 2.82 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024604969643315117		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.024604969643315117 | validation: 0.03385789524534468]
	TIME [epoch: 2.83 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019759229630692862		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.019759229630692862 | validation: 0.032352188052658874]
	TIME [epoch: 2.82 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021031514029553826		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.021031514029553826 | validation: 0.03360842082399584]
	TIME [epoch: 2.83 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01946125258894242		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.01946125258894242 | validation: 0.03513888252334651]
	TIME [epoch: 2.82 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01939687577890997		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.01939687577890997 | validation: 0.03397632451603107]
	TIME [epoch: 2.82 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019603021952903765		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.019603021952903765 | validation: 0.03273969276070744]
	TIME [epoch: 2.82 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020242810156751886		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.020242810156751886 | validation: 0.04257235609056871]
	TIME [epoch: 2.82 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0250425563133813		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.0250425563133813 | validation: 0.04308298196581557]
	TIME [epoch: 2.83 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02782695313228974		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.02782695313228974 | validation: 0.03763086075412926]
	TIME [epoch: 2.83 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0226880891438942		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.0226880891438942 | validation: 0.02599275442065159]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019555343615026526		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.019555343615026526 | validation: 0.04160831924964006]
	TIME [epoch: 2.82 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019546189766058892		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.019546189766058892 | validation: 0.029877058619909025]
	TIME [epoch: 2.82 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021274983481240402		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.021274983481240402 | validation: 0.03674117663751282]
	TIME [epoch: 2.82 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0227341395038945		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.0227341395038945 | validation: 0.028841206844651114]
	TIME [epoch: 2.82 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021778233442483596		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.021778233442483596 | validation: 0.037982552022585804]
	TIME [epoch: 2.82 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022122754218473813		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.022122754218473813 | validation: 0.0347780844142098]
	TIME [epoch: 2.83 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025837751157078603		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.025837751157078603 | validation: 0.043899444752090434]
	TIME [epoch: 2.83 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026540048131291877		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.026540048131291877 | validation: 0.03328781246821697]
	TIME [epoch: 2.83 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0198291278218309		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.0198291278218309 | validation: 0.031737419996961856]
	TIME [epoch: 2.82 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018744397648032655		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.018744397648032655 | validation: 0.0377970854314451]
	TIME [epoch: 2.83 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020264054506466872		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.020264054506466872 | validation: 0.027469725794030755]
	TIME [epoch: 2.82 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018375508588619072		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.018375508588619072 | validation: 0.03125406830779084]
	TIME [epoch: 2.83 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01813029826255054		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.01813029826255054 | validation: 0.028357410972769583]
	TIME [epoch: 2.82 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018278233156135203		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.018278233156135203 | validation: 0.027370204419617208]
	TIME [epoch: 2.82 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02049529709122885		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.02049529709122885 | validation: 0.040393885939142726]
	TIME [epoch: 2.82 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0228732055802113		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.0228732055802113 | validation: 0.028016001547286964]
	TIME [epoch: 2.83 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024905862588145106		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.024905862588145106 | validation: 0.04515149826640372]
	TIME [epoch: 2.82 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025041898426271757		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.025041898426271757 | validation: 0.027790351502131716]
	TIME [epoch: 2.82 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020506176199361372		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.020506176199361372 | validation: 0.03541980410955386]
	TIME [epoch: 2.83 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019742713425173527		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.019742713425173527 | validation: 0.03638964580370825]
	TIME [epoch: 2.83 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020670607370317066		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.020670607370317066 | validation: 0.028205559767706126]
	TIME [epoch: 2.82 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021141343595716705		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.021141343595716705 | validation: 0.03264864560533148]
	TIME [epoch: 2.83 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01892009110586807		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.01892009110586807 | validation: 0.029319011793886254]
	TIME [epoch: 2.82 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018526765718677122		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.018526765718677122 | validation: 0.02439358367292126]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_858.pth
	Model improved!!!
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019398615769459757		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.019398615769459757 | validation: 0.03530879068329447]
	TIME [epoch: 2.82 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01888133838103688		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.01888133838103688 | validation: 0.029139779666529544]
	TIME [epoch: 2.82 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01910708976522258		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.01910708976522258 | validation: 0.041675765761361276]
	TIME [epoch: 2.82 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02180236111961559		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.02180236111961559 | validation: 0.033559593311684933]
	TIME [epoch: 2.82 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02626291655885211		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.02626291655885211 | validation: 0.04469982733211733]
	TIME [epoch: 2.82 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028200790293911985		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.028200790293911985 | validation: 0.030000471488439963]
	TIME [epoch: 2.82 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020573568420589706		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.020573568420589706 | validation: 0.03043398847064334]
	TIME [epoch: 2.82 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01774112412754764		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.01774112412754764 | validation: 0.03075217819605094]
	TIME [epoch: 2.82 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018178379589933586		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.018178379589933586 | validation: 0.02737641775421177]
	TIME [epoch: 2.82 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01750096182118319		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.01750096182118319 | validation: 0.03357961408025991]
	TIME [epoch: 2.82 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019615830553734063		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.019615830553734063 | validation: 0.02879258565749442]
	TIME [epoch: 2.82 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01911508832580056		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.01911508832580056 | validation: 0.027074400301207537]
	TIME [epoch: 2.83 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018880502253840522		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.018880502253840522 | validation: 0.036477862645428315]
	TIME [epoch: 2.82 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020243378737780957		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.020243378737780957 | validation: 0.026614415810331818]
	TIME [epoch: 2.83 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020030280339170453		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.020030280339170453 | validation: 0.03263050218796311]
	TIME [epoch: 2.83 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019438793725107963		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.019438793725107963 | validation: 0.029586511062137357]
	TIME [epoch: 2.83 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020425031693733563		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.020425031693733563 | validation: 0.030426084255002418]
	TIME [epoch: 2.83 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02615944916246701		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.02615944916246701 | validation: 0.0543978331718025]
	TIME [epoch: 2.83 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028557021990324696		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.028557021990324696 | validation: 0.028044807766435444]
	TIME [epoch: 2.83 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02011288149392059		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.02011288149392059 | validation: 0.029086464233435083]
	TIME [epoch: 2.83 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019003934110732663		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.019003934110732663 | validation: 0.038093635033446926]
	TIME [epoch: 2.83 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02008462137507256		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.02008462137507256 | validation: 0.02663945659331093]
	TIME [epoch: 2.82 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019271883248440484		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.019271883248440484 | validation: 0.02864339526440174]
	TIME [epoch: 2.82 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017956069675115996		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.017956069675115996 | validation: 0.03138539037828684]
	TIME [epoch: 2.83 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016965901703391503		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.016965901703391503 | validation: 0.025355283416962606]
	TIME [epoch: 2.82 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897387557836275		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.01897387557836275 | validation: 0.03702445642325262]
	TIME [epoch: 2.82 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019197308052952042		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.019197308052952042 | validation: 0.029442445716677236]
	TIME [epoch: 2.83 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019637493090126027		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.019637493090126027 | validation: 0.032852091088403444]
	TIME [epoch: 2.83 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020317081506784396		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.020317081506784396 | validation: 0.023192136692588662]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020268698336314853		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.020268698336314853 | validation: 0.039118936180977296]
	TIME [epoch: 2.83 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022074575541592225		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.022074575541592225 | validation: 0.028505575920891726]
	TIME [epoch: 2.83 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022080551900645666		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.022080551900645666 | validation: 0.038248523509157986]
	TIME [epoch: 2.83 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019151731428059388		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.019151731428059388 | validation: 0.023156101924535512]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_891.pth
	Model improved!!!
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0182257535248819		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.0182257535248819 | validation: 0.03117814394001254]
	TIME [epoch: 2.83 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017373343564135817		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.017373343564135817 | validation: 0.02409612156045106]
	TIME [epoch: 2.83 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017715852768825698		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.017715852768825698 | validation: 0.032930461290880224]
	TIME [epoch: 2.82 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018675080128259924		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.018675080128259924 | validation: 0.02568739654171841]
	TIME [epoch: 2.83 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02002586856131302		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.02002586856131302 | validation: 0.040700602083851546]
	TIME [epoch: 2.83 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021157129419905815		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.021157129419905815 | validation: 0.025017761727379052]
	TIME [epoch: 2.83 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018354608778248207		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.018354608778248207 | validation: 0.03109497371860467]
	TIME [epoch: 2.83 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021331285498851136		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.021331285498851136 | validation: 0.0295053029205026]
	TIME [epoch: 2.83 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02057842796028639		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.02057842796028639 | validation: 0.0291319129615205]
	TIME [epoch: 2.83 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01893117170599513		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.01893117170599513 | validation: 0.03579257380960593]
	TIME [epoch: 2.83 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01921686955618316		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.01921686955618316 | validation: 0.02834686554221323]
	TIME [epoch: 2.83 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017224959395630665		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.017224959395630665 | validation: 0.023458254592295558]
	TIME [epoch: 2.83 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01770473883728974		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.01770473883728974 | validation: 0.040402631411836525]
	TIME [epoch: 2.83 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022557607037280256		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.022557607037280256 | validation: 0.021945714601248747]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_905.pth
	Model improved!!!
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023460926858379888		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.023460926858379888 | validation: 0.037688566081566025]
	TIME [epoch: 2.83 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02115997155509056		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.02115997155509056 | validation: 0.02737250392045089]
	TIME [epoch: 2.83 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018030351815260654		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.018030351815260654 | validation: 0.03173832725930621]
	TIME [epoch: 2.83 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017130901544615967		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.017130901544615967 | validation: 0.0287762217730428]
	TIME [epoch: 2.83 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016374375451391407		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.016374375451391407 | validation: 0.026947881367832528]
	TIME [epoch: 2.83 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017426478267366794		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.017426478267366794 | validation: 0.03180274068268135]
	TIME [epoch: 2.83 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017470245421925382		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.017470245421925382 | validation: 0.02593229776760572]
	TIME [epoch: 2.83 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016763745569143066		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.016763745569143066 | validation: 0.026748597754964156]
	TIME [epoch: 2.83 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017778250008257702		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.017778250008257702 | validation: 0.035703312081918015]
	TIME [epoch: 2.83 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019697929861978047		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.019697929861978047 | validation: 0.022456814925194347]
	TIME [epoch: 2.82 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022202490736412703		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.022202490736412703 | validation: 0.03660085580907029]
	TIME [epoch: 2.83 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022441389061694305		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.022441389061694305 | validation: 0.033320532811554573]
	TIME [epoch: 2.82 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020583569126957517		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.020583569126957517 | validation: 0.0282845178790503]
	TIME [epoch: 2.82 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01819974881750007		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.01819974881750007 | validation: 0.026578502480682477]
	TIME [epoch: 2.83 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01668413385629978		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.01668413385629978 | validation: 0.02511266588977601]
	TIME [epoch: 2.82 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017705009389804408		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.017705009389804408 | validation: 0.03650114776978102]
	TIME [epoch: 2.83 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020152789567019665		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.020152789567019665 | validation: 0.024086974126151364]
	TIME [epoch: 2.82 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01789431506555079		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.01789431506555079 | validation: 0.03025215271927231]
	TIME [epoch: 2.82 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01697709089782026		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.01697709089782026 | validation: 0.026335001578468242]
	TIME [epoch: 2.83 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016708371796917768		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.016708371796917768 | validation: 0.028589758891472195]
	TIME [epoch: 2.83 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017289632873199207		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.017289632873199207 | validation: 0.025578171552161223]
	TIME [epoch: 2.83 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017555050132184773		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.017555050132184773 | validation: 0.027431789251929017]
	TIME [epoch: 2.83 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017742186522041455		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.017742186522041455 | validation: 0.027680191312041816]
	TIME [epoch: 2.83 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017956108623261684		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.017956108623261684 | validation: 0.02650036051780922]
	TIME [epoch: 2.83 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017389219294072847		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.017389219294072847 | validation: 0.03835197192774449]
	TIME [epoch: 2.83 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022354571947681038		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.022354571947681038 | validation: 0.028116162943977875]
	TIME [epoch: 2.83 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022015355179989628		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.022015355179989628 | validation: 0.03762519678920393]
	TIME [epoch: 2.83 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018836015308590002		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.018836015308590002 | validation: 0.02616005007684448]
	TIME [epoch: 2.83 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018705307292749492		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.018705307292749492 | validation: 0.02421825287760252]
	TIME [epoch: 2.83 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016326705634028917		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.016326705634028917 | validation: 0.031872307403491666]
	TIME [epoch: 2.83 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017242451025971245		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.017242451025971245 | validation: 0.03188567171947121]
	TIME [epoch: 2.83 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01629121147111946		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.01629121147111946 | validation: 0.02597240442226513]
	TIME [epoch: 2.83 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018533431858345824		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.018533431858345824 | validation: 0.03448193193034226]
	TIME [epoch: 2.83 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01762899554822407		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.01762899554822407 | validation: 0.022427189153707828]
	TIME [epoch: 2.83 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0175883789476182		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.0175883789476182 | validation: 0.04014638100233369]
	TIME [epoch: 2.82 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018875658699334327		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.018875658699334327 | validation: 0.024605927125094674]
	TIME [epoch: 2.83 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016205872426440095		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.016205872426440095 | validation: 0.028104566969074464]
	TIME [epoch: 2.82 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017325398339240336		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.017325398339240336 | validation: 0.029139043709762027]
	TIME [epoch: 2.83 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016949886323756075		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.016949886323756075 | validation: 0.02661423421333663]
	TIME [epoch: 2.82 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017532483220113554		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.017532483220113554 | validation: 0.03395301905597675]
	TIME [epoch: 2.83 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01896158893059859		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.01896158893059859 | validation: 0.025760588716137636]
	TIME [epoch: 2.83 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020493421367841602		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.020493421367841602 | validation: 0.03540408594408959]
	TIME [epoch: 2.83 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018708128276719378		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.018708128276719378 | validation: 0.022176774142712442]
	TIME [epoch: 2.83 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017356084941430847		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.017356084941430847 | validation: 0.032930319214565164]
	TIME [epoch: 2.83 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017408364773035718		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.017408364773035718 | validation: 0.021630302561449512]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_950.pth
	Model improved!!!
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018316793414430793		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.018316793414430793 | validation: 0.028501044795923205]
	TIME [epoch: 2.83 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019004639839495543		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.019004639839495543 | validation: 0.024222533758841848]
	TIME [epoch: 2.82 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017104358840629587		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.017104358840629587 | validation: 0.027877483404078387]
	TIME [epoch: 2.82 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016192641740758167		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.016192641740758167 | validation: 0.028600201500798474]
	TIME [epoch: 2.83 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015662997083275725		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.015662997083275725 | validation: 0.025150039100409305]
	TIME [epoch: 2.82 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017164077711216388		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.017164077711216388 | validation: 0.03006608752274692]
	TIME [epoch: 2.83 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016358656988014503		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.016358656988014503 | validation: 0.030407131925776466]
	TIME [epoch: 2.82 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01651801429055423		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.01651801429055423 | validation: 0.032118397442463996]
	TIME [epoch: 2.83 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01597392261636436		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.01597392261636436 | validation: 0.02702047339128315]
	TIME [epoch: 2.83 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016162635114602752		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.016162635114602752 | validation: 0.02751240524381833]
	TIME [epoch: 2.82 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016847961623937306		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.016847961623937306 | validation: 0.02840878151195817]
	TIME [epoch: 2.82 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016514413396666667		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.016514413396666667 | validation: 0.024185368584121215]
	TIME [epoch: 2.82 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02099952823200562		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.02099952823200562 | validation: 0.04213356327840341]
	TIME [epoch: 2.82 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024697232627949938		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.024697232627949938 | validation: 0.028384062346188812]
	TIME [epoch: 2.82 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016899188654312947		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.016899188654312947 | validation: 0.023743679747283808]
	TIME [epoch: 2.83 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0160441245551961		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.0160441245551961 | validation: 0.027477397968721395]
	TIME [epoch: 2.82 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01725210835157868		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.01725210835157868 | validation: 0.027220403214978484]
	TIME [epoch: 2.82 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016820437087084375		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.016820437087084375 | validation: 0.02806832781040347]
	TIME [epoch: 2.82 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015772408399421733		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.015772408399421733 | validation: 0.02287924676015878]
	TIME [epoch: 2.82 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015388139226561659		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.015388139226561659 | validation: 0.023649811108581376]
	TIME [epoch: 2.82 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01652765418016343		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.01652765418016343 | validation: 0.03181847601397031]
	TIME [epoch: 2.83 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016977647513736763		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.016977647513736763 | validation: 0.02185133771746225]
	TIME [epoch: 2.82 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016525041245220938		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.016525041245220938 | validation: 0.021917772570032593]
	TIME [epoch: 2.82 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01718293703916291		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.01718293703916291 | validation: 0.035922270408186165]
	TIME [epoch: 2.82 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01979422422896636		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.01979422422896636 | validation: 0.021493342943905515]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_975.pth
	Model improved!!!
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01816245208202192		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.01816245208202192 | validation: 0.02449294604282184]
	TIME [epoch: 2.82 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01568451630680206		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.01568451630680206 | validation: 0.028526824931752973]
	TIME [epoch: 2.82 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016322506789101198		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.016322506789101198 | validation: 0.02112841751086855]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_978.pth
	Model improved!!!
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01714474511882297		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.01714474511882297 | validation: 0.0350339157513605]
	TIME [epoch: 2.82 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01923681870398556		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.01923681870398556 | validation: 0.02884691347908447]
	TIME [epoch: 2.82 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01723502000173938		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.01723502000173938 | validation: 0.02703439686991248]
	TIME [epoch: 2.82 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0166787012919172		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.0166787012919172 | validation: 0.03080292147661321]
	TIME [epoch: 2.82 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01571272767671305		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.01571272767671305 | validation: 0.025564679022497663]
	TIME [epoch: 2.82 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015522292975559163		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.015522292975559163 | validation: 0.02330027411378053]
	TIME [epoch: 2.82 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015678075625223102		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.015678075625223102 | validation: 0.026727313883978688]
	TIME [epoch: 2.82 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016260440660540335		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.016260440660540335 | validation: 0.019591961089719813]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_986.pth
	Model improved!!!
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017758588184506326		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.017758588184506326 | validation: 0.026434731643525267]
	TIME [epoch: 2.82 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016417947109889426		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.016417947109889426 | validation: 0.022613600222727705]
	TIME [epoch: 2.82 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01550203540045708		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.01550203540045708 | validation: 0.02439105234585817]
	TIME [epoch: 2.82 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015200090911819399		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.015200090911819399 | validation: 0.021284503523908517]
	TIME [epoch: 2.82 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015900009120712353		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.015900009120712353 | validation: 0.02961329244164698]
	TIME [epoch: 2.82 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017048974362943364		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.017048974362943364 | validation: 0.022067623101757194]
	TIME [epoch: 2.82 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018464010268374218		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.018464010268374218 | validation: 0.03516168909487736]
	TIME [epoch: 2.82 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016961202530384682		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.016961202530384682 | validation: 0.024601353971070074]
	TIME [epoch: 2.82 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015554901150962568		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.015554901150962568 | validation: 0.030585271155052187]
	TIME [epoch: 2.81 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0186077333946096		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.0186077333946096 | validation: 0.020232844537263973]
	TIME [epoch: 2.82 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015395261346277906		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.015395261346277906 | validation: 0.03131221261169357]
	TIME [epoch: 2.82 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015553224434788562		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.015553224434788562 | validation: 0.023119967550632472]
	TIME [epoch: 2.86 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016511708548528282		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.016511708548528282 | validation: 0.028496209647272633]
	TIME [epoch: 2.82 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01693923771753536		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.01693923771753536 | validation: 0.020955131832672294]
	TIME [epoch: 2.82 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014319941188395274		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.014319941188395274 | validation: 0.032869460577483876]
	TIME [epoch: 192 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016258863062341707		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.016258863062341707 | validation: 0.02589372092316408]
	TIME [epoch: 6.07 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016259697695579015		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.016259697695579015 | validation: 0.029661278506863266]
	TIME [epoch: 6.05 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014904197462388557		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.014904197462388557 | validation: 0.025640559917570097]
	TIME [epoch: 6.06 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015497385570159406		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.015497385570159406 | validation: 0.02644007660346123]
	TIME [epoch: 6.05 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015782449146388267		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.015782449146388267 | validation: 0.027750820507677233]
	TIME [epoch: 6.05 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01575087291509834		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.01575087291509834 | validation: 0.03426425308550448]
	TIME [epoch: 6.06 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01554239071197296		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.01554239071197296 | validation: 0.023763474508385486]
	TIME [epoch: 6.05 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015499040067520974		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.015499040067520974 | validation: 0.024776172104229777]
	TIME [epoch: 6.05 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014829741318426053		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.014829741318426053 | validation: 0.021791656547472206]
	TIME [epoch: 6.05 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015347045283821772		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.015347045283821772 | validation: 0.02859012599131047]
	TIME [epoch: 6.06 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01633136991709127		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.01633136991709127 | validation: 0.01997387214112231]
	TIME [epoch: 6.05 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017663128813926697		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.017663128813926697 | validation: 0.03194917683346693]
	TIME [epoch: 6.06 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015554377686838658		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.015554377686838658 | validation: 0.025567108298694376]
	TIME [epoch: 6.05 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01595686171621952		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.01595686171621952 | validation: 0.0213200043663235]
	TIME [epoch: 6.06 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018116073970642778		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.018116073970642778 | validation: 0.03209444397402742]
	TIME [epoch: 6.05 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017817134908329806		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.017817134908329806 | validation: 0.0205257657280324]
	TIME [epoch: 6.04 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017682213352275676		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.017682213352275676 | validation: 0.030858158377898015]
	TIME [epoch: 6.05 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01528526812722182		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.01528526812722182 | validation: 0.024213541510398465]
	TIME [epoch: 6.05 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014916558335817292		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.014916558335817292 | validation: 0.024640707837966205]
	TIME [epoch: 6.05 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016130373762737137		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.016130373762737137 | validation: 0.028747068250236565]
	TIME [epoch: 6.05 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01775047422534927		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.01775047422534927 | validation: 0.02223608255015952]
	TIME [epoch: 6.05 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014388388104543194		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.014388388104543194 | validation: 0.024174065594571626]
	TIME [epoch: 6.06 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016227622429765772		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.016227622429765772 | validation: 0.024748125616887176]
	TIME [epoch: 6.05 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015477934192463101		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.015477934192463101 | validation: 0.029134532902247802]
	TIME [epoch: 6.05 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016314457225732517		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.016314457225732517 | validation: 0.024567259672375455]
	TIME [epoch: 6.05 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015540155532175208		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.015540155532175208 | validation: 0.02646492002969071]
	TIME [epoch: 6.05 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01522426090478422		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.01522426090478422 | validation: 0.022598833439659606]
	TIME [epoch: 6.05 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015063951611712888		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.015063951611712888 | validation: 0.03022491060417766]
	TIME [epoch: 6.06 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016838317324368954		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.016838317324368954 | validation: 0.023117299991152654]
	TIME [epoch: 6.06 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016577040539041358		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.016577040539041358 | validation: 0.027737292005648097]
	TIME [epoch: 6.06 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015574795870347225		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.015574795870347225 | validation: 0.01843082226726387]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_1032.pth
	Model improved!!!
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014808955997493442		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.014808955997493442 | validation: 0.02454788769830012]
	TIME [epoch: 6.05 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014454381396260003		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.014454381396260003 | validation: 0.02674386685226481]
	TIME [epoch: 6.05 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016743113433442033		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.016743113433442033 | validation: 0.021339651560642103]
	TIME [epoch: 6.05 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01466523327303555		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.01466523327303555 | validation: 0.02520298803112099]
	TIME [epoch: 6.05 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014520494083597573		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.014520494083597573 | validation: 0.028702577810085873]
	TIME [epoch: 6.06 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014790712004941371		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.014790712004941371 | validation: 0.026641198066144657]
	TIME [epoch: 6.05 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01392917439602842		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.01392917439602842 | validation: 0.02086991825705209]
	TIME [epoch: 6.06 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01594385965439122		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.01594385965439122 | validation: 0.021389891799826678]
	TIME [epoch: 6.06 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014674011662200209		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.014674011662200209 | validation: 0.026621389978201827]
	TIME [epoch: 6.06 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015131347441827061		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.015131347441827061 | validation: 0.023008086891995672]
	TIME [epoch: 6.05 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01593541810624074		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.01593541810624074 | validation: 0.025781971774163137]
	TIME [epoch: 6.06 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014787203690453393		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.014787203690453393 | validation: 0.019895268715049]
	TIME [epoch: 6.05 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01782272560751301		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.01782272560751301 | validation: 0.036570780337343844]
	TIME [epoch: 6.06 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01669649946587378		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.01669649946587378 | validation: 0.020614606745653265]
	TIME [epoch: 6.06 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014974591442371494		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.014974591442371494 | validation: 0.02394665274456903]
	TIME [epoch: 6.07 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014505192732136499		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.014505192732136499 | validation: 0.02574412281240687]
	TIME [epoch: 6.05 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015743911303299128		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.015743911303299128 | validation: 0.026760240426727946]
	TIME [epoch: 6.06 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015742639422968396		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.015742639422968396 | validation: 0.027800678618167964]
	TIME [epoch: 6.05 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015323103322349098		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.015323103322349098 | validation: 0.02852701492181897]
	TIME [epoch: 6.06 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015803244855332726		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.015803244855332726 | validation: 0.0226532998290663]
	TIME [epoch: 6.05 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017474549162034825		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.017474549162034825 | validation: 0.03495870731686001]
	TIME [epoch: 6.05 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015283478760373047		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.015283478760373047 | validation: 0.03183763280968018]
	TIME [epoch: 6.05 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0156115043242679		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.0156115043242679 | validation: 0.025169276187861158]
	TIME [epoch: 6.05 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01571097622248979		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.01571097622248979 | validation: 0.026851190747820164]
	TIME [epoch: 6.05 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015144816248954236		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.015144816248954236 | validation: 0.024747362114643536]
	TIME [epoch: 6.05 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014208653530578173		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.014208653530578173 | validation: 0.024176834149601423]
	TIME [epoch: 6.05 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01484068065090097		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.01484068065090097 | validation: 0.02114714096362358]
	TIME [epoch: 6.06 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015479923712046704		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.015479923712046704 | validation: 0.026728098851975315]
	TIME [epoch: 6.05 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015097699037458423		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.015097699037458423 | validation: 0.02519475559590162]
	TIME [epoch: 6.05 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015163802729881458		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.015163802729881458 | validation: 0.03153101775857664]
	TIME [epoch: 6.05 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015906564382114494		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.015906564382114494 | validation: 0.02221772256253686]
	TIME [epoch: 6.05 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013759173367910984		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.013759173367910984 | validation: 0.020041758668304367]
	TIME [epoch: 6.05 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014634747975552033		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.014634747975552033 | validation: 0.026453022507787173]
	TIME [epoch: 6.05 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015043678168140118		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.015043678168140118 | validation: 0.020695299444520643]
	TIME [epoch: 6.05 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014604790911111032		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.014604790911111032 | validation: 0.02126361726593842]
	TIME [epoch: 6.05 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014468904043018187		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.014468904043018187 | validation: 0.025204787189737267]
	TIME [epoch: 6.05 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014135142646629215		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.014135142646629215 | validation: 0.02551740868130603]
	TIME [epoch: 6.05 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014926644722204525		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.014926644722204525 | validation: 0.023748617396172546]
	TIME [epoch: 6.06 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014561483157942074		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.014561483157942074 | validation: 0.021959879171576692]
	TIME [epoch: 6.05 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015200439946763735		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.015200439946763735 | validation: 0.029472045344244482]
	TIME [epoch: 6.05 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01728195942582205		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.01728195942582205 | validation: 0.021560657440506528]
	TIME [epoch: 6.05 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015457526897978236		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.015457526897978236 | validation: 0.02324405720261945]
	TIME [epoch: 6.06 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014615324960880104		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.014615324960880104 | validation: 0.02207411391086397]
	TIME [epoch: 6.06 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01379492841127652		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.01379492841127652 | validation: 0.028430993784999782]
	TIME [epoch: 6.05 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015090328602077754		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.015090328602077754 | validation: 0.01954049678098271]
	TIME [epoch: 6.05 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01571014159168462		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.01571014159168462 | validation: 0.02455814552305613]
	TIME [epoch: 6.05 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015814547162527403		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.015814547162527403 | validation: 0.026804671359504074]
	TIME [epoch: 6.05 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014704133413682784		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.014704133413682784 | validation: 0.02085394521977504]
	TIME [epoch: 6.04 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014593143330552795		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.014593143330552795 | validation: 0.023072966750225177]
	TIME [epoch: 6.05 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015102149873014385		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.015102149873014385 | validation: 0.025314818860224242]
	TIME [epoch: 6.04 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014560101531248752		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.014560101531248752 | validation: 0.02224649608186842]
	TIME [epoch: 6.06 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013387450402040764		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.013387450402040764 | validation: 0.0276421781606509]
	TIME [epoch: 6.05 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01436718357600278		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.01436718357600278 | validation: 0.028245538084940916]
	TIME [epoch: 6.06 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01416556099327186		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.01416556099327186 | validation: 0.019978047666771328]
	TIME [epoch: 6.05 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014861359157443327		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.014861359157443327 | validation: 0.023932466392681585]
	TIME [epoch: 6.05 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014033096835786312		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.014033096835786312 | validation: 0.02493362217959884]
	TIME [epoch: 6.05 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013110755245992325		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.013110755245992325 | validation: 0.018000600957434932]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_1089.pth
	Model improved!!!
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015813485294926		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.015813485294926 | validation: 0.026899181855823143]
	TIME [epoch: 6.06 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013888167319407563		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.013888167319407563 | validation: 0.026864389946732504]
	TIME [epoch: 6.05 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014152990177341845		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.014152990177341845 | validation: 0.01866247296281286]
	TIME [epoch: 6.05 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012833318864192677		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.012833318864192677 | validation: 0.022677067543719488]
	TIME [epoch: 6.06 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013727933678931143		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.013727933678931143 | validation: 0.02500553284294036]
	TIME [epoch: 6.06 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013859677846631032		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.013859677846631032 | validation: 0.020454420932811457]
	TIME [epoch: 6.06 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014665267834058422		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.014665267834058422 | validation: 0.02192039762591628]
	TIME [epoch: 6.05 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014288558609869082		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.014288558609869082 | validation: 0.026648702731857224]
	TIME [epoch: 6.05 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014490449314430186		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.014490449314430186 | validation: 0.02466649001193856]
	TIME [epoch: 6.05 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013698122621829305		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.013698122621829305 | validation: 0.024118619181277792]
	TIME [epoch: 6.04 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014133093419057162		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.014133093419057162 | validation: 0.01948114305448878]
	TIME [epoch: 6.06 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01425546773585873		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.01425546773585873 | validation: 0.027741256602441346]
	TIME [epoch: 6.05 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014593467269294532		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.014593467269294532 | validation: 0.020111150471737074]
	TIME [epoch: 6.06 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01404594921708715		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.01404594921708715 | validation: 0.022707014365097734]
	TIME [epoch: 6.05 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014074390969629794		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.014074390969629794 | validation: 0.026157371726123993]
	TIME [epoch: 6.06 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014013895993281178		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.014013895993281178 | validation: 0.023466876356022105]
	TIME [epoch: 6.05 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014742927371323052		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.014742927371323052 | validation: 0.023530227255491178]
	TIME [epoch: 6.06 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015186959949877817		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.015186959949877817 | validation: 0.02089464997421895]
	TIME [epoch: 6.05 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013604931194641554		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.013604931194641554 | validation: 0.020395456238070844]
	TIME [epoch: 6.06 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01391614647001391		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.01391614647001391 | validation: 0.023067218226661603]
	TIME [epoch: 6.06 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014276126023349621		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.014276126023349621 | validation: 0.021271838473970617]
	TIME [epoch: 6.08 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013168457397781542		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.013168457397781542 | validation: 0.020083294510387775]
	TIME [epoch: 6.07 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013503960980072907		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.013503960980072907 | validation: 0.023648494772358065]
	TIME [epoch: 6.08 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013946974860311948		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.013946974860311948 | validation: 0.0212436449255521]
	TIME [epoch: 6.06 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013590432660429976		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.013590432660429976 | validation: 0.023569076080901252]
	TIME [epoch: 6.07 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01461182932429742		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.01461182932429742 | validation: 0.021377641159259355]
	TIME [epoch: 6.06 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013345281064706747		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.013345281064706747 | validation: 0.02056065069294454]
	TIME [epoch: 6.07 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014581856678242064		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.014581856678242064 | validation: 0.033221442036555485]
	TIME [epoch: 6.06 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014854815907884343		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.014854815907884343 | validation: 0.02380769014315717]
	TIME [epoch: 6.06 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013764069104619456		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.013764069104619456 | validation: 0.025779912220827983]
	TIME [epoch: 6.06 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01363633635075714		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.01363633635075714 | validation: 0.020087051492516207]
	TIME [epoch: 6.07 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014021551775313208		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.014021551775313208 | validation: 0.030358294699609358]
	TIME [epoch: 6.06 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01471949533040656		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.01471949533040656 | validation: 0.015138876194897233]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_1122.pth
	Model improved!!!
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014475916720450994		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.014475916720450994 | validation: 0.020706881639698582]
	TIME [epoch: 6.06 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014017320421729308		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.014017320421729308 | validation: 0.03203966665646139]
	TIME [epoch: 6.06 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013894141526652914		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.013894141526652914 | validation: 0.02177074205202857]
	TIME [epoch: 6.06 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012884239944895008		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.012884239944895008 | validation: 0.027039411676313376]
	TIME [epoch: 6.06 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013777959317378848		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.013777959317378848 | validation: 0.02087109796451643]
	TIME [epoch: 6.06 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014494953651393905		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.014494953651393905 | validation: 0.020208233782483688]
	TIME [epoch: 6.06 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013470028262721452		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.013470028262721452 | validation: 0.02182135665080558]
	TIME [epoch: 6.05 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013627512423648845		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.013627512423648845 | validation: 0.024896499944449514]
	TIME [epoch: 6.07 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013566026042968755		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.013566026042968755 | validation: 0.024261756979363225]
	TIME [epoch: 6.05 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013656894429718531		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.013656894429718531 | validation: 0.017785090072346233]
	TIME [epoch: 6.06 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01490541071430676		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.01490541071430676 | validation: 0.022779745962409004]
	TIME [epoch: 6.05 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013362985429861994		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.013362985429861994 | validation: 0.020366064805194452]
	TIME [epoch: 6.05 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014303688959273008		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.014303688959273008 | validation: 0.019098441090438092]
	TIME [epoch: 6.05 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013616845995832117		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.013616845995832117 | validation: 0.0226552842744734]
	TIME [epoch: 6.05 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012491718275266886		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.012491718275266886 | validation: 0.023695259575227546]
	TIME [epoch: 6.06 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01387958123201017		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.01387958123201017 | validation: 0.026485971015839528]
	TIME [epoch: 6.06 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013888631207847437		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.013888631207847437 | validation: 0.022326227338341078]
	TIME [epoch: 6.06 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013327154750673417		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.013327154750673417 | validation: 0.022799678858187157]
	TIME [epoch: 6.05 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012840905659092896		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.012840905659092896 | validation: 0.018035444247069012]
	TIME [epoch: 6.04 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013236307069228773		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.013236307069228773 | validation: 0.02390510350292089]
	TIME [epoch: 6.05 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013369048516778786		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.013369048516778786 | validation: 0.02377613014898108]
	TIME [epoch: 6.04 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013228325199989701		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.013228325199989701 | validation: 0.018767999651440905]
	TIME [epoch: 6.05 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0141700018210615		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.0141700018210615 | validation: 0.02387043816167055]
	TIME [epoch: 6.05 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012547098426259852		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.012547098426259852 | validation: 0.027020434908084326]
	TIME [epoch: 6.05 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014341383504980099		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.014341383504980099 | validation: 0.01872262219938079]
	TIME [epoch: 6.05 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014631473454873273		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.014631473454873273 | validation: 0.029062868358653196]
	TIME [epoch: 6.06 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013384374403424167		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.013384374403424167 | validation: 0.024375338861245644]
	TIME [epoch: 6.04 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013264411962468895		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.013264411962468895 | validation: 0.01935576334111544]
	TIME [epoch: 6.04 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014913248175933905		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.014913248175933905 | validation: 0.031176887787429542]
	TIME [epoch: 6.04 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01423119360131421		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.01423119360131421 | validation: 0.02047250979482057]
	TIME [epoch: 6.04 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013390685257918596		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.013390685257918596 | validation: 0.022073383404507055]
	TIME [epoch: 6.04 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012167893147585727		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.012167893147585727 | validation: 0.018699570833290147]
	TIME [epoch: 6.04 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012531639605079805		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.012531639605079805 | validation: 0.021504962664812347]
	TIME [epoch: 6.04 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013397475007465229		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.013397475007465229 | validation: 0.01986831045331157]
	TIME [epoch: 6.04 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013050394420603007		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.013050394420603007 | validation: 0.02355956800131781]
	TIME [epoch: 6.05 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012336984872536302		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.012336984872536302 | validation: 0.023521024583557305]
	TIME [epoch: 6.05 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01391184736288275		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.01391184736288275 | validation: 0.016337152707503824]
	TIME [epoch: 6.05 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012391087042743321		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.012391087042743321 | validation: 0.021450864743808362]
	TIME [epoch: 6.05 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013954615747202963		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.013954615747202963 | validation: 0.020099006023259147]
	TIME [epoch: 6.04 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014232592288574023		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.014232592288574023 | validation: 0.025858241635764913]
	TIME [epoch: 6.04 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014638449677066236		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.014638449677066236 | validation: 0.022741378913557278]
	TIME [epoch: 6.05 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014960621756444494		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.014960621756444494 | validation: 0.021447484139065744]
	TIME [epoch: 6.05 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013315315358818629		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.013315315358818629 | validation: 0.01961862473995668]
	TIME [epoch: 6.05 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013589869559642658		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.013589869559642658 | validation: 0.020551556404293837]
	TIME [epoch: 6.05 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013554917746522161		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.013554917746522161 | validation: 0.022197329935544664]
	TIME [epoch: 6.05 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013048096097912154		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.013048096097912154 | validation: 0.01787790415448396]
	TIME [epoch: 6.05 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012288772366501024		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.012288772366501024 | validation: 0.020832163956593975]
	TIME [epoch: 6.04 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01329101903706814		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.01329101903706814 | validation: 0.018986733690421354]
	TIME [epoch: 6.05 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012478122725174496		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.012478122725174496 | validation: 0.02573553779202499]
	TIME [epoch: 6.05 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013362054398503083		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.013362054398503083 | validation: 0.025320024058809554]
	TIME [epoch: 6.05 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012741596464715385		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.012741596464715385 | validation: 0.01979697989321858]
	TIME [epoch: 6.05 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013273068167804356		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.013273068167804356 | validation: 0.018311070739650572]
	TIME [epoch: 6.05 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013670735353740868		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.013670735353740868 | validation: 0.021191961449029406]
	TIME [epoch: 6.05 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013164514501232367		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.013164514501232367 | validation: 0.022961001650769943]
	TIME [epoch: 6.05 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012645327810949656		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.012645327810949656 | validation: 0.01778173591580723]
	TIME [epoch: 6.04 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014392113776132007		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.014392113776132007 | validation: 0.022372913736798585]
	TIME [epoch: 6.04 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013537252653009691		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.013537252653009691 | validation: 0.021704915603590714]
	TIME [epoch: 6.04 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012960580536770015		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.012960580536770015 | validation: 0.024583310387649604]
	TIME [epoch: 6.04 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012832612588972375		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.012832612588972375 | validation: 0.021523645310686457]
	TIME [epoch: 6.04 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011762885290783317		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.011762885290783317 | validation: 0.01951871042672078]
	TIME [epoch: 6.04 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011747766529893013		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.011747766529893013 | validation: 0.022205243031479294]
	TIME [epoch: 6.04 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013554590076817035		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.013554590076817035 | validation: 0.023871896589527587]
	TIME [epoch: 6.05 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012312636648527678		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.012312636648527678 | validation: 0.022549025223962727]
	TIME [epoch: 6.04 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012616004782414798		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.012616004782414798 | validation: 0.01627510769822148]
	TIME [epoch: 6.04 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014728517204663108		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.014728517204663108 | validation: 0.020477146923732804]
	TIME [epoch: 6.05 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013043313719098368		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.013043313719098368 | validation: 0.024502023540280773]
	TIME [epoch: 6.05 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012680736271963143		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.012680736271963143 | validation: 0.022945781112968723]
	TIME [epoch: 6.05 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01380982581031199		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.01380982581031199 | validation: 0.022847224135444533]
	TIME [epoch: 6.04 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014004381779557021		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.014004381779557021 | validation: 0.02308942676797976]
	TIME [epoch: 6.05 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013268843399241122		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.013268843399241122 | validation: 0.020700942189978123]
	TIME [epoch: 6.05 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012881969731805552		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.012881969731805552 | validation: 0.02457035020955395]
	TIME [epoch: 6.05 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013591326482802507		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.013591326482802507 | validation: 0.021902896688094187]
	TIME [epoch: 6.05 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01342321433194758		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.01342321433194758 | validation: 0.02410129375397845]
	TIME [epoch: 6.05 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012426704388051362		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.012426704388051362 | validation: 0.020310607650804603]
	TIME [epoch: 6.05 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012477979907802342		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.012477979907802342 | validation: 0.01943858124336241]
	TIME [epoch: 6.05 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013107453960142132		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.013107453960142132 | validation: 0.019139070587057796]
	TIME [epoch: 6.05 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012220073022794278		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.012220073022794278 | validation: 0.019681308776417328]
	TIME [epoch: 6.05 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013271160860938195		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.013271160860938195 | validation: 0.023045796805326868]
	TIME [epoch: 6.05 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013872337408348565		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.013872337408348565 | validation: 0.02004279191725097]
	TIME [epoch: 6.05 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012983005557857724		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.012983005557857724 | validation: 0.02164339448686986]
	TIME [epoch: 6.05 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01197152240834916		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.01197152240834916 | validation: 0.01914047462595475]
	TIME [epoch: 6.04 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012404479566898009		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.012404479566898009 | validation: 0.025667190885308513]
	TIME [epoch: 6.04 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012440917007039679		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.012440917007039679 | validation: 0.019124630178054703]
	TIME [epoch: 6.04 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01316040109906986		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.01316040109906986 | validation: 0.02291392979296232]
	TIME [epoch: 6.05 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012607460665054986		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.012607460665054986 | validation: 0.02074209645636187]
	TIME [epoch: 6.04 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012565563210446528		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.012565563210446528 | validation: 0.0168326226992381]
	TIME [epoch: 6.04 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011709876276743594		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.011709876276743594 | validation: 0.024472029735080336]
	TIME [epoch: 6.05 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012138992408453647		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.012138992408453647 | validation: 0.026556073487781485]
	TIME [epoch: 6.05 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012606555856989894		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.012606555856989894 | validation: 0.023351678566238345]
	TIME [epoch: 6.05 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012056435764371085		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.012056435764371085 | validation: 0.020229966551236458]
	TIME [epoch: 6.05 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013741775010031368		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.013741775010031368 | validation: 0.018347858548291962]
	TIME [epoch: 6.04 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012763584713995134		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.012763584713995134 | validation: 0.02503931442272488]
	TIME [epoch: 6.05 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01349834257849508		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.01349834257849508 | validation: 0.021971319490614073]
	TIME [epoch: 6.04 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012812719306011786		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.012812719306011786 | validation: 0.021779295408531754]
	TIME [epoch: 6.04 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013441606511813536		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.013441606511813536 | validation: 0.02055128765373785]
	TIME [epoch: 6.05 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013237864293235454		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.013237864293235454 | validation: 0.02232436442002929]
	TIME [epoch: 6.05 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013675818380878134		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.013675818380878134 | validation: 0.024073814269636718]
	TIME [epoch: 6.05 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013427618724762832		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.013427618724762832 | validation: 0.018248237051146754]
	TIME [epoch: 6.05 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01256446497344227		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.01256446497344227 | validation: 0.022834109836286854]
	TIME [epoch: 6.05 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013237562916260659		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.013237562916260659 | validation: 0.02002106679958974]
	TIME [epoch: 6.04 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013380541258737635		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.013380541258737635 | validation: 0.020787846915639775]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_145623/states/model_phi1_4a_v_mmd1_1223.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4235.236 seconds.
