Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2268426740

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6583428614068465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6583428614068465 | validation: 4.54211036883475]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8041340509774098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8041340509774098 | validation: 6.953844262337295]
	TIME [epoch: 0.827 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.3486448253959775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3486448253959775 | validation: 6.425739233150198]
	TIME [epoch: 0.701 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.795217539732221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.795217539732221 | validation: 2.5260511811709003]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.397942353088561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.397942353088561 | validation: 2.9756157301610955]
	TIME [epoch: 0.709 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4339845877905284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4339845877905284 | validation: 3.082747020294719]
	TIME [epoch: 0.704 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.152209682394191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.152209682394191 | validation: 3.0310489411366017]
	TIME [epoch: 0.705 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6219977775667314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6219977775667314 | validation: 2.8486553999093043]
	TIME [epoch: 0.704 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3475662431158444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3475662431158444 | validation: 2.5542662928030753]
	TIME [epoch: 0.704 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1921106847823784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1921106847823784 | validation: 3.0368768154363255]
	TIME [epoch: 0.703 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7404537399820255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7404537399820255 | validation: 2.3838880473798345]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1829041196685095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1829041196685095 | validation: 2.1625333509847047]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9425624097083574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9425624097083574 | validation: 1.9173183233306448]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8210654436234712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8210654436234712 | validation: 1.8109465899885948]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7878519776693151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7878519776693151 | validation: 1.8804767143896457]
	TIME [epoch: 0.703 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7751897613182874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7751897613182874 | validation: 1.7069438494235265]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7332622494349255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7332622494349255 | validation: 1.734056558947768]
	TIME [epoch: 0.708 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6914722867382654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6914722867382654 | validation: 1.5507781015694004]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6492772674497291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6492772674497291 | validation: 1.6315104408726022]
	TIME [epoch: 0.709 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6396504170891957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6396504170891957 | validation: 1.490848434593974]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6213886012814414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6213886012814414 | validation: 1.6259602898333503]
	TIME [epoch: 0.705 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6511031992779506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6511031992779506 | validation: 1.4759899819649425]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5801397614594874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5801397614594874 | validation: 1.4872270007953756]
	TIME [epoch: 0.702 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5371548468384715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5371548468384715 | validation: 1.3389593592486484]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4745625828216549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4745625828216549 | validation: 1.3763750948872084]
	TIME [epoch: 0.703 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4441173472065947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4441173472065947 | validation: 1.2636540951357063]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.41860593477348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.41860593477348 | validation: 1.305675285150992]
	TIME [epoch: 0.704 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4163916089227546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4163916089227546 | validation: 1.2588083624158353]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4345711339676366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4345711339676366 | validation: 1.3756652053965273]
	TIME [epoch: 0.703 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5218530804694188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5218530804694188 | validation: 1.2249519148043504]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3859054814257241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3859054814257241 | validation: 1.164847231760224]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3152765197030192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3152765197030192 | validation: 1.0906604539706073]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2703573430407968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2703573430407968 | validation: 1.1530026931890944]
	TIME [epoch: 0.704 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.251671008662294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.251671008662294 | validation: 1.0387386124628823]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2277100448957794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2277100448957794 | validation: 1.0795862199558606]
	TIME [epoch: 0.702 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2343152176256664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2343152176256664 | validation: 1.0094755458894686]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2703375286475742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2703375286475742 | validation: 1.1735778782699542]
	TIME [epoch: 0.705 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3592443891848296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3592443891848296 | validation: 0.9912753152356903]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2222729641446881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2222729641446881 | validation: 0.9409081155930136]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1621237936880489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1621237936880489 | validation: 0.9710546485139725]
	TIME [epoch: 0.707 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124020716514865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.124020716514865 | validation: 0.9514057724586936]
	TIME [epoch: 0.705 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1054552693306285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1054552693306285 | validation: 0.9971117107568553]
	TIME [epoch: 0.703 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0785377613913256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0785377613913256 | validation: 1.1499286113919047]
	TIME [epoch: 0.701 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0836592475678746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0836592475678746 | validation: 0.933567204007114]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2093582022876297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2093582022876297 | validation: 1.4490191755724589]
	TIME [epoch: 0.705 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2508294198158578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2508294198158578 | validation: 0.7634800235064452]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0408509362317353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0408509362317353 | validation: 1.1426856161267607]
	TIME [epoch: 0.709 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1216080988747419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1216080988747419 | validation: 0.8232746555523622]
	TIME [epoch: 0.704 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1819356725429655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1819356725429655 | validation: 0.9579884940105189]
	TIME [epoch: 0.702 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9561572092975544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9561572092975544 | validation: 1.0246038528396566]
	TIME [epoch: 0.703 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9878858944192684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9878858944192684 | validation: 0.7472946696676381]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0621051732694522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0621051732694522 | validation: 1.008276424546774]
	TIME [epoch: 0.706 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9520083951604826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9520083951604826 | validation: 0.7401818029560664]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465155075798112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9465155075798112 | validation: 1.0510986897232333]
	TIME [epoch: 0.706 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507097452438464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507097452438464 | validation: 0.6955272413584059]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9717022489397386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9717022489397386 | validation: 0.9954040407936762]
	TIME [epoch: 0.711 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266460364678935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9266460364678935 | validation: 0.8292943784118982]
	TIME [epoch: 0.705 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013718796634689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9013718796634689 | validation: 0.8451792381715992]
	TIME [epoch: 0.704 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9068700627336574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9068700627336574 | validation: 0.9371316363342193]
	TIME [epoch: 0.703 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277253891877021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9277253891877021 | validation: 0.820144809631925]
	TIME [epoch: 0.705 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0104493032672088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0104493032672088 | validation: 0.9220558206983904]
	TIME [epoch: 0.703 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9381403603461508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9381403603461508 | validation: 0.7859357537393374]
	TIME [epoch: 0.703 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107350027179755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9107350027179755 | validation: 0.8657649698982759]
	TIME [epoch: 0.702 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777114551215188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8777114551215188 | validation: 0.7785747202600779]
	TIME [epoch: 0.704 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8829532947369576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8829532947369576 | validation: 0.8914471180800702]
	TIME [epoch: 0.702 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8773413357404455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8773413357404455 | validation: 0.6749374716593559]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248959100013727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9248959100013727 | validation: 1.0600514148166995]
	TIME [epoch: 0.707 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9433969469183174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9433969469183174 | validation: 0.612939932516229]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0270342434813073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0270342434813073 | validation: 0.9593739152445981]
	TIME [epoch: 0.705 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937369121154695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937369121154695 | validation: 0.9418604665919326]
	TIME [epoch: 0.707 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9572098612979661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9572098612979661 | validation: 0.7943518294996215]
	TIME [epoch: 0.705 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9519925661694555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9519925661694555 | validation: 0.7340718988345392]
	TIME [epoch: 0.703 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8534171901049403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8534171901049403 | validation: 0.954653223048349]
	TIME [epoch: 0.704 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767335738874298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767335738874298 | validation: 0.6459964517051606]
	TIME [epoch: 0.704 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823209900418804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823209900418804 | validation: 0.8058591414476434]
	TIME [epoch: 0.706 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375899889147306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8375899889147306 | validation: 0.8034477640407929]
	TIME [epoch: 0.703 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8255002701487066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8255002701487066 | validation: 0.7304936061468854]
	TIME [epoch: 0.705 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826601455805922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.826601455805922 | validation: 0.8446918581905017]
	TIME [epoch: 0.704 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334056169466653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8334056169466653 | validation: 0.6491953024321943]
	TIME [epoch: 0.703 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869600573944631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.869600573944631 | validation: 0.9794023950495703]
	TIME [epoch: 0.702 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841835589899016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841835589899016 | validation: 0.6964436568430433]
	TIME [epoch: 0.703 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9207377189411406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9207377189411406 | validation: 1.0266035972436893]
	TIME [epoch: 0.703 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9896390867951873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9896390867951873 | validation: 0.9188820381992742]
	TIME [epoch: 0.703 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9524635201989267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9524635201989267 | validation: 0.6170788281585531]
	TIME [epoch: 0.703 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9861558870148502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9861558870148502 | validation: 0.703648263576639]
	TIME [epoch: 0.704 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167382950191849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167382950191849 | validation: 0.8882968295520604]
	TIME [epoch: 0.704 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741316903859025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741316903859025 | validation: 0.6413399626857468]
	TIME [epoch: 0.702 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060839656890136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9060839656890136 | validation: 0.7363121391270658]
	TIME [epoch: 0.702 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292151481762201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8292151481762201 | validation: 0.9080849471123363]
	TIME [epoch: 0.703 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8602779578326144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8602779578326144 | validation: 0.6813117688694482]
	TIME [epoch: 0.703 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8532902000499444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8532902000499444 | validation: 0.8559172584227949]
	TIME [epoch: 0.702 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599468496328154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599468496328154 | validation: 0.7728395690895548]
	TIME [epoch: 0.707 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8544900446897796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8544900446897796 | validation: 0.8048718866093483]
	TIME [epoch: 0.708 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9144477370001048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9144477370001048 | validation: 0.8152534994538091]
	TIME [epoch: 0.704 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8826424063724089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8826424063724089 | validation: 0.7299303421967036]
	TIME [epoch: 0.703 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666336742651359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8666336742651359 | validation: 0.7432160674081181]
	TIME [epoch: 0.702 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361035495274421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8361035495274421 | validation: 0.7403143494651317]
	TIME [epoch: 0.703 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229265814315737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229265814315737 | validation: 0.7160902529578896]
	TIME [epoch: 0.704 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149905481276046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149905481276046 | validation: 0.7258737300681569]
	TIME [epoch: 0.702 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276832683569643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276832683569643 | validation: 0.7809048841974454]
	TIME [epoch: 0.708 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.866701540957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.866701540957 | validation: 0.8468757933548496]
	TIME [epoch: 0.708 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9433290398654236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9433290398654236 | validation: 0.7844730964582621]
	TIME [epoch: 0.704 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706647531993476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706647531993476 | validation: 0.7413347644044088]
	TIME [epoch: 0.703 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326923748530359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326923748530359 | validation: 0.7344784974316343]
	TIME [epoch: 0.703 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097368443161541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8097368443161541 | validation: 0.7610517075800944]
	TIME [epoch: 0.704 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106446155476775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8106446155476775 | validation: 0.7057417671247435]
	TIME [epoch: 0.702 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8144228973471503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8144228973471503 | validation: 0.8459893416843622]
	TIME [epoch: 0.703 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854635797755208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.854635797755208 | validation: 0.735439141813798]
	TIME [epoch: 0.704 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834106132497859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834106132497859 | validation: 0.8752008756438031]
	TIME [epoch: 0.704 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127213091503593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9127213091503593 | validation: 0.7739749685427854]
	TIME [epoch: 0.703 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426090980340106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426090980340106 | validation: 0.5699763689984182]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9189380682702247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9189380682702247 | validation: 0.92424049386307]
	TIME [epoch: 0.707 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752245811211993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752245811211993 | validation: 0.5955513255209349]
	TIME [epoch: 0.704 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371164736759025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8371164736759025 | validation: 0.7026005036995688]
	TIME [epoch: 0.703 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932795469302777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932795469302777 | validation: 0.7472612115087486]
	TIME [epoch: 0.702 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803059734111949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803059734111949 | validation: 0.6638878831181421]
	TIME [epoch: 0.703 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818908633350975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7818908633350975 | validation: 0.7637321403287789]
	TIME [epoch: 0.702 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830246326163293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7830246326163293 | validation: 0.6384088559551695]
	TIME [epoch: 0.702 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8166732782286673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8166732782286673 | validation: 1.0054474452122693]
	TIME [epoch: 0.704 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9335355134749901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9335355134749901 | validation: 0.9075353340094626]
	TIME [epoch: 0.702 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0022977933162873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0022977933162873 | validation: 0.6637002357794997]
	TIME [epoch: 0.702 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0005755228223747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0005755228223747 | validation: 0.6948603567017438]
	TIME [epoch: 0.704 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793579118588291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7793579118588291 | validation: 0.821695668293426]
	TIME [epoch: 0.705 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672546627148724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672546627148724 | validation: 0.6868237345546011]
	TIME [epoch: 0.703 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936069201733104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8936069201733104 | validation: 0.6460161557051498]
	TIME [epoch: 0.703 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7724484640933862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7724484640933862 | validation: 0.7524718047358352]
	TIME [epoch: 0.704 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798811703636036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.798811703636036 | validation: 0.5937608499232303]
	TIME [epoch: 0.702 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7994023651773964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7994023651773964 | validation: 0.7056913094211237]
	TIME [epoch: 0.703 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763894415607199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763894415607199 | validation: 0.6139916116649586]
	TIME [epoch: 0.702 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721865789642824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7721865789642824 | validation: 0.6929204463273266]
	TIME [epoch: 0.702 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720433853212489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720433853212489 | validation: 0.6941086061505217]
	TIME [epoch: 0.702 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860739897375024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7860739897375024 | validation: 0.845147820973412]
	TIME [epoch: 0.703 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451731258266449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451731258266449 | validation: 1.2138099378151876]
	TIME [epoch: 0.701 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1538877179285074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1538877179285074 | validation: 0.7158168972028222]
	TIME [epoch: 0.703 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764795982423391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764795982423391 | validation: 0.6290135709871343]
	TIME [epoch: 0.703 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481433138809018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8481433138809018 | validation: 0.867976978558918]
	TIME [epoch: 0.702 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598092258664031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8598092258664031 | validation: 0.6965627877298811]
	TIME [epoch: 0.702 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803112475197247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803112475197247 | validation: 0.5697169224623457]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973933500510535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973933500510535 | validation: 0.8050702174978581]
	TIME [epoch: 0.706 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063052783563426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8063052783563426 | validation: 0.5922565836395561]
	TIME [epoch: 0.704 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8172241339463219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8172241339463219 | validation: 0.7606168896433807]
	TIME [epoch: 0.705 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820455381974125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820455381974125 | validation: 0.6982606586908799]
	TIME [epoch: 0.702 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518883814069906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8518883814069906 | validation: 0.708865183760182]
	TIME [epoch: 0.7 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8204880812848043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8204880812848043 | validation: 0.7591960028057333]
	TIME [epoch: 0.7 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261102728113392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8261102728113392 | validation: 0.6991549314996928]
	TIME [epoch: 0.703 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834238548436315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7834238548436315 | validation: 0.6718347927407242]
	TIME [epoch: 0.702 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865574515391461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7865574515391461 | validation: 0.7305513788875171]
	TIME [epoch: 0.701 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7945713124529036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7945713124529036 | validation: 0.69113371972743]
	TIME [epoch: 0.7 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427507682724514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427507682724514 | validation: 0.7316717056117823]
	TIME [epoch: 0.702 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135262465407931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135262465407931 | validation: 0.7034097648428861]
	TIME [epoch: 0.701 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210366147488655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8210366147488655 | validation: 0.6987549096101184]
	TIME [epoch: 0.7 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883608441014788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7883608441014788 | validation: 0.7165767731461798]
	TIME [epoch: 0.699 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826766386372801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7826766386372801 | validation: 0.6661288570728605]
	TIME [epoch: 0.701 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964819180027337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7964819180027337 | validation: 0.8585744435981829]
	TIME [epoch: 0.701 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8410562108397784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8410562108397784 | validation: 0.6923019408611596]
	TIME [epoch: 0.699 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8050833466708531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8050833466708531 | validation: 0.676070542537782]
	TIME [epoch: 0.698 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044763888502579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044763888502579 | validation: 0.7272994152857408]
	TIME [epoch: 0.702 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871092583572916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7871092583572916 | validation: 0.6216819644711142]
	TIME [epoch: 0.701 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080977096801243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080977096801243 | validation: 0.7454526729726496]
	TIME [epoch: 0.7 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8009616026947899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8009616026947899 | validation: 0.5886444593281233]
	TIME [epoch: 0.699 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8071853435455516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8071853435455516 | validation: 0.7185981146248406]
	TIME [epoch: 0.7 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805764371475685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7805764371475685 | validation: 0.6003470902638398]
	TIME [epoch: 0.7 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650039674770989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650039674770989 | validation: 0.6852577574664113]
	TIME [epoch: 0.699 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572362388021179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572362388021179 | validation: 0.70134119458924]
	TIME [epoch: 0.699 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.802729150472452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.802729150472452 | validation: 0.7842121905908779]
	TIME [epoch: 0.7 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.912565074590709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.912565074590709 | validation: 0.9795488318102585]
	TIME [epoch: 0.699 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9215027560528273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9215027560528273 | validation: 0.6274597413965982]
	TIME [epoch: 0.7 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7403380269577312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7403380269577312 | validation: 0.5995768238661355]
	TIME [epoch: 0.699 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433885698923592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433885698923592 | validation: 0.7459240471724066]
	TIME [epoch: 0.7 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707506826099859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7707506826099859 | validation: 0.637412102199575]
	TIME [epoch: 0.7 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530602018266092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7530602018266092 | validation: 0.6643950825376672]
	TIME [epoch: 0.699 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8066021540987524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8066021540987524 | validation: 0.7623103788490802]
	TIME [epoch: 0.7 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8882158403434821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8882158403434821 | validation: 0.653878833377199]
	TIME [epoch: 0.701 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8507783493815305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8507783493815305 | validation: 0.6398184305813324]
	TIME [epoch: 0.701 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336740665727396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336740665727396 | validation: 0.6482745714096704]
	TIME [epoch: 0.699 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7144397515806113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7144397515806113 | validation: 0.6112694078011153]
	TIME [epoch: 0.7 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192617773153895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192617773153895 | validation: 0.6628671574576026]
	TIME [epoch: 0.701 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274356054384528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274356054384528 | validation: 0.6862530301754428]
	TIME [epoch: 0.701 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880623613613648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7880623613613648 | validation: 0.8354051046429479]
	TIME [epoch: 0.698 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9533210417081065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9533210417081065 | validation: 0.8461049062205186]
	TIME [epoch: 0.7 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764911667536663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8764911667536663 | validation: 0.5822636099673357]
	TIME [epoch: 0.7 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7192691459656451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7192691459656451 | validation: 0.6647853490370524]
	TIME [epoch: 0.704 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611059108226617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611059108226617 | validation: 0.7348495790635712]
	TIME [epoch: 0.705 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942994626372425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942994626372425 | validation: 0.637918419296487]
	TIME [epoch: 0.703 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592644331387441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7592644331387441 | validation: 0.6269850875495913]
	TIME [epoch: 0.703 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393160426525526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393160426525526 | validation: 0.6397585615313206]
	TIME [epoch: 0.701 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7442964661421146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7442964661421146 | validation: 0.6731599178903865]
	TIME [epoch: 0.7 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663784680365415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7663784680365415 | validation: 0.6746247970844245]
	TIME [epoch: 0.7 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8016763485856973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8016763485856973 | validation: 0.7259265283239862]
	TIME [epoch: 0.701 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918287831983513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7918287831983513 | validation: 0.6055079788800541]
	TIME [epoch: 0.699 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151323886839286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151323886839286 | validation: 0.6521846055921922]
	TIME [epoch: 0.699 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896984283724739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6896984283724739 | validation: 0.579205989582447]
	TIME [epoch: 0.702 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004391669519093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7004391669519093 | validation: 0.7160984698278137]
	TIME [epoch: 0.709 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495231376453353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495231376453353 | validation: 0.6877142229859592]
	TIME [epoch: 0.7 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579146585100107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8579146585100107 | validation: 0.6860574929406051]
	TIME [epoch: 0.701 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533494882998095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533494882998095 | validation: 0.6219595581105026]
	TIME [epoch: 0.7 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7075334866070582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7075334866070582 | validation: 0.5330415608811858]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6931254376423703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6931254376423703 | validation: 0.7922669980376343]
	TIME [epoch: 0.705 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517465792731678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517465792731678 | validation: 0.7327867982973317]
	TIME [epoch: 0.705 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379077003818555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8379077003818555 | validation: 0.6447068708038369]
	TIME [epoch: 0.703 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516744004124468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516744004124468 | validation: 0.6425307081942191]
	TIME [epoch: 176 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165366068944148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165366068944148 | validation: 0.5563892946259444]
	TIME [epoch: 1.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6981578238782093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6981578238782093 | validation: 0.7496107363150722]
	TIME [epoch: 1.38 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346586431566248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7346586431566248 | validation: 0.680182426207012]
	TIME [epoch: 1.38 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742999962208774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742999962208774 | validation: 0.64380844360162]
	TIME [epoch: 1.38 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8165150639294032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8165150639294032 | validation: 0.6417904713467247]
	TIME [epoch: 1.38 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7321997273606019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7321997273606019 | validation: 0.5651049165134265]
	TIME [epoch: 1.38 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.683604801949873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.683604801949873 | validation: 0.5890587075673548]
	TIME [epoch: 1.38 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663253413355029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6663253413355029 | validation: 0.641679750752437]
	TIME [epoch: 1.38 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6801410217287367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6801410217287367 | validation: 0.5501713314652481]
	TIME [epoch: 1.37 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6977093200809436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6977093200809436 | validation: 0.6840315780721852]
	TIME [epoch: 1.38 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552989724337471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552989724337471 | validation: 0.6887682001451744]
	TIME [epoch: 1.37 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.824256930015884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.824256930015884 | validation: 0.6810427257918599]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8302749361731202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8302749361731202 | validation: 0.5793954658402828]
	TIME [epoch: 1.37 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326319674579592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326319674579592 | validation: 0.56013978960235]
	TIME [epoch: 1.38 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6868340096215431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6868340096215431 | validation: 0.5797803797964216]
	TIME [epoch: 1.38 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592039488349333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592039488349333 | validation: 0.596593444939633]
	TIME [epoch: 1.38 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6692275654677868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6692275654677868 | validation: 0.4956192103606034]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690621261346881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690621261346881 | validation: 0.5779703567504679]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.649960165556267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.649960165556267 | validation: 0.5397499956378164]
	TIME [epoch: 1.38 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653033659694394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653033659694394 | validation: 0.5927224466106108]
	TIME [epoch: 1.38 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853173069113195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6853173069113195 | validation: 0.5908119602028299]
	TIME [epoch: 1.38 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068064303950945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068064303950945 | validation: 0.7995187092779094]
	TIME [epoch: 1.38 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9361633442880833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9361633442880833 | validation: 0.838607090263046]
	TIME [epoch: 1.38 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961542864478482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7961542864478482 | validation: 0.4860505847444844]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6647510089130546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6647510089130546 | validation: 0.5995667404568755]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6846404054940538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6846404054940538 | validation: 0.5634502730523779]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6697475772029209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6697475772029209 | validation: 0.523748791983435]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6749145616771147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6749145616771147 | validation: 0.6757533340762055]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7087447149995721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7087447149995721 | validation: 0.6548300198132044]
	TIME [epoch: 1.38 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034481855024649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8034481855024649 | validation: 0.6426452168110293]
	TIME [epoch: 1.38 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762472081064933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762472081064933 | validation: 0.5336329595891945]
	TIME [epoch: 1.38 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6450233174965111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6450233174965111 | validation: 0.6250608141889155]
	TIME [epoch: 1.38 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6757572570102991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6757572570102991 | validation: 0.5149721253259845]
	TIME [epoch: 1.38 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652171378253712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652171378253712 | validation: 0.5885801691900769]
	TIME [epoch: 1.38 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6990967591813724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6990967591813724 | validation: 0.610721554662724]
	TIME [epoch: 1.38 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383072466540904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7383072466540904 | validation: 0.7545386459556325]
	TIME [epoch: 1.38 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8343791878893361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8343791878893361 | validation: 0.546723324362158]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729893266928761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7729893266928761 | validation: 0.621866795927718]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019962589685804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7019962589685804 | validation: 0.5357514522915772]
	TIME [epoch: 1.38 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372345950026036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6372345950026036 | validation: 0.5656943462764424]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6379491320440237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6379491320440237 | validation: 0.5352054103103828]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265396296504774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6265396296504774 | validation: 0.5379980394539023]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6344030831856725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6344030831856725 | validation: 0.5098200860817016]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6383321003085934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6383321003085934 | validation: 0.5526709279321567]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653136147243106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653136147243106 | validation: 0.7005478058762571]
	TIME [epoch: 1.38 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790303489594966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790303489594966 | validation: 0.6091951947751508]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8057178785673849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8057178785673849 | validation: 0.6380135989006853]
	TIME [epoch: 1.38 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213490676237629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7213490676237629 | validation: 0.49913486508318705]
	TIME [epoch: 1.38 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6226622261103789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6226622261103789 | validation: 0.6052828191021735]
	TIME [epoch: 1.38 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623416331676291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6623416331676291 | validation: 0.5344983305332802]
	TIME [epoch: 1.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503961617736762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503961617736762 | validation: 0.5307779784181423]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391099471318976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6391099471318976 | validation: 0.5467616886240719]
	TIME [epoch: 1.38 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6187184730310579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6187184730310579 | validation: 0.629894777717566]
	TIME [epoch: 1.38 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66783353022774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66783353022774 | validation: 0.592666907172222]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623503852243098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7623503852243098 | validation: 0.6555588159472842]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7874847253867736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7874847253867736 | validation: 0.5059954298088304]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320108931702599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6320108931702599 | validation: 0.6630986308551654]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723653385896111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6723653385896111 | validation: 0.48670514689482847]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668161398604646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668161398604646 | validation: 0.5859866482091823]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763276517414825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6763276517414825 | validation: 0.4852188954224905]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6231349863714031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6231349863714031 | validation: 0.6326454858078416]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6523326856764682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6523326856764682 | validation: 0.5389587266912458]
	TIME [epoch: 1.38 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.672392123398279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.672392123398279 | validation: 0.5623478868214636]
	TIME [epoch: 1.38 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6947745097501132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6947745097501132 | validation: 0.5448080125810212]
	TIME [epoch: 1.38 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307736337636898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6307736337636898 | validation: 0.6666135004399893]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6530615112327157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6530615112327157 | validation: 0.479900419221928]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6547864001180903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6547864001180903 | validation: 0.5694336326693544]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427253107594066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427253107594066 | validation: 0.4780907677257735]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.578861332497594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.578861332497594 | validation: 0.6536453739357495]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621189844193358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621189844193358 | validation: 0.5433508777248366]
	TIME [epoch: 1.37 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661613912278178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661613912278178 | validation: 0.560580828239362]
	TIME [epoch: 1.37 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359133497535305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7359133497535305 | validation: 0.5457724756277669]
	TIME [epoch: 1.87 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6188114051478203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6188114051478203 | validation: 0.7590522840926606]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7083784857893858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7083784857893858 | validation: 0.4971611330939168]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6555488935908633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6555488935908633 | validation: 0.5883864316308668]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674391319711456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.674391319711456 | validation: 0.4718411412267696]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965322956584211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5965322956584211 | validation: 0.5411673054063423]
	TIME [epoch: 1.38 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5782782013577855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5782782013577855 | validation: 0.5267337145385745]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5701386082848428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5701386082848428 | validation: 0.5496980010082758]
	TIME [epoch: 1.38 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5919720845492135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5919720845492135 | validation: 0.5453824554212053]
	TIME [epoch: 1.38 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6585952860568286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6585952860568286 | validation: 0.5595575053556895]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6037650694444101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6037650694444101 | validation: 0.4502419513885997]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529179100463297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5529179100463297 | validation: 0.6415853758527315]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.572941953973444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.572941953973444 | validation: 0.556031375195432]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220237101590183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220237101590183 | validation: 0.5471353854760179]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975089818698459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6975089818698459 | validation: 0.5170074376843323]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5963091577816274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5963091577816274 | validation: 0.5333378612880016]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6176642321567749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6176642321567749 | validation: 0.5932275049914496]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782647726324603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782647726324603 | validation: 0.6823923968602825]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691755559300682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691755559300682 | validation: 0.4256873098142087]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725797196395721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5725797196395721 | validation: 0.4647503438632873]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.500385446984243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.500385446984243 | validation: 0.6539703264945773]
	TIME [epoch: 1.45 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5728949457059114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5728949457059114 | validation: 0.5254422039771887]
	TIME [epoch: 1.38 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7213397904801641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7213397904801641 | validation: 0.6342644596007733]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200203643974042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7200203643974042 | validation: 0.4830895185097746]
	TIME [epoch: 1.38 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786080128196709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5786080128196709 | validation: 0.5115305308386295]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5482065906683308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5482065906683308 | validation: 0.49971920496856587]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5071662193387476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5071662193387476 | validation: 0.5003244889555979]
	TIME [epoch: 1.38 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48436591197320084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48436591197320084 | validation: 0.4528652030025163]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295018395385023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5295018395385023 | validation: 0.8558313994824218]
	TIME [epoch: 1.38 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831703857586364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831703857586364 | validation: 0.4087827419501411]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326014277418443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326014277418443 | validation: 0.5872234496168078]
	TIME [epoch: 1.38 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110009520970246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110009520970246 | validation: 0.47813579606957224]
	TIME [epoch: 1.37 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5933466919628163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5933466919628163 | validation: 0.4516881207002828]
	TIME [epoch: 1.38 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530745209399024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.530745209399024 | validation: 0.5158422763563447]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5085371260405749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5085371260405749 | validation: 0.48158666037235226]
	TIME [epoch: 1.37 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4781453125818594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4781453125818594 | validation: 0.43572563909438566]
	TIME [epoch: 1.37 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4635397242870726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4635397242870726 | validation: 0.5781690645448336]
	TIME [epoch: 1.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5055472999771761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5055472999771761 | validation: 0.5623900463883589]
	TIME [epoch: 1.37 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062397484858136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7062397484858136 | validation: 0.46979723550955177]
	TIME [epoch: 1.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5236137153189079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5236137153189079 | validation: 0.5129384969061656]
	TIME [epoch: 1.37 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45239898969616277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45239898969616277 | validation: 0.5956412835840365]
	TIME [epoch: 1.37 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312965951413901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7312965951413901 | validation: 0.4571914411660366]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44749852577012894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44749852577012894 | validation: 0.6271957025120442]
	TIME [epoch: 1.37 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5156659216979398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5156659216979398 | validation: 0.6756416424397921]
	TIME [epoch: 1.37 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991033502600239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7991033502600239 | validation: 0.532450783971901]
	TIME [epoch: 1.37 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872194670136241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6872194670136241 | validation: 0.42524584107808894]
	TIME [epoch: 1.37 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5218112596846624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5218112596846624 | validation: 0.5205146825052406]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4732146191091453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4732146191091453 | validation: 0.4642404886307314]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4411601588714245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4411601588714245 | validation: 0.5433997050170086]
	TIME [epoch: 1.37 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46345271428403645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46345271428403645 | validation: 0.4579650415862956]
	TIME [epoch: 1.37 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5987533668681359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5987533668681359 | validation: 0.38419544004007833]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3952128266269881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3952128266269881 | validation: 0.6262049031434795]
	TIME [epoch: 1.38 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5446367390208046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5446367390208046 | validation: 0.6241729627184072]
	TIME [epoch: 1.38 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7306464707822216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7306464707822216 | validation: 0.5711055365083086]
	TIME [epoch: 1.38 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6229614705606921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6229614705606921 | validation: 0.387421856572324]
	TIME [epoch: 1.38 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4764071094771154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4764071094771154 | validation: 0.49716899914444657]
	TIME [epoch: 1.37 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43560158531782367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43560158531782367 | validation: 0.4471208917277732]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3886722829234998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3886722829234998 | validation: 0.5327475445908413]
	TIME [epoch: 1.37 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4341672138898064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4341672138898064 | validation: 0.42817190498839497]
	TIME [epoch: 1.37 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6183937237281761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6183937237281761 | validation: 0.36645018911856186]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3850735961010645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3850735961010645 | validation: 0.7003403250772382]
	TIME [epoch: 1.37 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6165290423465969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6165290423465969 | validation: 0.5534887139272622]
	TIME [epoch: 1.37 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6746743969388739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6746743969388739 | validation: 0.5698938121553564]
	TIME [epoch: 1.38 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.586774697167778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.586774697167778 | validation: 0.3916261031029841]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46074021564840534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46074021564840534 | validation: 0.4754090887136229]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40178194277403656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40178194277403656 | validation: 0.45774626774294147]
	TIME [epoch: 1.37 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3680631980994011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3680631980994011 | validation: 0.4783551607268291]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3808036729335681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3808036729335681 | validation: 0.3747360527497082]
	TIME [epoch: 1.38 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49243367273343636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49243367273343636 | validation: 0.4214686865473043]
	TIME [epoch: 1.37 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.349397448841348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.349397448841348 | validation: 0.3791995852484226]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37561870425318417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37561870425318417 | validation: 0.7522068661130851]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720535499876833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6720535499876833 | validation: 0.5072530046963636]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612988709459535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.612988709459535 | validation: 0.3407275718656604]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503423316326315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503423316326315 | validation: 0.5184694660991562]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453137155917767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.453137155917767 | validation: 0.4708872172623054]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3663151173910205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3663151173910205 | validation: 0.38568311752302065]
	TIME [epoch: 1.37 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32901563008562507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32901563008562507 | validation: 0.4175185534337496]
	TIME [epoch: 1.37 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183197631407913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3183197631407913 | validation: 0.34177573298603275]
	TIME [epoch: 1.37 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2909758140455439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2909758140455439 | validation: 0.4128415570789152]
	TIME [epoch: 1.37 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896166103564617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2896166103564617 | validation: 0.5085516692503329]
	TIME [epoch: 1.37 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6918633853177355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6918633853177355 | validation: 0.3377799952529909]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33186441152559726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33186441152559726 | validation: 0.44126381847179347]
	TIME [epoch: 1.38 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29692369033642413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29692369033642413 | validation: 0.5178564204865731]
	TIME [epoch: 1.38 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653409936177227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6653409936177227 | validation: 0.43260462678044]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3223183070493108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3223183070493108 | validation: 0.3932527084336137]
	TIME [epoch: 1.37 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29249166873284077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29249166873284077 | validation: 0.4349852328278093]
	TIME [epoch: 1.37 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46284656972703275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46284656972703275 | validation: 0.6641228019063107]
	TIME [epoch: 1.37 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5314694146917803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5314694146917803 | validation: 0.3955412902330565]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663614380563035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6663614380563035 | validation: 0.41947865658787187]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5531470476369219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5531470476369219 | validation: 0.43310183436575134]
	TIME [epoch: 1.37 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38924220512213853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38924220512213853 | validation: 0.4642408115743642]
	TIME [epoch: 1.37 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3466828019929937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3466828019929937 | validation: 0.37359674420586225]
	TIME [epoch: 1.37 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31194508465831844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31194508465831844 | validation: 0.39269032603179665]
	TIME [epoch: 1.37 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142082903972973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3142082903972973 | validation: 0.3261221577326167]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375470685057435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3375470685057435 | validation: 0.5127358276649991]
	TIME [epoch: 1.38 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3877117150339804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3877117150339804 | validation: 0.4057010006012524]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491517636163664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5491517636163664 | validation: 0.32920863765572883]
	TIME [epoch: 1.38 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3175877084266921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3175877084266921 | validation: 0.5832900958123688]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5353050881826793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5353050881826793 | validation: 0.4288766504334975]
	TIME [epoch: 1.38 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40329489906971044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40329489906971044 | validation: 0.3102992084032437]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.302013019712257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.302013019712257 | validation: 0.4383072073728778]
	TIME [epoch: 1.37 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30441921445191883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30441921445191883 | validation: 0.3520775368785359]
	TIME [epoch: 1.37 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3152226491877665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3152226491877665 | validation: 0.3872271477044333]
	TIME [epoch: 1.37 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3261936319526005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3261936319526005 | validation: 0.3382875429294364]
	TIME [epoch: 1.37 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37679188134477437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37679188134477437 | validation: 0.5526440643714857]
	TIME [epoch: 1.37 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39240452375679397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39240452375679397 | validation: 0.38367689669437594]
	TIME [epoch: 1.38 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853871861218006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3853871861218006 | validation: 0.35075814851817805]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27216369720618083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27216369720618083 | validation: 0.35301554002422963]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23995811497652558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23995811497652558 | validation: 0.3259852060407725]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23108233133021144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23108233133021144 | validation: 0.3519967817389446]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38381166511198167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38381166511198167 | validation: 0.6299728528534253]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5473245061196557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5473245061196557 | validation: 0.4575290419440178]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5135248434243651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5135248434243651 | validation: 0.2944064704426098]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749043355458445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749043355458445 | validation: 0.5447595671906027]
	TIME [epoch: 1.38 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4359835965537367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4359835965537367 | validation: 0.366593227957549]
	TIME [epoch: 1.37 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31672323754824133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31672323754824133 | validation: 0.28874493343691443]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2816933679806368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2816933679806368 | validation: 0.38279701439761094]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25503492298548014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25503492298548014 | validation: 0.3216375591287799]
	TIME [epoch: 1.37 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2545243472045008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2545243472045008 | validation: 0.43767921767236634]
	TIME [epoch: 1.38 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878254157333423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3878254157333423 | validation: 0.3391879109569698]
	TIME [epoch: 1.37 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45286368152375844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45286368152375844 | validation: 0.5562588226227846]
	TIME [epoch: 1.37 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33905585212485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33905585212485 | validation: 0.43440408386463014]
	TIME [epoch: 1.37 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2699454557145861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2699454557145861 | validation: 0.28839357853345293]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2693205755365108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2693205755365108 | validation: 0.3864845327134613]
	TIME [epoch: 1.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2976459397982513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2976459397982513 | validation: 0.3423446713966601]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3874751460561754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3874751460561754 | validation: 0.3107780422262141]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2456656594143005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2456656594143005 | validation: 0.36148589765787387]
	TIME [epoch: 1.37 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22622884422682416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22622884422682416 | validation: 0.3907046969068995]
	TIME [epoch: 1.37 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22943471990838057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22943471990838057 | validation: 0.3198144071410389]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23024581643884126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23024581643884126 | validation: 0.3540281806265972]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21744094796630578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21744094796630578 | validation: 0.4371190026992004]
	TIME [epoch: 1.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3577435584555832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3577435584555832 | validation: 0.5860748805459993]
	TIME [epoch: 1.38 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9087099878473762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9087099878473762 | validation: 0.4170915919158253]
	TIME [epoch: 1.38 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550636809526721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7550636809526721 | validation: 0.34072695030894523]
	TIME [epoch: 1.37 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25963930962324594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25963930962324594 | validation: 0.4894693602202962]
	TIME [epoch: 1.38 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3697479220825823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3697479220825823 | validation: 0.33434776114060444]
	TIME [epoch: 1.38 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.353246755673514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.353246755673514 | validation: 0.3303984855278264]
	TIME [epoch: 1.38 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3693859338789091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3693859338789091 | validation: 0.3623673307529686]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2426510689062799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2426510689062799 | validation: 0.34678591899912115]
	TIME [epoch: 1.38 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2332541460505602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2332541460505602 | validation: 0.3045934623925228]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2303150926072519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2303150926072519 | validation: 0.28652148468764976]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792207007231215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2792207007231215 | validation: 0.6717715688057635]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5307345358950946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5307345358950946 | validation: 0.3382806440817163]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35139760194098935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35139760194098935 | validation: 0.26838226291982864]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2344724633893467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2344724633893467 | validation: 0.41502608138524383]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502010319671298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502010319671298 | validation: 0.3067637549089766]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26092136688132506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26092136688132506 | validation: 0.35056217160697617]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23923403157365294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23923403157365294 | validation: 0.335706928566146]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2227847291832711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2227847291832711 | validation: 0.43780756104156193]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2911323332544296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2911323332544296 | validation: 0.3439682606433321]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.482038864106966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.482038864106966 | validation: 0.26841257641977817]
	TIME [epoch: 1.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17897127828417764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17897127828417764 | validation: 0.383966311275004]
	TIME [epoch: 1.37 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22268776928955583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22268776928955583 | validation: 0.2940315211409898]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27253400964768143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27253400964768143 | validation: 0.38458456990343937]
	TIME [epoch: 1.38 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2813942222923433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2813942222923433 | validation: 0.34731940214165813]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3383597591283171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3383597591283171 | validation: 0.3306759135614858]
	TIME [epoch: 1.37 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2527814045038878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2527814045038878 | validation: 0.38303395107382293]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3061710152757631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3061710152757631 | validation: 0.44686674645579694]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.327429456645085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.327429456645085 | validation: 0.28237530069423283]
	TIME [epoch: 1.4 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31077374455332146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31077374455332146 | validation: 0.2874730155068414]
	TIME [epoch: 1.38 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17685352044190267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17685352044190267 | validation: 0.26234435339185486]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15937047328970047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15937047328970047 | validation: 0.22550917805832454]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16902350132730057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16902350132730057 | validation: 0.34629245331497005]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2177903027687598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2177903027687598 | validation: 0.34203938656837973]
	TIME [epoch: 1.38 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4881661229350388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4881661229350388 | validation: 0.2366212975808582]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506129199189554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1506129199189554 | validation: 0.5067246348801888]
	TIME [epoch: 1.38 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36815861313564385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36815861313564385 | validation: 0.33122781546637053]
	TIME [epoch: 1.38 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4635474541116432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4635474541116432 | validation: 0.25850573041431907]
	TIME [epoch: 1.38 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25856654471908735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25856654471908735 | validation: 0.4927114837633035]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40922416170796155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40922416170796155 | validation: 0.3368037223645578]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23568278048940808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23568278048940808 | validation: 0.29585386098042293]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3314995387416896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3314995387416896 | validation: 0.46069032281041866]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25025189476889703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25025189476889703 | validation: 0.2653718969991736]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15576344481305868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15576344481305868 | validation: 0.2092221660838791]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773567757327404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1773567757327404 | validation: 0.3221676876361625]
	TIME [epoch: 1.38 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19951577699711778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19951577699711778 | validation: 0.3206776799488975]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3293924480398696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3293924480398696 | validation: 0.3287175013461643]
	TIME [epoch: 1.38 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30125086665655304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30125086665655304 | validation: 0.362242203147251]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3300614363675352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3300614363675352 | validation: 0.2911127680649125]
	TIME [epoch: 1.38 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404591905347216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2404591905347216 | validation: 0.32071875113461756]
	TIME [epoch: 1.38 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22526749795607856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22526749795607856 | validation: 0.49822863688235963]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2867288781659589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2867288781659589 | validation: 0.27526524804883057]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2008950007554909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2008950007554909 | validation: 0.27863404922635654]
	TIME [epoch: 1.38 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18747800594674185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18747800594674185 | validation: 0.26892327887433193]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23631098869990588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23631098869990588 | validation: 0.3305194275617179]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23584489547349755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23584489547349755 | validation: 0.3158404486457373]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2926627543432728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2926627543432728 | validation: 0.36085024789805165]
	TIME [epoch: 1.38 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2495469547685287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2495469547685287 | validation: 0.29301374057262125]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24843213594161015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24843213594161015 | validation: 0.2609417166630894]
	TIME [epoch: 1.37 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638649743582834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638649743582834 | validation: 0.2639533314867097]
	TIME [epoch: 1.37 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14469372141134765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14469372141134765 | validation: 0.2387046044181742]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16893226938600422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16893226938600422 | validation: 0.48612224198697424]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3214309618596281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3214309618596281 | validation: 0.38186278833381715]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4539976599032543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4539976599032543 | validation: 0.2527888284671967]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1909821765370897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1909821765370897 | validation: 0.29660274326268066]
	TIME [epoch: 1.38 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1884233307417919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1884233307417919 | validation: 0.31262541843537117]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3056922334942647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3056922334942647 | validation: 0.2597691850186302]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551950251260882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551950251260882 | validation: 0.2977487007457351]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14847055308536258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14847055308536258 | validation: 0.20821004885634423]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_471.pth
	Model improved!!!
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14457768908487154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14457768908487154 | validation: 0.3560154938337805]
	TIME [epoch: 1.37 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23012388242305565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23012388242305565 | validation: 0.2999033670588523]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37564650231190916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37564650231190916 | validation: 0.21667031657223745]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477774764961485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477774764961485 | validation: 0.2970907301679734]
	TIME [epoch: 1.38 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16747464392610983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16747464392610983 | validation: 0.23207163128212882]
	TIME [epoch: 1.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2279302900108346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2279302900108346 | validation: 0.42665760356677407]
	TIME [epoch: 1.38 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3281184330909567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3281184330909567 | validation: 0.451883113335754]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35927086344684556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35927086344684556 | validation: 0.27329258341620066]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15810933575197011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15810933575197011 | validation: 0.264996454456275]
	TIME [epoch: 1.37 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464218290406633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1464218290406633 | validation: 0.23095254533192067]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1909341269134306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1909341269134306 | validation: 0.3175062503536832]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2410110876971416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2410110876971416 | validation: 0.2768144062687234]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27331172350727817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27331172350727817 | validation: 0.44114861515318826]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2778804369898381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2778804369898381 | validation: 0.27653827569685524]
	TIME [epoch: 1.38 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17487418683265432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17487418683265432 | validation: 0.2705150162725172]
	TIME [epoch: 1.38 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13405723038298561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13405723038298561 | validation: 0.21692159765552452]
	TIME [epoch: 1.38 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11678296131225825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11678296131225825 | validation: 0.23371405894737293]
	TIME [epoch: 1.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13244410734780956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13244410734780956 | validation: 0.3891939928669909]
	TIME [epoch: 1.38 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2828998857724478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2828998857724478 | validation: 0.4324766246097526]
	TIME [epoch: 1.38 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5254620804907338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5254620804907338 | validation: 0.286826561539362]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2128779919197155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2128779919197155 | validation: 0.5122100177562584]
	TIME [epoch: 1.38 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4127259039672632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4127259039672632 | validation: 0.2150035961750324]
	TIME [epoch: 1.38 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16996724075136357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16996724075136357 | validation: 0.19076013831204985]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12447770632905902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447770632905902 | validation: 0.24580298346341697]
	TIME [epoch: 1.37 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12361012393717843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12361012393717843 | validation: 0.18987745722887742]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_496.pth
	Model improved!!!
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1625745024721997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1625745024721997 | validation: 0.30864880334071004]
	TIME [epoch: 1.38 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.180836887329626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.180836887329626 | validation: 0.2845505917162819]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25304267174358713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25304267174358713 | validation: 0.2966624794993082]
	TIME [epoch: 1.37 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2568569030562411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2568569030562411 | validation: 0.24548368424395434]
	TIME [epoch: 1.37 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1555641663710165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1555641663710165 | validation: 0.2144301997736954]
	TIME [epoch: 181 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12462262338810096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12462262338810096 | validation: 0.20463095500073325]
	TIME [epoch: 2.73 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13702974886197272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13702974886197272 | validation: 0.3670761228368929]
	TIME [epoch: 2.77 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2556694924099348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2556694924099348 | validation: 0.46852937751411244]
	TIME [epoch: 2.71 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42614801075606795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42614801075606795 | validation: 0.26456801028126736]
	TIME [epoch: 2.72 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16289587515563178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16289587515563178 | validation: 0.15657235030405892]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10774056064723936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10774056064723936 | validation: 0.22056901367992943]
	TIME [epoch: 2.72 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11151288233204318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11151288233204318 | validation: 0.2205080822289073]
	TIME [epoch: 2.71 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14296284196903528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14296284196903528 | validation: 0.3651033143114793]
	TIME [epoch: 2.72 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22418083790579452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22418083790579452 | validation: 0.29709833572918354]
	TIME [epoch: 2.71 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28539644094291816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28539644094291816 | validation: 0.3139785064431597]
	TIME [epoch: 2.72 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22258321018577767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22258321018577767 | validation: 0.20342966613739244]
	TIME [epoch: 2.71 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12624460079592534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12624460079592534 | validation: 0.24186738398284327]
	TIME [epoch: 2.72 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387934183280216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1387934183280216 | validation: 0.2599559180363202]
	TIME [epoch: 2.72 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2073369294294819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2073369294294819 | validation: 0.3810693029792828]
	TIME [epoch: 2.72 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2880107498156216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2880107498156216 | validation: 0.3125812736229849]
	TIME [epoch: 2.72 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27838241298112865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27838241298112865 | validation: 0.25514720428305965]
	TIME [epoch: 2.72 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16258078251661354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16258078251661354 | validation: 0.1794814517762885]
	TIME [epoch: 2.72 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11906796989933334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11906796989933334 | validation: 0.22200001264386382]
	TIME [epoch: 2.72 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14303453626229257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14303453626229257 | validation: 0.23543035885466052]
	TIME [epoch: 2.72 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19484825384333662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19484825384333662 | validation: 0.41325835731567456]
	TIME [epoch: 2.72 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2674438866871883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2674438866871883 | validation: 0.2401141159309196]
	TIME [epoch: 2.71 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18050403490098751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18050403490098751 | validation: 0.42268823112543336]
	TIME [epoch: 2.71 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27552905754006346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27552905754006346 | validation: 0.21422390374035324]
	TIME [epoch: 2.72 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18516986878681238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18516986878681238 | validation: 0.24069062396888646]
	TIME [epoch: 2.71 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14500608434642515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14500608434642515 | validation: 0.2738010672081977]
	TIME [epoch: 2.71 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15990167274906092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15990167274906092 | validation: 0.2170277322971555]
	TIME [epoch: 2.72 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286186200405609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12286186200405609 | validation: 0.19569366993432374]
	TIME [epoch: 2.72 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11079404881909183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11079404881909183 | validation: 0.20865056655166733]
	TIME [epoch: 2.72 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11494205509674485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11494205509674485 | validation: 0.1744642051819703]
	TIME [epoch: 2.72 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20475445781465218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20475445781465218 | validation: 0.4515000677487542]
	TIME [epoch: 2.72 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34963193023146827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34963193023146827 | validation: 0.2845881905504394]
	TIME [epoch: 2.72 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2606264383128432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2606264383128432 | validation: 0.2029845666385345]
	TIME [epoch: 2.72 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11595429092324423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11595429092324423 | validation: 0.21901123516141677]
	TIME [epoch: 2.72 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10314847038334993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10314847038334993 | validation: 0.14859756537454852]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10787056188843108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10787056188843108 | validation: 0.242023578889085]
	TIME [epoch: 2.72 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12524720985481347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12524720985481347 | validation: 0.13450306711572405]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15631055194318502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15631055194318502 | validation: 0.35024782489809153]
	TIME [epoch: 2.72 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21930968673656942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21930968673656942 | validation: 0.24722710477342213]
	TIME [epoch: 2.71 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26940626924497657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26940626924497657 | validation: 0.20331855878219254]
	TIME [epoch: 2.72 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19236349551542417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19236349551542417 | validation: 0.2589458305402335]
	TIME [epoch: 2.71 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18139609122091563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18139609122091563 | validation: 0.20574370073909867]
	TIME [epoch: 2.72 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1473922913291958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1473922913291958 | validation: 0.26925918990711467]
	TIME [epoch: 2.72 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16455544406313344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16455544406313344 | validation: 0.424665506962128]
	TIME [epoch: 2.77 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19913880806847664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19913880806847664 | validation: 0.15390310265896853]
	TIME [epoch: 2.71 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0869409753859695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0869409753859695 | validation: 0.16774720325067338]
	TIME [epoch: 2.72 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10946023973905288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10946023973905288 | validation: 0.26519276004683995]
	TIME [epoch: 2.72 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614606601767656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614606601767656 | validation: 0.37584365321872576]
	TIME [epoch: 2.72 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2717831468555501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2717831468555501 | validation: 0.20663266875742164]
	TIME [epoch: 2.72 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18623442592903192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18623442592903192 | validation: 0.22965754354351633]
	TIME [epoch: 2.72 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.134322324413263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.134322324413263 | validation: 0.18627496821721723]
	TIME [epoch: 2.72 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13066654402465652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13066654402465652 | validation: 0.23980295781423672]
	TIME [epoch: 2.72 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15959470944285886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15959470944285886 | validation: 0.334904862027118]
	TIME [epoch: 2.72 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20612616004742002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20612616004742002 | validation: 0.2003787876011734]
	TIME [epoch: 2.72 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14486369832457455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14486369832457455 | validation: 0.237116901292464]
	TIME [epoch: 2.72 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13467057609039298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13467057609039298 | validation: 0.17534027065938682]
	TIME [epoch: 2.72 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12671323035315615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12671323035315615 | validation: 0.19586146441888955]
	TIME [epoch: 2.72 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13955195229805267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13955195229805267 | validation: 0.2590868541837494]
	TIME [epoch: 2.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22182083848345954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22182083848345954 | validation: 0.38400950964764863]
	TIME [epoch: 2.72 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558715150255813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2558715150255813 | validation: 0.22707442029994965]
	TIME [epoch: 2.71 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18655866131523383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18655866131523383 | validation: 0.2006754553724379]
	TIME [epoch: 2.72 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362958287646947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12362958287646947 | validation: 0.21629400585503544]
	TIME [epoch: 2.72 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255651539586825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1255651539586825 | validation: 0.16073211025310494]
	TIME [epoch: 2.72 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12286547413977093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12286547413977093 | validation: 0.25688274627874214]
	TIME [epoch: 2.71 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16092799530287263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16092799530287263 | validation: 0.18933416217882504]
	TIME [epoch: 2.72 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16528306699778786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16528306699778786 | validation: 0.20899411547364305]
	TIME [epoch: 2.72 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1346061614239211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1346061614239211 | validation: 0.1718080398790363]
	TIME [epoch: 2.72 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09544493335438933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09544493335438933 | validation: 0.1428491526004021]
	TIME [epoch: 2.72 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08065660947075824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08065660947075824 | validation: 0.15045508165594126]
	TIME [epoch: 2.72 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07487512929716142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07487512929716142 | validation: 0.1172471153411395]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0831441065948804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0831441065948804 | validation: 0.23893200037232964]
	TIME [epoch: 2.79 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11288298694448184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11288298694448184 | validation: 0.16975832117659517]
	TIME [epoch: 2.72 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20151879700801215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20151879700801215 | validation: 0.6141245823602997]
	TIME [epoch: 2.72 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5202640599431894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5202640599431894 | validation: 0.23430035764803359]
	TIME [epoch: 2.72 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21259439369136637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21259439369136637 | validation: 0.38834702951928635]
	TIME [epoch: 2.72 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22009472325988327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22009472325988327 | validation: 0.26589747721214446]
	TIME [epoch: 2.71 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17829639147801246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17829639147801246 | validation: 0.21352809694701064]
	TIME [epoch: 2.72 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14657115055960968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14657115055960968 | validation: 0.15034948756206748]
	TIME [epoch: 2.72 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13346942241220386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13346942241220386 | validation: 0.1560777707758103]
	TIME [epoch: 2.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12370747255384797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12370747255384797 | validation: 0.2923482174693773]
	TIME [epoch: 2.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19423998985757582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19423998985757582 | validation: 0.1681782497737741]
	TIME [epoch: 2.72 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16531709010062132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16531709010062132 | validation: 0.20864046153680338]
	TIME [epoch: 2.72 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12696684445243317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12696684445243317 | validation: 0.21577955713695274]
	TIME [epoch: 2.72 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12467044443330495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12467044443330495 | validation: 0.18598618748857432]
	TIME [epoch: 2.71 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10882318541549546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10882318541549546 | validation: 0.16565130283601964]
	TIME [epoch: 2.72 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11407629499351847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11407629499351847 | validation: 0.17380633780569849]
	TIME [epoch: 2.72 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1386412855043321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1386412855043321 | validation: 0.35242296177521104]
	TIME [epoch: 2.71 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23964306868378096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23964306868378096 | validation: 0.18339794359623396]
	TIME [epoch: 2.72 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1694989751964697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1694989751964697 | validation: 0.1691775772854478]
	TIME [epoch: 2.72 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264896212327834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11264896212327834 | validation: 0.21578130847969712]
	TIME [epoch: 2.72 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12682268823804413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12682268823804413 | validation: 0.22870152286985854]
	TIME [epoch: 2.72 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13992067974757358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13992067974757358 | validation: 0.19465114843533632]
	TIME [epoch: 2.72 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17554533750673407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17554533750673407 | validation: 0.3197324171481699]
	TIME [epoch: 2.72 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2263663865614693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2263663865614693 | validation: 0.1700746605695951]
	TIME [epoch: 2.72 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18824268322263954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18824268322263954 | validation: 0.17401465120506926]
	TIME [epoch: 2.71 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10829974752234929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10829974752234929 | validation: 0.14465051213703825]
	TIME [epoch: 2.72 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507255689649368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507255689649368 | validation: 0.12206206955367183]
	TIME [epoch: 2.72 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.084920099692303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.084920099692303 | validation: 0.17239635288985533]
	TIME [epoch: 2.71 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09038908989930952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09038908989930952 | validation: 0.11929944375104179]
	TIME [epoch: 2.71 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14722309939409595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14722309939409595 | validation: 0.35200973409429076]
	TIME [epoch: 2.72 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24454513508066933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24454513508066933 | validation: 0.2134096069511153]
	TIME [epoch: 2.71 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19965911363705197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19965911363705197 | validation: 0.20174729425049762]
	TIME [epoch: 2.71 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1205650998822772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1205650998822772 | validation: 0.21943655642864268]
	TIME [epoch: 2.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10215369145748572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10215369145748572 | validation: 0.19261342696570882]
	TIME [epoch: 2.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11532822871570432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11532822871570432 | validation: 0.3122311472050151]
	TIME [epoch: 2.72 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21885798647955654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21885798647955654 | validation: 0.20313562208890437]
	TIME [epoch: 2.72 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20848450658220175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20848450658220175 | validation: 0.24254064010973186]
	TIME [epoch: 2.72 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14421902620618454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14421902620618454 | validation: 0.13820109454125426]
	TIME [epoch: 2.72 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06917222779844474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06917222779844474 | validation: 0.11086706431852034]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050145819274428675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050145819274428675 | validation: 0.09053092635727117]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05056824838734602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05056824838734602 | validation: 0.1243546595389315]
	TIME [epoch: 2.73 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06996970924481309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06996970924481309 | validation: 0.19340686212243907]
	TIME [epoch: 2.72 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24766361970372677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24766361970372677 | validation: 0.44681084185420805]
	TIME [epoch: 2.72 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3511117920619921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3511117920619921 | validation: 0.25849486588383164]
	TIME [epoch: 2.72 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19905045208655067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19905045208655067 | validation: 0.15117301785073134]
	TIME [epoch: 2.72 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935519888697696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0935519888697696 | validation: 0.14442543893260473]
	TIME [epoch: 2.72 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08440751852022307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08440751852022307 | validation: 0.13221439118858114]
	TIME [epoch: 2.72 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09249638008209703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09249638008209703 | validation: 0.2084412998561557]
	TIME [epoch: 2.71 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839632478925768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11839632478925768 | validation: 0.16280836357556325]
	TIME [epoch: 2.72 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18736334945221877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18736334945221877 | validation: 0.23057738578685438]
	TIME [epoch: 2.71 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12620863895618875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12620863895618875 | validation: 0.14013106374515988]
	TIME [epoch: 2.71 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080091795594497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.080091795594497 | validation: 0.1464530316565531]
	TIME [epoch: 2.71 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09998627810341244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09998627810341244 | validation: 0.2047554305907353]
	TIME [epoch: 2.71 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1458576753294005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1458576753294005 | validation: 0.23299802732872574]
	TIME [epoch: 2.71 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23941357956588807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23941357956588807 | validation: 0.36085949367401304]
	TIME [epoch: 2.72 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23068179160339589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23068179160339589 | validation: 0.12964478160108164]
	TIME [epoch: 2.71 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10930284507754304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10930284507754304 | validation: 0.12946988380494126]
	TIME [epoch: 2.71 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674135969058764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0674135969058764 | validation: 0.14249767471873837]
	TIME [epoch: 2.71 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744118687738803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0744118687738803 | validation: 0.138208278547693]
	TIME [epoch: 2.71 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0979917101136558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0979917101136558 | validation: 0.17545683300085418]
	TIME [epoch: 2.71 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225307236771517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225307236771517 | validation: 0.20040594514207677]
	TIME [epoch: 2.71 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338952953953315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1338952953953315 | validation: 0.1885177098619617]
	TIME [epoch: 2.71 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225268117811837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225268117811837 | validation: 0.1401672966471171]
	TIME [epoch: 2.72 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617880178450131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1617880178450131 | validation: 0.3856466561294137]
	TIME [epoch: 2.71 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2915459046133795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2915459046133795 | validation: 0.13383742497566575]
	TIME [epoch: 2.71 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1437595255020382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1437595255020382 | validation: 0.15472157627856975]
	TIME [epoch: 2.71 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166212568383646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08166212568383646 | validation: 0.15615379236909255]
	TIME [epoch: 2.72 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08148127573353765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08148127573353765 | validation: 0.12650375558963234]
	TIME [epoch: 2.71 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09399436180362049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09399436180362049 | validation: 0.16086958254457678]
	TIME [epoch: 2.71 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10762145150933229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10762145150933229 | validation: 0.14440457641770563]
	TIME [epoch: 2.71 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12763327319492995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12763327319492995 | validation: 0.16736489711470087]
	TIME [epoch: 2.71 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10623365694475878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10623365694475878 | validation: 0.15689947321317743]
	TIME [epoch: 2.71 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13419155810319217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13419155810319217 | validation: 0.3961959285648396]
	TIME [epoch: 2.72 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29275351837208524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29275351837208524 | validation: 0.18660477125491057]
	TIME [epoch: 2.72 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16791285833588204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16791285833588204 | validation: 0.1299943843197777]
	TIME [epoch: 2.71 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08408198479417095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08408198479417095 | validation: 0.14003607809082405]
	TIME [epoch: 2.71 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08036557853586684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08036557853586684 | validation: 0.10699142790485683]
	TIME [epoch: 2.71 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08050604829261453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08050604829261453 | validation: 0.12965366202824713]
	TIME [epoch: 2.72 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707698090641453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707698090641453 | validation: 0.15597214292636183]
	TIME [epoch: 2.72 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09341428011934422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09341428011934422 | validation: 0.16808467554433176]
	TIME [epoch: 2.72 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10992489498025673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10992489498025673 | validation: 0.18739332891098837]
	TIME [epoch: 2.71 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12878732871505583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12878732871505583 | validation: 0.18880361018637348]
	TIME [epoch: 2.72 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13080483140668386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13080483140668386 | validation: 0.17647527833843263]
	TIME [epoch: 2.71 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2413393226896281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2413393226896281 | validation: 0.2909158159318192]
	TIME [epoch: 2.71 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17864314728803649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17864314728803649 | validation: 0.13589398021782567]
	TIME [epoch: 2.71 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09642390936914946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09642390936914946 | validation: 0.1203485287217323]
	TIME [epoch: 2.72 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07998088468727356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07998088468727356 | validation: 0.16118893049422633]
	TIME [epoch: 2.71 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1057300215091826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1057300215091826 | validation: 0.17078862535442457]
	TIME [epoch: 2.71 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14788871220408883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14788871220408883 | validation: 0.1635719704372017]
	TIME [epoch: 2.72 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11466055451258958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11466055451258958 | validation: 0.25072387662278744]
	TIME [epoch: 2.71 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14624144556134977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14624144556134977 | validation: 0.15398336151777856]
	TIME [epoch: 2.72 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14872713370133409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14872713370133409 | validation: 0.1931364960820154]
	TIME [epoch: 2.71 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13780119652167744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13780119652167744 | validation: 0.1914312119240664]
	TIME [epoch: 2.72 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12153687724240415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12153687724240415 | validation: 0.16087192722201193]
	TIME [epoch: 2.71 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247070925579035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09247070925579035 | validation: 0.10825996252426462]
	TIME [epoch: 2.71 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07331051426650967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07331051426650967 | validation: 0.10605579411689608]
	TIME [epoch: 2.72 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056142519124594406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056142519124594406 | validation: 0.07737885418777075]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_667.pth
	Model improved!!!
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05802856727377202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05802856727377202 | validation: 0.1040960938260302]
	TIME [epoch: 2.72 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557568430802553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557568430802553 | validation: 0.2968197463026357]
	TIME [epoch: 2.72 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2186171588882538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2186171588882538 | validation: 0.2074148251242085]
	TIME [epoch: 2.72 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22285343332097823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22285343332097823 | validation: 0.13103981277432103]
	TIME [epoch: 2.72 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697108541933016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08697108541933016 | validation: 0.1289040885097999]
	TIME [epoch: 2.72 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014930636849205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08014930636849205 | validation: 0.18502861152445388]
	TIME [epoch: 2.72 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10422532154490935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10422532154490935 | validation: 0.1934549074621842]
	TIME [epoch: 2.72 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13083803332822647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13083803332822647 | validation: 0.15274938590985984]
	TIME [epoch: 2.73 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10428229472318064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10428229472318064 | validation: 0.14758227252378373]
	TIME [epoch: 2.72 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14567147644321587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14567147644321587 | validation: 0.4242377629392913]
	TIME [epoch: 2.73 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3040142354175156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3040142354175156 | validation: 0.1556686830817333]
	TIME [epoch: 2.72 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14404644537443434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14404644537443434 | validation: 0.10905459636329647]
	TIME [epoch: 2.72 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0603439178562088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0603439178562088 | validation: 0.11541249073703315]
	TIME [epoch: 2.72 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050067056706900905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050067056706900905 | validation: 0.07778930826955671]
	TIME [epoch: 2.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282451974961449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04282451974961449 | validation: 0.09362297821881255]
	TIME [epoch: 2.72 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0421029004646633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0421029004646633 | validation: 0.09076565890862051]
	TIME [epoch: 2.72 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06141020385162542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06141020385162542 | validation: 0.1457586363057856]
	TIME [epoch: 2.72 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13288639056448892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13288639056448892 | validation: 0.32550994070940514]
	TIME [epoch: 2.72 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25381570642990947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25381570642990947 | validation: 0.16295113057702326]
	TIME [epoch: 2.72 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11156167836056553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11156167836056553 | validation: 0.12111963664819148]
	TIME [epoch: 2.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05999987263489258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05999987263489258 | validation: 0.10400157791428587]
	TIME [epoch: 2.73 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06299120959060074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06299120959060074 | validation: 0.167926431577098]
	TIME [epoch: 2.72 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0999129187716084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0999129187716084 | validation: 0.35426807852983094]
	TIME [epoch: 2.72 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22461696844179055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22461696844179055 | validation: 0.2085812636606078]
	TIME [epoch: 2.72 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16236648599391085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16236648599391085 | validation: 0.17394002534866246]
	TIME [epoch: 2.72 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17428471710953783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17428471710953783 | validation: 0.3173991086551087]
	TIME [epoch: 2.72 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2397863588949074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2397863588949074 | validation: 0.1322040131684768]
	TIME [epoch: 2.72 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08147333777826127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08147333777826127 | validation: 0.1125501504911334]
	TIME [epoch: 2.72 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05521232744708495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05521232744708495 | validation: 0.10638470744184941]
	TIME [epoch: 2.72 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06418075783321184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06418075783321184 | validation: 0.11725596056270843]
	TIME [epoch: 2.72 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07490783573919671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07490783573919671 | validation: 0.18671839944542953]
	TIME [epoch: 2.72 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12229309317388422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12229309317388422 | validation: 0.1443633568561928]
	TIME [epoch: 2.72 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10152916666703896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10152916666703896 | validation: 0.16243263753195844]
	TIME [epoch: 2.72 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10971340015162753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10971340015162753 | validation: 0.17739718318424266]
	TIME [epoch: 2.72 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14147181920987276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14147181920987276 | validation: 0.18141760734385237]
	TIME [epoch: 2.73 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14153364124718829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14153364124718829 | validation: 0.13757931308999533]
	TIME [epoch: 2.72 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11155906536393989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11155906536393989 | validation: 0.09746247487074694]
	TIME [epoch: 2.72 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05593071967916279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05593071967916279 | validation: 0.06639773288695179]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_705.pth
	Model improved!!!
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03617854370540115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03617854370540115 | validation: 0.06621574456067485]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03122203490413744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03122203490413744 | validation: 0.0756573665963416]
	TIME [epoch: 2.72 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710364161701832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04710364161701832 | validation: 0.2708529246614952]
	TIME [epoch: 2.74 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19060631351944282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19060631351944282 | validation: 0.22294955726017737]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29932380748464993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29932380748464993 | validation: 0.14666062889210646]
	TIME [epoch: 2.73 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11203778355411555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11203778355411555 | validation: 0.2296487593903553]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18794764308673245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18794764308673245 | validation: 0.1773364302163123]
	TIME [epoch: 2.73 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11100304217976276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11100304217976276 | validation: 0.10280771824358204]
	TIME [epoch: 2.73 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056878459233593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06056878459233593 | validation: 0.09113636157839072]
	TIME [epoch: 2.72 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04376140258676427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04376140258676427 | validation: 0.0720381885777683]
	TIME [epoch: 2.73 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041075809534188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041075809534188 | validation: 0.08882584789348098]
	TIME [epoch: 2.72 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05550122349143995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05550122349143995 | validation: 0.20281023787798197]
	TIME [epoch: 2.73 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13980461058246232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13980461058246232 | validation: 0.253720914926306]
	TIME [epoch: 2.72 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106290409491119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3106290409491119 | validation: 0.20876939969864924]
	TIME [epoch: 2.73 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1578732738994127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1578732738994127 | validation: 0.16566083063778247]
	TIME [epoch: 2.72 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11280483343465102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11280483343465102 | validation: 0.11883193629878885]
	TIME [epoch: 2.73 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08272620244330207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08272620244330207 | validation: 0.10505931198569166]
	TIME [epoch: 2.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05827909379271244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05827909379271244 | validation: 0.08407244329208353]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07695368448637135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07695368448637135 | validation: 0.15845558833795015]
	TIME [epoch: 2.73 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09400764307962411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09400764307962411 | validation: 0.11282119574978422]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09726028099600725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09726028099600725 | validation: 0.1424238608187238]
	TIME [epoch: 2.72 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08914820550388797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08914820550388797 | validation: 0.12154745421918872]
	TIME [epoch: 2.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895960435769673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0895960435769673 | validation: 0.15371654133545068]
	TIME [epoch: 2.73 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10895216380593725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10895216380593725 | validation: 0.15223746205829305]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12352551926285012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12352551926285012 | validation: 0.12402866745650663]
	TIME [epoch: 2.73 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362850692380658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08362850692380658 | validation: 0.09454422549691707]
	TIME [epoch: 2.72 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07868047390131412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07868047390131412 | validation: 0.10656178503556135]
	TIME [epoch: 2.73 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06567077581810218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06567077581810218 | validation: 0.07368662869828506]
	TIME [epoch: 2.72 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058505731073830426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058505731073830426 | validation: 0.09436121332686934]
	TIME [epoch: 2.72 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06375874245238702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06375874245238702 | validation: 0.24502565467768173]
	TIME [epoch: 2.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17156673934774674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17156673934774674 | validation: 0.19050558093848835]
	TIME [epoch: 2.72 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.222835563493503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.222835563493503 | validation: 0.08931072805360485]
	TIME [epoch: 2.72 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05933442590371465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05933442590371465 | validation: 0.0953430490055318]
	TIME [epoch: 2.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052076645422680236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052076645422680236 | validation: 0.1354516919950536]
	TIME [epoch: 2.72 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07220306368133034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07220306368133034 | validation: 0.12763661526960077]
	TIME [epoch: 2.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09507334379837953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09507334379837953 | validation: 0.12469125149946812]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918763548090901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0918763548090901 | validation: 0.14374502107952103]
	TIME [epoch: 2.72 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12307094623823661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12307094623823661 | validation: 0.3281206152043205]
	TIME [epoch: 2.73 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2306442810627125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2306442810627125 | validation: 0.17184070156766817]
	TIME [epoch: 2.72 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1654651443460275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1654651443460275 | validation: 0.09677498858601535]
	TIME [epoch: 2.73 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06454607619131691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06454607619131691 | validation: 0.08914154575722724]
	TIME [epoch: 2.72 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04168858692492505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04168858692492505 | validation: 0.07582016144134818]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04408150775266671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04408150775266671 | validation: 0.09122813312522912]
	TIME [epoch: 2.72 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04706706742758659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04706706742758659 | validation: 0.08506068515830895]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05998631353174621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05998631353174621 | validation: 0.1623456192040847]
	TIME [epoch: 2.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11802819751597442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11802819751597442 | validation: 0.1986725371078827]
	TIME [epoch: 2.73 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2020071601712965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2020071601712965 | validation: 0.24010317096245692]
	TIME [epoch: 2.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17960984261069635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17960984261069635 | validation: 0.10117314310309711]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07031181618127272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07031181618127272 | validation: 0.0558709701141496]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_754.pth
	Model improved!!!
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036790124022489316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036790124022489316 | validation: 0.08036452890407514]
	TIME [epoch: 2.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04856016918208024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04856016918208024 | validation: 0.09399765946193733]
	TIME [epoch: 2.73 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08066247397996293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08066247397996293 | validation: 0.19891628679567822]
	TIME [epoch: 2.73 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12459833967935117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12459833967935117 | validation: 0.14498178224969613]
	TIME [epoch: 2.73 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12266866596858346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12266866596858346 | validation: 0.10692267974396417]
	TIME [epoch: 2.72 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07498309109685489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07498309109685489 | validation: 0.0782036665849513]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04740165426291007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04740165426291007 | validation: 0.068268885308418]
	TIME [epoch: 2.72 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262717674755141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04262717674755141 | validation: 0.15984009177360425]
	TIME [epoch: 2.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1103382601734036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1103382601734036 | validation: 0.22810907933329796]
	TIME [epoch: 2.73 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23346672403270824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23346672403270824 | validation: 0.10236465134430409]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07479678347865326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07479678347865326 | validation: 0.07678589783594998]
	TIME [epoch: 2.72 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032690032464086576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032690032464086576 | validation: 0.07955062674389131]
	TIME [epoch: 2.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03765526627954853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03765526627954853 | validation: 0.07040040580117639]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047971845892585685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047971845892585685 | validation: 0.08812052698301785]
	TIME [epoch: 2.72 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823427405123544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05823427405123544 | validation: 0.10825228224505341]
	TIME [epoch: 2.73 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955607630940078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0955607630940078 | validation: 0.17430000173896296]
	TIME [epoch: 2.73 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13195445424504482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13195445424504482 | validation: 0.17909261907042073]
	TIME [epoch: 2.73 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13391058474562567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13391058474562567 | validation: 0.10613571383073098]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06130030127565595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06130030127565595 | validation: 0.08706174573291008]
	TIME [epoch: 2.73 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04224151288910543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04224151288910543 | validation: 0.10706444636109694]
	TIME [epoch: 2.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10218205816946037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10218205816946037 | validation: 0.2963521953413765]
	TIME [epoch: 2.73 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4145395303992475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4145395303992475 | validation: 0.21356551962606174]
	TIME [epoch: 2.73 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1797590745397288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1797590745397288 | validation: 0.29229222545826977]
	TIME [epoch: 2.73 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2243532315038765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2243532315038765 | validation: 0.07614275478780984]
	TIME [epoch: 2.72 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06585150866633827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06585150866633827 | validation: 0.14547957951688326]
	TIME [epoch: 2.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09597618177877157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09597618177877157 | validation: 0.11238018010590095]
	TIME [epoch: 2.72 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06274440047046163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06274440047046163 | validation: 0.06017401229800171]
	TIME [epoch: 2.73 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031212075722450914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031212075722450914 | validation: 0.07704691401435507]
	TIME [epoch: 2.72 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03226249350281485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03226249350281485 | validation: 0.06623433709543354]
	TIME [epoch: 2.72 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03397555106842204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03397555106842204 | validation: 0.060760267596080886]
	TIME [epoch: 2.72 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034024402217664645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034024402217664645 | validation: 0.06361477333552518]
	TIME [epoch: 2.73 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048412564353924596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048412564353924596 | validation: 0.1367006577206293]
	TIME [epoch: 2.73 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11022245696401388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11022245696401388 | validation: 0.29629426489886]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23151060245709804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23151060245709804 | validation: 0.19403199956231132]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16511575502766146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16511575502766146 | validation: 0.1553389054384111]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11207457478623618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11207457478623618 | validation: 0.13040354245418426]
	TIME [epoch: 2.72 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08981117930610505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08981117930610505 | validation: 0.10012180816310895]
	TIME [epoch: 2.73 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049254084512330785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049254084512330785 | validation: 0.04830452595999266]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027760610433271573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027760610433271573 | validation: 0.04690976432548986]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_793.pth
	Model improved!!!
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025384975296955326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025384975296955326 | validation: 0.04698335968091105]
	TIME [epoch: 2.72 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024753377864754404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024753377864754404 | validation: 0.058014534703568627]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699823473621849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03699823473621849 | validation: 0.12638914328395262]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13417437731721119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13417437731721119 | validation: 0.3830699620133314]
	TIME [epoch: 2.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840450954113474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2840450954113474 | validation: 0.15260515851573284]
	TIME [epoch: 2.72 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14391725531130894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14391725531130894 | validation: 0.15536508813500963]
	TIME [epoch: 2.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10657908727357214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10657908727357214 | validation: 0.1536689966351481]
	TIME [epoch: 2.71 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1275911272730821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1275911272730821 | validation: 0.10684065674797301]
	TIME [epoch: 2.73 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05354645151181426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05354645151181426 | validation: 0.061628503590870604]
	TIME [epoch: 2.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031351753624287625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031351753624287625 | validation: 0.058924139091916385]
	TIME [epoch: 2.72 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029609651153951148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029609651153951148 | validation: 0.06388115410498872]
	TIME [epoch: 2.72 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02514395079368309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02514395079368309 | validation: 0.03702241377189615]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022627379815834403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022627379815834403 | validation: 0.04954932818868866]
	TIME [epoch: 2.72 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026431399309412403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026431399309412403 | validation: 0.05439100109568276]
	TIME [epoch: 2.72 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054491822178474036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054491822178474036 | validation: 0.1607926117299676]
	TIME [epoch: 2.72 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15156661289915077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15156661289915077 | validation: 0.26544182876719197]
	TIME [epoch: 2.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533553390182325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2533553390182325 | validation: 0.29222837641618726]
	TIME [epoch: 2.72 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21823574872580814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21823574872580814 | validation: 0.14509084207843137]
	TIME [epoch: 2.72 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11150728782556929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11150728782556929 | validation: 0.13014768253491024]
	TIME [epoch: 2.72 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08679668783252766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08679668783252766 | validation: 0.08807890580085297]
	TIME [epoch: 2.72 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05872798088636161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05872798088636161 | validation: 0.09503278989768937]
	TIME [epoch: 2.72 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726851851409489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04726851851409489 | validation: 0.07593294301518289]
	TIME [epoch: 2.73 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05024450635214874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05024450635214874 | validation: 0.09912400424796941]
	TIME [epoch: 2.72 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05162315146918225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05162315146918225 | validation: 0.06477405641292461]
	TIME [epoch: 2.78 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050165070765487645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050165070765487645 | validation: 0.1017525167333321]
	TIME [epoch: 2.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140068489243811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06140068489243811 | validation: 0.1201573329677958]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09111796598722137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09111796598722137 | validation: 0.10326035785377191]
	TIME [epoch: 2.73 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07856766355455587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07856766355455587 | validation: 0.10345704761929375]
	TIME [epoch: 2.72 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192351325605678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192351325605678 | validation: 0.10853004786490733]
	TIME [epoch: 2.72 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09611493840198543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09611493840198543 | validation: 0.15078058237391145]
	TIME [epoch: 2.72 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11852636757046853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11852636757046853 | validation: 0.12198713704165909]
	TIME [epoch: 2.82 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12052655062856012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12052655062856012 | validation: 0.0835593771285073]
	TIME [epoch: 2.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054732992218564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054732992218564 | validation: 0.04709224044931165]
	TIME [epoch: 2.72 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029824008419789556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029824008419789556 | validation: 0.04688436852648012]
	TIME [epoch: 2.74 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027756987901222364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027756987901222364 | validation: 0.12527962060254513]
	TIME [epoch: 2.74 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897265469034618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06897265469034618 | validation: 0.16330551983772215]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18552950668923765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18552950668923765 | validation: 0.14451144504516394]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10924014177383025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10924014177383025 | validation: 0.11445342986508718]
	TIME [epoch: 2.73 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860256900134676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0860256900134676 | validation: 0.12202004849100839]
	TIME [epoch: 2.84 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209137661814558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07209137661814558 | validation: 0.0756513176827266]
	TIME [epoch: 2.73 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058864968533056596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058864968533056596 | validation: 0.07742385871260332]
	TIME [epoch: 2.73 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044126448329625236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044126448329625236 | validation: 0.05740268645971556]
	TIME [epoch: 2.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04544892421800378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04544892421800378 | validation: 0.07456967776076985]
	TIME [epoch: 2.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04799711601404365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04799711601404365 | validation: 0.0602889277487103]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06722452046625951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06722452046625951 | validation: 0.11698659059095529]
	TIME [epoch: 2.73 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08775549398200294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08775549398200294 | validation: 0.16309393447270268]
	TIME [epoch: 2.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11128646840555487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11128646840555487 | validation: 0.12743955048492336]
	TIME [epoch: 2.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07921357272046399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07921357272046399 | validation: 0.07920298604923504]
	TIME [epoch: 2.72 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310146087696052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06310146087696052 | validation: 0.13271388186319183]
	TIME [epoch: 2.72 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08973997056831422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08973997056831422 | validation: 0.17765643438824053]
	TIME [epoch: 2.72 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.137530080808889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.137530080808889 | validation: 0.1740827125598953]
	TIME [epoch: 2.73 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12209124460581616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12209124460581616 | validation: 0.10168932660626524]
	TIME [epoch: 2.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07674509791496396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07674509791496396 | validation: 0.0622012501553341]
	TIME [epoch: 2.72 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03554549020200951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03554549020200951 | validation: 0.05246224070693011]
	TIME [epoch: 2.72 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025282872108715684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025282872108715684 | validation: 0.049457802734637436]
	TIME [epoch: 2.73 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027559496429798613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027559496429798613 | validation: 0.06665992129042511]
	TIME [epoch: 2.72 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04702744523128392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04702744523128392 | validation: 0.14064805830617733]
	TIME [epoch: 2.72 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12269551584686156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12269551584686156 | validation: 0.21448035223173553]
	TIME [epoch: 2.72 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14373999957043676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14373999957043676 | validation: 0.12257059999986815]
	TIME [epoch: 2.72 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08951669852770838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08951669852770838 | validation: 0.12038370074535867]
	TIME [epoch: 2.73 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08409081108980548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08409081108980548 | validation: 0.13248010901220605]
	TIME [epoch: 2.73 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284782005493468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284782005493468 | validation: 0.1299781797057992]
	TIME [epoch: 2.73 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340967426518363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07340967426518363 | validation: 0.0629116238174121]
	TIME [epoch: 2.73 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041075080247079246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041075080247079246 | validation: 0.041676478792291]
	TIME [epoch: 2.73 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256908493358856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03256908493358856 | validation: 0.05858966945372865]
	TIME [epoch: 2.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038041462874496945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038041462874496945 | validation: 0.09827978047121035]
	TIME [epoch: 2.73 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07143344284957458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07143344284957458 | validation: 0.16097526034610213]
	TIME [epoch: 2.72 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531032009120768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531032009120768 | validation: 0.27371080148303667]
	TIME [epoch: 2.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20027512352055346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20027512352055346 | validation: 0.09172972266754087]
	TIME [epoch: 2.71 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06309759132095628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06309759132095628 | validation: 0.05865172811372478]
	TIME [epoch: 2.71 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025113357605306596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025113357605306596 | validation: 0.0493790494242701]
	TIME [epoch: 2.71 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02956875591095898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02956875591095898 | validation: 0.06385657302273472]
	TIME [epoch: 2.71 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026742308907995747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026742308907995747 | validation: 0.050212391640021]
	TIME [epoch: 2.71 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029998712564096645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029998712564096645 | validation: 0.056970218464382494]
	TIME [epoch: 2.71 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03803385706286367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03803385706286367 | validation: 0.09145492838134495]
	TIME [epoch: 2.71 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07302463753371827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07302463753371827 | validation: 0.16405451692675088]
	TIME [epoch: 2.71 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366125884675822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1366125884675822 | validation: 0.16836902765658746]
	TIME [epoch: 2.71 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14242772049278427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14242772049278427 | validation: 0.0766678004539426]
	TIME [epoch: 2.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06903998160769176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06903998160769176 | validation: 0.054918412688332564]
	TIME [epoch: 2.72 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032625606016016814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032625606016016814 | validation: 0.03541728852873223]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02701816572316977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02701816572316977 | validation: 0.07032597710121634]
	TIME [epoch: 2.73 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04095023992535741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04095023992535741 | validation: 0.14120862736084708]
	TIME [epoch: 2.73 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11351843467606802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351843467606802 | validation: 0.1864753076835185]
	TIME [epoch: 2.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21881320765156376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21881320765156376 | validation: 0.07787636484919996]
	TIME [epoch: 2.73 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0454311108366333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0454311108366333 | validation: 0.073037161817977]
	TIME [epoch: 2.71 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044814019204534344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044814019204534344 | validation: 0.08475595012885562]
	TIME [epoch: 2.71 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053664294473531785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053664294473531785 | validation: 0.06281843728666205]
	TIME [epoch: 2.71 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05111847229159269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05111847229159269 | validation: 0.08334195458405258]
	TIME [epoch: 2.71 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07587794351482825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07587794351482825 | validation: 0.13388425059498404]
	TIME [epoch: 2.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11833763468125608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11833763468125608 | validation: 0.13973285912799752]
	TIME [epoch: 2.71 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12346335692957641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12346335692957641 | validation: 0.10118658628045396]
	TIME [epoch: 2.71 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07277829908623494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07277829908623494 | validation: 0.06879318017629535]
	TIME [epoch: 2.71 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043243482827020244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043243482827020244 | validation: 0.04965724759106821]
	TIME [epoch: 2.71 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03579339437617019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03579339437617019 | validation: 0.0798328199339025]
	TIME [epoch: 2.71 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04807333357139264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04807333357139264 | validation: 0.07187211331185103]
	TIME [epoch: 2.71 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054916561533013344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054916561533013344 | validation: 0.11776613407375236]
	TIME [epoch: 2.71 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07667475507046577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07667475507046577 | validation: 0.08533018531781077]
	TIME [epoch: 2.71 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484006036987688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06484006036987688 | validation: 0.07910863115571137]
	TIME [epoch: 2.71 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044899330253057465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044899330253057465 | validation: 0.04819607031442527]
	TIME [epoch: 2.71 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03412547151974366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03412547151974366 | validation: 0.08058378359071894]
	TIME [epoch: 2.71 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508384528870907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06508384528870907 | validation: 0.1758139703423631]
	TIME [epoch: 2.72 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14990065602355868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14990065602355868 | validation: 0.18999604225193512]
	TIME [epoch: 2.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1724596509764495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1724596509764495 | validation: 0.11859675846632722]
	TIME [epoch: 2.72 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016676465186534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09016676465186534 | validation: 0.09514081119514593]
	TIME [epoch: 2.73 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07092887544345891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07092887544345891 | validation: 0.0541816497494581]
	TIME [epoch: 2.72 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05063287897812839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05063287897812839 | validation: 0.05706156643607701]
	TIME [epoch: 2.72 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033647280793266815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033647280793266815 | validation: 0.04698181955627974]
	TIME [epoch: 2.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02449265498783735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02449265498783735 | validation: 0.0384772992236733]
	TIME [epoch: 2.73 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020141531526134352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020141531526134352 | validation: 0.03550941402930554]
	TIME [epoch: 2.73 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018661739031953974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018661739031953974 | validation: 0.04158947660238849]
	TIME [epoch: 2.73 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026183066844014717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026183066844014717 | validation: 0.11111951513280194]
	TIME [epoch: 2.72 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08039128346852031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08039128346852031 | validation: 0.3748369222967609]
	TIME [epoch: 2.72 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3000581021728517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3000581021728517 | validation: 0.1496215416182569]
	TIME [epoch: 2.72 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498440306510535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498440306510535 | validation: 0.06983271978898746]
	TIME [epoch: 2.73 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906815984446399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03906815984446399 | validation: 0.06660660889772764]
	TIME [epoch: 2.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046517289389927496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046517289389927496 | validation: 0.11112382517679656]
	TIME [epoch: 2.72 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06095851725450938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06095851725450938 | validation: 0.06457380655699856]
	TIME [epoch: 2.73 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219308828843234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05219308828843234 | validation: 0.07545100716804781]
	TIME [epoch: 2.72 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042520488847860104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042520488847860104 | validation: 0.05873881130012744]
	TIME [epoch: 2.72 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05088232516238108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05088232516238108 | validation: 0.10677280318234841]
	TIME [epoch: 2.72 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07679924867843356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07679924867843356 | validation: 0.13610154460517457]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254449079121694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10254449079121694 | validation: 0.08133810741571332]
	TIME [epoch: 2.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07244058427751272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07244058427751272 | validation: 0.07848088223152994]
	TIME [epoch: 2.72 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05408389192508396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05408389192508396 | validation: 0.09964085702121528]
	TIME [epoch: 2.73 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06362672017044527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06362672017044527 | validation: 0.11586582980494414]
	TIME [epoch: 2.73 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003518788521664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09003518788521664 | validation: 0.06487900712778805]
	TIME [epoch: 2.74 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0530324521552777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0530324521552777 | validation: 0.06280146304546352]
	TIME [epoch: 2.73 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03213703229782116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03213703229782116 | validation: 0.04274663500471583]
	TIME [epoch: 2.72 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026570676357971976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026570676357971976 | validation: 0.05079073581677271]
	TIME [epoch: 2.73 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026597221396261066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026597221396261066 | validation: 0.036882873817034756]
	TIME [epoch: 2.72 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03964098168115394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03964098168115394 | validation: 0.08755918279707202]
	TIME [epoch: 2.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08769778134595178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08769778134595178 | validation: 0.1742740865496395]
	TIME [epoch: 2.73 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1674672564455556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1674672564455556 | validation: 0.14440055075732014]
	TIME [epoch: 2.73 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12179961821147929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12179961821147929 | validation: 0.11003707629119786]
	TIME [epoch: 2.71 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08631838552650141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08631838552650141 | validation: 0.06639342716183681]
	TIME [epoch: 2.71 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04605428863868909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04605428863868909 | validation: 0.07392892210874437]
	TIME [epoch: 2.71 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045498692988008196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045498692988008196 | validation: 0.06027293468135068]
	TIME [epoch: 2.71 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046553695978054054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046553695978054054 | validation: 0.09062848646584769]
	TIME [epoch: 2.71 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424588818788486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0424588818788486 | validation: 0.0481526892216905]
	TIME [epoch: 2.71 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03674872893019242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03674872893019242 | validation: 0.0729214697634693]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03428442864771656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03428442864771656 | validation: 0.03657151183260136]
	TIME [epoch: 2.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0325723335389852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0325723335389852 | validation: 0.08337655330307553]
	TIME [epoch: 2.73 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048803859094740075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048803859094740075 | validation: 0.09146725657964846]
	TIME [epoch: 2.72 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930607618346437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07930607618346437 | validation: 0.1726069104206551]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13450980623476944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13450980623476944 | validation: 0.22249590619654167]
	TIME [epoch: 2.73 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18606157138344645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18606157138344645 | validation: 0.10548943722156195]
	TIME [epoch: 2.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12452685349503753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12452685349503753 | validation: 0.06796297402974114]
	TIME [epoch: 2.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04096032356021064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04096032356021064 | validation: 0.06313318152091815]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036728107772419916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036728107772419916 | validation: 0.043864910832595985]
	TIME [epoch: 2.73 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033940602558744884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033940602558744884 | validation: 0.05291133011902113]
	TIME [epoch: 2.73 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040693076623899385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040693076623899385 | validation: 0.08311103280634934]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05252886760952317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05252886760952317 | validation: 0.08442747430111683]
	TIME [epoch: 2.73 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638709558136793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638709558136793 | validation: 0.1028622117215372]
	TIME [epoch: 2.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08446825747592682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08446825747592682 | validation: 0.13894166698541302]
	TIME [epoch: 2.73 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10736562318330331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10736562318330331 | validation: 0.10744498888843304]
	TIME [epoch: 2.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07751025307693547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07751025307693547 | validation: 0.04259735668734482]
	TIME [epoch: 2.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031851997042819685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031851997042819685 | validation: 0.0390429502446355]
	TIME [epoch: 2.71 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016339249822312305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016339249822312305 | validation: 0.046910645080323866]
	TIME [epoch: 2.72 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01767812483641882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01767812483641882 | validation: 0.03973808434851578]
	TIME [epoch: 2.72 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022413193280627106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022413193280627106 | validation: 0.07557864245752359]
	TIME [epoch: 2.71 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820739332024731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03820739332024731 | validation: 0.11791090851307656]
	TIME [epoch: 2.72 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640385572328323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07640385572328323 | validation: 0.1311405738970629]
	TIME [epoch: 2.72 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768738003993085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09768738003993085 | validation: 0.06866174383476538]
	TIME [epoch: 2.72 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532201213803885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05532201213803885 | validation: 0.08420639067711988]
	TIME [epoch: 2.71 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043033728724063566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043033728724063566 | validation: 0.06787696385766208]
	TIME [epoch: 2.72 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05591811362840852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05591811362840852 | validation: 0.08677542535914123]
	TIME [epoch: 2.71 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10616177476135619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10616177476135619 | validation: 0.15446596127805245]
	TIME [epoch: 2.75 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15419909960034348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15419909960034348 | validation: 0.21260085159941258]
	TIME [epoch: 2.71 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14511870429072718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14511870429072718 | validation: 0.09595610116590225]
	TIME [epoch: 2.72 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06877395086457161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06877395086457161 | validation: 0.03244893130405237]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_963.pth
	Model improved!!!
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029513617369113093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029513617369113093 | validation: 0.04945410881562661]
	TIME [epoch: 2.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027967160289297785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027967160289297785 | validation: 0.040023268796984035]
	TIME [epoch: 2.72 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029154860542263558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029154860542263558 | validation: 0.05194762122256827]
	TIME [epoch: 2.72 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02957284914693127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02957284914693127 | validation: 0.04768282974192187]
	TIME [epoch: 2.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041059297637619355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041059297637619355 | validation: 0.11190717425667492]
	TIME [epoch: 2.72 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07737451282863217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07737451282863217 | validation: 0.10102164859301364]
	TIME [epoch: 2.71 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534952967442602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06534952967442602 | validation: 0.0854983166978095]
	TIME [epoch: 2.72 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624955615514721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04624955615514721 | validation: 0.0561631247104345]
	TIME [epoch: 2.72 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04338842666704646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04338842666704646 | validation: 0.09766021073501571]
	TIME [epoch: 2.72 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0936544089720915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0936544089720915 | validation: 0.15266401291087056]
	TIME [epoch: 2.72 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14707320199702376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14707320199702376 | validation: 0.12887709126758856]
	TIME [epoch: 2.72 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10783355724711963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10783355724711963 | validation: 0.07513959908633505]
	TIME [epoch: 2.72 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047481872935949174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047481872935949174 | validation: 0.05385612993499196]
	TIME [epoch: 2.72 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03130413759204319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03130413759204319 | validation: 0.03161793668538944]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_977.pth
	Model improved!!!
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024563392251386477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024563392251386477 | validation: 0.05212182018594084]
	TIME [epoch: 2.71 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026446807791616563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026446807791616563 | validation: 0.04370262929513348]
	TIME [epoch: 2.71 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03395983640953856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03395983640953856 | validation: 0.09037370181621358]
	TIME [epoch: 2.72 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059649478075331185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059649478075331185 | validation: 0.0924565444500316]
	TIME [epoch: 2.73 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06890562213678929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06890562213678929 | validation: 0.08078336028358957]
	TIME [epoch: 2.73 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051776678776942316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051776678776942316 | validation: 0.057117288152041146]
	TIME [epoch: 2.73 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476516638388749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476516638388749 | validation: 0.10428012133931715]
	TIME [epoch: 2.73 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0929443583251798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0929443583251798 | validation: 0.15639173894637728]
	TIME [epoch: 2.72 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15925022057038782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15925022057038782 | validation: 0.1323322043115711]
	TIME [epoch: 2.72 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606166866664091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10606166866664091 | validation: 0.06368400279665591]
	TIME [epoch: 2.73 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03596925819079088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03596925819079088 | validation: 0.048230710904811694]
	TIME [epoch: 2.73 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02290773216282628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02290773216282628 | validation: 0.031724452098944825]
	TIME [epoch: 2.73 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025388259895071662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025388259895071662 | validation: 0.06509544407764382]
	TIME [epoch: 2.72 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03917468720555867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03917468720555867 | validation: 0.0986393351830989]
	TIME [epoch: 2.72 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06925209559510853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06925209559510853 | validation: 0.13261420516046304]
	TIME [epoch: 2.72 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1105833513112678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1105833513112678 | validation: 0.06290308907462398]
	TIME [epoch: 2.72 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077599904836642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05077599904836642 | validation: 0.07863419440360708]
	TIME [epoch: 2.72 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05908906499327474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05908906499327474 | validation: 0.10167452798051937]
	TIME [epoch: 2.73 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08513572959620216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08513572959620216 | validation: 0.10767055791357848]
	TIME [epoch: 2.72 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09660805609121846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09660805609121846 | validation: 0.08253759473460588]
	TIME [epoch: 2.72 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05567708556598713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05567708556598713 | validation: 0.07271119390998937]
	TIME [epoch: 2.72 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813179930963291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03813179930963291 | validation: 0.029278685833045603]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_999.pth
	Model improved!!!
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03350430896848754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03350430896848754 | validation: 0.07215544515250638]
	TIME [epoch: 2.72 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04059572855942762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04059572855942762 | validation: 0.07096927411523808]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057973313520281754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057973313520281754 | validation: 0.09476031034924082]
	TIME [epoch: 5.84 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06877940629497965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06877940629497965 | validation: 0.0883505635346529]
	TIME [epoch: 5.83 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07413828471418256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07413828471418256 | validation: 0.07461133381105342]
	TIME [epoch: 5.83 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06233648581239143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06233648581239143 | validation: 0.08307777615309812]
	TIME [epoch: 5.84 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061288866820358195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061288866820358195 | validation: 0.06871894390645882]
	TIME [epoch: 5.83 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07282324342693132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07282324342693132 | validation: 0.06714520108879324]
	TIME [epoch: 5.83 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049842936569374885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049842936569374885 | validation: 0.03771487708399829]
	TIME [epoch: 5.84 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033780315115097964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033780315115097964 | validation: 0.04724802727518507]
	TIME [epoch: 5.83 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0275627975631015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0275627975631015 | validation: 0.04212584749915985]
	TIME [epoch: 5.83 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03209056106956183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03209056106956183 | validation: 0.09701352941210889]
	TIME [epoch: 5.84 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060666813944184605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060666813944184605 | validation: 0.11548246877204907]
	TIME [epoch: 5.84 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08412018594080951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08412018594080951 | validation: 0.12177273217645848]
	TIME [epoch: 5.84 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743514863155052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08743514863155052 | validation: 0.1292300711031]
	TIME [epoch: 5.84 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10335129830434273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10335129830434273 | validation: 0.11862912687070898]
	TIME [epoch: 5.84 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11651706223416816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11651706223416816 | validation: 0.06395392507310957]
	TIME [epoch: 5.84 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404079345974019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0404079345974019 | validation: 0.06370617902217271]
	TIME [epoch: 5.83 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02987796966407403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02987796966407403 | validation: 0.024801328366571097]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1018.pth
	Model improved!!!
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023750681609352772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023750681609352772 | validation: 0.04366281106466692]
	TIME [epoch: 5.83 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016100704927763162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016100704927763162 | validation: 0.022793191673474503]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1020.pth
	Model improved!!!
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01531624673389223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01531624673389223 | validation: 0.033528888800158134]
	TIME [epoch: 5.83 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016091878054847943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016091878054847943 | validation: 0.022534271025098473]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023479825337289206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023479825337289206 | validation: 0.08737454689230854]
	TIME [epoch: 5.85 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06154197647758539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06154197647758539 | validation: 0.1876693093906364]
	TIME [epoch: 5.84 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13936106348429128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13936106348429128 | validation: 0.14263773277873762]
	TIME [epoch: 5.83 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10785697426073718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10785697426073718 | validation: 0.09088604374465736]
	TIME [epoch: 5.83 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252156647275584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252156647275584 | validation: 0.16326827332775254]
	TIME [epoch: 5.83 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16090781416948235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16090781416948235 | validation: 0.10792290353022449]
	TIME [epoch: 5.83 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09002084139138221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09002084139138221 | validation: 0.09145794223122154]
	TIME [epoch: 5.84 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0422411061817012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0422411061817012 | validation: 0.03602416437749395]
	TIME [epoch: 5.83 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023580204699507615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023580204699507615 | validation: 0.04414593209730944]
	TIME [epoch: 5.83 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020199062774477736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020199062774477736 | validation: 0.034776506218708814]
	TIME [epoch: 5.84 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020567231988393306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020567231988393306 | validation: 0.04084510852064136]
	TIME [epoch: 5.84 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02539937305251571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02539937305251571 | validation: 0.06176316772178103]
	TIME [epoch: 5.84 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519487312402194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0519487312402194 | validation: 0.13176924568030265]
	TIME [epoch: 5.83 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12684301847648813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12684301847648813 | validation: 0.1721092918367842]
	TIME [epoch: 5.83 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15584880840019585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15584880840019585 | validation: 0.1225863093763262]
	TIME [epoch: 5.84 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09074643003547254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09074643003547254 | validation: 0.0668640471927478]
	TIME [epoch: 5.84 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04694411728147263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04694411728147263 | validation: 0.03802480950823183]
	TIME [epoch: 5.84 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410671235731448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03410671235731448 | validation: 0.04799805749898559]
	TIME [epoch: 5.84 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03466610792279958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03466610792279958 | validation: 0.04897939341992457]
	TIME [epoch: 5.83 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0318400762095988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0318400762095988 | validation: 0.0443690141017739]
	TIME [epoch: 5.83 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030167499437112423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030167499437112423 | validation: 0.05319552965422225]
	TIME [epoch: 5.84 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03577555505154437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03577555505154437 | validation: 0.05908565846808156]
	TIME [epoch: 5.84 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05053570531391038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05053570531391038 | validation: 0.09590178010676371]
	TIME [epoch: 5.83 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08022818081333313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08022818081333313 | validation: 0.13000547450901548]
	TIME [epoch: 5.83 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10101526954208499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10101526954208499 | validation: 0.08901861185376825]
	TIME [epoch: 5.84 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534041473648404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06534041473648404 | validation: 0.05189433488939125]
	TIME [epoch: 5.83 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028295807739402115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028295807739402115 | validation: 0.02835574225432689]
	TIME [epoch: 5.83 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015039656849685628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015039656849685628 | validation: 0.0446042694695902]
	TIME [epoch: 5.83 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021602062022126153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021602062022126153 | validation: 0.04214915919638475]
	TIME [epoch: 5.84 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034490656736757795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034490656736757795 | validation: 0.10283489467617475]
	TIME [epoch: 5.84 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06358019629310464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06358019629310464 | validation: 0.10387938181782497]
	TIME [epoch: 5.83 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11432857023939502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11432857023939502 | validation: 0.12912125203709396]
	TIME [epoch: 5.83 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09923739970247994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09923739970247994 | validation: 0.11666353298283715]
	TIME [epoch: 5.83 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07861313450635861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07861313450635861 | validation: 0.08494573655498466]
	TIME [epoch: 5.84 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06863928408887818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06863928408887818 | validation: 0.08399524706576696]
	TIME [epoch: 5.83 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0543200903725537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0543200903725537 | validation: 0.05340756384653214]
	TIME [epoch: 5.84 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03856026941714526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03856026941714526 | validation: 0.04764422394364594]
	TIME [epoch: 5.83 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03085146389919946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03085146389919946 | validation: 0.04249493035554506]
	TIME [epoch: 5.83 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02793381598876894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02793381598876894 | validation: 0.043857994517457935]
	TIME [epoch: 5.83 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223832150946446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03223832150946446 | validation: 0.05864522434769055]
	TIME [epoch: 5.84 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055973363728589194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055973363728589194 | validation: 0.11791218495217999]
	TIME [epoch: 5.83 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09145111579149737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09145111579149737 | validation: 0.12572146321567598]
	TIME [epoch: 5.83 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10793957864065029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10793957864065029 | validation: 0.07583054185482725]
	TIME [epoch: 5.83 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055743120467093434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055743120467093434 | validation: 0.036840684176632464]
	TIME [epoch: 5.83 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020780989641201574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020780989641201574 | validation: 0.022160639289950025]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1067.pth
	Model improved!!!
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016185961131452993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016185961131452993 | validation: 0.048960639076955095]
	TIME [epoch: 5.83 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02804881579716884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02804881579716884 | validation: 0.055520891324354796]
	TIME [epoch: 5.84 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06243656384582524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06243656384582524 | validation: 0.1437436560957976]
	TIME [epoch: 5.83 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10181120671314442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10181120671314442 | validation: 0.07421919900771994]
	TIME [epoch: 5.83 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07130819794753855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07130819794753855 | validation: 0.04746950018175679]
	TIME [epoch: 5.83 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04214823954776829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04214823954776829 | validation: 0.08258086888445573]
	TIME [epoch: 5.84 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04502481318431745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04502481318431745 | validation: 0.09218746718567505]
	TIME [epoch: 5.84 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683559989401721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0683559989401721 | validation: 0.1012985530812911]
	TIME [epoch: 5.84 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746071834528242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746071834528242 | validation: 0.07264585799508076]
	TIME [epoch: 5.83 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06713633603837313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06713633603837313 | validation: 0.05274050308981612]
	TIME [epoch: 5.84 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743295289040193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03743295289040193 | validation: 0.04389364555031287]
	TIME [epoch: 5.84 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02482606202002632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02482606202002632 | validation: 0.031871098405315566]
	TIME [epoch: 5.83 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02412446891750302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02412446891750302 | validation: 0.04314085812303762]
	TIME [epoch: 5.84 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03021199332539535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03021199332539535 | validation: 0.05564454971627289]
	TIME [epoch: 5.83 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05203225422178739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05203225422178739 | validation: 0.10949195106599019]
	TIME [epoch: 5.83 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08113913935699531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08113913935699531 | validation: 0.08512417265509958]
	TIME [epoch: 5.83 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08607798510954517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08607798510954517 | validation: 0.0812001745942765]
	TIME [epoch: 5.83 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04758502010164745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04758502010164745 | validation: 0.034289560595277456]
	TIME [epoch: 5.84 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02698016432554027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02698016432554027 | validation: 0.10195200903520536]
	TIME [epoch: 5.83 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741090954037308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06741090954037308 | validation: 0.16451849480302536]
	TIME [epoch: 5.83 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15654452837679328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15654452837679328 | validation: 0.1531186834967972]
	TIME [epoch: 5.83 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10263620227717751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10263620227717751 | validation: 0.04182690941205234]
	TIME [epoch: 5.84 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02837943644803099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02837943644803099 | validation: 0.02635041903972545]
	TIME [epoch: 5.84 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01740877875900646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01740877875900646 | validation: 0.05380919687697468]
	TIME [epoch: 5.83 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02293784131379138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02293784131379138 | validation: 0.03771733120606601]
	TIME [epoch: 5.83 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027172952428233334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027172952428233334 | validation: 0.06064564270175176]
	TIME [epoch: 5.83 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03592037546139214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03592037546139214 | validation: 0.05324846566149424]
	TIME [epoch: 5.85 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043988663980360804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043988663980360804 | validation: 0.07891682968842352]
	TIME [epoch: 5.84 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052403272941955306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052403272941955306 | validation: 0.05509310994858693]
	TIME [epoch: 5.83 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0547346093882924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0547346093882924 | validation: 0.0715753254990031]
	TIME [epoch: 5.83 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344070032077458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08344070032077458 | validation: 0.12848763313498437]
	TIME [epoch: 5.83 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10206401982249774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10206401982249774 | validation: 0.10051635144206335]
	TIME [epoch: 5.84 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08509755997096907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08509755997096907 | validation: 0.04050370160854444]
	TIME [epoch: 5.83 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02804355496354252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02804355496354252 | validation: 0.023950371440576313]
	TIME [epoch: 5.84 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012433456090461053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012433456090461053 | validation: 0.02053923446412792]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1102.pth
	Model improved!!!
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017478843653729956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017478843653729956 | validation: 0.03061730098952431]
	TIME [epoch: 5.81 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02243712363397178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02243712363397178 | validation: 0.03335605936045467]
	TIME [epoch: 5.81 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029532525243917324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029532525243917324 | validation: 0.06148192298927063]
	TIME [epoch: 5.82 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06209429169831575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06209429169831575 | validation: 0.19965324068306364]
	TIME [epoch: 5.82 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18113618386664299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18113618386664299 | validation: 0.18663926839707645]
	TIME [epoch: 5.81 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14107454594573818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14107454594573818 | validation: 0.042832577903104053]
	TIME [epoch: 5.84 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027653249442755202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027653249442755202 | validation: 0.05530630830899339]
	TIME [epoch: 5.83 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02691437194883724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02691437194883724 | validation: 0.059227271685423105]
	TIME [epoch: 5.83 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03930778387577723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03930778387577723 | validation: 0.05595744032399851]
	TIME [epoch: 5.83 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760624278611693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04760624278611693 | validation: 0.07385686472692952]
	TIME [epoch: 5.83 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05075758790056993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05075758790056993 | validation: 0.04874504373317206]
	TIME [epoch: 5.83 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949928491636357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05949928491636357 | validation: 0.05664675855541323]
	TIME [epoch: 5.84 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03920344273646245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03920344273646245 | validation: 0.0295004057997165]
	TIME [epoch: 5.83 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025089265150577682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025089265150577682 | validation: 0.03484303687223513]
	TIME [epoch: 5.83 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018926582929032622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018926582929032622 | validation: 0.02639148710765579]
	TIME [epoch: 5.83 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023930256715577247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023930256715577247 | validation: 0.09471063244647848]
	TIME [epoch: 5.83 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06253215753493355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06253215753493355 | validation: 0.12817733098605189]
	TIME [epoch: 5.83 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10135075160547574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10135075160547574 | validation: 0.1284560257831243]
	TIME [epoch: 5.83 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09334368149337269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09334368149337269 | validation: 0.11357143065507838]
	TIME [epoch: 5.83 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08512667001242917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08512667001242917 | validation: 0.10547983948045646]
	TIME [epoch: 5.83 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1004378008651559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1004378008651559 | validation: 0.05742004952312139]
	TIME [epoch: 5.83 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036520729688409886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036520729688409886 | validation: 0.048464492012654875]
	TIME [epoch: 5.84 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020416501136291378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020416501136291378 | validation: 0.028896265470708338]
	TIME [epoch: 5.83 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014501659337845696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014501659337845696 | validation: 0.03293272413398105]
	TIME [epoch: 5.83 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013891765597052083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013891765597052083 | validation: 0.024845607395241998]
	TIME [epoch: 5.83 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01762003549165044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01762003549165044 | validation: 0.035983262600443544]
	TIME [epoch: 5.83 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02722943485136884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02722943485136884 | validation: 0.04263570938375536]
	TIME [epoch: 5.83 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055564650508707576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055564650508707576 | validation: 0.08791336051937734]
	TIME [epoch: 5.83 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08603234290381241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08603234290381241 | validation: 0.11290788944963281]
	TIME [epoch: 5.83 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09828978564676731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09828978564676731 | validation: 0.13809064365519247]
	TIME [epoch: 5.83 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09824889794401251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09824889794401251 | validation: 0.13508902555204647]
	TIME [epoch: 5.83 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12506928630427042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12506928630427042 | validation: 0.048092751326700406]
	TIME [epoch: 5.83 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03518985438360306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03518985438360306 | validation: 0.034024611728613895]
	TIME [epoch: 5.84 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01891125050798489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01891125050798489 | validation: 0.042556898914761956]
	TIME [epoch: 5.83 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020273915404911243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020273915404911243 | validation: 0.02922561056226225]
	TIME [epoch: 5.83 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020921474005625017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020921474005625017 | validation: 0.03943344531455309]
	TIME [epoch: 5.83 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01986436738570834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01986436738570834 | validation: 0.027761607713891336]
	TIME [epoch: 5.83 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027561484134429257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027561484134429257 | validation: 0.0713927177084042]
	TIME [epoch: 5.83 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05443426833300625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05443426833300625 | validation: 0.11622962894978035]
	TIME [epoch: 5.84 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11262425068625667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11262425068625667 | validation: 0.14465360818196718]
	TIME [epoch: 5.83 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11557099224776712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11557099224776712 | validation: 0.06719985343126075]
	TIME [epoch: 5.83 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05971022288405891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05971022288405891 | validation: 0.042660181619348585]
	TIME [epoch: 5.83 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024707662994847446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024707662994847446 | validation: 0.03737317998805513]
	TIME [epoch: 5.83 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022287980999642337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022287980999642337 | validation: 0.03301604585026359]
	TIME [epoch: 5.83 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501332794941108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03501332794941108 | validation: 0.10362297182636643]
	TIME [epoch: 5.83 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06304625033073481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06304625033073481 | validation: 0.06830287085848556]
	TIME [epoch: 5.83 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04970721979761637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04970721979761637 | validation: 0.06416510842727584]
	TIME [epoch: 5.83 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036813894700362546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036813894700362546 | validation: 0.05038639181908703]
	TIME [epoch: 5.83 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039520377217877795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039520377217877795 | validation: 0.0596383025340993]
	TIME [epoch: 5.83 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06219246028524314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06219246028524314 | validation: 0.11775877705552631]
	TIME [epoch: 5.83 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08696499442957968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08696499442957968 | validation: 0.10024556940991106]
	TIME [epoch: 5.83 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08346016510974233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08346016510974233 | validation: 0.059907140021567155]
	TIME [epoch: 5.84 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043782694183395664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043782694183395664 | validation: 0.028337533613069277]
	TIME [epoch: 5.83 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02033108398092752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02033108398092752 | validation: 0.020755926873424015]
	TIME [epoch: 5.84 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013622131107182286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013622131107182286 | validation: 0.01621791565851337]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1157.pth
	Model improved!!!
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01378933023559184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01378933023559184 | validation: 0.04756499717923616]
	TIME [epoch: 5.81 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028374856833047076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028374856833047076 | validation: 0.11020497283066683]
	TIME [epoch: 5.81 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08977658505130771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08977658505130771 | validation: 0.14843995490438358]
	TIME [epoch: 5.81 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12120935534736889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12120935534736889 | validation: 0.04891418625685787]
	TIME [epoch: 5.8 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028539929953398692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028539929953398692 | validation: 0.045626141468272444]
	TIME [epoch: 5.81 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03807670318453948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03807670318453948 | validation: 0.06649259550779177]
	TIME [epoch: 5.82 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031195873001596743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031195873001596743 | validation: 0.06297360843065206]
	TIME [epoch: 5.81 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05056180952663521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05056180952663521 | validation: 0.13349938078327722]
	TIME [epoch: 5.81 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12699400890012005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12699400890012005 | validation: 0.12049873370472129]
	TIME [epoch: 5.81 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10471814872168417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10471814872168417 | validation: 0.06447586525034237]
	TIME [epoch: 5.84 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04816237052439888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04816237052439888 | validation: 0.029166383115426388]
	TIME [epoch: 5.84 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018363579162507592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018363579162507592 | validation: 0.023330851609839165]
	TIME [epoch: 5.84 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013296003887682924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013296003887682924 | validation: 0.020174740316269802]
	TIME [epoch: 5.84 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014683431770611035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014683431770611035 | validation: 0.021599147634800267]
	TIME [epoch: 5.82 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01615870485828236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01615870485828236 | validation: 0.024869046674217077]
	TIME [epoch: 5.83 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02282351408795454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02282351408795454 | validation: 0.04099502636557596]
	TIME [epoch: 5.83 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043947977708019136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043947977708019136 | validation: 0.08676463235530307]
	TIME [epoch: 5.83 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09227739799716991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09227739799716991 | validation: 0.18623769530199313]
	TIME [epoch: 5.84 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1503963521595267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1503963521595267 | validation: 0.16309301281065935]
	TIME [epoch: 5.84 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13607869527788866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13607869527788866 | validation: 0.03711270658515993]
	TIME [epoch: 5.83 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026446126301500037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026446126301500037 | validation: 0.0637355002214548]
	TIME [epoch: 5.81 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0406450684785645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0406450684785645 | validation: 0.06488754505812498]
	TIME [epoch: 5.81 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03275988798874046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03275988798874046 | validation: 0.039978348553716084]
	TIME [epoch: 5.8 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02537978731797377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02537978731797377 | validation: 0.04048141228682232]
	TIME [epoch: 5.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0245630333707658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0245630333707658 | validation: 0.06047489320474454]
	TIME [epoch: 5.81 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027804795220752958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027804795220752958 | validation: 0.0675777538201152]
	TIME [epoch: 5.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05899920925368682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05899920925368682 | validation: 0.11483437497649573]
	TIME [epoch: 5.81 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620013234385195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08620013234385195 | validation: 0.05689416603141339]
	TIME [epoch: 5.81 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645178327134923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0645178327134923 | validation: 0.02871360370522591]
	TIME [epoch: 5.84 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026372600716253806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026372600716253806 | validation: 0.03806166908370826]
	TIME [epoch: 5.84 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020809091343157952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020809091343157952 | validation: 0.05062989032004598]
	TIME [epoch: 5.84 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03802664998329362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03802664998329362 | validation: 0.12168027855363968]
	TIME [epoch: 5.83 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08863497391406071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08863497391406071 | validation: 0.10726654602341533]
	TIME [epoch: 5.81 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09580368475665148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09580368475665148 | validation: 0.08874764638269028]
	TIME [epoch: 5.81 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06887450290237446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06887450290237446 | validation: 0.057078781166451535]
	TIME [epoch: 5.81 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040750282102015006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040750282102015006 | validation: 0.034749965407862786]
	TIME [epoch: 5.81 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129790189336486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03129790189336486 | validation: 0.05252766554000169]
	TIME [epoch: 5.81 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025125883519992436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025125883519992436 | validation: 0.028114155890531668]
	TIME [epoch: 5.8 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01988688453605961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01988688453605961 | validation: 0.0419349439342026]
	TIME [epoch: 5.81 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01834496088185357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01834496088185357 | validation: 0.017900301143546616]
	TIME [epoch: 5.81 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020820169979332417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020820169979332417 | validation: 0.030948514920722415]
	TIME [epoch: 5.81 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162708309586441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03162708309586441 | validation: 0.04563923552015223]
	TIME [epoch: 5.82 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060466699143149664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060466699143149664 | validation: 0.09626926614213732]
	TIME [epoch: 5.81 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08812394166967924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08812394166967924 | validation: 0.18115489640632043]
	TIME [epoch: 5.85 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12880435296527215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12880435296527215 | validation: 0.16175520756556194]
	TIME [epoch: 5.83 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13144955875994876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13144955875994876 | validation: 0.10422650496022678]
	TIME [epoch: 5.84 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0667219970891376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0667219970891376 | validation: 0.05729728661428343]
	TIME [epoch: 5.83 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03504336094003188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03504336094003188 | validation: 0.04877265204473735]
	TIME [epoch: 5.84 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721387698146858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03721387698146858 | validation: 0.04954717087635027]
	TIME [epoch: 5.84 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02479144911046615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02479144911046615 | validation: 0.0326510464743426]
	TIME [epoch: 5.84 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017437378759532766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017437378759532766 | validation: 0.024858700408148516]
	TIME [epoch: 5.83 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013198228451647158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013198228451647158 | validation: 0.017620996048412264]
	TIME [epoch: 5.83 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010905723756583875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010905723756583875 | validation: 0.01912421579926478]
	TIME [epoch: 5.82 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012489367108909044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012489367108909044 | validation: 0.029435157782169187]
	TIME [epoch: 5.8 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024591671989694693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024591671989694693 | validation: 0.07271135099716172]
	TIME [epoch: 5.84 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07919665295114282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07919665295114282 | validation: 0.13749877848422795]
	TIME [epoch: 5.84 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1257407427453469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1257407427453469 | validation: 0.07539591299130974]
	TIME [epoch: 5.85 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07261489845644369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07261489845644369 | validation: 0.021320950025961682]
	TIME [epoch: 5.82 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015251783177281321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015251783177281321 | validation: 0.03772649690433682]
	TIME [epoch: 5.8 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022326990183465574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022326990183465574 | validation: 0.050846634928123025]
	TIME [epoch: 5.8 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05163812892546429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05163812892546429 | validation: 0.14954965444473742]
	TIME [epoch: 5.81 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13205993545130987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13205993545130987 | validation: 0.09508956610815271]
	TIME [epoch: 5.8 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622488444465463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07622488444465463 | validation: 0.06333926629067699]
	TIME [epoch: 5.82 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055985147204609814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055985147204609814 | validation: 0.05605486862587539]
	TIME [epoch: 5.83 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034857342480004896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034857342480004896 | validation: 0.03674114151420005]
	TIME [epoch: 5.84 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01730405937725167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01730405937725167 | validation: 0.02452250472788059]
	TIME [epoch: 5.84 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012153588167712956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012153588167712956 | validation: 0.02911046470454023]
	TIME [epoch: 5.83 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010591254829190077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010591254829190077 | validation: 0.012901257058122074]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1225.pth
	Model improved!!!
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011272782212585587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011272782212585587 | validation: 0.03696127888238439]
	TIME [epoch: 5.8 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014893252400533265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014893252400533265 | validation: 0.043492567694119]
	TIME [epoch: 5.8 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04409474006468309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04409474006468309 | validation: 0.17998208702137555]
	TIME [epoch: 5.81 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13925676714292348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13925676714292348 | validation: 0.16921535125223713]
	TIME [epoch: 5.8 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12867294062539023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12867294062539023 | validation: 0.07887898580902761]
	TIME [epoch: 5.8 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09576954709539572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09576954709539572 | validation: 0.07168876801864066]
	TIME [epoch: 5.8 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039560534279573276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039560534279573276 | validation: 0.030892600662173177]
	TIME [epoch: 5.81 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019409959555899775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019409959555899775 | validation: 0.022110203319855026]
	TIME [epoch: 5.81 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014913194168552626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014913194168552626 | validation: 0.024987834973227808]
	TIME [epoch: 5.8 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015012295502171638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015012295502171638 | validation: 0.020300181393182473]
	TIME [epoch: 5.8 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019300260567291214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019300260567291214 | validation: 0.04821600202375514]
	TIME [epoch: 5.84 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033388161013933174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033388161013933174 | validation: 0.06867104342264088]
	TIME [epoch: 5.83 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061753186410531805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061753186410531805 | validation: 0.08946565611401025]
	TIME [epoch: 5.84 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534626066948755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06534626066948755 | validation: 0.04961773476056073]
	TIME [epoch: 5.83 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047628869189060875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047628869189060875 | validation: 0.039107818873479]
	TIME [epoch: 5.84 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03532535772577414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03532535772577414 | validation: 0.10481452089136174]
	TIME [epoch: 5.84 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07278100171402423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07278100171402423 | validation: 0.11217835394235505]
	TIME [epoch: 5.84 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08902177221610807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08902177221610807 | validation: 0.0975772714441524]
	TIME [epoch: 5.83 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06181512919476135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06181512919476135 | validation: 0.03604371936144938]
	TIME [epoch: 5.83 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020378896738115663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020378896738115663 | validation: 0.025485296828715167]
	TIME [epoch: 5.83 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022884084363879377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022884084363879377 | validation: 0.05111184254113258]
	TIME [epoch: 5.8 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024181246642637877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024181246642637877 | validation: 0.03690876237257539]
	TIME [epoch: 5.81 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025964018876779252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025964018876779252 | validation: 0.04429425902044032]
	TIME [epoch: 5.81 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03978231828966076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03978231828966076 | validation: 0.06946752023622993]
	TIME [epoch: 5.8 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06757851090641527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06757851090641527 | validation: 0.08343774948427513]
	TIME [epoch: 5.8 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08443772957012194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08443772957012194 | validation: 0.1111493997386698]
	TIME [epoch: 5.8 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180487465313417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08180487465313417 | validation: 0.05348252608738266]
	TIME [epoch: 5.8 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05213870410270439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05213870410270439 | validation: 0.05531736070323215]
	TIME [epoch: 5.8 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393003840878119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03393003840878119 | validation: 0.02740714786337022]
	TIME [epoch: 5.81 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019793376871927584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019793376871927584 | validation: 0.041725162893939384]
	TIME [epoch: 5.8 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02238089999861594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02238089999861594 | validation: 0.034579110350201385]
	TIME [epoch: 5.81 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02567065481440552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02567065481440552 | validation: 0.047333344204210495]
	TIME [epoch: 5.81 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03121535372314019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03121535372314019 | validation: 0.04570428401487491]
	TIME [epoch: 5.81 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04637833678970525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04637833678970525 | validation: 0.06943191862119732]
	TIME [epoch: 5.8 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055262377888233745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055262377888233745 | validation: 0.03661533768016485]
	TIME [epoch: 5.81 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04835215249853694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04835215249853694 | validation: 0.026007137508197543]
	TIME [epoch: 5.8 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171255784298576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02171255784298576 | validation: 0.014001343453774984]
	TIME [epoch: 5.8 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010798108077773065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010798108077773065 | validation: 0.015480602606688411]
	TIME [epoch: 5.8 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012788325927116335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012788325927116335 | validation: 0.07489364742038976]
	TIME [epoch: 5.8 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286301862058409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04286301862058409 | validation: 0.20746482826949927]
	TIME [epoch: 5.82 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1856409980153171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1856409980153171 | validation: 0.18261855463874918]
	TIME [epoch: 5.81 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16717859100685778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16717859100685778 | validation: 0.07114815920566099]
	TIME [epoch: 5.81 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059210836888420576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059210836888420576 | validation: 0.0764792148029535]
	TIME [epoch: 5.81 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07317567547503982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07317567547503982 | validation: 0.06551357012036561]
	TIME [epoch: 5.81 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03096343650452027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03096343650452027 | validation: 0.04761609537589004]
	TIME [epoch: 5.81 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023624434376159194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023624434376159194 | validation: 0.02176374566172038]
	TIME [epoch: 5.84 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0216005670842814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0216005670842814 | validation: 0.04633361592533713]
	TIME [epoch: 5.85 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01445441810787616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01445441810787616 | validation: 0.025961029422024818]
	TIME [epoch: 5.82 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013284504142435809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013284504142435809 | validation: 0.017132466664408163]
	TIME [epoch: 5.82 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011347327216207419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011347327216207419 | validation: 0.02374178959372533]
	TIME [epoch: 5.82 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013575142425170245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013575142425170245 | validation: 0.04225298921889305]
	TIME [epoch: 5.81 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03308556954303742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03308556954303742 | validation: 0.13157198880000462]
	TIME [epoch: 5.81 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11283874271927756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11283874271927756 | validation: 0.16082176334408135]
	TIME [epoch: 5.82 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17310644958271282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17310644958271282 | validation: 0.0883823190073108]
	TIME [epoch: 5.84 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058432539298609844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058432539298609844 | validation: 0.023484797162995875]
	TIME [epoch: 5.83 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016824724974516356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016824724974516356 | validation: 0.039572099575227615]
	TIME [epoch: 5.83 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026944884812360764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026944884812360764 | validation: 0.038227206124493124]
	TIME [epoch: 5.83 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025207771714707145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025207771714707145 | validation: 0.02741950632979594]
	TIME [epoch: 5.82 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017597298081484535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017597298081484535 | validation: 0.025818747539853706]
	TIME [epoch: 5.83 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012387654194905808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012387654194905808 | validation: 0.014377691959947059]
	TIME [epoch: 5.82 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012749966624275513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012749966624275513 | validation: 0.026953185888618805]
	TIME [epoch: 5.83 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01857694392377773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01857694392377773 | validation: 0.03700412109217902]
	TIME [epoch: 5.83 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03265902216333271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03265902216333271 | validation: 0.07988697298532021]
	TIME [epoch: 5.83 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663741619867066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07663741619867066 | validation: 0.11896092453710683]
	TIME [epoch: 5.83 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12296639879388443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12296639879388443 | validation: 0.13533241187145592]
	TIME [epoch: 5.82 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11284191122393235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11284191122393235 | validation: 0.04331678231028713]
	TIME [epoch: 5.83 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400534355869467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03400534355869467 | validation: 0.06576016181166651]
	TIME [epoch: 5.83 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03692616649341287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03692616649341287 | validation: 0.05952342618325549]
	TIME [epoch: 5.83 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734623710420581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04734623710420581 | validation: 0.04161178346997627]
	TIME [epoch: 5.83 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026682479144640392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026682479144640392 | validation: 0.02633894394626676]
	TIME [epoch: 5.83 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020624255266465347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020624255266465347 | validation: 0.03682976208315202]
	TIME [epoch: 5.83 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01696436686402312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01696436686402312 | validation: 0.022918186794210406]
	TIME [epoch: 5.83 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02174220867925687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02174220867925687 | validation: 0.07551034820322391]
	TIME [epoch: 5.83 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126339757770443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04126339757770443 | validation: 0.0585412168123676]
	TIME [epoch: 5.82 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048941269481371716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048941269481371716 | validation: 0.10584080154791131]
	TIME [epoch: 5.88 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05809096524724163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05809096524724163 | validation: 0.035421313413225644]
	TIME [epoch: 5.8 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02989111367602817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02989111367602817 | validation: 0.03875261672396198]
	TIME [epoch: 5.81 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159028111764572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03159028111764572 | validation: 0.05857365329586885]
	TIME [epoch: 5.81 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045444208765597946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045444208765597946 | validation: 0.09050223931331018]
	TIME [epoch: 5.81 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08510417601681795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08510417601681795 | validation: 0.09970357109202643]
	TIME [epoch: 5.8 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1024534322074755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1024534322074755 | validation: 0.10173311739899776]
	TIME [epoch: 5.81 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926129674310567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06926129674310567 | validation: 0.04205915952899928]
	TIME [epoch: 5.81 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344938709182542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0344938709182542 | validation: 0.04054421425300875]
	TIME [epoch: 5.82 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021932039766447354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021932039766447354 | validation: 0.020534753579380352]
	TIME [epoch: 5.82 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0170940984210196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0170940984210196 | validation: 0.024751125163804445]
	TIME [epoch: 5.82 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012447451262466984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012447451262466984 | validation: 0.017156795220252985]
	TIME [epoch: 5.83 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01670942566864551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01670942566864551 | validation: 0.029901235926094838]
	TIME [epoch: 5.82 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023770352115512088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023770352115512088 | validation: 0.04236623257484723]
	TIME [epoch: 5.82 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04309067652291226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04309067652291226 | validation: 0.08137462123448519]
	TIME [epoch: 5.82 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06460726569078137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06460726569078137 | validation: 0.06321840336358317]
	TIME [epoch: 5.83 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061779107083716714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061779107083716714 | validation: 0.054414071977031055]
	TIME [epoch: 5.83 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02839786042449515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02839786042449515 | validation: 0.02090430618624156]
	TIME [epoch: 5.83 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01208076090602802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01208076090602802 | validation: 0.054746267649199425]
	TIME [epoch: 5.82 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025314947250445932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025314947250445932 | validation: 0.1219866362036005]
	TIME [epoch: 5.82 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1161258942165852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1161258942165852 | validation: 0.21163445758289898]
	TIME [epoch: 5.83 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1767562626837881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1767562626837881 | validation: 0.07492379093394357]
	TIME [epoch: 5.84 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05863011075435802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05863011075435802 | validation: 0.053037502835357214]
	TIME [epoch: 5.84 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061206835076601004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061206835076601004 | validation: 0.08231412592861215]
	TIME [epoch: 5.84 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037139100303089076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037139100303089076 | validation: 0.04371874826270533]
	TIME [epoch: 5.81 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02047535277634455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02047535277634455 | validation: 0.03164700198346988]
	TIME [epoch: 5.82 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026885436485313236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026885436485313236 | validation: 0.05480095430660367]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_191340/states/model_phi1_4a_v_mmd2_1326.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4633.622 seconds.
