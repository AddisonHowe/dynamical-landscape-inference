Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2923907760

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2578691802668605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2578691802668605 | validation: 4.335747093376445]
	TIME [epoch: 161 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.552393731452047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.552393731452047 | validation: 4.424435742017003]
	TIME [epoch: 2.67 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.64443082684709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.64443082684709 | validation: 3.8255354467146625]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.135423471620453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.135423471620453 | validation: 3.7578059237458903]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.130360764928659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.130360764928659 | validation: 3.681608009439677]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.888796505846641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.888796505846641 | validation: 3.7717706698398077]
	TIME [epoch: 2.67 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9720671780436305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9720671780436305 | validation: 3.5559051098910275]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9942857855164666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9942857855164666 | validation: 3.477779637891295]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.842088175938966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.842088175938966 | validation: 3.3768510966630068]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.649704244935882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.649704244935882 | validation: 3.3547882812843044]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6221972078671136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6221972078671136 | validation: 3.3417973682807594]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.648649469610666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.648649469610666 | validation: 3.2590503964470914]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4882603568766455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4882603568766455 | validation: 3.251139427480942]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4628305268425827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4628305268425827 | validation: 3.15519306850233]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3562649616469424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3562649616469424 | validation: 2.8364014320059527]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.017111484457696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.017111484457696 | validation: 1.9861705896883732]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3191499756994283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3191499756994283 | validation: 2.1193346934302624]
	TIME [epoch: 2.67 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.295926260683237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.295926260683237 | validation: 1.558744997301174]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9199118998958467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9199118998958467 | validation: 1.278307031416497]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7764950936980786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7764950936980786 | validation: 0.982581786859754]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.160053908593532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.160053908593532 | validation: 1.082516323376873]
	TIME [epoch: 2.67 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3769014290624548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3769014290624548 | validation: 1.3018124555272914]
	TIME [epoch: 2.67 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8310955850806618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8310955850806618 | validation: 1.6115635641381225]
	TIME [epoch: 2.67 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.981592608097008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.981592608097008 | validation: 1.0200972865098332]
	TIME [epoch: 2.66 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2623713871198932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2623713871198932 | validation: 1.5410998864740533]
	TIME [epoch: 2.66 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7067773196466742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7067773196466742 | validation: 1.1490884850834941]
	TIME [epoch: 2.67 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4467541593663955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4467541593663955 | validation: 1.1081171023777596]
	TIME [epoch: 2.66 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.369537098010038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.369537098010038 | validation: 0.98922243729573]
	TIME [epoch: 2.67 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1886484618938389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1886484618938389 | validation: 1.0708374519323525]
	TIME [epoch: 2.66 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1327658867968922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1327658867968922 | validation: 0.9513406984948727]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0255989700547474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0255989700547474 | validation: 0.9023784186467229]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0651679868693817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0651679868693817 | validation: 0.9288087124865779]
	TIME [epoch: 2.66 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0467749544093028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0467749544093028 | validation: 0.9714109098269659]
	TIME [epoch: 2.65 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0206519897784079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0206519897784079 | validation: 0.8846663202670767]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0646377491598722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0646377491598722 | validation: 0.9054472541784361]
	TIME [epoch: 2.64 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0027836267743928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0027836267743928 | validation: 0.9003873372878712]
	TIME [epoch: 2.66 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0190438066318428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0190438066318428 | validation: 0.9212798009158348]
	TIME [epoch: 2.67 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0640721900767196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0640721900767196 | validation: 0.8973167143388221]
	TIME [epoch: 2.65 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9914466146743007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9914466146743007 | validation: 0.8703789177469563]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0284837912729077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0284837912729077 | validation: 0.912539088727899]
	TIME [epoch: 2.66 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2523822008129135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2523822008129135 | validation: 0.8840283885590922]
	TIME [epoch: 2.64 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.045447229704769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.045447229704769 | validation: 0.9940821244248913]
	TIME [epoch: 2.65 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2501556013217983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2501556013217983 | validation: 0.8512956313269875]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9930650934177397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9930650934177397 | validation: 0.9078791197941636]
	TIME [epoch: 2.66 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0100673446012685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0100673446012685 | validation: 0.8205447442311771]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0010055354039797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0010055354039797 | validation: 0.8752516555123826]
	TIME [epoch: 2.65 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9935284457316257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9935284457316257 | validation: 0.8483166787012708]
	TIME [epoch: 2.64 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9589834737141031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9589834737141031 | validation: 0.7924497099001176]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9666477818678908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9666477818678908 | validation: 0.829353045361064]
	TIME [epoch: 2.66 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9289266793695551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9289266793695551 | validation: 0.8380939724925878]
	TIME [epoch: 2.65 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9386933092992612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9386933092992612 | validation: 0.777990830509661]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9532274260973914		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.9532274260973914 | validation: 0.7633602387895162]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450207870116224		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9450207870116224 | validation: 0.7547437307974566]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.93155857440217		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.93155857440217 | validation: 0.8623656539762984]
	TIME [epoch: 2.66 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957960791576796		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.957960791576796 | validation: 0.8325672591164509]
	TIME [epoch: 2.66 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022502584675543		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.022502584675543 | validation: 0.9508637866141258]
	TIME [epoch: 2.66 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0464122213645177		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 1.0464122213645177 | validation: 0.8671604547072822]
	TIME [epoch: 2.67 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0658848889952615		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.0658848889952615 | validation: 0.8433190786266489]
	TIME [epoch: 2.66 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0722741289356559		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.0722741289356559 | validation: 0.918545859620588]
	TIME [epoch: 2.66 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2569000876997307		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.2569000876997307 | validation: 0.8565220593985656]
	TIME [epoch: 2.66 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0612890243639979		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.0612890243639979 | validation: 0.8286301186150533]
	TIME [epoch: 2.66 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0110909361448994		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 1.0110909361448994 | validation: 0.7958170552800193]
	TIME [epoch: 2.64 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284585714560781		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9284585714560781 | validation: 0.7973203088525636]
	TIME [epoch: 2.66 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380480561869746		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9380480561869746 | validation: 0.7734193233108484]
	TIME [epoch: 2.65 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9253556148095958		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9253556148095958 | validation: 0.759910127131094]
	TIME [epoch: 2.66 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960458935986644		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.8960458935986644 | validation: 0.7871479044309795]
	TIME [epoch: 2.66 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9018961972538964		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.9018961972538964 | validation: 0.7488218796673366]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9034634622177029		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.9034634622177029 | validation: 0.7877086411474682]
	TIME [epoch: 2.68 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077725828274037		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9077725828274037 | validation: 0.7521044208275491]
	TIME [epoch: 2.67 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833865808704577		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8833865808704577 | validation: 0.7508453557515795]
	TIME [epoch: 2.67 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.904362198566976		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.904362198566976 | validation: 0.8088326107887917]
	TIME [epoch: 2.68 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971821054000343		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8971821054000343 | validation: 0.747861328986425]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834612324866952		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.8834612324866952 | validation: 0.7613118070360092]
	TIME [epoch: 2.66 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810299190392258		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.8810299190392258 | validation: 0.7461380921297586]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9344321836002908		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9344321836002908 | validation: 0.7970073380156973]
	TIME [epoch: 2.66 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9671198399052437		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.9671198399052437 | validation: 0.7567595340419118]
	TIME [epoch: 2.66 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.936617897109754		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.936617897109754 | validation: 0.7971015599374542]
	TIME [epoch: 2.66 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9683771527146431		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.9683771527146431 | validation: 0.8490403362620438]
	TIME [epoch: 2.66 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9425421051364176		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9425421051364176 | validation: 0.8254438318705569]
	TIME [epoch: 2.66 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9982992075760914		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.9982992075760914 | validation: 0.7267675214545062]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8773799729065966		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8773799729065966 | validation: 0.8711717168845265]
	TIME [epoch: 2.62 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782917678227716		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.9782917678227716 | validation: 0.7842901718768895]
	TIME [epoch: 2.61 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9322793565715222		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.9322793565715222 | validation: 0.7699425258969533]
	TIME [epoch: 2.65 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9259605110492194		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.9259605110492194 | validation: 0.773174741456694]
	TIME [epoch: 2.64 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980241556504907		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8980241556504907 | validation: 0.7531950753744994]
	TIME [epoch: 2.66 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775653467713542		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8775653467713542 | validation: 0.7362430451647595]
	TIME [epoch: 2.67 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669884782340637		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8669884782340637 | validation: 0.7307657137056816]
	TIME [epoch: 2.67 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722727678951969		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.8722727678951969 | validation: 0.7444381652123384]
	TIME [epoch: 2.66 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8995802983215788		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.8995802983215788 | validation: 0.7454844984607277]
	TIME [epoch: 2.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891146721425361		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.891146721425361 | validation: 0.7601589196590157]
	TIME [epoch: 2.66 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091928348322762		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9091928348322762 | validation: 0.7382272149257125]
	TIME [epoch: 2.66 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9298000532394812		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.9298000532394812 | validation: 0.7640770977743246]
	TIME [epoch: 2.65 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134123834973314		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.9134123834973314 | validation: 0.7584040881956742]
	TIME [epoch: 2.66 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8872337392457422		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.8872337392457422 | validation: 0.7175318399868109]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8978644043247573		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8978644043247573 | validation: 0.7906902883271654]
	TIME [epoch: 2.65 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9455088207112317		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9455088207112317 | validation: 0.7408255856890018]
	TIME [epoch: 2.66 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953924318707043		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8953924318707043 | validation: 0.7982741612259802]
	TIME [epoch: 2.66 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748420753387924		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9748420753387924 | validation: 0.7540354286596065]
	TIME [epoch: 2.66 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887843278783853		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8887843278783853 | validation: 0.7407515532194424]
	TIME [epoch: 2.66 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857932172218685		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8857932172218685 | validation: 0.7032554458059851]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8608401235755596		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8608401235755596 | validation: 0.7789961236150618]
	TIME [epoch: 2.66 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007611353200041		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.9007611353200041 | validation: 0.7180236840977051]
	TIME [epoch: 2.65 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732622345205681		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8732622345205681 | validation: 0.726790175799218]
	TIME [epoch: 2.66 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8606648286481504		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8606648286481504 | validation: 0.7198216572701779]
	TIME [epoch: 2.65 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612856082539068		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8612856082539068 | validation: 0.8408477654163614]
	TIME [epoch: 2.65 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0205795390224166		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 1.0205795390224166 | validation: 0.7828906001816844]
	TIME [epoch: 2.66 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9058907893151567		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9058907893151567 | validation: 0.7792147073560248]
	TIME [epoch: 2.66 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.924654230855806		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.924654230855806 | validation: 0.7553030386897263]
	TIME [epoch: 2.65 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859633286413778		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.859633286413778 | validation: 0.7063497363150764]
	TIME [epoch: 2.65 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479363012952078		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8479363012952078 | validation: 0.7017115197908037]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568188754664656		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8568188754664656 | validation: 0.7549875923999408]
	TIME [epoch: 2.66 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640785336854571		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8640785336854571 | validation: 0.7348490622232613]
	TIME [epoch: 2.65 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.874950398096365		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.874950398096365 | validation: 0.8030873305871801]
	TIME [epoch: 2.65 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9868715499618453		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9868715499618453 | validation: 0.7532253087199396]
	TIME [epoch: 2.65 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9289006480433172		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9289006480433172 | validation: 0.7754228335825589]
	TIME [epoch: 2.65 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9016073068988695		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9016073068988695 | validation: 0.714784575994559]
	TIME [epoch: 2.66 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8753517912352298		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8753517912352298 | validation: 0.7363226509372292]
	TIME [epoch: 2.65 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9611591127540003		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.9611591127540003 | validation: 0.7854659097236777]
	TIME [epoch: 2.65 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9466362703773991		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.9466362703773991 | validation: 0.7178508554017571]
	TIME [epoch: 2.65 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8684109421092555		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8684109421092555 | validation: 0.7102390024010559]
	TIME [epoch: 2.65 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524041581511834		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8524041581511834 | validation: 0.7339043586503422]
	TIME [epoch: 2.65 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959153711414585		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8959153711414585 | validation: 0.725749458569341]
	TIME [epoch: 2.65 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486989838677207		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8486989838677207 | validation: 0.7086161857765757]
	TIME [epoch: 2.65 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8477965953531978		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8477965953531978 | validation: 0.7075720166294964]
	TIME [epoch: 2.66 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653093064283429		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8653093064283429 | validation: 0.7278199515170851]
	TIME [epoch: 2.65 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8816512440621285		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8816512440621285 | validation: 0.7374222823312749]
	TIME [epoch: 2.65 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8785435752028695		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8785435752028695 | validation: 0.6974867326485006]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341592872246756		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8341592872246756 | validation: 0.700187805964557]
	TIME [epoch: 2.66 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8307901310726223		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8307901310726223 | validation: 0.7345182965698398]
	TIME [epoch: 2.65 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8709246034077975		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8709246034077975 | validation: 0.8507375413617263]
	TIME [epoch: 2.66 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.994270530057446		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.994270530057446 | validation: 0.7315618863338633]
	TIME [epoch: 2.65 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667001169849621		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8667001169849621 | validation: 0.7145448700284763]
	TIME [epoch: 2.66 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8544589727254102		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8544589727254102 | validation: 0.7057947680727694]
	TIME [epoch: 2.66 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8203390601049116		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8203390601049116 | validation: 0.704208179622701]
	TIME [epoch: 2.65 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182444496913712		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8182444496913712 | validation: 0.7202025161769848]
	TIME [epoch: 2.66 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245964841634995		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8245964841634995 | validation: 0.6835204537112891]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.798986520147786		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.798986520147786 | validation: 0.7328177553367191]
	TIME [epoch: 2.67 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058268802102674		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8058268802102674 | validation: 0.7653278044403011]
	TIME [epoch: 2.66 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771622628154048		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8771622628154048 | validation: 1.2864714532608683]
	TIME [epoch: 2.67 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2987073194664793		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 1.2987073194664793 | validation: 0.9950792590954948]
	TIME [epoch: 2.67 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2133916559177675		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 1.2133916559177675 | validation: 0.6909823619383109]
	TIME [epoch: 2.65 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199238128007216		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8199238128007216 | validation: 0.7403220546723364]
	TIME [epoch: 2.64 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8872252875813442		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8872252875813442 | validation: 0.6877507723486075]
	TIME [epoch: 2.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8154856689027339		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8154856689027339 | validation: 0.683662145631082]
	TIME [epoch: 2.64 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114871224584934		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8114871224584934 | validation: 0.6669813464851826]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024965201318375		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8024965201318375 | validation: 0.6694401644541703]
	TIME [epoch: 2.67 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906336319619797		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7906336319619797 | validation: 0.6632876744013756]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894301784044189		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7894301784044189 | validation: 0.6504539578265752]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_148.pth
	Model improved!!!
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629670962762524		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7629670962762524 | validation: 0.6589090507067469]
	TIME [epoch: 2.66 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398267002014708		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7398267002014708 | validation: 0.6738816880492096]
	TIME [epoch: 2.67 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312845312340209		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7312845312340209 | validation: 0.6326801892415962]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7132153157399739		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7132153157399739 | validation: 0.7614492444079709]
	TIME [epoch: 2.63 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439337778801314		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7439337778801314 | validation: 0.8646219669082754]
	TIME [epoch: 2.63 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9846366927837514		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9846366927837514 | validation: 1.3258942400283154]
	TIME [epoch: 2.64 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.347188862079708		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 1.347188862079708 | validation: 0.6991975781494157]
	TIME [epoch: 2.64 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8412608738811272		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.8412608738811272 | validation: 0.6449791105711226]
	TIME [epoch: 2.66 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876331208869314		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7876331208869314 | validation: 0.6613739354114445]
	TIME [epoch: 2.65 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872057159348552		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7872057159348552 | validation: 0.656198738100437]
	TIME [epoch: 2.66 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7635799952273654		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7635799952273654 | validation: 0.6573177833005392]
	TIME [epoch: 2.64 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621705941284787		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7621705941284787 | validation: 0.6300168797042787]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370095996343775		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.7370095996343775 | validation: 0.6326048131143494]
	TIME [epoch: 2.65 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7141235008119958		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.7141235008119958 | validation: 0.6175576893668643]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973920846818349		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.6973920846818349 | validation: 0.6297737677991049]
	TIME [epoch: 2.68 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6870408611170976		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.6870408611170976 | validation: 0.6227503752359583]
	TIME [epoch: 2.66 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6683613009929176		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.6683613009929176 | validation: 0.6280040211486336]
	TIME [epoch: 2.66 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633624862842598		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6633624862842598 | validation: 0.7007609143228862]
	TIME [epoch: 2.66 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434722734798863		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7434722734798863 | validation: 1.0952048702788908]
	TIME [epoch: 2.66 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2020974378391218		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 1.2020974378391218 | validation: 0.6615350377430732]
	TIME [epoch: 2.65 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677768535854901		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.7677768535854901 | validation: 0.6463488069916132]
	TIME [epoch: 2.66 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443740207521857		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7443740207521857 | validation: 0.6841106153262624]
	TIME [epoch: 2.66 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434623851003488		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7434623851003488 | validation: 0.643003991801229]
	TIME [epoch: 2.67 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.673153076874989		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.673153076874989 | validation: 0.5654022709865796]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6395192191059538		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6395192191059538 | validation: 0.5749071468800505]
	TIME [epoch: 2.65 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6300409918050955		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6300409918050955 | validation: 0.5795689758830289]
	TIME [epoch: 2.66 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265833016387674		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.6265833016387674 | validation: 0.6312955752433741]
	TIME [epoch: 2.66 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6603120996779687		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6603120996779687 | validation: 0.8337260010183688]
	TIME [epoch: 2.64 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8970740785578576		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.8970740785578576 | validation: 0.6389831730989113]
	TIME [epoch: 2.65 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908540024428845		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.6908540024428845 | validation: 0.5640044978259625]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320018076037751		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6320018076037751 | validation: 0.5637695769755995]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269619044226014		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6269619044226014 | validation: 0.7656950593720179]
	TIME [epoch: 2.66 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471459161611828		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7471459161611828 | validation: 0.9691766914890277]
	TIME [epoch: 2.65 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0811422213864361		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 1.0811422213864361 | validation: 0.5748247777155105]
	TIME [epoch: 2.65 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502673753839988		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6502673753839988 | validation: 0.6880168230089565]
	TIME [epoch: 2.65 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550447173428042		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7550447173428042 | validation: 0.5632411005662752]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6202130596174028		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6202130596174028 | validation: 0.5662209032504941]
	TIME [epoch: 2.65 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6002351487256776		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6002351487256776 | validation: 0.62863661911876]
	TIME [epoch: 2.65 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6589826563175464		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6589826563175464 | validation: 0.6930109876891786]
	TIME [epoch: 2.69 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671111468562137		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7671111468562137 | validation: 0.5600372709535506]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6075768342401348		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.6075768342401348 | validation: 0.5436136607604708]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5807619088950333		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.5807619088950333 | validation: 0.5301013953350856]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5808294113577723		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5808294113577723 | validation: 0.5755927459490505]
	TIME [epoch: 2.65 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5944702406010733		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5944702406010733 | validation: 0.6220621849331753]
	TIME [epoch: 2.65 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886762340997474		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.6886762340997474 | validation: 0.6039154010248056]
	TIME [epoch: 2.64 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6416843780918764		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6416843780918764 | validation: 0.5641393372067885]
	TIME [epoch: 2.65 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6250557444314746		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6250557444314746 | validation: 0.5116987695251772]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625204976234828		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.5625204976234828 | validation: 0.5484475501388623]
	TIME [epoch: 2.63 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.555931363210834		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.555931363210834 | validation: 0.6221505280341127]
	TIME [epoch: 2.64 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6490194252644611		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6490194252644611 | validation: 0.667919395251998]
	TIME [epoch: 2.62 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7101295849508873		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7101295849508873 | validation: 0.7034014413115504]
	TIME [epoch: 2.67 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501604292787366		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7501604292787366 | validation: 0.5706272510752127]
	TIME [epoch: 2.63 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314319112415734		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.6314319112415734 | validation: 0.5568734233892754]
	TIME [epoch: 170 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6138032754324642		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6138032754324642 | validation: 0.5117452323620325]
	TIME [epoch: 5.74 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5507217715066134		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.5507217715066134 | validation: 0.5542815411150882]
	TIME [epoch: 5.72 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5582770495871059		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.5582770495871059 | validation: 0.5174803574483443]
	TIME [epoch: 5.73 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5392269127022068		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5392269127022068 | validation: 0.518060795801146]
	TIME [epoch: 5.75 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571308995489986		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.5571308995489986 | validation: 0.5516666603337862]
	TIME [epoch: 5.74 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5869903893617899		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.5869903893617899 | validation: 0.5385270386030049]
	TIME [epoch: 5.76 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5791430846156367		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5791430846156367 | validation: 0.5733095127875076]
	TIME [epoch: 5.73 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320727695870272		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6320727695870272 | validation: 0.5367732530715158]
	TIME [epoch: 5.74 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5610459468491328		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.5610459468491328 | validation: 0.4888913112039995]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5409755996607182		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5409755996607182 | validation: 0.5038311482352715]
	TIME [epoch: 5.75 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5284723456796904		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.5284723456796904 | validation: 0.5520012271207659]
	TIME [epoch: 5.72 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5848057408675282		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.5848057408675282 | validation: 0.48108052094111353]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5330330916728917		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.5330330916728917 | validation: 0.496996291464669]
	TIME [epoch: 5.75 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5285572295863		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.5285572295863 | validation: 0.45637926792308864]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48670193237917486		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.48670193237917486 | validation: 0.44722642270371094]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48976583283876224		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.48976583283876224 | validation: 0.4923336200022506]
	TIME [epoch: 5.76 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49394043822918043		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.49394043822918043 | validation: 0.5342602481860041]
	TIME [epoch: 5.72 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5796709240839791		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.5796709240839791 | validation: 0.6677799710531187]
	TIME [epoch: 5.72 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637231739388151		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6637231739388151 | validation: 0.5999139157263379]
	TIME [epoch: 5.72 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827068218528044		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6827068218528044 | validation: 0.5219534642913715]
	TIME [epoch: 5.72 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845624082804061		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.5845624082804061 | validation: 0.4937629341893768]
	TIME [epoch: 5.75 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5633350397797628		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.5633350397797628 | validation: 0.4527100760866821]
	TIME [epoch: 5.72 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47855524713315645		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.47855524713315645 | validation: 0.4827687772658563]
	TIME [epoch: 5.72 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48084012567030765		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.48084012567030765 | validation: 0.4423764463260101]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4684063665661444		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.4684063665661444 | validation: 0.4406832031589467]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4649436284485051		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.4649436284485051 | validation: 0.46788930919303645]
	TIME [epoch: 5.77 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48474217086076843		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.48474217086076843 | validation: 0.5060272550482282]
	TIME [epoch: 5.77 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4999662712306092		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.4999662712306092 | validation: 0.5683011999805209]
	TIME [epoch: 5.77 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6194771403624729		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6194771403624729 | validation: 0.44427837159609784]
	TIME [epoch: 5.77 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175522118303916		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.5175522118303916 | validation: 0.463140020570569]
	TIME [epoch: 5.73 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.497620621712128		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.497620621712128 | validation: 0.5332422441315953]
	TIME [epoch: 5.75 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883189216120887		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5883189216120887 | validation: 0.5105737940048548]
	TIME [epoch: 5.74 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5046634344014569		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5046634344014569 | validation: 0.42566745301883835]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48205103221737133		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.48205103221737133 | validation: 0.4111296556721838]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4502043949368725		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.4502043949368725 | validation: 0.40671966326458914]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43372148728127896		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.43372148728127896 | validation: 0.4036541283155298]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41951401091699747		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.41951401091699747 | validation: 0.4114724825257051]
	TIME [epoch: 5.78 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41677520072738966		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.41677520072738966 | validation: 0.4195920081240024]
	TIME [epoch: 5.75 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4434412754398687		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.4434412754398687 | validation: 0.5065820352611733]
	TIME [epoch: 5.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4966507534841667		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.4966507534841667 | validation: 0.5627611188300234]
	TIME [epoch: 5.78 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699579899182018		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6699579899182018 | validation: 0.573081754061928]
	TIME [epoch: 5.74 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832861261957739		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6832861261957739 | validation: 0.5224049595343859]
	TIME [epoch: 5.75 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5826008584955057		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5826008584955057 | validation: 0.40194269854009285]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41979612367798785		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.41979612367798785 | validation: 0.4411133502601409]
	TIME [epoch: 5.69 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4410416159018412		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.4410416159018412 | validation: 0.3763888899140603]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40306015789019567		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.40306015789019567 | validation: 0.3760353127151319]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014591754262908		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.4014591754262908 | validation: 0.3548743370222577]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3984125268473902		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.3984125268473902 | validation: 0.40182258967290707]
	TIME [epoch: 5.78 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4018663553281077		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.4018663553281077 | validation: 0.4482231731514421]
	TIME [epoch: 5.79 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48851777641894667		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.48851777641894667 | validation: 0.4800284797818103]
	TIME [epoch: 5.77 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4949846078787624		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.4949846078787624 | validation: 0.4772778198266034]
	TIME [epoch: 5.77 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5319256032837791		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5319256032837791 | validation: 0.37371026694874776]
	TIME [epoch: 5.77 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38736642124212667		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.38736642124212667 | validation: 0.37928014583909675]
	TIME [epoch: 5.75 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3861113910467082		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3861113910467082 | validation: 0.4097208608566411]
	TIME [epoch: 5.77 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42324357302215554		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.42324357302215554 | validation: 0.3591754337887659]
	TIME [epoch: 5.77 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36402857377938785		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.36402857377938785 | validation: 0.35043170261121026]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36028599610468787		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.36028599610468787 | validation: 0.3473094035271918]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3727978163236776		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.3727978163236776 | validation: 0.4431547680223493]
	TIME [epoch: 5.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4134318814141213		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.4134318814141213 | validation: 0.4935853148156203]
	TIME [epoch: 5.78 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.548285067084808		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.548285067084808 | validation: 0.36086243212478325]
	TIME [epoch: 5.75 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38024957526556874		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.38024957526556874 | validation: 0.5114991882413641]
	TIME [epoch: 5.77 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4860442958067019		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.4860442958067019 | validation: 0.503772582929033]
	TIME [epoch: 5.77 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947609721105396		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5947609721105396 | validation: 0.3900362668069768]
	TIME [epoch: 5.75 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40391230805707684		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.40391230805707684 | validation: 0.4602800476891686]
	TIME [epoch: 5.77 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4521758171153167		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.4521758171153167 | validation: 0.3856073907336248]
	TIME [epoch: 5.76 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42509808617547323		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.42509808617547323 | validation: 0.3492945631117693]
	TIME [epoch: 5.74 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3727182864539115		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.3727182864539115 | validation: 0.38698257573871875]
	TIME [epoch: 5.77 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3729600103316309		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.3729600103316309 | validation: 0.3104093002463795]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.326468869929837		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.326468869929837 | validation: 0.2951956335459907]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200118741282543		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.3200118741282543 | validation: 0.30924873809428594]
	TIME [epoch: 5.77 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3122920279293744		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.3122920279293744 | validation: 0.3058930014874748]
	TIME [epoch: 5.78 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3068629064994908		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.3068629064994908 | validation: 0.2927595116353063]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3144235056200244		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.3144235056200244 | validation: 0.3339878624557777]
	TIME [epoch: 5.78 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3238295816785697		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.3238295816785697 | validation: 0.36326441988384184]
	TIME [epoch: 5.75 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922664058722908		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.3922664058722908 | validation: 0.32792068837765814]
	TIME [epoch: 5.77 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3389517011843904		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.3389517011843904 | validation: 0.2927594186897389]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3045302577545029		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.3045302577545029 | validation: 0.3691411629525201]
	TIME [epoch: 5.77 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3243483802358679		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.3243483802358679 | validation: 0.3511867668701809]
	TIME [epoch: 5.76 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38924566329276533		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.38924566329276533 | validation: 0.2652352212110203]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29946973503496077		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.29946973503496077 | validation: 0.3691523061387256]
	TIME [epoch: 5.79 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3218980787370744		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.3218980787370744 | validation: 0.4159185058244482]
	TIME [epoch: 5.79 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44841103209311556		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.44841103209311556 | validation: 0.2663428095347279]
	TIME [epoch: 5.78 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29045565566744225		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.29045565566744225 | validation: 0.37747794111198985]
	TIME [epoch: 5.78 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37613140735669703		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.37613140735669703 | validation: 0.41107909280709987]
	TIME [epoch: 5.78 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45912852296233736		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.45912852296233736 | validation: 0.2994847372019687]
	TIME [epoch: 5.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32754764195688496		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.32754764195688496 | validation: 0.34692975298715756]
	TIME [epoch: 5.78 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178384218927959		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.3178384218927959 | validation: 0.26504929148334644]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978876608290076		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.2978876608290076 | validation: 0.24353222366152322]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2416072211375281		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.2416072211375281 | validation: 0.25495902909261775]
	TIME [epoch: 5.76 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24444468307258163		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.24444468307258163 | validation: 0.227160392322995]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2494412693270128		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.2494412693270128 | validation: 0.25945846600255895]
	TIME [epoch: 5.78 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2517365866290434		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.2517365866290434 | validation: 0.273855159416994]
	TIME [epoch: 5.77 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29616250412727646		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.29616250412727646 | validation: 0.21679956942521006]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22354056303712655		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.22354056303712655 | validation: 0.2544271637452236]
	TIME [epoch: 5.76 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22076544061004902		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.22076544061004902 | validation: 0.21270135430768225]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2320603836290813		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.2320603836290813 | validation: 0.34803431154663883]
	TIME [epoch: 5.76 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282855386366311		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.282855386366311 | validation: 0.39026091310788424]
	TIME [epoch: 5.77 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45516250303211975		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.45516250303211975 | validation: 0.2445672717204983]
	TIME [epoch: 5.77 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27228954347229745		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.27228954347229745 | validation: 0.6169323771110422]
	TIME [epoch: 5.77 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5082829127963782		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.5082829127963782 | validation: 0.32467640577111256]
	TIME [epoch: 5.76 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3741804342317262		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3741804342317262 | validation: 0.2553949252646241]
	TIME [epoch: 5.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30921939731626613		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.30921939731626613 | validation: 0.3237391844677293]
	TIME [epoch: 5.77 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603785291091596		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.2603785291091596 | validation: 0.21156446032144505]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_304.pth
	Model improved!!!
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20424355687955287		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.20424355687955287 | validation: 0.17932098728294021]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20857822084102318		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.20857822084102318 | validation: 0.2176115867673179]
	TIME [epoch: 5.77 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20669570536975698		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.20669570536975698 | validation: 0.18968722937250374]
	TIME [epoch: 5.77 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19698755782978933		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.19698755782978933 | validation: 0.21041809818106363]
	TIME [epoch: 5.77 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20167021506040889		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.20167021506040889 | validation: 0.21764983845878866]
	TIME [epoch: 5.77 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21812849259455555		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.21812849259455555 | validation: 0.2627036634163486]
	TIME [epoch: 5.76 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2269621590944091		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.2269621590944091 | validation: 0.2565466427707036]
	TIME [epoch: 5.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628003719272436		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2628003719272436 | validation: 0.1910536006901943]
	TIME [epoch: 5.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18533761552373762		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.18533761552373762 | validation: 0.2059746252455605]
	TIME [epoch: 5.78 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2015938506779192		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.2015938506779192 | validation: 0.278565733437431]
	TIME [epoch: 5.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27978902365280883		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.27978902365280883 | validation: 0.23035651669904775]
	TIME [epoch: 5.76 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20283953892053672		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.20283953892053672 | validation: 0.1793692055696804]
	TIME [epoch: 5.76 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21260622661752443		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.21260622661752443 | validation: 0.29196473251091243]
	TIME [epoch: 5.75 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21473287049543038		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.21473287049543038 | validation: 0.19963172709204346]
	TIME [epoch: 5.75 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24469243094700457		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.24469243094700457 | validation: 0.2422089718512429]
	TIME [epoch: 5.76 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165453160442355		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.2165453160442355 | validation: 0.2222072371001123]
	TIME [epoch: 5.74 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24457047458046574		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.24457047458046574 | validation: 0.17561033888883412]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17086852801902772		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.17086852801902772 | validation: 0.23408180559404307]
	TIME [epoch: 5.75 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18122243446591785		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.18122243446591785 | validation: 0.16841037355013333]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_323.pth
	Model improved!!!
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20498988781411792		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.20498988781411792 | validation: 0.3028210481756337]
	TIME [epoch: 5.75 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24303608485743483		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.24303608485743483 | validation: 0.2968772641729283]
	TIME [epoch: 5.78 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3376693477298811		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.3376693477298811 | validation: 0.2533411077633396]
	TIME [epoch: 5.76 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002526007135801		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.3002526007135801 | validation: 0.2888312634199407]
	TIME [epoch: 5.78 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22998768418451543		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.22998768418451543 | validation: 0.2909495939234394]
	TIME [epoch: 5.77 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.300799176426421		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.300799176426421 | validation: 0.15283400731012453]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1908389943739889		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.1908389943739889 | validation: 0.34574654947601924]
	TIME [epoch: 5.76 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2893847316295348		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.2893847316295348 | validation: 0.22844425018519116]
	TIME [epoch: 5.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26234004386626664		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.26234004386626664 | validation: 0.1992924064505815]
	TIME [epoch: 5.75 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20664092573417747		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.20664092573417747 | validation: 0.23430136986703412]
	TIME [epoch: 5.75 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19547962033576355		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.19547962033576355 | validation: 0.16274254913641867]
	TIME [epoch: 5.76 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17594141164457283		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.17594141164457283 | validation: 0.19639254705065695]
	TIME [epoch: 5.75 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16139449098371292		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.16139449098371292 | validation: 0.14743345181504572]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16001208551557355		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.16001208551557355 | validation: 0.15433755688925538]
	TIME [epoch: 5.75 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15923871722643934		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.15923871722643934 | validation: 0.14523442880233436]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15889593426270573		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.15889593426270573 | validation: 0.19117425602306581]
	TIME [epoch: 5.75 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1575426259534539		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.1575426259534539 | validation: 0.15307467818892972]
	TIME [epoch: 5.76 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17294304108642608		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.17294304108642608 | validation: 0.2515328315281012]
	TIME [epoch: 5.77 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19375935096479327		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.19375935096479327 | validation: 0.2126237042770061]
	TIME [epoch: 5.76 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25405538110368553		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.25405538110368553 | validation: 0.17171860141251624]
	TIME [epoch: 5.75 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1537771819396393		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.1537771819396393 | validation: 0.16899917542243786]
	TIME [epoch: 5.78 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14673013681496977		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.14673013681496977 | validation: 0.15981708115703677]
	TIME [epoch: 5.76 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15875233338158964		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.15875233338158964 | validation: 0.2472103605631705]
	TIME [epoch: 5.76 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21665645305865827		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.21665645305865827 | validation: 0.28043342711780256]
	TIME [epoch: 5.77 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32778204836392777		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.32778204836392777 | validation: 0.20842278727385952]
	TIME [epoch: 5.77 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2215439127332501		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.2215439127332501 | validation: 0.4590179020711988]
	TIME [epoch: 5.76 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35498974620539103		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.35498974620539103 | validation: 0.21845681904679168]
	TIME [epoch: 5.75 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27818579937299487		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.27818579937299487 | validation: 0.22518401442121735]
	TIME [epoch: 5.75 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21512002715582632		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.21512002715582632 | validation: 0.2263113369233466]
	TIME [epoch: 5.75 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18682332915193547		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.18682332915193547 | validation: 0.1411182353216195]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688316681564085		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.1688316681564085 | validation: 0.1971402441540339]
	TIME [epoch: 5.75 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16073923008528224		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.16073923008528224 | validation: 0.1398338706461583]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14500091427032957		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.14500091427032957 | validation: 0.15374103254505542]
	TIME [epoch: 5.78 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145366439901626		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.145366439901626 | validation: 0.13485379284973095]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1397598468836903		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.1397598468836903 | validation: 0.12895028744542067]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13608612265487424		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.13608612265487424 | validation: 0.13490243968606436]
	TIME [epoch: 5.77 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14035551976043195		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.14035551976043195 | validation: 0.16253270891109192]
	TIME [epoch: 5.76 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13771917669681125		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.13771917669681125 | validation: 0.1625587396383711]
	TIME [epoch: 5.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15955060420091396		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.15955060420091396 | validation: 0.3564631753706487]
	TIME [epoch: 5.77 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26453193776210193		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.26453193776210193 | validation: 0.29719078364788076]
	TIME [epoch: 5.75 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36233770440118873		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.36233770440118873 | validation: 0.30419018871729614]
	TIME [epoch: 5.77 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30717218864921997		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.30717218864921997 | validation: 0.2526395902522231]
	TIME [epoch: 5.77 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.205631800702162		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.205631800702162 | validation: 0.13126446384593018]
	TIME [epoch: 5.76 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1544060834351202		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.1544060834351202 | validation: 0.20752299453746892]
	TIME [epoch: 5.77 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16015610179715714		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.16015610179715714 | validation: 0.1370800248863495]
	TIME [epoch: 5.77 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14799378445942543		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.14799378445942543 | validation: 0.14632541576204985]
	TIME [epoch: 5.76 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15563763000749845		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.15563763000749845 | validation: 0.20555880213397248]
	TIME [epoch: 5.76 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15931597802415476		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.15931597802415476 | validation: 0.13940591509753106]
	TIME [epoch: 5.76 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18030865582629577		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.18030865582629577 | validation: 0.24976999559535323]
	TIME [epoch: 5.76 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17837075757957316		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.17837075757957316 | validation: 0.1578287185722047]
	TIME [epoch: 5.78 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19433795526846098		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.19433795526846098 | validation: 0.1365270487184114]
	TIME [epoch: 5.77 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1370752656079445		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.1370752656079445 | validation: 0.19004838533294235]
	TIME [epoch: 5.78 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14884615733326392		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.14884615733326392 | validation: 0.12081826815395186]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.152481856464678		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.152481856464678 | validation: 0.26569531627443627]
	TIME [epoch: 5.76 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17840757778440242		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.17840757778440242 | validation: 0.16533474977249699]
	TIME [epoch: 5.77 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.203507478609175		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.203507478609175 | validation: 0.13729848971549694]
	TIME [epoch: 5.77 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13742141340121192		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.13742141340121192 | validation: 0.17133700723100576]
	TIME [epoch: 5.77 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14318067703846915		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.14318067703846915 | validation: 0.15018734702608738]
	TIME [epoch: 5.75 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15661185183840654		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.15661185183840654 | validation: 0.1842219220635276]
	TIME [epoch: 5.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16791732556574068		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.16791732556574068 | validation: 0.21852920597077646]
	TIME [epoch: 5.77 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18828080168087177		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.18828080168087177 | validation: 0.21242099085612598]
	TIME [epoch: 5.78 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24180314808971748		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.24180314808971748 | validation: 0.17115219757862926]
	TIME [epoch: 5.77 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13600673245675637		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.13600673245675637 | validation: 0.19808901350857133]
	TIME [epoch: 5.76 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17998637486726796		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.17998637486726796 | validation: 0.16689549159264697]
	TIME [epoch: 5.77 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19559866282757873		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.19559866282757873 | validation: 0.14073356336521148]
	TIME [epoch: 5.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12888241400271164		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.12888241400271164 | validation: 0.14346102313619255]
	TIME [epoch: 5.77 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12953578485347866		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.12953578485347866 | validation: 0.1310263671159337]
	TIME [epoch: 5.75 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13273727908479785		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.13273727908479785 | validation: 0.15504447158458082]
	TIME [epoch: 5.76 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14001272631303485		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.14001272631303485 | validation: 0.15775091011834183]
	TIME [epoch: 5.75 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16674056130791898		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.16674056130791898 | validation: 0.1689583578471947]
	TIME [epoch: 5.77 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14398196600581928		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.14398196600581928 | validation: 0.1278247279723381]
	TIME [epoch: 5.75 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14356345036464654		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.14356345036464654 | validation: 0.23724600377647376]
	TIME [epoch: 5.75 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15650122013267204		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.15650122013267204 | validation: 0.12402826478161227]
	TIME [epoch: 5.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16131380717414318		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.16131380717414318 | validation: 0.20492632290544963]
	TIME [epoch: 5.75 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14546259794455108		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.14546259794455108 | validation: 0.11016520963704424]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_398.pth
	Model improved!!!
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14537557318323047		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.14537557318323047 | validation: 0.1447981407715669]
	TIME [epoch: 5.76 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1273714096460225		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.1273714096460225 | validation: 0.17942550838630722]
	TIME [epoch: 5.76 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15019900994798321		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.15019900994798321 | validation: 0.1407962217826142]
	TIME [epoch: 5.76 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14235782764455773		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.14235782764455773 | validation: 0.12593930128629402]
	TIME [epoch: 5.77 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15486350218317524		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.15486350218317524 | validation: 0.1961776248667194]
	TIME [epoch: 5.75 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14026647616237667		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.14026647616237667 | validation: 0.1253912106634474]
	TIME [epoch: 5.77 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17942019063614734		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.17942019063614734 | validation: 0.23773285354558796]
	TIME [epoch: 5.75 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768722715589304		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.1768722715589304 | validation: 0.20217373203157632]
	TIME [epoch: 5.76 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1920474524709573		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.1920474524709573 | validation: 0.2137961208661129]
	TIME [epoch: 5.76 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21564604086892336		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.21564604086892336 | validation: 0.10581296962068043]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12039746622000955		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.12039746622000955 | validation: 0.12815272077538326]
	TIME [epoch: 5.76 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11734706753408645		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.11734706753408645 | validation: 0.13204836384193047]
	TIME [epoch: 5.77 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13408626139961177		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.13408626139961177 | validation: 0.10258023391114668]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12744968213087016		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.12744968213087016 | validation: 0.20263007455701723]
	TIME [epoch: 5.75 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1321953407546436		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.1321953407546436 | validation: 0.10588294704017027]
	TIME [epoch: 5.77 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11627299262158783		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.11627299262158783 | validation: 0.11944282572464471]
	TIME [epoch: 5.76 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11059962444766594		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.11059962444766594 | validation: 0.12420522004756061]
	TIME [epoch: 5.77 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11049593267048931		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.11049593267048931 | validation: 0.09855210610479712]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1134909589695085		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.1134909589695085 | validation: 0.19471759840102074]
	TIME [epoch: 5.76 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306433127704042		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.1306433127704042 | validation: 0.12654123917548118]
	TIME [epoch: 5.77 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16514573477654496		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.16514573477654496 | validation: 0.2801522667341895]
	TIME [epoch: 5.75 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17107027768784008		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.17107027768784008 | validation: 0.2416866536264735]
	TIME [epoch: 5.77 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502408514183748		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2502408514183748 | validation: 0.1086099981121327]
	TIME [epoch: 5.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13270984543017794		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.13270984543017794 | validation: 0.34867043269024345]
	TIME [epoch: 5.77 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.273925939460349		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.273925939460349 | validation: 0.19630746969122773]
	TIME [epoch: 5.75 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21316236722248305		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.21316236722248305 | validation: 0.16834158589988704]
	TIME [epoch: 5.77 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17664251857280178		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.17664251857280178 | validation: 0.167926545547118]
	TIME [epoch: 5.75 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14945499200066467		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.14945499200066467 | validation: 0.11559252298310417]
	TIME [epoch: 5.76 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1143987108882196		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.1143987108882196 | validation: 0.1340371055046888]
	TIME [epoch: 5.77 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1210038162182671		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.1210038162182671 | validation: 0.13483003764882884]
	TIME [epoch: 5.77 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12598438927113917		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.12598438927113917 | validation: 0.11080591719901234]
	TIME [epoch: 5.75 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11062656652976166		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.11062656652976166 | validation: 0.10495255789994391]
	TIME [epoch: 5.77 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10913829552531681		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.10913829552531681 | validation: 0.24718513608795095]
	TIME [epoch: 5.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431928666196214		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.1431928666196214 | validation: 0.10886583747706516]
	TIME [epoch: 5.77 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1630381222541725		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.1630381222541725 | validation: 0.14181965368374053]
	TIME [epoch: 5.76 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11291443959489765		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.11291443959489765 | validation: 0.14578486610721245]
	TIME [epoch: 5.77 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11173739315275849		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.11173739315275849 | validation: 0.11474927833889369]
	TIME [epoch: 5.76 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11963390499236515		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.11963390499236515 | validation: 0.15446748570535723]
	TIME [epoch: 5.77 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13468819077575306		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.13468819077575306 | validation: 0.14117189367693941]
	TIME [epoch: 5.77 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12355714589372817		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.12355714589372817 | validation: 0.11207874229065654]
	TIME [epoch: 5.77 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13954782709769967		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.13954782709769967 | validation: 0.19714899359407556]
	TIME [epoch: 5.76 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13594817150528704		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.13594817150528704 | validation: 0.10129438431200928]
	TIME [epoch: 5.76 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13589191189137034		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.13589191189137034 | validation: 0.18317738099342717]
	TIME [epoch: 5.77 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479340681028664		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.1479340681028664 | validation: 0.1670656529450328]
	TIME [epoch: 5.76 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1693779663171778		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.1693779663171778 | validation: 0.14542892906193394]
	TIME [epoch: 5.77 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11645400834010242		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.11645400834010242 | validation: 0.10456862329155248]
	TIME [epoch: 5.76 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10154494159211076		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.10154494159211076 | validation: 0.12465091203703811]
	TIME [epoch: 5.78 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10405436122406507		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.10405436122406507 | validation: 0.11791014932221047]
	TIME [epoch: 5.78 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10081975711249878		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.10081975711249878 | validation: 0.10007966109380084]
	TIME [epoch: 5.78 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.098735670125077		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.098735670125077 | validation: 0.1440757535702209]
	TIME [epoch: 5.77 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10705986542315923		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.10705986542315923 | validation: 0.11989554831240141]
	TIME [epoch: 5.76 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14769433861109232		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.14769433861109232 | validation: 0.36751398028422866]
	TIME [epoch: 5.77 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22029601182515424		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.22029601182515424 | validation: 0.14192097165968232]
	TIME [epoch: 5.76 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19664321223315134		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.19664321223315134 | validation: 0.09623534109101058]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10172100076261636		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.10172100076261636 | validation: 0.22934856460174552]
	TIME [epoch: 5.76 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13349135124184322		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.13349135124184322 | validation: 0.09917757331276827]
	TIME [epoch: 5.76 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10891806502766738		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.10891806502766738 | validation: 0.1089136516170906]
	TIME [epoch: 5.77 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365964006768465		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.10365964006768465 | validation: 0.15970308495771435]
	TIME [epoch: 5.76 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11359815876482328		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.11359815876482328 | validation: 0.1197522990507744]
	TIME [epoch: 5.78 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546209991808577		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.11546209991808577 | validation: 0.1569790261090257]
	TIME [epoch: 5.77 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12664778258496817		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.12664778258496817 | validation: 0.1414074928277819]
	TIME [epoch: 5.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11796085301720044		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.11796085301720044 | validation: 0.09848708506641812]
	TIME [epoch: 5.77 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13224360363958707		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.13224360363958707 | validation: 0.15043877171233128]
	TIME [epoch: 5.78 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11458239370660604		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.11458239370660604 | validation: 0.11427510431685442]
	TIME [epoch: 5.76 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11219539160966996		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.11219539160966996 | validation: 0.13451911791286417]
	TIME [epoch: 5.77 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13076027648688587		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.13076027648688587 | validation: 0.12240605573197248]
	TIME [epoch: 5.78 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11109882698047001		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.11109882698047001 | validation: 0.1038739049562793]
	TIME [epoch: 5.78 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021787124063281		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.1021787124063281 | validation: 0.12627750483348352]
	TIME [epoch: 5.77 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10011756731616357		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.10011756731616357 | validation: 0.10292281993252826]
	TIME [epoch: 5.78 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09793688424262083		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.09793688424262083 | validation: 0.08748219957753572]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080308688561055		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.1080308688561055 | validation: 0.24894451397762538]
	TIME [epoch: 5.77 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15703010342627077		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.15703010342627077 | validation: 0.1944452864132707]
	TIME [epoch: 5.78 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23121983181893463		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.23121983181893463 | validation: 0.10482999950894159]
	TIME [epoch: 5.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0976016187594044		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.0976016187594044 | validation: 0.2599773858633802]
	TIME [epoch: 5.75 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16732738859285276		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.16732738859285276 | validation: 0.14380312840445142]
	TIME [epoch: 5.78 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18710562785683202		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.18710562785683202 | validation: 0.10058960522605193]
	TIME [epoch: 5.77 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09949900822370678		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.09949900822370678 | validation: 0.22179142101904847]
	TIME [epoch: 5.76 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15555994324898223		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.15555994324898223 | validation: 0.11706842567870115]
	TIME [epoch: 5.75 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12752300295080457		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.12752300295080457 | validation: 0.09505313341451163]
	TIME [epoch: 5.77 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09284634516253493		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.09284634516253493 | validation: 0.14225334754343893]
	TIME [epoch: 5.75 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511918453956178		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.09511918453956178 | validation: 0.08803424065445348]
	TIME [epoch: 5.77 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0937485076504294		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.0937485076504294 | validation: 0.10520681987555036]
	TIME [epoch: 5.78 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719072129900119		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.09719072129900119 | validation: 0.11693732859234172]
	TIME [epoch: 5.78 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09447250719787009		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.09447250719787009 | validation: 0.08348909679958723]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11446557150374893		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.11446557150374893 | validation: 0.12514387435057883]
	TIME [epoch: 5.77 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09147300431166233		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.09147300431166233 | validation: 0.09945675134467813]
	TIME [epoch: 5.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09311982608289153		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.09311982608289153 | validation: 0.093910802854398]
	TIME [epoch: 5.76 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08879895559764743		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.08879895559764743 | validation: 0.11109890654324897]
	TIME [epoch: 5.77 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09270151580047366		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.09270151580047366 | validation: 0.13056930687771479]
	TIME [epoch: 5.74 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12493026005177778		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.12493026005177778 | validation: 0.20902245761947716]
	TIME [epoch: 5.78 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21917882416272555		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.21917882416272555 | validation: 0.17585352357476947]
	TIME [epoch: 5.74 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18270870300762224		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.18270870300762224 | validation: 0.16030238736742716]
	TIME [epoch: 5.79 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11291328584591268		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.11291328584591268 | validation: 0.14043295632557623]
	TIME [epoch: 5.77 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12245061240186729		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.12245061240186729 | validation: 0.0823947310963407]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11009149084635475		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.11009149084635475 | validation: 0.18366169477151573]
	TIME [epoch: 5.75 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1201718313123619		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.1201718313123619 | validation: 0.08529131911609322]
	TIME [epoch: 5.77 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09391678050347288		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.09391678050347288 | validation: 0.09759140966105352]
	TIME [epoch: 5.75 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1033772597757908		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.1033772597757908 | validation: 0.1346632883955792]
	TIME [epoch: 5.77 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11061100940027521		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.11061100940027521 | validation: 0.12728258742099752]
	TIME [epoch: 5.75 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10191539522035907		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.10191539522035907 | validation: 0.08975688526174373]
	TIME [epoch: 5.78 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458849031864838		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.09458849031864838 | validation: 0.1570958485488072]
	TIME [epoch: 5.76 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09627639077042212		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.09627639077042212 | validation: 0.08183516447044131]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11233695757789092		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.11233695757789092 | validation: 0.13730870996011788]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10588796994366989		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.10588796994366989 | validation: 0.09926872754575004]
	TIME [epoch: 12.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149326823652421		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.1149326823652421 | validation: 0.20156908311150704]
	TIME [epoch: 12.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18209880703389877		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.18209880703389877 | validation: 0.07969155742475292]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11089686682951104		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.11089686682951104 | validation: 0.15676014039857022]
	TIME [epoch: 12.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1025179634929899		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.1025179634929899 | validation: 0.09423130062006423]
	TIME [epoch: 12.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09930498216301209		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.09930498216301209 | validation: 0.1060624455469696]
	TIME [epoch: 12.4 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0872495964390377		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.0872495964390377 | validation: 0.1376960253257701]
	TIME [epoch: 12.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09972628415796851		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.09972628415796851 | validation: 0.11234839772481822]
	TIME [epoch: 12.4 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09603255722276828		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.09603255722276828 | validation: 0.07673988642579473]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10291649412113228		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.10291649412113228 | validation: 0.14953305032510064]
	TIME [epoch: 12.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10556032710808833		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.10556032710808833 | validation: 0.07847431959076145]
	TIME [epoch: 12.3 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10970489068635773		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.10970489068635773 | validation: 0.1980677328528817]
	TIME [epoch: 12.3 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11294557239125932		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.11294557239125932 | validation: 0.08620561567138713]
	TIME [epoch: 12.3 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11339348350508825		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.11339348350508825 | validation: 0.14301591515712314]
	TIME [epoch: 12.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09025231959344802		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.09025231959344802 | validation: 0.08181232018459]
	TIME [epoch: 12.3 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08590225158654131		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.08590225158654131 | validation: 0.11954642565282421]
	TIME [epoch: 12.3 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1667889149255026		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1667889149255026 | validation: 0.15277068162232835]
	TIME [epoch: 12.3 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10568872395400568		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.10568872395400568 | validation: 0.18945445309782455]
	TIME [epoch: 12.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14340614647787922		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.14340614647787922 | validation: 0.14530850338253268]
	TIME [epoch: 12.3 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12912107112182894		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.12912107112182894 | validation: 0.09272350347300332]
	TIME [epoch: 12.3 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11477318487107134		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.11477318487107134 | validation: 0.11104614274490753]
	TIME [epoch: 12.3 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08688103446394775		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.08688103446394775 | validation: 0.1613078957458792]
	TIME [epoch: 12.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09443621342139132		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.09443621342139132 | validation: 0.08088345890258533]
	TIME [epoch: 12.3 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09981223712023367		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.09981223712023367 | validation: 0.11338959770858835]
	TIME [epoch: 12.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09435132994191057		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.09435132994191057 | validation: 0.1335539194250294]
	TIME [epoch: 12.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10354584075787177		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.10354584075787177 | validation: 0.11822847794014205]
	TIME [epoch: 12.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10107335646557364		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.10107335646557364 | validation: 0.0776533621151844]
	TIME [epoch: 12.3 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08996539211204002		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.08996539211204002 | validation: 0.12988808802530458]
	TIME [epoch: 12.3 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08889050302436875		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.08889050302436875 | validation: 0.08423638240623636]
	TIME [epoch: 12.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08405315574123001		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.08405315574123001 | validation: 0.09770337815134224]
	TIME [epoch: 12.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09388931775314578		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.09388931775314578 | validation: 0.1699731823821233]
	TIME [epoch: 12.3 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11158706999863068		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.11158706999863068 | validation: 0.14723998228483168]
	TIME [epoch: 12.3 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16470230644976874		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.16470230644976874 | validation: 0.1793167151645532]
	TIME [epoch: 12.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16701078702219982		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.16701078702219982 | validation: 0.1703573809906098]
	TIME [epoch: 12.3 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1085394642547379		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.1085394642547379 | validation: 0.12710656050748345]
	TIME [epoch: 12.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12321953674438227		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.12321953674438227 | validation: 0.07849186324623136]
	TIME [epoch: 12.3 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09288088143899006		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.09288088143899006 | validation: 0.18151070856759977]
	TIME [epoch: 12.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10213812953906906		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.10213812953906906 | validation: 0.08450160970650848]
	TIME [epoch: 12.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08778487583729139		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.08778487583729139 | validation: 0.12570408144816705]
	TIME [epoch: 12.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09822113975864329		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.09822113975864329 | validation: 0.08353489088715778]
	TIME [epoch: 12.3 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09637471053533336		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.09637471053533336 | validation: 0.14635688164251875]
	TIME [epoch: 12.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08765605411773258		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.08765605411773258 | validation: 0.07453175502109226]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08220424606378021		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.08220424606378021 | validation: 0.0924042624708931]
	TIME [epoch: 12.3 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08109444561988369		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.08109444561988369 | validation: 0.11402114877753283]
	TIME [epoch: 12.3 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0944110593722868		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.0944110593722868 | validation: 0.0970446896107241]
	TIME [epoch: 12.3 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10355326833183835		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.10355326833183835 | validation: 0.18818376821159283]
	TIME [epoch: 12.3 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11559250961288943		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.11559250961288943 | validation: 0.10125896325708306]
	TIME [epoch: 12.3 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14134406786167533		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.14134406786167533 | validation: 0.1613253420539644]
	TIME [epoch: 12.3 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09371628315319548		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.09371628315319548 | validation: 0.09108004653640187]
	TIME [epoch: 12.3 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08514981152932904		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.08514981152932904 | validation: 0.09284464775824477]
	TIME [epoch: 12.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07871729346216698		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.07871729346216698 | validation: 0.09806928206772463]
	TIME [epoch: 12.3 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07622803249345413		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.07622803249345413 | validation: 0.09758298569446232]
	TIME [epoch: 12.3 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571915987668827		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.07571915987668827 | validation: 0.09411165695741437]
	TIME [epoch: 12.3 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07870264362350372		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.07870264362350372 | validation: 0.11638203740115954]
	TIME [epoch: 12.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07941168877737265		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.07941168877737265 | validation: 0.08807649260924046]
	TIME [epoch: 12.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848735797191504		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.07848735797191504 | validation: 0.0890113496375059]
	TIME [epoch: 12.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306284697521855		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.08306284697521855 | validation: 0.14381418067110951]
	TIME [epoch: 12.4 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09923148046484141		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.09923148046484141 | validation: 0.13368108386798824]
	TIME [epoch: 12.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17877016744817223		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.17877016744817223 | validation: 0.1582417114570115]
	TIME [epoch: 12.3 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09979884411417214		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.09979884411417214 | validation: 0.08749880406417566]
	TIME [epoch: 12.3 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08807366612421831		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.08807366612421831 | validation: 0.11804286941776293]
	TIME [epoch: 12.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09747137704137213		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.09747137704137213 | validation: 0.11804875587760207]
	TIME [epoch: 12.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08726853223554938		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.08726853223554938 | validation: 0.07066907742060456]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08910325819005804		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.08910325819005804 | validation: 0.12674055924050207]
	TIME [epoch: 12.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07929317314937563		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.07929317314937563 | validation: 0.08084672109420421]
	TIME [epoch: 12.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0750946181112606		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.0750946181112606 | validation: 0.06922614784941923]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08357868484305656		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.08357868484305656 | validation: 0.21530322285447223]
	TIME [epoch: 12.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10948645246679686		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.10948645246679686 | validation: 0.08775950808669995]
	TIME [epoch: 12.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10416101106983494		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.10416101106983494 | validation: 0.1642062745395798]
	TIME [epoch: 12.3 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11510370017532683		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.11510370017532683 | validation: 0.11458841822536198]
	TIME [epoch: 12.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11594283006656488		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.11594283006656488 | validation: 0.10869835113430365]
	TIME [epoch: 12.3 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07829122050872038		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.07829122050872038 | validation: 0.06586937336576179]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08361926906295394		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.08361926906295394 | validation: 0.07760319598360228]
	TIME [epoch: 12.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08437066833949879		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.08437066833949879 | validation: 0.13214702714993026]
	TIME [epoch: 12.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858827590920364		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.0858827590920364 | validation: 0.06753801758812698]
	TIME [epoch: 12.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1030959227420284		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.1030959227420284 | validation: 0.1489938373840806]
	TIME [epoch: 12.4 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08838079366512876		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.08838079366512876 | validation: 0.06927512900472277]
	TIME [epoch: 12.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07647292776833402		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.07647292776833402 | validation: 0.07651247276754236]
	TIME [epoch: 12.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07531184546756756		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.07531184546756756 | validation: 0.09306813922760686]
	TIME [epoch: 12.3 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07538510962409829		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.07538510962409829 | validation: 0.0730960610047001]
	TIME [epoch: 12.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677259924166722		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.07677259924166722 | validation: 0.11522702555583611]
	TIME [epoch: 12.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477060317073329		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.08477060317073329 | validation: 0.07586515217409684]
	TIME [epoch: 12.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09619070350112381		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.09619070350112381 | validation: 0.250980394220582]
	TIME [epoch: 12.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12995112607048007		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.12995112607048007 | validation: 0.0942968617508743]
	TIME [epoch: 12.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07094682904963456		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.07094682904963456 | validation: 0.06689918216462258]
	TIME [epoch: 12.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839725248243192		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.0839725248243192 | validation: 0.1506467430987403]
	TIME [epoch: 12.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0891131083041811		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.0891131083041811 | validation: 0.10069561960584002]
	TIME [epoch: 12.3 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1037697409494308		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.1037697409494308 | validation: 0.14690494778286653]
	TIME [epoch: 12.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0930884282355402		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.0930884282355402 | validation: 0.06793737537667878]
	TIME [epoch: 12.3 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08976708168572252		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.08976708168572252 | validation: 0.11264903226922587]
	TIME [epoch: 12.3 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07829251522025782		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.07829251522025782 | validation: 0.07863242134337445]
	TIME [epoch: 12.3 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07405842322475094		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.07405842322475094 | validation: 0.08364548046081484]
	TIME [epoch: 12.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07041567209805863		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.07041567209805863 | validation: 0.10408123343004534]
	TIME [epoch: 12.3 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207772633422858		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.07207772633422858 | validation: 0.06975050630297869]
	TIME [epoch: 12.3 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07628673588164576		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.07628673588164576 | validation: 0.12277034152429933]
	TIME [epoch: 12.3 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09382954744202243		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.09382954744202243 | validation: 0.08227643511276028]
	TIME [epoch: 12.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10776755436992737		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.10776755436992737 | validation: 0.2083862521518797]
	TIME [epoch: 12.3 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243059447180336		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.10243059447180336 | validation: 0.06836889077335044]
	TIME [epoch: 12.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07734951396674135		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.07734951396674135 | validation: 0.07102114519568374]
	TIME [epoch: 12.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07222912897182884		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.07222912897182884 | validation: 0.1270830578914926]
	TIME [epoch: 12.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0782877523984839		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.0782877523984839 | validation: 0.08009704293806102]
	TIME [epoch: 12.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118390936758806		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.07118390936758806 | validation: 0.08235985009501896]
	TIME [epoch: 12.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07024846979952014		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.07024846979952014 | validation: 0.08630240663598543]
	TIME [epoch: 12.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665970056423774		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.07665970056423774 | validation: 0.16202563989002317]
	TIME [epoch: 12.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10499036966682183		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.10499036966682183 | validation: 0.11783358844472847]
	TIME [epoch: 12.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331536786755169		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.1331536786755169 | validation: 0.095159120484339]
	TIME [epoch: 12.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763443656732808		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.0763443656732808 | validation: 0.11619228743158634]
	TIME [epoch: 12.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07696872010704514		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.07696872010704514 | validation: 0.05972451240102964]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07986864378557733		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.07986864378557733 | validation: 0.12433715828583568]
	TIME [epoch: 12.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09013143698871527		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.09013143698871527 | validation: 0.07671793364129097]
	TIME [epoch: 12.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08492085967405157		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.08492085967405157 | validation: 0.12452288721544749]
	TIME [epoch: 12.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07237740235244654		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.07237740235244654 | validation: 0.0666356291120294]
	TIME [epoch: 12.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07512867809821519		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.07512867809821519 | validation: 0.09099393519666521]
	TIME [epoch: 12.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06719692984672765		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.06719692984672765 | validation: 0.08350475291995213]
	TIME [epoch: 12.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557240415756786		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.07557240415756786 | validation: 0.09236976074783702]
	TIME [epoch: 12.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07937075056742258		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.07937075056742258 | validation: 0.059562103071419896]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07567622497710799		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.07567622497710799 | validation: 0.16140525111258205]
	TIME [epoch: 12.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08085579643515384		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.08085579643515384 | validation: 0.06277351921094819]
	TIME [epoch: 12.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08751622228794723		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.08751622228794723 | validation: 0.14605367901584893]
	TIME [epoch: 12.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219357517779647		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.09219357517779647 | validation: 0.08781048134993133]
	TIME [epoch: 12.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09896424506229862		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.09896424506229862 | validation: 0.09054720422788239]
	TIME [epoch: 12.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10743740670546015		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.10743740670546015 | validation: 0.101216659723318]
	TIME [epoch: 12.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07421042041306633		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.07421042041306633 | validation: 0.15705869904208603]
	TIME [epoch: 12.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08659233872878006		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.08659233872878006 | validation: 0.0714120461867074]
	TIME [epoch: 12.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07369413184110839		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.07369413184110839 | validation: 0.06605990532435488]
	TIME [epoch: 12.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07279398098734632		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.07279398098734632 | validation: 0.0864016768503752]
	TIME [epoch: 12.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970901816179739		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.07970901816179739 | validation: 0.16352083776852241]
	TIME [epoch: 12.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08965645343108573		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.08965645343108573 | validation: 0.06473357009032583]
	TIME [epoch: 12.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660256395279843		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.08660256395279843 | validation: 0.1026260119191286]
	TIME [epoch: 12.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08923802096068859		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.08923802096068859 | validation: 0.0714513127249453]
	TIME [epoch: 12.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07234169893393523		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07234169893393523 | validation: 0.11324490981362836]
	TIME [epoch: 12.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07790154606209551		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.07790154606209551 | validation: 0.0814693561419027]
	TIME [epoch: 12.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06918374070527188		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.06918374070527188 | validation: 0.05621887821508592]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07396185762831756		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.07396185762831756 | validation: 0.17895255672776803]
	TIME [epoch: 12.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08815006426044417		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.08815006426044417 | validation: 0.0782935911973591]
	TIME [epoch: 12.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0833280426974323		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.0833280426974323 | validation: 0.0960791531503502]
	TIME [epoch: 12.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08089114031434667		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.08089114031434667 | validation: 0.05762224470351818]
	TIME [epoch: 12.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0707334098561999		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.0707334098561999 | validation: 0.0803073269668522]
	TIME [epoch: 12.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848086960073206		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.06848086960073206 | validation: 0.09136770615501799]
	TIME [epoch: 12.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770821378406831		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.07770821378406831 | validation: 0.23894091199382556]
	TIME [epoch: 12.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25119082081382177		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.25119082081382177 | validation: 0.07822756484818938]
	TIME [epoch: 12.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717944606353623		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.09717944606353623 | validation: 0.1295975965804502]
	TIME [epoch: 12.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08193943511718832		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.08193943511718832 | validation: 0.14070336658544572]
	TIME [epoch: 12.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014940552938388		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.08014940552938388 | validation: 0.06941110447514867]
	TIME [epoch: 12.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06989651264660396		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.06989651264660396 | validation: 0.08318135245018463]
	TIME [epoch: 12.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07503564489293826		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.07503564489293826 | validation: 0.0707131427138299]
	TIME [epoch: 12.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07204056351191121		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.07204056351191121 | validation: 0.09500448468347472]
	TIME [epoch: 12.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694703776528331		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.06694703776528331 | validation: 0.08866017185698272]
	TIME [epoch: 12.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08166833769815224		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.08166833769815224 | validation: 0.05964949591718131]
	TIME [epoch: 12.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08615110492703468		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.08615110492703468 | validation: 0.09346339187945911]
	TIME [epoch: 12.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06926467597979304		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.06926467597979304 | validation: 0.09214737808563528]
	TIME [epoch: 12.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630380738384038		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.0630380738384038 | validation: 0.07099383257371408]
	TIME [epoch: 12.3 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06582207104440664		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.06582207104440664 | validation: 0.07391883211383304]
	TIME [epoch: 12.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06538387270155104		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.06538387270155104 | validation: 0.07666821710026361]
	TIME [epoch: 12.3 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06782992287825014		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.06782992287825014 | validation: 0.0813044664194529]
	TIME [epoch: 12.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06417592491497175		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.06417592491497175 | validation: 0.1291711207981545]
	TIME [epoch: 12.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806925631587012		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.0806925631587012 | validation: 0.1115124860467852]
	TIME [epoch: 12.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.146574279313618		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.146574279313618 | validation: 0.11711977088258307]
	TIME [epoch: 12.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08102672313416534		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.08102672313416534 | validation: 0.09189937330315]
	TIME [epoch: 12.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.065532688610861		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.065532688610861 | validation: 0.06884988123862049]
	TIME [epoch: 12.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07425925656613401		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.07425925656613401 | validation: 0.08556680173996112]
	TIME [epoch: 12.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679650551074719		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.06679650551074719 | validation: 0.06859448118735738]
	TIME [epoch: 12.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0657356144378441		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.0657356144378441 | validation: 0.07885913149504607]
	TIME [epoch: 12.3 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06456138160857529		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.06456138160857529 | validation: 0.08185058398531327]
	TIME [epoch: 12.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06183522473324106		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.06183522473324106 | validation: 0.07902110738478796]
	TIME [epoch: 12.3 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06195420174823291		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.06195420174823291 | validation: 0.05724614582977055]
	TIME [epoch: 12.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06571350386506666		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.06571350386506666 | validation: 0.1043236575120498]
	TIME [epoch: 12.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063243551124489		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.07063243551124489 | validation: 0.07264273872316077]
	TIME [epoch: 12.3 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798124986978243		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.0798124986978243 | validation: 0.09746955589504191]
	TIME [epoch: 12.3 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09552713202346556		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.09552713202346556 | validation: 0.05362783965696269]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0672413759361363		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.0672413759361363 | validation: 0.10677994762404458]
	TIME [epoch: 12.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251048718320183		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.06251048718320183 | validation: 0.06505516539239778]
	TIME [epoch: 12.3 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464601271772207		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.06464601271772207 | validation: 0.07751011673077478]
	TIME [epoch: 12.3 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06104508464975766		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.06104508464975766 | validation: 0.0877144380443295]
	TIME [epoch: 12.3 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897021045366729		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.06897021045366729 | validation: 0.08086445401624648]
	TIME [epoch: 12.3 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654267808276735		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.0654267808276735 | validation: 0.06630342439866568]
	TIME [epoch: 12.3 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06720468640072547		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.06720468640072547 | validation: 0.11773136136577378]
	TIME [epoch: 12.3 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871431025088086		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.06871431025088086 | validation: 0.05673118727979964]
	TIME [epoch: 12.3 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849093278112971		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.0849093278112971 | validation: 0.12620870393169845]
	TIME [epoch: 12.3 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08253183805491088		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.08253183805491088 | validation: 0.06498369198749303]
	TIME [epoch: 12.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08850433588752499		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.08850433588752499 | validation: 0.09971377407219827]
	TIME [epoch: 12.3 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07067883632935822		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.07067883632935822 | validation: 0.05808555384254365]
	TIME [epoch: 12.3 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06539091075551279		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.06539091075551279 | validation: 0.06731087677754101]
	TIME [epoch: 12.3 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236886996235147		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.06236886996235147 | validation: 0.07848990413511002]
	TIME [epoch: 12.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0706084097006865		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.0706084097006865 | validation: 0.07489652876604379]
	TIME [epoch: 12.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08265169731784973		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.08265169731784973 | validation: 0.1402521951373228]
	TIME [epoch: 12.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07741818244279967		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.07741818244279967 | validation: 0.06279980488546226]
	TIME [epoch: 12.3 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07626727920442558		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.07626727920442558 | validation: 0.07233052121940657]
	TIME [epoch: 12.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06841407227300829		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06841407227300829 | validation: 0.13976099629742036]
	TIME [epoch: 12.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835758957494311		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.0835758957494311 | validation: 0.07757768696756277]
	TIME [epoch: 12.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386961152039838		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.06386961152039838 | validation: 0.064476687579344]
	TIME [epoch: 12.3 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08377869581630935		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.08377869581630935 | validation: 0.08218493945883551]
	TIME [epoch: 12.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06770196171652325		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06770196171652325 | validation: 0.09037926959361049]
	TIME [epoch: 12.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06629956073480019		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.06629956073480019 | validation: 0.06752741811225728]
	TIME [epoch: 12.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06250864224414035		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.06250864224414035 | validation: 0.05339065596826185]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06305768204816496		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.06305768204816496 | validation: 0.10495542077410891]
	TIME [epoch: 12.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414484376887142		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.06414484376887142 | validation: 0.061276262016176676]
	TIME [epoch: 12.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07708551552535897		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.07708551552535897 | validation: 0.1558945419931045]
	TIME [epoch: 12.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07963118705138184		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.07963118705138184 | validation: 0.05341595874373156]
	TIME [epoch: 12.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07766587887643293		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.07766587887643293 | validation: 0.08836791985885034]
	TIME [epoch: 12.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07495270489313188		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.07495270489313188 | validation: 0.05264723588188072]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0614527326191952		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.0614527326191952 | validation: 0.0741565287832098]
	TIME [epoch: 12.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06069730346739674		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.06069730346739674 | validation: 0.09803814760578553]
	TIME [epoch: 12.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06694557208912782		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.06694557208912782 | validation: 0.060459676768257836]
	TIME [epoch: 12.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07430852876521747		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.07430852876521747 | validation: 0.10151037256827018]
	TIME [epoch: 12.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06277836464583417		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06277836464583417 | validation: 0.07846228588213061]
	TIME [epoch: 12.1 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06308616415557598		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.06308616415557598 | validation: 0.10595487360561368]
	TIME [epoch: 12.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226440649503572		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.07226440649503572 | validation: 0.07519501560641648]
	TIME [epoch: 12.1 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06995448675987329		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.06995448675987329 | validation: 0.09485707712944774]
	TIME [epoch: 12.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09611582636843254		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.09611582636843254 | validation: 0.13890631117392377]
	TIME [epoch: 12.1 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707647724057883		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.07707647724057883 | validation: 0.05684253493141368]
	TIME [epoch: 12.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06484232056618264		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.06484232056618264 | validation: 0.06442114683209645]
	TIME [epoch: 12.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060140862037871376		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.060140862037871376 | validation: 0.1286184268699336]
	TIME [epoch: 12.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1663360360032123		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.1663360360032123 | validation: 0.07190945875948718]
	TIME [epoch: 12.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07969387597643704		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.07969387597643704 | validation: 0.09019242794121655]
	TIME [epoch: 12.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.074425880007027		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.074425880007027 | validation: 0.15158403085831282]
	TIME [epoch: 12.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798342261542512		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0798342261542512 | validation: 0.05926982884165395]
	TIME [epoch: 12.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06916482478668462		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.06916482478668462 | validation: 0.0744269708971312]
	TIME [epoch: 12.1 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317067065630749		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.06317067065630749 | validation: 0.07054616242170679]
	TIME [epoch: 12.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062395179586350394		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.062395179586350394 | validation: 0.060621426654681815]
	TIME [epoch: 12.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061152886370376414		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.061152886370376414 | validation: 0.06495513405822079]
	TIME [epoch: 12.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05980504434070803		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.05980504434070803 | validation: 0.05214912408535999]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06257377480772004		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.06257377480772004 | validation: 0.10858060702434491]
	TIME [epoch: 12.3 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06980759965030624		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.06980759965030624 | validation: 0.060470251752361516]
	TIME [epoch: 12.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843026372059509		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.06843026372059509 | validation: 0.10174357764380614]
	TIME [epoch: 12.3 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07139270755268143		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.07139270755268143 | validation: 0.0647208496195747]
	TIME [epoch: 12.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06713866045877379		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.06713866045877379 | validation: 0.08106581969492013]
	TIME [epoch: 12.3 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06342778293182202		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.06342778293182202 | validation: 0.06189789017052251]
	TIME [epoch: 12.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0594966842681084		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.0594966842681084 | validation: 0.05403233385932606]
	TIME [epoch: 12.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142999474620048		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.06142999474620048 | validation: 0.08245376270527428]
	TIME [epoch: 12.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059364666149939946		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.059364666149939946 | validation: 0.06178891569226629]
	TIME [epoch: 12.3 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317918344511471		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.06317918344511471 | validation: 0.08005118717845955]
	TIME [epoch: 12.1 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667127423918702		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.09667127423918702 | validation: 0.14883979324867558]
	TIME [epoch: 12.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557153936451784		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.07557153936451784 | validation: 0.11426232065729985]
	TIME [epoch: 12.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843955798048461		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.0843955798048461 | validation: 0.10659207613640778]
	TIME [epoch: 12.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06457368926765675		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.06457368926765675 | validation: 0.049342302161989884]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_737.pth
	Model improved!!!
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06284168574968245		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.06284168574968245 | validation: 0.05323947335972499]
	TIME [epoch: 12.3 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06634649157364061		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.06634649157364061 | validation: 0.10975722408444155]
	TIME [epoch: 12.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07049430216411506		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.07049430216411506 | validation: 0.06177585470791476]
	TIME [epoch: 12.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07049131570964973		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.07049131570964973 | validation: 0.09816683539587126]
	TIME [epoch: 12.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07472311671993304		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.07472311671993304 | validation: 0.05901458265482232]
	TIME [epoch: 12.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06269997677093157		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.06269997677093157 | validation: 0.07148603814496234]
	TIME [epoch: 12.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057919419385125466		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.057919419385125466 | validation: 0.07747570765681566]
	TIME [epoch: 12.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05879262040842922		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05879262040842922 | validation: 0.07066875420321257]
	TIME [epoch: 12.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05947771836801801		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.05947771836801801 | validation: 0.0510523747124091]
	TIME [epoch: 12.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05778414149616227		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.05778414149616227 | validation: 0.07847574526842636]
	TIME [epoch: 12.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06178434589821336		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.06178434589821336 | validation: 0.06643334579170147]
	TIME [epoch: 12.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05902439555650345		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.05902439555650345 | validation: 0.05702837778568348]
	TIME [epoch: 12.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707988664072077		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.07707988664072077 | validation: 0.13580899895324447]
	TIME [epoch: 12.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08566417511150755		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.08566417511150755 | validation: 0.06205835919979078]
	TIME [epoch: 12.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425740992754128		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.06425740992754128 | validation: 0.06230563559989544]
	TIME [epoch: 12.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05818328680024987		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.05818328680024987 | validation: 0.06070484538292538]
	TIME [epoch: 12.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060605313329371406		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.060605313329371406 | validation: 0.050945920858443544]
	TIME [epoch: 12.2 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06201388447311135		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.06201388447311135 | validation: 0.06560634348479881]
	TIME [epoch: 12.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05704758521305044		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.05704758521305044 | validation: 0.06775052800014623]
	TIME [epoch: 12.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739140667204737		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.05739140667204737 | validation: 0.054785320445160296]
	TIME [epoch: 12.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06052533048111293		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.06052533048111293 | validation: 0.11073475351721418]
	TIME [epoch: 12.2 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06401181057813149		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.06401181057813149 | validation: 0.058419538328043225]
	TIME [epoch: 12.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07019275930354274		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.07019275930354274 | validation: 0.0992150988190284]
	TIME [epoch: 12.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575064300448463		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.06575064300448463 | validation: 0.06209030644140006]
	TIME [epoch: 12.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06467275040470434		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.06467275040470434 | validation: 0.07582206652077128]
	TIME [epoch: 12.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058560009953451536		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.058560009953451536 | validation: 0.05081549323221271]
	TIME [epoch: 12.2 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07096661445287525		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.07096661445287525 | validation: 0.07120115368654056]
	TIME [epoch: 12.3 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06016158378926321		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.06016158378926321 | validation: 0.08919277677287568]
	TIME [epoch: 12.2 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06410827462144912		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.06410827462144912 | validation: 0.05459633249904863]
	TIME [epoch: 12.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07041462652357391		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.07041462652357391 | validation: 0.08587356927762102]
	TIME [epoch: 12.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551892859127005		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.06551892859127005 | validation: 0.06716122657891363]
	TIME [epoch: 12.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660680502703389		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05660680502703389 | validation: 0.0652678692235871]
	TIME [epoch: 12.1 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05790200756336757		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05790200756336757 | validation: 0.05460793462700068]
	TIME [epoch: 12.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05756784379800064		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.05756784379800064 | validation: 0.06129316442442561]
	TIME [epoch: 12.1 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05711694307028404		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.05711694307028404 | validation: 0.0780008654146766]
	TIME [epoch: 12.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06301454369931692		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.06301454369931692 | validation: 0.050369910900918836]
	TIME [epoch: 12.2 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645988269901913		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.0645988269901913 | validation: 0.0930821660666639]
	TIME [epoch: 12.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06043085139603415		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.06043085139603415 | validation: 0.05545132128918831]
	TIME [epoch: 12.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05547156195864202		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.05547156195864202 | validation: 0.07149029842636133]
	TIME [epoch: 12.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09411173971997742		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.09411173971997742 | validation: 0.0769529853366987]
	TIME [epoch: 12.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294457463061942		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.06294457463061942 | validation: 0.08089645460536264]
	TIME [epoch: 12.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07963885356611465		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.07963885356611465 | validation: 0.1437452102160736]
	TIME [epoch: 12.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06955361773879752		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.06955361773879752 | validation: 0.05029047200470632]
	TIME [epoch: 12.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059194775653547735		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.059194775653547735 | validation: 0.05490509865453644]
	TIME [epoch: 12.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05653506983321353		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05653506983321353 | validation: 0.0644056634461378]
	TIME [epoch: 12.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0551270909980152		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.0551270909980152 | validation: 0.0710440172138555]
	TIME [epoch: 12.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05967575651400557		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.05967575651400557 | validation: 0.11314454527865814]
	TIME [epoch: 12.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060866394197511574		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.060866394197511574 | validation: 0.049747071972147644]
	TIME [epoch: 12.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331882216187788		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.06331882216187788 | validation: 0.08450109717977133]
	TIME [epoch: 12.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06811343260814962		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.06811343260814962 | validation: 0.061084316523125605]
	TIME [epoch: 12.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06100367030396259		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.06100367030396259 | validation: 0.07244930401643566]
	TIME [epoch: 12.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05437740341002525		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.05437740341002525 | validation: 0.05735986246235234]
	TIME [epoch: 12.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056614784200583086		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.056614784200583086 | validation: 0.08306474419725779]
	TIME [epoch: 12.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08188922699387013		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.08188922699387013 | validation: 0.1336178407950786]
	TIME [epoch: 12.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759972390438453		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06759972390438453 | validation: 0.04601598779226989]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05745510943953871		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.05745510943953871 | validation: 0.07891492267797592]
	TIME [epoch: 12.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0597080618744744		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.0597080618744744 | validation: 0.052603973526619344]
	TIME [epoch: 12.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05405408829600802		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05405408829600802 | validation: 0.06061485747119414]
	TIME [epoch: 12.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05486658411091994		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.05486658411091994 | validation: 0.058052093947826136]
	TIME [epoch: 12.3 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055926576562338265		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.055926576562338265 | validation: 0.06820703714861014]
	TIME [epoch: 12.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05426227034236195		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.05426227034236195 | validation: 0.055629553191922136]
	TIME [epoch: 12.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055441757169669974		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.055441757169669974 | validation: 0.1025393032903643]
	TIME [epoch: 12.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06907881980068795		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.06907881980068795 | validation: 0.06359248227627845]
	TIME [epoch: 12.2 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060776467765927954		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.060776467765927954 | validation: 0.07809859898564803]
	TIME [epoch: 12.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06244251023506894		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.06244251023506894 | validation: 0.051751086586347576]
	TIME [epoch: 12.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06408456915919278		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.06408456915919278 | validation: 0.101523451991158]
	TIME [epoch: 12.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062340469383604095		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.062340469383604095 | validation: 0.04628366422746886]
	TIME [epoch: 12.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06230456281554059		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.06230456281554059 | validation: 0.05946187556125904]
	TIME [epoch: 12.2 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053733002778172195		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.053733002778172195 | validation: 0.09234134073461593]
	TIME [epoch: 12.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05946693892269007		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.05946693892269007 | validation: 0.05751045502242265]
	TIME [epoch: 12.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06609398905135631		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.06609398905135631 | validation: 0.07787379452453291]
	TIME [epoch: 12.4 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286511438608834		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.06286511438608834 | validation: 0.05617845484220555]
	TIME [epoch: 12.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055634181439760594		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.055634181439760594 | validation: 0.21152872962816868]
	TIME [epoch: 12.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1568070729075822		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.1568070729075822 | validation: 0.17822534507862794]
	TIME [epoch: 12.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09833545765884813		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.09833545765884813 | validation: 0.08034336613091617]
	TIME [epoch: 12.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06384383870745933		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.06384383870745933 | validation: 0.04954889660256789]
	TIME [epoch: 12.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06651792870390458		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.06651792870390458 | validation: 0.049300587637481165]
	TIME [epoch: 12.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707225619187226		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.05707225619187226 | validation: 0.05696788867072881]
	TIME [epoch: 12.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684580472217153		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.05684580472217153 | validation: 0.04814837408678918]
	TIME [epoch: 12.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05626704974912813		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.05626704974912813 | validation: 0.0786585919604219]
	TIME [epoch: 12.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05623147947911619		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.05623147947911619 | validation: 0.07688497884056697]
	TIME [epoch: 12.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05727262092906489		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.05727262092906489 | validation: 0.04646988007908381]
	TIME [epoch: 12.2 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05596391567160128		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.05596391567160128 | validation: 0.05622843869521191]
	TIME [epoch: 12.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655939543712572		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.05655939543712572 | validation: 0.057345067700892094]
	TIME [epoch: 12.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053338809872264616		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.053338809872264616 | validation: 0.10047591711747508]
	TIME [epoch: 12.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06354559603569673		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.06354559603569673 | validation: 0.058140181261006285]
	TIME [epoch: 12.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055688680254621856		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.055688680254621856 | validation: 0.08640948643085927]
	TIME [epoch: 12.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058755749691922966		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.058755749691922966 | validation: 0.08585545697806218]
	TIME [epoch: 12.2 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05691162687201684		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.05691162687201684 | validation: 0.05211798470064767]
	TIME [epoch: 12.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662473453276971		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.06662473453276971 | validation: 0.08933634657382326]
	TIME [epoch: 12.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06528694920717701		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.06528694920717701 | validation: 0.0634321267836681]
	TIME [epoch: 12.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05644949454520367		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.05644949454520367 | validation: 0.06020235023543857]
	TIME [epoch: 12.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414324941618416		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.06414324941618416 | validation: 0.09237317262350629]
	TIME [epoch: 12.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263489145540481		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.06263489145540481 | validation: 0.051983910619806]
	TIME [epoch: 12.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051932221685097424		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.051932221685097424 | validation: 0.05628448869767029]
	TIME [epoch: 12.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055840340483438194		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.055840340483438194 | validation: 0.07865722622582011]
	TIME [epoch: 12.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05771048072244686		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.05771048072244686 | validation: 0.050474863806923446]
	TIME [epoch: 12.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05395325732248088		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.05395325732248088 | validation: 0.059030731210818005]
	TIME [epoch: 12.2 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054538317935954005		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.054538317935954005 | validation: 0.06302819612317931]
	TIME [epoch: 12.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05398693413912686		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.05398693413912686 | validation: 0.07281506779077931]
	TIME [epoch: 12.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531278142129767		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.05531278142129767 | validation: 0.05845612573309975]
	TIME [epoch: 12.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052956742851346335		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.052956742851346335 | validation: 0.08965753084995924]
	TIME [epoch: 12.2 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059056820116010156		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.059056820116010156 | validation: 0.05533408240768625]
	TIME [epoch: 12.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052643261602255444		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.052643261602255444 | validation: 0.04581036939786908]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_841.pth
	Model improved!!!
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949540390979531		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.05949540390979531 | validation: 0.07740055283599852]
	TIME [epoch: 12.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057330757415072035		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.057330757415072035 | validation: 0.059870124626690696]
	TIME [epoch: 12.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05819773644973052		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.05819773644973052 | validation: 0.07546510626734937]
	TIME [epoch: 12.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05512988728529601		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.05512988728529601 | validation: 0.05095342848307016]
	TIME [epoch: 12.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060846403987301304		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.060846403987301304 | validation: 0.10701311670194201]
	TIME [epoch: 12.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06591433008829559		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.06591433008829559 | validation: 0.048507392812845274]
	TIME [epoch: 12.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05917698938413276		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.05917698938413276 | validation: 0.0493914042584436]
	TIME [epoch: 12.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054996894276703456		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.054996894276703456 | validation: 0.06801937300006689]
	TIME [epoch: 12.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05181657946206639		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.05181657946206639 | validation: 0.05057505879563978]
	TIME [epoch: 12.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058986666296921564		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.058986666296921564 | validation: 0.08140134092395562]
	TIME [epoch: 12.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055019714494418656		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.055019714494418656 | validation: 0.06758175774233262]
	TIME [epoch: 12.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05628290479412163		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.05628290479412163 | validation: 0.05360874828817254]
	TIME [epoch: 12.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052947219346441185		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.052947219346441185 | validation: 0.06766879340260139]
	TIME [epoch: 12.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05183534392364136		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.05183534392364136 | validation: 0.04456172347387586]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05660961589050158		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.05660961589050158 | validation: 0.04588529415443007]
	TIME [epoch: 12.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056622302405806506		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.056622302405806506 | validation: 0.11029125683362956]
	TIME [epoch: 12.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06124389120418421		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.06124389120418421 | validation: 0.05227702479878252]
	TIME [epoch: 12.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053512260843179504		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.053512260843179504 | validation: 0.05161207861611753]
	TIME [epoch: 12.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05012789006531511		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.05012789006531511 | validation: 0.10410183049225603]
	TIME [epoch: 12.2 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337485972134038		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.06337485972134038 | validation: 0.061234724490854875]
	TIME [epoch: 12.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06037217644077009		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.06037217644077009 | validation: 0.06092348171401826]
	TIME [epoch: 12.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05284381472476217		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.05284381472476217 | validation: 0.04947497530758979]
	TIME [epoch: 12.2 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0498499109315648		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.0498499109315648 | validation: 0.0640629635992583]
	TIME [epoch: 12.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051122397634330845		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.051122397634330845 | validation: 0.049828441354578736]
	TIME [epoch: 12.2 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05177399358897521		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.05177399358897521 | validation: 0.0653382119879977]
	TIME [epoch: 12.2 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315195290800739		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.05315195290800739 | validation: 0.05453387245224302]
	TIME [epoch: 12.2 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06363280242666029		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.06363280242666029 | validation: 0.05202182103617445]
	TIME [epoch: 12.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05218461098553224		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.05218461098553224 | validation: 0.06373766368255081]
	TIME [epoch: 12.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051140877542206385		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.051140877542206385 | validation: 0.07301524020679186]
	TIME [epoch: 12.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05158906447674501		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.05158906447674501 | validation: 0.057307761203750786]
	TIME [epoch: 12.3 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05948574568104832		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.05948574568104832 | validation: 0.09408295353845715]
	TIME [epoch: 12.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06837855180763237		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.06837855180763237 | validation: 0.041969527543288115]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06865057815596172		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.06865057815596172 | validation: 0.060996338663305505]
	TIME [epoch: 12.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04989785676700837		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.04989785676700837 | validation: 0.1288686072574384]
	TIME [epoch: 12.2 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08254289983134416		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.08254289983134416 | validation: 0.08054134758685888]
	TIME [epoch: 12.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692805263838571		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.05692805263838571 | validation: 0.04730675798514651]
	TIME [epoch: 12.2 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635361716412637		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.05635361716412637 | validation: 0.05846820547908843]
	TIME [epoch: 12.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05120561051253679		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.05120561051253679 | validation: 0.04769415689576825]
	TIME [epoch: 12.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05096180940483822		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.05096180940483822 | validation: 0.060141721073712774]
	TIME [epoch: 12.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053004909781990274		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.053004909781990274 | validation: 0.0573291302405578]
	TIME [epoch: 12.2 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04953769057677902		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.04953769057677902 | validation: 0.045308900234467236]
	TIME [epoch: 12.2 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053687761131104836		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.053687761131104836 | validation: 0.07420391865965663]
	TIME [epoch: 12.2 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05526364912032248		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.05526364912032248 | validation: 0.04749321511152928]
	TIME [epoch: 12.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050234392011069745		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.050234392011069745 | validation: 0.05084015623399257]
	TIME [epoch: 12.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049382483895623254		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.049382483895623254 | validation: 0.07300933243660028]
	TIME [epoch: 12.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05202029020866906		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.05202029020866906 | validation: 0.04704133153605026]
	TIME [epoch: 12.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052993484299611586		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.052993484299611586 | validation: 0.06574411809393953]
	TIME [epoch: 12.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971107607618354		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.04971107607618354 | validation: 0.050850835749044115]
	TIME [epoch: 12.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050248896300746644		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.050248896300746644 | validation: 0.05521924022009225]
	TIME [epoch: 12.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05235365667986059		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.05235365667986059 | validation: 0.04140274842234834]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_891.pth
	Model improved!!!
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05285503374614872		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.05285503374614872 | validation: 0.0741287976171741]
	TIME [epoch: 12.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05426877235151691		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.05426877235151691 | validation: 0.0503550650967511]
	TIME [epoch: 12.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05480927794256517		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.05480927794256517 | validation: 0.05167550840591032]
	TIME [epoch: 12.2 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052856127324646666		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.052856127324646666 | validation: 0.05657040243731748]
	TIME [epoch: 12.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05039543202250323		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.05039543202250323 | validation: 0.06294971790671285]
	TIME [epoch: 12.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06405236609618271		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.06405236609618271 | validation: 0.07636833028461028]
	TIME [epoch: 12.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06355910757268983		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.06355910757268983 | validation: 0.04650872625772835]
	TIME [epoch: 12.2 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06153636535339176		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.06153636535339176 | validation: 0.05895434815869516]
	TIME [epoch: 12.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04946916736386266		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.04946916736386266 | validation: 0.050742525196506086]
	TIME [epoch: 12.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905807214285117		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.04905807214285117 | validation: 0.05064398798789056]
	TIME [epoch: 12.2 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05720139200915074		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.05720139200915074 | validation: 0.08363496026532118]
	TIME [epoch: 12.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058315150456937116		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.058315150456937116 | validation: 0.04648957657872156]
	TIME [epoch: 12.2 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05322776717583242		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.05322776717583242 | validation: 0.050225564350882615]
	TIME [epoch: 12.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998286073230317		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.04998286073230317 | validation: 0.07600902659303851]
	TIME [epoch: 12.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05380010528309875		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.05380010528309875 | validation: 0.05473687618870376]
	TIME [epoch: 12.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053854342972174916		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.053854342972174916 | validation: 0.05595295960205529]
	TIME [epoch: 12.1 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04912528399115882		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.04912528399115882 | validation: 0.05150807922622136]
	TIME [epoch: 12.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04982606929283875		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.04982606929283875 | validation: 0.04353934995434722]
	TIME [epoch: 12.1 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052378483400463055		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.052378483400463055 | validation: 0.06864761674845828]
	TIME [epoch: 12.2 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05300004605807418		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.05300004605807418 | validation: 0.04208590478885822]
	TIME [epoch: 12.1 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05062704388045999		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.05062704388045999 | validation: 0.06921692422442621]
	TIME [epoch: 12.2 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05349575693048693		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.05349575693048693 | validation: 0.04387260708712914]
	TIME [epoch: 12.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267021742732188		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.05267021742732188 | validation: 0.07205064609405164]
	TIME [epoch: 12.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054509213882591466		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.054509213882591466 | validation: 0.05189518996887064]
	TIME [epoch: 12.1 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04891479516279425		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.04891479516279425 | validation: 0.04574364731436199]
	TIME [epoch: 12.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05161310078748389		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.05161310078748389 | validation: 0.07029468654042702]
	TIME [epoch: 12.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05313048299810518		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.05313048299810518 | validation: 0.042411711026076396]
	TIME [epoch: 12.3 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05554598191144061		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.05554598191144061 | validation: 0.07350686781224627]
	TIME [epoch: 12.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050197192252318654		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.050197192252318654 | validation: 0.2602617626629239]
	TIME [epoch: 12.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1058144573654199		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.1058144573654199 | validation: 0.0699291724418379]
	TIME [epoch: 12.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05749023997669962		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.05749023997669962 | validation: 0.04081155163591497]
	TIME [epoch: 12.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_922.pth
	Model improved!!!
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056719141084456394		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.056719141084456394 | validation: 0.04636112959464608]
	TIME [epoch: 12.1 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049258800309698494		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.049258800309698494 | validation: 0.0637506823166915]
	TIME [epoch: 12.1 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051486996748425946		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.051486996748425946 | validation: 0.05216093316086541]
	TIME [epoch: 12.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04887672181177043		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.04887672181177043 | validation: 0.06471304872160348]
	TIME [epoch: 12.2 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05090224096112368		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.05090224096112368 | validation: 0.042686923408829486]
	TIME [epoch: 12.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05127384830841789		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.05127384830841789 | validation: 0.045528019914529955]
	TIME [epoch: 12.2 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04846862378032763		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.04846862378032763 | validation: 0.04929228693315938]
	TIME [epoch: 12.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05170698921240198		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.05170698921240198 | validation: 0.05060755759913495]
	TIME [epoch: 12.2 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048237355961585744		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.048237355961585744 | validation: 0.05264198507949609]
	TIME [epoch: 12.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04938013443566229		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.04938013443566229 | validation: 0.07107712022000472]
	TIME [epoch: 12.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050876682120727		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.050876682120727 | validation: 0.04934335974083562]
	TIME [epoch: 12.2 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049010760881198864		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.049010760881198864 | validation: 0.06670226222935613]
	TIME [epoch: 12.1 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052890580726220666		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.052890580726220666 | validation: 0.042642868062581385]
	TIME [epoch: 12.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053841015053067764		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.053841015053067764 | validation: 0.05075481243580998]
	TIME [epoch: 12.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049726011731916736		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.049726011731916736 | validation: 0.05331763574028531]
	TIME [epoch: 12.3 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07144407683740711		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.07144407683740711 | validation: 0.052561786605090116]
	TIME [epoch: 12.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058391962530474		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.058391962530474 | validation: 0.05337597011263298]
	TIME [epoch: 12.3 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04832513561215288		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.04832513561215288 | validation: 0.24839906625664598]
	TIME [epoch: 12.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14775064515167471		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.14775064515167471 | validation: 0.20202477185491313]
	TIME [epoch: 12.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382759627387299		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.1382759627387299 | validation: 0.07507052134435849]
	TIME [epoch: 12.1 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07230441978024421		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.07230441978024421 | validation: 0.04462805035286347]
	TIME [epoch: 12.1 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986346155064415		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.05986346155064415 | validation: 0.04172913510028177]
	TIME [epoch: 12.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055707242141352316		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.055707242141352316 | validation: 0.051803166836785874]
	TIME [epoch: 12.3 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05311436614857384		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.05311436614857384 | validation: 0.0881272032386743]
	TIME [epoch: 12.2 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05154349095207584		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.05154349095207584 | validation: 0.07133704603215393]
	TIME [epoch: 12.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049920462088427354		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.049920462088427354 | validation: 0.06306008725175337]
	TIME [epoch: 12.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04928075259512496		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.04928075259512496 | validation: 0.05500377546703074]
	TIME [epoch: 12.2 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973641398338054		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.04973641398338054 | validation: 0.05369958358233447]
	TIME [epoch: 12.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04703180402412076		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.04703180402412076 | validation: 0.054645217704568844]
	TIME [epoch: 12.2 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04818669162684932		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.04818669162684932 | validation: 0.05540572901738986]
	TIME [epoch: 12.2 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04735222202842135		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.04735222202842135 | validation: 0.056302952843618484]
	TIME [epoch: 12.1 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04681118014938289		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.04681118014938289 | validation: 0.05389164684745136]
	TIME [epoch: 12.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048537337523498696		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.048537337523498696 | validation: 0.04750819203500332]
	TIME [epoch: 12.1 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048914636796738686		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.048914636796738686 | validation: 0.046968065742964166]
	TIME [epoch: 12.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047575018709083476		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.047575018709083476 | validation: 0.052241952765549854]
	TIME [epoch: 12.2 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048217219845750295		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.048217219845750295 | validation: 0.05217819069781082]
	TIME [epoch: 12.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765635088976745		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.04765635088976745 | validation: 0.06533076951584642]
	TIME [epoch: 12.2 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04828310556722114		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.04828310556722114 | validation: 0.0542074034269153]
	TIME [epoch: 12.2 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04688742288672725		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.04688742288672725 | validation: 0.043506982093296356]
	TIME [epoch: 12.1 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050613834843643256		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.050613834843643256 | validation: 0.046642251104304436]
	TIME [epoch: 12.2 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04985004021651778		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.04985004021651778 | validation: 0.06588902255230528]
	TIME [epoch: 12.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04838325032349587		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.04838325032349587 | validation: 0.06159968556758393]
	TIME [epoch: 12.2 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049000671945753		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.049000671945753 | validation: 0.046385964076698996]
	TIME [epoch: 12.1 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04991310896347515		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.04991310896347515 | validation: 0.04693931335525883]
	TIME [epoch: 12.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04830728863880841		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.04830728863880841 | validation: 0.059185940102428974]
	TIME [epoch: 12.2 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047300132903773245		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.047300132903773245 | validation: 0.06816114911247022]
	TIME [epoch: 12.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048716208097697625		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.048716208097697625 | validation: 0.04972054845553689]
	TIME [epoch: 12.2 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045848135085960494		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.045848135085960494 | validation: 0.050403580769555346]
	TIME [epoch: 12.1 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04950862994296825		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.04950862994296825 | validation: 0.04384194963666388]
	TIME [epoch: 12.2 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04871790198227892		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.04871790198227892 | validation: 0.05729606892090854]
	TIME [epoch: 12.2 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051427593141808635		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.051427593141808635 | validation: 0.07269834488508162]
	TIME [epoch: 12.2 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05072769300374625		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.05072769300374625 | validation: 0.048549937031332126]
	TIME [epoch: 12.2 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486449630086551		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.0486449630086551 | validation: 0.049934824445907945]
	TIME [epoch: 12.2 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047632798870916544		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.047632798870916544 | validation: 0.04483427583200028]
	TIME [epoch: 12.2 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05015168371082142		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.05015168371082142 | validation: 0.05177785369703597]
	TIME [epoch: 12.2 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641892881602473		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.04641892881602473 | validation: 0.04966638079313468]
	TIME [epoch: 12.2 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752787207142153		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.04752787207142153 | validation: 0.06304167962944214]
	TIME [epoch: 12.2 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0500897447198972		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.0500897447198972 | validation: 0.04721738016584501]
	TIME [epoch: 12.2 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054024057195902045		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.054024057195902045 | validation: 0.05255795379165787]
	TIME [epoch: 12.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048459615868656644		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.048459615868656644 | validation: 0.07672183387919672]
	TIME [epoch: 12.2 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107589676444329		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.05107589676444329 | validation: 0.055650921082798246]
	TIME [epoch: 12.2 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054335323492858634		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.054335323492858634 | validation: 0.04508792237798951]
	TIME [epoch: 12.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04864051406232717		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.04864051406232717 | validation: 0.057878480969818596]
	TIME [epoch: 12.2 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047578992623712006		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.047578992623712006 | validation: 0.056060084793179366]
	TIME [epoch: 12.2 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047035312888816384		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.047035312888816384 | validation: 0.0484252167959792]
	TIME [epoch: 12.2 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641630030304301		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.04641630030304301 | validation: 0.04411517287971837]
	TIME [epoch: 12.2 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048317397851789165		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.048317397851789165 | validation: 0.07006824069435293]
	TIME [epoch: 12.2 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410592191785657		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.05410592191785657 | validation: 0.040200163478259714]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_990.pth
	Model improved!!!
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035419454309173		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.05035419454309173 | validation: 0.05037408034381374]
	TIME [epoch: 12.2 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04897100998539799		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.04897100998539799 | validation: 0.05078461445314791]
	TIME [epoch: 12.2 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04801467627087069		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.04801467627087069 | validation: 0.044562810328992036]
	TIME [epoch: 12.2 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05086176165786042		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.05086176165786042 | validation: 0.041290624546283086]
	TIME [epoch: 12.2 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053682144345610965		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.053682144345610965 | validation: 0.0644553117620654]
	TIME [epoch: 12.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050227180602405705		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.050227180602405705 | validation: 0.05263707964988793]
	TIME [epoch: 12.2 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050603131532540944		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.050603131532540944 | validation: 0.05630019462939937]
	TIME [epoch: 12.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047178818634877794		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.047178818634877794 | validation: 0.05379921528772233]
	TIME [epoch: 12.2 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04752366342659348		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.04752366342659348 | validation: 0.04057292548578535]
	TIME [epoch: 12.2 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04714202788717449		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.04714202788717449 | validation: 0.06482593039033226]
	TIME [epoch: 12.2 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04950872296607434		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.04950872296607434 | validation: 0.04810756013857497]
	TIME [epoch: 191 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05262881026957289		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.05262881026957289 | validation: 0.06346385830929298]
	TIME [epoch: 25.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000220019414396		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.05000220019414396 | validation: 0.06466417241284517]
	TIME [epoch: 25.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05748131019646833		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.05748131019646833 | validation: 0.044758573726436636]
	TIME [epoch: 25.2 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04740837598967179		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.04740837598967179 | validation: 0.05757641703490806]
	TIME [epoch: 25.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049247843282459236		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.049247843282459236 | validation: 0.04532683422442449]
	TIME [epoch: 25.3 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945350065881849		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.04945350065881849 | validation: 0.06561600013787097]
	TIME [epoch: 25.3 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055129223969614095		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.055129223969614095 | validation: 0.11227230969142275]
	TIME [epoch: 25.3 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06125081573345201		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.06125081573345201 | validation: 0.0459303323292464]
	TIME [epoch: 25.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757971962859758		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.04757971962859758 | validation: 0.03916448934173661]
	TIME [epoch: 25.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035749358727252		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.05035749358727252 | validation: 0.04471549015241514]
	TIME [epoch: 25.3 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04817594554697726		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.04817594554697726 | validation: 0.059874821499875476]
	TIME [epoch: 25.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048660453305990635		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.048660453305990635 | validation: 0.05016457807901094]
	TIME [epoch: 25.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04610069261976905		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.04610069261976905 | validation: 0.041315427219144524]
	TIME [epoch: 25.2 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046980067982150375		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.046980067982150375 | validation: 0.0396334664446799]
	TIME [epoch: 25.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04869483328200981		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.04869483328200981 | validation: 0.05478935550843611]
	TIME [epoch: 25.3 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047543422094874235		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.047543422094874235 | validation: 0.06047067475319572]
	TIME [epoch: 25.3 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049577149124210004		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.049577149124210004 | validation: 0.062249462296375305]
	TIME [epoch: 25.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045073070790323556		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.045073070790323556 | validation: 0.05907460098232597]
	TIME [epoch: 25.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04884056838606433		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.04884056838606433 | validation: 0.04764611545318858]
	TIME [epoch: 25.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05228587864514255		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.05228587864514255 | validation: 0.05715385548417465]
	TIME [epoch: 25.1 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835302948306433		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.0835302948306433 | validation: 0.05123879245950588]
	TIME [epoch: 25.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06296525562442132		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.06296525562442132 | validation: 0.12260279226067304]
	TIME [epoch: 25.1 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06793017651668808		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.06793017651668808 | validation: 0.18110160493032207]
	TIME [epoch: 25.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0832516527215132		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.0832516527215132 | validation: 0.17685300268711424]
	TIME [epoch: 25.1 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08377369125348963		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.08377369125348963 | validation: 0.08295615901778859]
	TIME [epoch: 25.2 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0604338676021318		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.0604338676021318 | validation: 0.0612907327760947]
	TIME [epoch: 25.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053959465855502986		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.053959465855502986 | validation: 0.05652543073585851]
	TIME [epoch: 25.2 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930976119039312		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.04930976119039312 | validation: 0.05259794874214564]
	TIME [epoch: 25.2 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04859927410258239		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.04859927410258239 | validation: 0.05364956542941113]
	TIME [epoch: 25.2 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048162991024333246		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.048162991024333246 | validation: 0.04927860732067222]
	TIME [epoch: 25.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638915542146959		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.04638915542146959 | validation: 0.04887602872999383]
	TIME [epoch: 25.2 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04699261514374947		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.04699261514374947 | validation: 0.0501574089375296]
	TIME [epoch: 25.1 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04738662482611464		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.04738662482611464 | validation: 0.04822283455698534]
	TIME [epoch: 25.2 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04544724118779363		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.04544724118779363 | validation: 0.054779880028406384]
	TIME [epoch: 25.1 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04590192244962747		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.04590192244962747 | validation: 0.048401511163627335]
	TIME [epoch: 25.2 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048913871524614086		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.048913871524614086 | validation: 0.05021042439589133]
	TIME [epoch: 25.1 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047138676408698156		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.047138676408698156 | validation: 0.050320566953017]
	TIME [epoch: 25.2 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04801429241213791		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.04801429241213791 | validation: 0.043850167884185644]
	TIME [epoch: 25.2 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726114376839043		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.04726114376839043 | validation: 0.041628179897679696]
	TIME [epoch: 25.2 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048728767215876094		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.048728767215876094 | validation: 0.05012126064128645]
	TIME [epoch: 25.2 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04525282634760685		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.04525282634760685 | validation: 0.05262992468527176]
	TIME [epoch: 25.2 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045867310236488594		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.045867310236488594 | validation: 0.05472648467240587]
	TIME [epoch: 25.1 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04803803837957554		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.04803803837957554 | validation: 0.048385079433584756]
	TIME [epoch: 25.2 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046599401848034974		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.046599401848034974 | validation: 0.05105987156062151]
	TIME [epoch: 25 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04731328427665704		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.04731328427665704 | validation: 0.052245276247734035]
	TIME [epoch: 25.2 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837097058305572		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.04837097058305572 | validation: 0.04943173508688629]
	TIME [epoch: 25.1 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732587281727513		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.04732587281727513 | validation: 0.0532068287420638]
	TIME [epoch: 25.2 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475095981497372		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.04475095981497372 | validation: 0.042588617624890246]
	TIME [epoch: 25.1 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04707983133924092		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.04707983133924092 | validation: 0.04061795604042674]
	TIME [epoch: 25.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676764309463466		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.04676764309463466 | validation: 0.055683764180769514]
	TIME [epoch: 25.1 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047649803182078704		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.047649803182078704 | validation: 0.052049195437065945]
	TIME [epoch: 25.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760739884018771		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.04760739884018771 | validation: 0.04790655731772388]
	TIME [epoch: 25.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04858668146837793		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.04858668146837793 | validation: 0.06396571999028171]
	TIME [epoch: 25.3 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048200046697656984		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.048200046697656984 | validation: 0.040036721407998105]
	TIME [epoch: 25.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04573732497344107		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.04573732497344107 | validation: 0.04023738771217821]
	TIME [epoch: 25.3 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04670187140452585		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.04670187140452585 | validation: 0.04958864492276526]
	TIME [epoch: 25.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04862931894088149		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.04862931894088149 | validation: 0.052781773919878205]
	TIME [epoch: 25.3 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660705798063516		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.04660705798063516 | validation: 0.04402244444564182]
	TIME [epoch: 25.1 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04666036961037007		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.04666036961037007 | validation: 0.0542609283615133]
	TIME [epoch: 25.2 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04628381170240488		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.04628381170240488 | validation: 0.049040081354029555]
	TIME [epoch: 25.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04636986354998774		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.04636986354998774 | validation: 0.05499922708123905]
	TIME [epoch: 25.1 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945830734544983		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.04945830734544983 | validation: 0.04823264443773378]
	TIME [epoch: 25.3 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05033310118520483		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.05033310118520483 | validation: 0.06003327252507515]
	TIME [epoch: 25.3 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04872829032270458		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.04872829032270458 | validation: 0.05418285135995107]
	TIME [epoch: 25.2 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04654190051414373		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.04654190051414373 | validation: 0.07103686574977024]
	TIME [epoch: 25.3 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04840948384902658		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.04840948384902658 | validation: 0.04152837124461233]
	TIME [epoch: 25 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048529956156305794		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.048529956156305794 | validation: 0.07133488524994701]
	TIME [epoch: 25.2 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06285815146468753		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.06285815146468753 | validation: 0.05934222552595948]
	TIME [epoch: 25.1 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055892996326921025		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.055892996326921025 | validation: 0.04787353248939411]
	TIME [epoch: 25.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692313137275166		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.04692313137275166 | validation: 0.04902144888981759]
	TIME [epoch: 25.1 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0513280450100108		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.0513280450100108 | validation: 0.0591487074489708]
	TIME [epoch: 25.3 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048017064216986344		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.048017064216986344 | validation: 0.059545868350330905]
	TIME [epoch: 25.1 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930679742992983		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.04930679742992983 | validation: 0.0461319033046618]
	TIME [epoch: 25.3 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04566491596251424		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.04566491596251424 | validation: 0.0488415846101558]
	TIME [epoch: 25 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04465246874045672		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.04465246874045672 | validation: 0.04757935176637956]
	TIME [epoch: 25.3 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04535070362656434		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.04535070362656434 | validation: 0.04928359531628525]
	TIME [epoch: 25 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04618965133541229		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.04618965133541229 | validation: 0.04507362374279153]
	TIME [epoch: 25.1 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436714445049171		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.04436714445049171 | validation: 0.03876666083025391]
	TIME [epoch: 25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_1079.pth
	Model improved!!!
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551613347388424		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.04551613347388424 | validation: 0.05190176963064096]
	TIME [epoch: 25.2 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04609509653356119		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.04609509653356119 | validation: 0.06197910504683172]
	TIME [epoch: 25.2 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04573028399777005		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.04573028399777005 | validation: 0.05314577141807474]
	TIME [epoch: 25.2 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04560336275571555		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.04560336275571555 | validation: 0.04111010153332594]
	TIME [epoch: 25.2 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045326196156810696		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.045326196156810696 | validation: 0.043852035088931596]
	TIME [epoch: 25.2 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532063882998326		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.0532063882998326 | validation: 0.050075195977661316]
	TIME [epoch: 25.2 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734456347367386		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.04734456347367386 | validation: 0.0442943186614028]
	TIME [epoch: 25.2 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615929097291326		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.04615929097291326 | validation: 0.07057686279088259]
	TIME [epoch: 25 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048344320931773294		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.048344320931773294 | validation: 0.06338644261915401]
	TIME [epoch: 25.2 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726074413267688		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.04726074413267688 | validation: 0.04690802170433163]
	TIME [epoch: 25.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04899655022167874		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.04899655022167874 | validation: 0.0523547183238527]
	TIME [epoch: 25.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046876859485414875		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.046876859485414875 | validation: 0.04523098828819705]
	TIME [epoch: 25.2 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04694351285753756		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.04694351285753756 | validation: 0.044609697588471414]
	TIME [epoch: 25.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581234812103686		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.04581234812103686 | validation: 0.08257101334013414]
	TIME [epoch: 25.2 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053488635041939625		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.053488635041939625 | validation: 0.048044643872360175]
	TIME [epoch: 25.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04806585871048137		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.04806585871048137 | validation: 0.04489766180010282]
	TIME [epoch: 25.2 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047454407740586		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.047454407740586 | validation: 0.054694722257697365]
	TIME [epoch: 25.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04708443086827763		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.04708443086827763 | validation: 0.048951661742584585]
	TIME [epoch: 25.1 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045793076533589785		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.045793076533589785 | validation: 0.04688635655533372]
	TIME [epoch: 25.2 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04371600175487858		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.04371600175487858 | validation: 0.05702189557579625]
	TIME [epoch: 25 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04640499724263622		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.04640499724263622 | validation: 0.04929438461318354]
	TIME [epoch: 25.2 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04522781445988223		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.04522781445988223 | validation: 0.05418633423928876]
	TIME [epoch: 25 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588213347942986		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.04588213347942986 | validation: 0.05369259568757944]
	TIME [epoch: 25.2 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616592854934348		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.04616592854934348 | validation: 0.05349536260340398]
	TIME [epoch: 25.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046654175991231554		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.046654175991231554 | validation: 0.04612035457608671]
	TIME [epoch: 25.1 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921454806322651		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.04921454806322651 | validation: 0.056572956033531435]
	TIME [epoch: 25.3 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04654178017139398		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.04654178017139398 | validation: 0.04499480202228115]
	TIME [epoch: 25 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04821099064907413		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.04821099064907413 | validation: 0.04684383463419343]
	TIME [epoch: 25.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04529202150736226		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.04529202150736226 | validation: 0.05039945887227211]
	TIME [epoch: 25.1 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047002211479329335		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.047002211479329335 | validation: 0.05190809958169628]
	TIME [epoch: 25.3 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04583361581478504		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.04583361581478504 | validation: 0.043560198107475016]
	TIME [epoch: 25.2 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0461675480817426		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.0461675480817426 | validation: 0.04091614856424628]
	TIME [epoch: 25.3 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04855276567606945		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.04855276567606945 | validation: 0.0430504480233797]
	TIME [epoch: 25.2 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045388804987217216		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.045388804987217216 | validation: 0.05613910916084297]
	TIME [epoch: 25.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04582627520835482		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.04582627520835482 | validation: 0.04240405092875169]
	TIME [epoch: 25.1 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04821219806598055		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.04821219806598055 | validation: 0.048927753718895496]
	TIME [epoch: 25.2 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04682865805393604		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.04682865805393604 | validation: 0.04288606596170087]
	TIME [epoch: 25.2 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046778376059963905		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.046778376059963905 | validation: 0.049163492799745225]
	TIME [epoch: 25.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046284915346733825		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.046284915346733825 | validation: 0.0419473814023681]
	TIME [epoch: 25.1 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046983856505014306		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.046983856505014306 | validation: 0.04143769438448785]
	TIME [epoch: 25.2 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497495562164833		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.04497495562164833 | validation: 0.12054206165762103]
	TIME [epoch: 25.1 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06374250023867989		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.06374250023867989 | validation: 0.15012094959506964]
	TIME [epoch: 25.3 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07516314176903754		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.07516314176903754 | validation: 0.11058643432749296]
	TIME [epoch: 25 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06045066387171218		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.06045066387171218 | validation: 0.061749515251175086]
	TIME [epoch: 25.2 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04973217225488382		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.04973217225488382 | validation: 0.05031527085672547]
	TIME [epoch: 25.1 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06035323454455661		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.06035323454455661 | validation: 0.07414409352565711]
	TIME [epoch: 25.3 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883637365218965		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.0883637365218965 | validation: 0.0718812053876821]
	TIME [epoch: 25.2 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005027843563457		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.08005027843563457 | validation: 0.049132138481520475]
	TIME [epoch: 25.3 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06361136625868044		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.06361136625868044 | validation: 0.047550044003244744]
	TIME [epoch: 25.1 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05617270165917244		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.05617270165917244 | validation: 0.0455300049194516]
	TIME [epoch: 25.2 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04656934125160743		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.04656934125160743 | validation: 0.052642582609967285]
	TIME [epoch: 25.3 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04385831193347025		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.04385831193347025 | validation: 0.059361042536864396]
	TIME [epoch: 25.3 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04554494155243773		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.04554494155243773 | validation: 0.05060572648330335]
	TIME [epoch: 25.3 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453070258522415		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.0453070258522415 | validation: 0.05959037769955203]
	TIME [epoch: 25.3 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678392549597634		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.04678392549597634 | validation: 0.05388745779185276]
	TIME [epoch: 25.2 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532533042683634		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.04532533042683634 | validation: 0.04768364110902266]
	TIME [epoch: 25.3 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416935840702473		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.04416935840702473 | validation: 0.05721943556664343]
	TIME [epoch: 25.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04638373111606515		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.04638373111606515 | validation: 0.04566792483146335]
	TIME [epoch: 25.2 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045010526470403135		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.045010526470403135 | validation: 0.04165718041443342]
	TIME [epoch: 25.2 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432669940426969		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.04432669940426969 | validation: 0.04372668878292301]
	TIME [epoch: 25.3 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044563393140468345		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.044563393140468345 | validation: 0.04605034895421439]
	TIME [epoch: 25.2 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474442188630416		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.04474442188630416 | validation: 0.04474857856792698]
	TIME [epoch: 25.2 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04546435484423343		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.04546435484423343 | validation: 0.04408984318110681]
	TIME [epoch: 25.1 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04392144892037992		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.04392144892037992 | validation: 0.048138120569212864]
	TIME [epoch: 25.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04427512208778115		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.04427512208778115 | validation: 0.06475317870975471]
	TIME [epoch: 25.2 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04829138789953388		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.04829138789953388 | validation: 0.046733352618363526]
	TIME [epoch: 25.3 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455412751493861		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.0455412751493861 | validation: 0.0453027912186452]
	TIME [epoch: 25.1 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05856231980554432		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.05856231980554432 | validation: 0.041795195484511306]
	TIME [epoch: 25.2 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05248686392365621		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.05248686392365621 | validation: 0.05516523918243232]
	TIME [epoch: 25.1 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0482543267262201		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.0482543267262201 | validation: 0.05512659874204953]
	TIME [epoch: 25.3 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04561491048617558		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.04561491048617558 | validation: 0.04708421243047133]
	TIME [epoch: 25.1 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04724849473931606		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.04724849473931606 | validation: 0.048671358748217736]
	TIME [epoch: 25.3 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04634864076386682		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.04634864076386682 | validation: 0.05439806932236107]
	TIME [epoch: 25.2 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04604555306519252		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.04604555306519252 | validation: 0.05617584081657089]
	TIME [epoch: 25.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04910845310964389		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.04910845310964389 | validation: 0.0513145197469671]
	TIME [epoch: 25.1 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04678557111706306		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.04678557111706306 | validation: 0.04474974393493863]
	TIME [epoch: 25.2 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04849827521212174		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.04849827521212174 | validation: 0.043928347053287825]
	TIME [epoch: 25.1 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04826367258056716		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.04826367258056716 | validation: 0.047795518364839654]
	TIME [epoch: 25.2 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04421503907486435		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.04421503907486435 | validation: 0.04135997304073228]
	TIME [epoch: 24.9 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04648999402997408		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.04648999402997408 | validation: 0.04686174172444403]
	TIME [epoch: 25.2 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044506732786383355		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.044506732786383355 | validation: 0.04978607351442721]
	TIME [epoch: 25 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045994364628239845		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.045994364628239845 | validation: 0.06043813530594893]
	TIME [epoch: 25.2 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672296183222799		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.04672296183222799 | validation: 0.05421336450987313]
	TIME [epoch: 25 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044309020207630445		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.044309020207630445 | validation: 0.0441972461955881]
	TIME [epoch: 25.2 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04442703705652312		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.04442703705652312 | validation: 0.04822220013664211]
	TIME [epoch: 25 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045826621174050806		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.045826621174050806 | validation: 0.04522898414570902]
	TIME [epoch: 25.2 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045944653286591336		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.045944653286591336 | validation: 0.05944575288616232]
	TIME [epoch: 25.1 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048903208444506394		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.048903208444506394 | validation: 0.0563471175195358]
	TIME [epoch: 25.2 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04823577183975015		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.04823577183975015 | validation: 0.041178790987364426]
	TIME [epoch: 25.1 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046449690953156424		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.046449690953156424 | validation: 0.03892979381676598]
	TIME [epoch: 25.2 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045519878519424635		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.045519878519424635 | validation: 0.05617112586864197]
	TIME [epoch: 25.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04555928128265306		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.04555928128265306 | validation: 0.06207254309188268]
	TIME [epoch: 25.2 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04819842442063871		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.04819842442063871 | validation: 0.12993219912087767]
	TIME [epoch: 24.9 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294008935442534		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.06294008935442534 | validation: 0.06759245452393063]
	TIME [epoch: 25.1 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048062030244348505		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.048062030244348505 | validation: 0.04545738976669712]
	TIME [epoch: 25.1 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048153788802963914		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.048153788802963914 | validation: 0.04323668379349961]
	TIME [epoch: 25.1 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046677034976209855		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.046677034976209855 | validation: 0.04133449537638219]
	TIME [epoch: 25 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044391572150018224		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.044391572150018224 | validation: 0.04839043018885284]
	TIME [epoch: 25.2 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045377286535892694		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.045377286535892694 | validation: 0.03969057113996523]
	TIME [epoch: 25.1 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043993797601710814		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.043993797601710814 | validation: 0.051196295734888236]
	TIME [epoch: 25.2 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0455440182789076		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.0455440182789076 | validation: 0.06202750179177431]
	TIME [epoch: 25.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132639/states/model_phi1_4c_v_mmd1_1180.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 13699.682 seconds.
