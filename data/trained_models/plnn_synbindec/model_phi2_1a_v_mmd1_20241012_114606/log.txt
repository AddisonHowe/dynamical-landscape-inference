Args:
Namespace(name='model_phi2_1a_v_mmd1', outdir='out/model_training/model_phi2_1a_v_mmd1', training_data='data/training_data/basic/data_phi2_1a/training', validation_data='data/training_data/basic/data_phi2_1a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 360024189

Training model...

Saving initial model state to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.37971212335437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.37971212335437 | validation: 3.6031554170751208]
	TIME [epoch: 107 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.1384269608782978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1384269608782978 | validation: 4.62450294619928]
	TIME [epoch: 13.3 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 4/4] avg loss: 3.0492698965925236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0492698965925236 | validation: 3.246271156971476]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.5608515244598244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5608515244598244 | validation: 2.760322451284237]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 4/4] avg loss: 2.155009950127847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.155009950127847 | validation: 1.3081219928399044]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.366665612056442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.366665612056442 | validation: 2.428972436628049]
	TIME [epoch: 13.2 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 4/4] avg loss: 1.2379796304737076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2379796304737076 | validation: 1.1068457182271725]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6894815056845374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6894815056845374 | validation: 0.802472226329916]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.6264057682826759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6264057682826759 | validation: 0.3742328851822295]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.567324112703754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.567324112703754 | validation: 0.5610533599134534]
	TIME [epoch: 13.2 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5174057803227065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5174057803227065 | validation: 0.4498894507810754]
	TIME [epoch: 13.2 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5226221681643135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5226221681643135 | validation: 0.4119590114192564]
	TIME [epoch: 13.2 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.47129878162503347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47129878162503347 | validation: 0.49037646762025766]
	TIME [epoch: 13.2 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4934755376879717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4934755376879717 | validation: 0.37843469690001386]
	TIME [epoch: 13.2 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5147601298618496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5147601298618496 | validation: 0.4721692134586204]
	TIME [epoch: 13.2 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41852696594818833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41852696594818833 | validation: 0.5170851270602019]
	TIME [epoch: 13.2 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4978510426873314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4978510426873314 | validation: 0.3985618277881238]
	TIME [epoch: 13.2 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4327621576595683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4327621576595683 | validation: 0.38073487357429725]
	TIME [epoch: 13.2 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.46785939459943926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46785939459943926 | validation: 0.5538310388447482]
	TIME [epoch: 13.2 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.45481816383800183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45481816383800183 | validation: 0.3280213005519257]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4829195651750744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4829195651750744 | validation: 0.48986555667516035]
	TIME [epoch: 13.2 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42289975209810843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42289975209810843 | validation: 0.3004776248074799]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3805199083779851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3805199083779851 | validation: 0.6337836772097891]
	TIME [epoch: 13.2 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.5016791929658122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5016791929658122 | validation: 0.3937863513594634]
	TIME [epoch: 13.2 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.40342721579837443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40342721579837443 | validation: 0.4469260742164057]
	TIME [epoch: 13.2 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.43908725890213135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43908725890213135 | validation: 0.39554485579174337]
	TIME [epoch: 13.2 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42439316902963864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42439316902963864 | validation: 0.31382095438732077]
	TIME [epoch: 13.2 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.367924525751527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.367924525751527 | validation: 0.39795142787722815]
	TIME [epoch: 13.2 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.42756922167241906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42756922167241906 | validation: 0.2915746809012226]
	TIME [epoch: 13.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4241426055919116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4241426055919116 | validation: 0.41713514549621933]
	TIME [epoch: 13.2 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3551889408262107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3551889408262107 | validation: 0.39319287809242565]
	TIME [epoch: 13.2 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3937158767747364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3937158767747364 | validation: 0.36299697828588817]
	TIME [epoch: 13.2 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3918919180593403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3918919180593403 | validation: 0.3148892941464739]
	TIME [epoch: 13.2 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36016117389330066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36016117389330066 | validation: 0.48833795097710125]
	TIME [epoch: 13.2 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3859151438820798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3859151438820798 | validation: 0.33277380013919156]
	TIME [epoch: 13.2 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3623510477825187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3623510477825187 | validation: 0.4131976723245854]
	TIME [epoch: 13.2 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.41497410972306603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41497410972306603 | validation: 0.26151696041752254]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4002099423998205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4002099423998205 | validation: 0.2683951890568892]
	TIME [epoch: 13.2 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.4066035573854284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4066035573854284 | validation: 0.4449734515275816]
	TIME [epoch: 13.2 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35850905614445994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35850905614445994 | validation: 0.2874820386877185]
	TIME [epoch: 13.2 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38223640951008225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38223640951008225 | validation: 0.29233878902955723]
	TIME [epoch: 13.2 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3646323567575219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3646323567575219 | validation: 0.2891199735966818]
	TIME [epoch: 13.2 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3659389455330603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3659389455330603 | validation: 0.3840643109757025]
	TIME [epoch: 13.2 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3480317963000004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3480317963000004 | validation: 0.29107785359839067]
	TIME [epoch: 13.1 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36990927484819225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36990927484819225 | validation: 0.39224667794201595]
	TIME [epoch: 13.1 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3744883954860938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3744883954860938 | validation: 0.3307345131177982]
	TIME [epoch: 13.2 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.35752245340955036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35752245340955036 | validation: 0.33515885652766986]
	TIME [epoch: 13.2 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.38664892864794265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38664892864794265 | validation: 0.32712890556739105]
	TIME [epoch: 13.2 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32818529337625896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32818529337625896 | validation: 0.33123730322562106]
	TIME [epoch: 13.2 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.39394627723448267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39394627723448267 | validation: 0.3226325943564525]
	TIME [epoch: 13.2 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3729848861180543		[learning rate: 0.0099735]
	Learning Rate: 0.00997347
	LOSS [training: 0.3729848861180543 | validation: 0.3252910467428047]
	TIME [epoch: 13.2 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32055881504959216		[learning rate: 0.0099382]
	Learning Rate: 0.0099382
	LOSS [training: 0.32055881504959216 | validation: 0.2984206964096322]
	TIME [epoch: 13.2 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3889430949673911		[learning rate: 0.0099031]
	Learning Rate: 0.00990306
	LOSS [training: 0.3889430949673911 | validation: 0.3664532585153112]
	TIME [epoch: 13.2 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.32286367034553515		[learning rate: 0.009868]
	Learning Rate: 0.00986804
	LOSS [training: 0.32286367034553515 | validation: 0.3836335780500809]
	TIME [epoch: 13.2 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3745340895553796		[learning rate: 0.0098331]
	Learning Rate: 0.00983314
	LOSS [training: 0.3745340895553796 | validation: 0.2608384798531818]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3675828175366696		[learning rate: 0.0097984]
	Learning Rate: 0.00979837
	LOSS [training: 0.3675828175366696 | validation: 0.2285804902918342]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30205606481266295		[learning rate: 0.0097637]
	Learning Rate: 0.00976372
	LOSS [training: 0.30205606481266295 | validation: 0.3238943984472629]
	TIME [epoch: 13.2 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.323001786002793		[learning rate: 0.0097292]
	Learning Rate: 0.0097292
	LOSS [training: 0.323001786002793 | validation: 0.3935334854334439]
	TIME [epoch: 13.2 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3524015866363083		[learning rate: 0.0096948]
	Learning Rate: 0.00969479
	LOSS [training: 0.3524015866363083 | validation: 0.32564322422740566]
	TIME [epoch: 13.2 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.36161630951092916		[learning rate: 0.0096605]
	Learning Rate: 0.00966051
	LOSS [training: 0.36161630951092916 | validation: 0.26611436962234064]
	TIME [epoch: 13.2 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2855433649381873		[learning rate: 0.0096263]
	Learning Rate: 0.00962635
	LOSS [training: 0.2855433649381873 | validation: 0.2618117115361333]
	TIME [epoch: 13.2 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3581716672847324		[learning rate: 0.0095923]
	Learning Rate: 0.00959231
	LOSS [training: 0.3581716672847324 | validation: 0.25108780275292597]
	TIME [epoch: 13.2 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.306584991402486		[learning rate: 0.0095584]
	Learning Rate: 0.00955839
	LOSS [training: 0.306584991402486 | validation: 0.28561603835533195]
	TIME [epoch: 13.2 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3285964932470773		[learning rate: 0.0095246]
	Learning Rate: 0.00952459
	LOSS [training: 0.3285964932470773 | validation: 0.22167285071884307]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31270870202286166		[learning rate: 0.0094909]
	Learning Rate: 0.00949091
	LOSS [training: 0.31270870202286166 | validation: 0.3534046754788992]
	TIME [epoch: 13.2 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.33536921171806733		[learning rate: 0.0094573]
	Learning Rate: 0.00945734
	LOSS [training: 0.33536921171806733 | validation: 0.2287497353412227]
	TIME [epoch: 13.2 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3194281694480167		[learning rate: 0.0094239]
	Learning Rate: 0.0094239
	LOSS [training: 0.3194281694480167 | validation: 0.26695005688212387]
	TIME [epoch: 13.2 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3188855815403183		[learning rate: 0.0093906]
	Learning Rate: 0.00939058
	LOSS [training: 0.3188855815403183 | validation: 0.24367324754542058]
	TIME [epoch: 13.2 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28749730095489207		[learning rate: 0.0093574]
	Learning Rate: 0.00935737
	LOSS [training: 0.28749730095489207 | validation: 0.22245472731411708]
	TIME [epoch: 13.2 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3491685788334055		[learning rate: 0.0093243]
	Learning Rate: 0.00932428
	LOSS [training: 0.3491685788334055 | validation: 0.21240280252915084]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29443607472350564		[learning rate: 0.0092913]
	Learning Rate: 0.00929131
	LOSS [training: 0.29443607472350564 | validation: 0.34481868793039117]
	TIME [epoch: 13.2 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3121481618365597		[learning rate: 0.0092585]
	Learning Rate: 0.00925845
	LOSS [training: 0.3121481618365597 | validation: 0.2556958708339116]
	TIME [epoch: 13.2 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28242261803263247		[learning rate: 0.0092257]
	Learning Rate: 0.00922571
	LOSS [training: 0.28242261803263247 | validation: 0.24721424055087415]
	TIME [epoch: 13.2 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.342290699710974		[learning rate: 0.0091931]
	Learning Rate: 0.00919309
	LOSS [training: 0.342290699710974 | validation: 0.2367553619635771]
	TIME [epoch: 13.2 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3083532735225315		[learning rate: 0.0091606]
	Learning Rate: 0.00916058
	LOSS [training: 0.3083532735225315 | validation: 0.21062088285688893]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2816214150193197		[learning rate: 0.0091282]
	Learning Rate: 0.00912819
	LOSS [training: 0.2816214150193197 | validation: 0.2572259827871055]
	TIME [epoch: 13.2 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3152457853868247		[learning rate: 0.0090959]
	Learning Rate: 0.00909591
	LOSS [training: 0.3152457853868247 | validation: 0.20788508415767712]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2674849786349519		[learning rate: 0.0090637]
	Learning Rate: 0.00906374
	LOSS [training: 0.2674849786349519 | validation: 0.24095883712201582]
	TIME [epoch: 13.2 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.30991033591747086		[learning rate: 0.0090317]
	Learning Rate: 0.00903169
	LOSS [training: 0.30991033591747086 | validation: 0.22154391593412542]
	TIME [epoch: 13.2 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26463538253785995		[learning rate: 0.0089998]
	Learning Rate: 0.00899976
	LOSS [training: 0.26463538253785995 | validation: 0.2458875768883132]
	TIME [epoch: 13.2 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.31245534703065675		[learning rate: 0.0089679]
	Learning Rate: 0.00896793
	LOSS [training: 0.31245534703065675 | validation: 0.2039804755116145]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2617876311011357		[learning rate: 0.0089362]
	Learning Rate: 0.00893622
	LOSS [training: 0.2617876311011357 | validation: 0.23250596961504247]
	TIME [epoch: 13.2 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3259914723513575		[learning rate: 0.0089046]
	Learning Rate: 0.00890462
	LOSS [training: 0.3259914723513575 | validation: 0.26422383636057223]
	TIME [epoch: 13.2 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28779163052878254		[learning rate: 0.0088731]
	Learning Rate: 0.00887313
	LOSS [training: 0.28779163052878254 | validation: 0.1912313306301603]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2821005656456702		[learning rate: 0.0088418]
	Learning Rate: 0.00884175
	LOSS [training: 0.2821005656456702 | validation: 0.2007834629682574]
	TIME [epoch: 13.2 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26359987427176457		[learning rate: 0.0088105]
	Learning Rate: 0.00881049
	LOSS [training: 0.26359987427176457 | validation: 0.2181259862826187]
	TIME [epoch: 13.2 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.28700017828753227		[learning rate: 0.0087793]
	Learning Rate: 0.00877933
	LOSS [training: 0.28700017828753227 | validation: 0.18421801750434613]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25019662207981985		[learning rate: 0.0087483]
	Learning Rate: 0.00874829
	LOSS [training: 0.25019662207981985 | validation: 0.20138053704721592]
	TIME [epoch: 13.2 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.3030291362363794		[learning rate: 0.0087174]
	Learning Rate: 0.00871735
	LOSS [training: 0.3030291362363794 | validation: 0.25067527529505174]
	TIME [epoch: 13.2 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27435464567535783		[learning rate: 0.0086865]
	Learning Rate: 0.00868653
	LOSS [training: 0.27435464567535783 | validation: 0.1804842735122433]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2878946564401042		[learning rate: 0.0086558]
	Learning Rate: 0.00865581
	LOSS [training: 0.2878946564401042 | validation: 0.2257185245518888]
	TIME [epoch: 13.2 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2630522100556256		[learning rate: 0.0086252]
	Learning Rate: 0.0086252
	LOSS [training: 0.2630522100556256 | validation: 0.20322996083057057]
	TIME [epoch: 13.2 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2877278699606903		[learning rate: 0.0085947]
	Learning Rate: 0.0085947
	LOSS [training: 0.2877278699606903 | validation: 0.17608636164767327]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26470584189600616		[learning rate: 0.0085643]
	Learning Rate: 0.00856431
	LOSS [training: 0.26470584189600616 | validation: 0.21054742826541045]
	TIME [epoch: 13.2 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2708072321118469		[learning rate: 0.008534]
	Learning Rate: 0.00853402
	LOSS [training: 0.2708072321118469 | validation: 0.18576700121963674]
	TIME [epoch: 13.2 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24690227389292754		[learning rate: 0.0085038]
	Learning Rate: 0.00850385
	LOSS [training: 0.24690227389292754 | validation: 0.18927576194288537]
	TIME [epoch: 13.2 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2742074800588974		[learning rate: 0.0084738]
	Learning Rate: 0.00847377
	LOSS [training: 0.2742074800588974 | validation: 0.2218360097274234]
	TIME [epoch: 13.2 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2645607458763992		[learning rate: 0.0084438]
	Learning Rate: 0.00844381
	LOSS [training: 0.2645607458763992 | validation: 0.2102130828244249]
	TIME [epoch: 13.2 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.29325189032750554		[learning rate: 0.008414]
	Learning Rate: 0.00841395
	LOSS [training: 0.29325189032750554 | validation: 0.17992164442106992]
	TIME [epoch: 13.2 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2510717169968868		[learning rate: 0.0083842]
	Learning Rate: 0.0083842
	LOSS [training: 0.2510717169968868 | validation: 0.21157999224929175]
	TIME [epoch: 13.2 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27437421693641834		[learning rate: 0.0083546]
	Learning Rate: 0.00835455
	LOSS [training: 0.27437421693641834 | validation: 0.2033317005983819]
	TIME [epoch: 13.2 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.268388262468241		[learning rate: 0.008325]
	Learning Rate: 0.00832501
	LOSS [training: 0.268388262468241 | validation: 0.17245960902709964]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24386896767401148		[learning rate: 0.0082956]
	Learning Rate: 0.00829557
	LOSS [training: 0.24386896767401148 | validation: 0.19834145066837156]
	TIME [epoch: 13.2 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2867449272637005		[learning rate: 0.0082662]
	Learning Rate: 0.00826623
	LOSS [training: 0.2867449272637005 | validation: 0.17163993670528102]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25454766103066495		[learning rate: 0.008237]
	Learning Rate: 0.008237
	LOSS [training: 0.25454766103066495 | validation: 0.21182709383567408]
	TIME [epoch: 13.2 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2654435507384757		[learning rate: 0.0082079]
	Learning Rate: 0.00820788
	LOSS [training: 0.2654435507384757 | validation: 0.18224841504969003]
	TIME [epoch: 13.2 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2593252067691301		[learning rate: 0.0081789]
	Learning Rate: 0.00817885
	LOSS [training: 0.2593252067691301 | validation: 0.22585329057246445]
	TIME [epoch: 13.2 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26677459881395205		[learning rate: 0.0081499]
	Learning Rate: 0.00814993
	LOSS [training: 0.26677459881395205 | validation: 0.1928389161827842]
	TIME [epoch: 13.2 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2594242250713031		[learning rate: 0.0081211]
	Learning Rate: 0.00812111
	LOSS [training: 0.2594242250713031 | validation: 0.23399973756588158]
	TIME [epoch: 13.2 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2629862043159461		[learning rate: 0.0080924]
	Learning Rate: 0.00809239
	LOSS [training: 0.2629862043159461 | validation: 0.18547571148131253]
	TIME [epoch: 13.2 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2540285147298415		[learning rate: 0.0080638]
	Learning Rate: 0.00806378
	LOSS [training: 0.2540285147298415 | validation: 0.18825361842014127]
	TIME [epoch: 13.2 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25575543951809365		[learning rate: 0.0080353]
	Learning Rate: 0.00803526
	LOSS [training: 0.25575543951809365 | validation: 0.2039445342697792]
	TIME [epoch: 13.2 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2730628993148108		[learning rate: 0.0080068]
	Learning Rate: 0.00800685
	LOSS [training: 0.2730628993148108 | validation: 0.1868259309329705]
	TIME [epoch: 13.2 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24860743115758305		[learning rate: 0.0079785]
	Learning Rate: 0.00797853
	LOSS [training: 0.24860743115758305 | validation: 0.16790502127045612]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2438772789520746		[learning rate: 0.0079503]
	Learning Rate: 0.00795032
	LOSS [training: 0.2438772789520746 | validation: 0.21388134844661336]
	TIME [epoch: 13.2 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27831890611260335		[learning rate: 0.0079222]
	Learning Rate: 0.00792221
	LOSS [training: 0.27831890611260335 | validation: 0.16534872647285812]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27251079095159525		[learning rate: 0.0078942]
	Learning Rate: 0.00789419
	LOSS [training: 0.27251079095159525 | validation: 0.1713944069572518]
	TIME [epoch: 13.2 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24377029155899874		[learning rate: 0.0078663]
	Learning Rate: 0.00786628
	LOSS [training: 0.24377029155899874 | validation: 0.1962437929503365]
	TIME [epoch: 13.2 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2601823435280377		[learning rate: 0.0078385]
	Learning Rate: 0.00783846
	LOSS [training: 0.2601823435280377 | validation: 0.1678933356503327]
	TIME [epoch: 13.2 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23430182998855742		[learning rate: 0.0078107]
	Learning Rate: 0.00781074
	LOSS [training: 0.23430182998855742 | validation: 0.1768426024532222]
	TIME [epoch: 13.2 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.26979247749264823		[learning rate: 0.0077831]
	Learning Rate: 0.00778312
	LOSS [training: 0.26979247749264823 | validation: 0.1575462326074873]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2391359542994442		[learning rate: 0.0077556]
	Learning Rate: 0.0077556
	LOSS [training: 0.2391359542994442 | validation: 0.17426908626522258]
	TIME [epoch: 13.2 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.27075586664368356		[learning rate: 0.0077282]
	Learning Rate: 0.00772817
	LOSS [training: 0.27075586664368356 | validation: 0.16100144764911112]
	TIME [epoch: 13.2 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23782196462461086		[learning rate: 0.0077008]
	Learning Rate: 0.00770085
	LOSS [training: 0.23782196462461086 | validation: 0.20089091645108195]
	TIME [epoch: 13.2 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.265041747616613		[learning rate: 0.0076736]
	Learning Rate: 0.00767362
	LOSS [training: 0.265041747616613 | validation: 0.18223896995140146]
	TIME [epoch: 13.2 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2497950491021509		[learning rate: 0.0076465]
	Learning Rate: 0.00764648
	LOSS [training: 0.2497950491021509 | validation: 0.17152144912701572]
	TIME [epoch: 13.2 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25396228118099934		[learning rate: 0.0076194]
	Learning Rate: 0.00761944
	LOSS [training: 0.25396228118099934 | validation: 0.1605454165380726]
	TIME [epoch: 13.2 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2372005452955201		[learning rate: 0.0075925]
	Learning Rate: 0.0075925
	LOSS [training: 0.2372005452955201 | validation: 0.22284819057671434]
	TIME [epoch: 13.2 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2583839455223425		[learning rate: 0.0075656]
	Learning Rate: 0.00756565
	LOSS [training: 0.2583839455223425 | validation: 0.16349405100481967]
	TIME [epoch: 13.2 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2446943159976887		[learning rate: 0.0075389]
	Learning Rate: 0.00753889
	LOSS [training: 0.2446943159976887 | validation: 0.21069963712527973]
	TIME [epoch: 13.2 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.257155091933471		[learning rate: 0.0075122]
	Learning Rate: 0.00751224
	LOSS [training: 0.257155091933471 | validation: 0.15632439891554303]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.240820931656125		[learning rate: 0.0074857]
	Learning Rate: 0.00748567
	LOSS [training: 0.240820931656125 | validation: 0.18836671165858182]
	TIME [epoch: 13.2 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23707873027869913		[learning rate: 0.0074592]
	Learning Rate: 0.0074592
	LOSS [training: 0.23707873027869913 | validation: 0.17153804808577658]
	TIME [epoch: 13.2 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2539253452613978		[learning rate: 0.0074328]
	Learning Rate: 0.00743282
	LOSS [training: 0.2539253452613978 | validation: 0.17660592438427575]
	TIME [epoch: 13.2 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23873061103122242		[learning rate: 0.0074065]
	Learning Rate: 0.00740654
	LOSS [training: 0.23873061103122242 | validation: 0.16875040074769687]
	TIME [epoch: 13.2 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24807023744440754		[learning rate: 0.0073803]
	Learning Rate: 0.00738035
	LOSS [training: 0.24807023744440754 | validation: 0.17115964150569915]
	TIME [epoch: 13.2 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2376555100808403		[learning rate: 0.0073543]
	Learning Rate: 0.00735425
	LOSS [training: 0.2376555100808403 | validation: 0.16903360149525654]
	TIME [epoch: 13.2 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24864599684135422		[learning rate: 0.0073282]
	Learning Rate: 0.00732825
	LOSS [training: 0.24864599684135422 | validation: 0.1676833395785901]
	TIME [epoch: 13.2 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24077755233870415		[learning rate: 0.0073023]
	Learning Rate: 0.00730233
	LOSS [training: 0.24077755233870415 | validation: 0.17203403822168384]
	TIME [epoch: 13.2 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23041995493453465		[learning rate: 0.0072765]
	Learning Rate: 0.00727651
	LOSS [training: 0.23041995493453465 | validation: 0.17005854202103438]
	TIME [epoch: 13.2 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.25044094106945647		[learning rate: 0.0072508]
	Learning Rate: 0.00725078
	LOSS [training: 0.25044094106945647 | validation: 0.19834469407058664]
	TIME [epoch: 13.2 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2436331984011289		[learning rate: 0.0072251]
	Learning Rate: 0.00722514
	LOSS [training: 0.2436331984011289 | validation: 0.14990610131965587]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2337259650200635		[learning rate: 0.0071996]
	Learning Rate: 0.00719959
	LOSS [training: 0.2337259650200635 | validation: 0.1899646204676907]
	TIME [epoch: 13.2 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2427402786127525		[learning rate: 0.0071741]
	Learning Rate: 0.00717413
	LOSS [training: 0.2427402786127525 | validation: 0.16960987137273054]
	TIME [epoch: 13.2 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24762274415036967		[learning rate: 0.0071488]
	Learning Rate: 0.00714876
	LOSS [training: 0.24762274415036967 | validation: 0.15539216035707357]
	TIME [epoch: 13.2 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22843335595172803		[learning rate: 0.0071235]
	Learning Rate: 0.00712348
	LOSS [training: 0.22843335595172803 | validation: 0.16598677416080243]
	TIME [epoch: 13.2 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24760150278283533		[learning rate: 0.0070983]
	Learning Rate: 0.00709829
	LOSS [training: 0.24760150278283533 | validation: 0.17189414855598345]
	TIME [epoch: 13.2 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23233703858122595		[learning rate: 0.0070732]
	Learning Rate: 0.00707319
	LOSS [training: 0.23233703858122595 | validation: 0.18010180975585838]
	TIME [epoch: 13.2 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22717499795314977		[learning rate: 0.0070482]
	Learning Rate: 0.00704818
	LOSS [training: 0.22717499795314977 | validation: 0.16673633324426865]
	TIME [epoch: 13.2 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24041317605951895		[learning rate: 0.0070233]
	Learning Rate: 0.00702325
	LOSS [training: 0.24041317605951895 | validation: 0.1858970892188474]
	TIME [epoch: 13.2 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.24321389916050637		[learning rate: 0.0069984]
	Learning Rate: 0.00699842
	LOSS [training: 0.24321389916050637 | validation: 0.15101560173083753]
	TIME [epoch: 13.2 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23431416103010846		[learning rate: 0.0069737]
	Learning Rate: 0.00697367
	LOSS [training: 0.23431416103010846 | validation: 0.18688536174704162]
	TIME [epoch: 13.2 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23203753889575837		[learning rate: 0.006949]
	Learning Rate: 0.00694901
	LOSS [training: 0.23203753889575837 | validation: 0.15294844815369296]
	TIME [epoch: 13.2 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2366128231570429		[learning rate: 0.0069244]
	Learning Rate: 0.00692444
	LOSS [training: 0.2366128231570429 | validation: 0.18141099532802468]
	TIME [epoch: 13.2 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23414480146485545		[learning rate: 0.0069]
	Learning Rate: 0.00689995
	LOSS [training: 0.23414480146485545 | validation: 0.17536588132556846]
	TIME [epoch: 13.2 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2320937747979569		[learning rate: 0.0068756]
	Learning Rate: 0.00687555
	LOSS [training: 0.2320937747979569 | validation: 0.16340840043288365]
	TIME [epoch: 13.2 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23228891595382656		[learning rate: 0.0068512]
	Learning Rate: 0.00685124
	LOSS [training: 0.23228891595382656 | validation: 0.16426917519829579]
	TIME [epoch: 13.2 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22697283944329735		[learning rate: 0.006827]
	Learning Rate: 0.00682701
	LOSS [training: 0.22697283944329735 | validation: 0.17533199479277112]
	TIME [epoch: 13.2 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2512731388834361		[learning rate: 0.0068029]
	Learning Rate: 0.00680287
	LOSS [training: 0.2512731388834361 | validation: 0.15003397822267753]
	TIME [epoch: 13.2 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2289706727703901		[learning rate: 0.0067788]
	Learning Rate: 0.00677882
	LOSS [training: 0.2289706727703901 | validation: 0.158595362205453]
	TIME [epoch: 13.2 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22883448616146465		[learning rate: 0.0067548]
	Learning Rate: 0.00675485
	LOSS [training: 0.22883448616146465 | validation: 0.16793076180016886]
	TIME [epoch: 13.2 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2252277975949344		[learning rate: 0.006731]
	Learning Rate: 0.00673096
	LOSS [training: 0.2252277975949344 | validation: 0.16275450788702778]
	TIME [epoch: 13.2 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22491757180708027		[learning rate: 0.0067072]
	Learning Rate: 0.00670716
	LOSS [training: 0.22491757180708027 | validation: 0.148346428053551]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_163.pth
	Model improved!!!
EPOCH 164/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22240080799098208		[learning rate: 0.0066834]
	Learning Rate: 0.00668344
	LOSS [training: 0.22240080799098208 | validation: 0.16507883514354293]
	TIME [epoch: 13.2 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22636396250869842		[learning rate: 0.0066598]
	Learning Rate: 0.0066598
	LOSS [training: 0.22636396250869842 | validation: 0.1596025926816597]
	TIME [epoch: 13.2 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22671280970599		[learning rate: 0.0066363]
	Learning Rate: 0.00663625
	LOSS [training: 0.22671280970599 | validation: 0.15815192351731916]
	TIME [epoch: 13.2 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22008609249458866		[learning rate: 0.0066128]
	Learning Rate: 0.00661279
	LOSS [training: 0.22008609249458866 | validation: 0.15241041199645627]
	TIME [epoch: 13.2 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.23644423563521336		[learning rate: 0.0065894]
	Learning Rate: 0.0065894
	LOSS [training: 0.23644423563521336 | validation: 0.15750894566279672]
	TIME [epoch: 13.2 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21842906700335074		[learning rate: 0.0065661]
	Learning Rate: 0.0065661
	LOSS [training: 0.21842906700335074 | validation: 0.16455208919387795]
	TIME [epoch: 13.2 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.219387559508338		[learning rate: 0.0065429]
	Learning Rate: 0.00654288
	LOSS [training: 0.219387559508338 | validation: 0.17050291460890904]
	TIME [epoch: 13.2 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21828037421749774		[learning rate: 0.0065197]
	Learning Rate: 0.00651975
	LOSS [training: 0.21828037421749774 | validation: 0.15011440867470782]
	TIME [epoch: 13.2 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20797800925200652		[learning rate: 0.0064967]
	Learning Rate: 0.00649669
	LOSS [training: 0.20797800925200652 | validation: 0.15597476793980164]
	TIME [epoch: 13.2 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22599973873440987		[learning rate: 0.0064737]
	Learning Rate: 0.00647372
	LOSS [training: 0.22599973873440987 | validation: 0.14785632615007704]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21773511196573966		[learning rate: 0.0064508]
	Learning Rate: 0.00645083
	LOSS [training: 0.21773511196573966 | validation: 0.19600729021886346]
	TIME [epoch: 13.2 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22713312974136607		[learning rate: 0.006428]
	Learning Rate: 0.00642801
	LOSS [training: 0.22713312974136607 | validation: 0.15979090902197285]
	TIME [epoch: 13.2 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.208408199855279		[learning rate: 0.0064053]
	Learning Rate: 0.00640529
	LOSS [training: 0.208408199855279 | validation: 0.18559539088503563]
	TIME [epoch: 13.2 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21204088786937023		[learning rate: 0.0063826]
	Learning Rate: 0.00638263
	LOSS [training: 0.21204088786937023 | validation: 0.16070719524149912]
	TIME [epoch: 13.2 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19554693347334		[learning rate: 0.0063601]
	Learning Rate: 0.00636006
	LOSS [training: 0.19554693347334 | validation: 0.15862944211631827]
	TIME [epoch: 13.2 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.22768073531875233		[learning rate: 0.0063376]
	Learning Rate: 0.00633757
	LOSS [training: 0.22768073531875233 | validation: 0.16804898114080918]
	TIME [epoch: 13.2 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.20249585188387714		[learning rate: 0.0063152]
	Learning Rate: 0.00631516
	LOSS [training: 0.20249585188387714 | validation: 0.1680551184319508]
	TIME [epoch: 13.2 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.21184573179487376		[learning rate: 0.0062928]
	Learning Rate: 0.00629283
	LOSS [training: 0.21184573179487376 | validation: 0.14973825751368094]
	TIME [epoch: 13.2 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19390285114893924		[learning rate: 0.0062706]
	Learning Rate: 0.00627058
	LOSS [training: 0.19390285114893924 | validation: 0.15558749269949668]
	TIME [epoch: 13.2 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2075863775686689		[learning rate: 0.0062484]
	Learning Rate: 0.00624841
	LOSS [training: 0.2075863775686689 | validation: 0.13882105067518027]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19156598335843739		[learning rate: 0.0062263]
	Learning Rate: 0.00622631
	LOSS [training: 0.19156598335843739 | validation: 0.1951890255407815]
	TIME [epoch: 13.2 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19322279845452		[learning rate: 0.0062043]
	Learning Rate: 0.00620429
	LOSS [training: 0.19322279845452 | validation: 0.17270358995943397]
	TIME [epoch: 13.2 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1884386965220351		[learning rate: 0.0061824]
	Learning Rate: 0.00618235
	LOSS [training: 0.1884386965220351 | validation: 0.14388983775453926]
	TIME [epoch: 13.2 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.179742781012093		[learning rate: 0.0061605]
	Learning Rate: 0.00616049
	LOSS [training: 0.179742781012093 | validation: 0.15857260565844622]
	TIME [epoch: 13.2 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1899338257124101		[learning rate: 0.0061387]
	Learning Rate: 0.00613871
	LOSS [training: 0.1899338257124101 | validation: 0.12431542402122375]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.19700031024314724		[learning rate: 0.006117]
	Learning Rate: 0.006117
	LOSS [training: 0.19700031024314724 | validation: 0.13673574552365753]
	TIME [epoch: 13.2 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1712447075685934		[learning rate: 0.0060954]
	Learning Rate: 0.00609537
	LOSS [training: 0.1712447075685934 | validation: 0.13269004464874928]
	TIME [epoch: 13.2 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1501398569582113		[learning rate: 0.0060738]
	Learning Rate: 0.00607382
	LOSS [training: 0.1501398569582113 | validation: 0.12671593905386352]
	TIME [epoch: 13.2 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17402326915411115		[learning rate: 0.0060523]
	Learning Rate: 0.00605234
	LOSS [training: 0.17402326915411115 | validation: 0.12110470366684609]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13360847179013965		[learning rate: 0.0060309]
	Learning Rate: 0.00603093
	LOSS [training: 0.13360847179013965 | validation: 0.14517738319803503]
	TIME [epoch: 13.2 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.2261491437932832		[learning rate: 0.0060096]
	Learning Rate: 0.00600961
	LOSS [training: 0.2261491437932832 | validation: 0.15072906879938708]
	TIME [epoch: 13.2 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14475888088968525		[learning rate: 0.0059884]
	Learning Rate: 0.00598836
	LOSS [training: 0.14475888088968525 | validation: 0.10761618744543688]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.16882238218257584		[learning rate: 0.0059672]
	Learning Rate: 0.00596718
	LOSS [training: 0.16882238218257584 | validation: 0.20564367846308315]
	TIME [epoch: 13.2 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1398080188602093		[learning rate: 0.0059461]
	Learning Rate: 0.00594608
	LOSS [training: 0.1398080188602093 | validation: 0.16027261005380983]
	TIME [epoch: 13.3 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14214754723318887		[learning rate: 0.0059251]
	Learning Rate: 0.00592505
	LOSS [training: 0.14214754723318887 | validation: 0.12515763788903297]
	TIME [epoch: 13.2 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15880510587375013		[learning rate: 0.0059041]
	Learning Rate: 0.0059041
	LOSS [training: 0.15880510587375013 | validation: 0.13851442716054907]
	TIME [epoch: 13.2 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13546850499208032		[learning rate: 0.0058832]
	Learning Rate: 0.00588322
	LOSS [training: 0.13546850499208032 | validation: 0.09823925598275994]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11436796830088497		[learning rate: 0.0058624]
	Learning Rate: 0.00586242
	LOSS [training: 0.11436796830088497 | validation: 0.10290742038913869]
	TIME [epoch: 118 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14080442155525505		[learning rate: 0.0058417]
	Learning Rate: 0.00584169
	LOSS [training: 0.14080442155525505 | validation: 0.10150953740348165]
	TIME [epoch: 25.4 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12296868388296893		[learning rate: 0.005821]
	Learning Rate: 0.00582103
	LOSS [training: 0.12296868388296893 | validation: 0.11123887478265415]
	TIME [epoch: 25.3 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1178243487459863		[learning rate: 0.0058004]
	Learning Rate: 0.00580045
	LOSS [training: 0.1178243487459863 | validation: 0.15130402278579314]
	TIME [epoch: 25.3 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18188037584929725		[learning rate: 0.0057799]
	Learning Rate: 0.00577994
	LOSS [training: 0.18188037584929725 | validation: 0.11468751618917458]
	TIME [epoch: 25.3 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.18441632176900216		[learning rate: 0.0057595]
	Learning Rate: 0.0057595
	LOSS [training: 0.18441632176900216 | validation: 0.1795399278940894]
	TIME [epoch: 25.3 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11703103948604038		[learning rate: 0.0057391]
	Learning Rate: 0.00573913
	LOSS [training: 0.11703103948604038 | validation: 0.14817739180285044]
	TIME [epoch: 25.2 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14387788928439932		[learning rate: 0.0057188]
	Learning Rate: 0.00571884
	LOSS [training: 0.14387788928439932 | validation: 0.08797837835467057]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12634563512786137		[learning rate: 0.0056986]
	Learning Rate: 0.00569861
	LOSS [training: 0.12634563512786137 | validation: 0.08729743838121758]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12205507445457757		[learning rate: 0.0056785]
	Learning Rate: 0.00567846
	LOSS [training: 0.12205507445457757 | validation: 0.1165620994944447]
	TIME [epoch: 25.3 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1616649690132916		[learning rate: 0.0056584]
	Learning Rate: 0.00565838
	LOSS [training: 0.1616649690132916 | validation: 0.09236026273405197]
	TIME [epoch: 25.3 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12365606074252979		[learning rate: 0.0056384]
	Learning Rate: 0.00563837
	LOSS [training: 0.12365606074252979 | validation: 0.10092401953757202]
	TIME [epoch: 25.3 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08307975154741083		[learning rate: 0.0056184]
	Learning Rate: 0.00561843
	LOSS [training: 0.08307975154741083 | validation: 0.1229311126815018]
	TIME [epoch: 25.3 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.14671430047189168		[learning rate: 0.0055986]
	Learning Rate: 0.00559857
	LOSS [training: 0.14671430047189168 | validation: 0.08691440366202927]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1254977882445311		[learning rate: 0.0055788]
	Learning Rate: 0.00557877
	LOSS [training: 0.1254977882445311 | validation: 0.08956499183685218]
	TIME [epoch: 25.3 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08753720699057949		[learning rate: 0.005559]
	Learning Rate: 0.00555904
	LOSS [training: 0.08753720699057949 | validation: 0.14264962407919712]
	TIME [epoch: 25.3 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.17584797320805204		[learning rate: 0.0055394]
	Learning Rate: 0.00553939
	LOSS [training: 0.17584797320805204 | validation: 0.08641811523659872]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11391247467323187		[learning rate: 0.0055198]
	Learning Rate: 0.0055198
	LOSS [training: 0.11391247467323187 | validation: 0.08295799107509409]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13068841373478848		[learning rate: 0.0055003]
	Learning Rate: 0.00550028
	LOSS [training: 0.13068841373478848 | validation: 0.07641461833996019]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08784222653390705		[learning rate: 0.0054808]
	Learning Rate: 0.00548083
	LOSS [training: 0.08784222653390705 | validation: 0.16181863030682941]
	TIME [epoch: 25.3 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11086259450504296		[learning rate: 0.0054614]
	Learning Rate: 0.00546145
	LOSS [training: 0.11086259450504296 | validation: 0.0758321011802852]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12678178993234338		[learning rate: 0.0054421]
	Learning Rate: 0.00544213
	LOSS [training: 0.12678178993234338 | validation: 0.23223319915604745]
	TIME [epoch: 25.3 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1519463361188367		[learning rate: 0.0054229]
	Learning Rate: 0.00542289
	LOSS [training: 0.1519463361188367 | validation: 0.10119240642976363]
	TIME [epoch: 25.3 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11153945449336895		[learning rate: 0.0054037]
	Learning Rate: 0.00540371
	LOSS [training: 0.11153945449336895 | validation: 0.08743810022668319]
	TIME [epoch: 25.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09045974278140745		[learning rate: 0.0053846]
	Learning Rate: 0.0053846
	LOSS [training: 0.09045974278140745 | validation: 0.08542584018306937]
	TIME [epoch: 25.3 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13926674345775186		[learning rate: 0.0053656]
	Learning Rate: 0.00536556
	LOSS [training: 0.13926674345775186 | validation: 0.09308495428677233]
	TIME [epoch: 25.3 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12373776943216003		[learning rate: 0.0053466]
	Learning Rate: 0.00534659
	LOSS [training: 0.12373776943216003 | validation: 0.09951515569402025]
	TIME [epoch: 25.3 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.15517192430346094		[learning rate: 0.0053277]
	Learning Rate: 0.00532768
	LOSS [training: 0.15517192430346094 | validation: 0.09358314821278294]
	TIME [epoch: 25.3 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1245770211656901		[learning rate: 0.0053088]
	Learning Rate: 0.00530885
	LOSS [training: 0.1245770211656901 | validation: 0.14828340596259612]
	TIME [epoch: 25.3 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10820315542992752		[learning rate: 0.0052901]
	Learning Rate: 0.00529007
	LOSS [training: 0.10820315542992752 | validation: 0.07836301189023101]
	TIME [epoch: 25.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09645049252542062		[learning rate: 0.0052714]
	Learning Rate: 0.00527136
	LOSS [training: 0.09645049252542062 | validation: 0.06466406250661022]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09662659591262632		[learning rate: 0.0052527]
	Learning Rate: 0.00525272
	LOSS [training: 0.09662659591262632 | validation: 0.06882159182268875]
	TIME [epoch: 25.3 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08593443669892144		[learning rate: 0.0052341]
	Learning Rate: 0.00523415
	LOSS [training: 0.08593443669892144 | validation: 0.16722530902571894]
	TIME [epoch: 25.2 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11130414783856374		[learning rate: 0.0052156]
	Learning Rate: 0.00521564
	LOSS [training: 0.11130414783856374 | validation: 0.07267239573709774]
	TIME [epoch: 25.3 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09145851516994519		[learning rate: 0.0051972]
	Learning Rate: 0.0051972
	LOSS [training: 0.09145851516994519 | validation: 0.09435876248643554]
	TIME [epoch: 25.2 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0982123399423356		[learning rate: 0.0051788]
	Learning Rate: 0.00517882
	LOSS [training: 0.0982123399423356 | validation: 0.1293763972521122]
	TIME [epoch: 25.3 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.12687943031709983		[learning rate: 0.0051605]
	Learning Rate: 0.00516051
	LOSS [training: 0.12687943031709983 | validation: 0.06766762802480182]
	TIME [epoch: 25.2 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07171843326105991		[learning rate: 0.0051423]
	Learning Rate: 0.00514226
	LOSS [training: 0.07171843326105991 | validation: 0.06057373991378337]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_238.pth
	Model improved!!!
EPOCH 239/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06592307262635633		[learning rate: 0.0051241]
	Learning Rate: 0.00512407
	LOSS [training: 0.06592307262635633 | validation: 0.08299886789362225]
	TIME [epoch: 25.2 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.13155418115989492		[learning rate: 0.005106]
	Learning Rate: 0.00510596
	LOSS [training: 0.13155418115989492 | validation: 0.09248418132372138]
	TIME [epoch: 25.3 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.1160857509231334		[learning rate: 0.0050879]
	Learning Rate: 0.0050879
	LOSS [training: 0.1160857509231334 | validation: 0.0766669477242852]
	TIME [epoch: 25.2 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09156906073952759		[learning rate: 0.0050699]
	Learning Rate: 0.00506991
	LOSS [training: 0.09156906073952759 | validation: 0.05907187112916881]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0841973815109909		[learning rate: 0.005052]
	Learning Rate: 0.00505198
	LOSS [training: 0.0841973815109909 | validation: 0.04993680620793539]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_243.pth
	Model improved!!!
EPOCH 244/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06434737948620757		[learning rate: 0.0050341]
	Learning Rate: 0.00503411
	LOSS [training: 0.06434737948620757 | validation: 0.07768960399572084]
	TIME [epoch: 25.3 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10649943080079144		[learning rate: 0.0050163]
	Learning Rate: 0.00501631
	LOSS [training: 0.10649943080079144 | validation: 0.14026879070901557]
	TIME [epoch: 25.3 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.11574728741684465		[learning rate: 0.0049986]
	Learning Rate: 0.00499857
	LOSS [training: 0.11574728741684465 | validation: 0.06930515120973016]
	TIME [epoch: 25.3 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07915434812614068		[learning rate: 0.0049809]
	Learning Rate: 0.0049809
	LOSS [training: 0.07915434812614068 | validation: 0.09764535828659673]
	TIME [epoch: 25.3 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07793162164575222		[learning rate: 0.0049633]
	Learning Rate: 0.00496329
	LOSS [training: 0.07793162164575222 | validation: 0.06030635702427847]
	TIME [epoch: 25.3 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09998317140364558		[learning rate: 0.0049457]
	Learning Rate: 0.00494573
	LOSS [training: 0.09998317140364558 | validation: 0.04880561044254578]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07810538859781044		[learning rate: 0.0049282]
	Learning Rate: 0.00492824
	LOSS [training: 0.07810538859781044 | validation: 0.06582563408780173]
	TIME [epoch: 25.3 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0726793289279799		[learning rate: 0.0049108]
	Learning Rate: 0.00491082
	LOSS [training: 0.0726793289279799 | validation: 0.04585346289612329]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08743800591943576		[learning rate: 0.0048935]
	Learning Rate: 0.00489345
	LOSS [training: 0.08743800591943576 | validation: 0.06405752583534707]
	TIME [epoch: 25.3 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09467315594617731		[learning rate: 0.0048761]
	Learning Rate: 0.00487615
	LOSS [training: 0.09467315594617731 | validation: 0.05889793924198835]
	TIME [epoch: 25.3 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08234022331182914		[learning rate: 0.0048589]
	Learning Rate: 0.00485891
	LOSS [training: 0.08234022331182914 | validation: 0.06240465540165083]
	TIME [epoch: 25.3 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06648209778577212		[learning rate: 0.0048417]
	Learning Rate: 0.00484172
	LOSS [training: 0.06648209778577212 | validation: 0.05720176734720016]
	TIME [epoch: 25.3 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07646629648726276		[learning rate: 0.0048246]
	Learning Rate: 0.0048246
	LOSS [training: 0.07646629648726276 | validation: 0.06849608817748362]
	TIME [epoch: 25.3 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08014213816709033		[learning rate: 0.0048075]
	Learning Rate: 0.00480754
	LOSS [training: 0.08014213816709033 | validation: 0.07499974084381406]
	TIME [epoch: 25.3 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058447213376543794		[learning rate: 0.0047905]
	Learning Rate: 0.00479054
	LOSS [training: 0.058447213376543794 | validation: 0.05355418262937299]
	TIME [epoch: 25.3 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08160588046111499		[learning rate: 0.0047736]
	Learning Rate: 0.0047736
	LOSS [training: 0.08160588046111499 | validation: 0.067022918410027]
	TIME [epoch: 25.3 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07098372798134199		[learning rate: 0.0047567]
	Learning Rate: 0.00475672
	LOSS [training: 0.07098372798134199 | validation: 0.08477306262644962]
	TIME [epoch: 25.3 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05006736249048289		[learning rate: 0.0047399]
	Learning Rate: 0.0047399
	LOSS [training: 0.05006736249048289 | validation: 0.04351297293062781]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.10082111726232036		[learning rate: 0.0047231]
	Learning Rate: 0.00472314
	LOSS [training: 0.10082111726232036 | validation: 0.07181084081529417]
	TIME [epoch: 25.4 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06343981851706224		[learning rate: 0.0047064]
	Learning Rate: 0.00470644
	LOSS [training: 0.06343981851706224 | validation: 0.03932402288287206]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09777450047505758		[learning rate: 0.0046898]
	Learning Rate: 0.00468979
	LOSS [training: 0.09777450047505758 | validation: 0.08571294561354852]
	TIME [epoch: 25.4 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0826803972515742		[learning rate: 0.0046732]
	Learning Rate: 0.00467321
	LOSS [training: 0.0826803972515742 | validation: 0.05507393857096857]
	TIME [epoch: 25.3 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.061283524591483444		[learning rate: 0.0046567]
	Learning Rate: 0.00465669
	LOSS [training: 0.061283524591483444 | validation: 0.04654812282765762]
	TIME [epoch: 25.3 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0878374417304731		[learning rate: 0.0046402]
	Learning Rate: 0.00464022
	LOSS [training: 0.0878374417304731 | validation: 0.08625657592964749]
	TIME [epoch: 25.3 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09187240919471523		[learning rate: 0.0046238]
	Learning Rate: 0.00462381
	LOSS [training: 0.09187240919471523 | validation: 0.04216930131835678]
	TIME [epoch: 25.3 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06484187364962615		[learning rate: 0.0046075]
	Learning Rate: 0.00460746
	LOSS [training: 0.06484187364962615 | validation: 0.04428214902470913]
	TIME [epoch: 25.3 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06301563539877122		[learning rate: 0.0045912]
	Learning Rate: 0.00459117
	LOSS [training: 0.06301563539877122 | validation: 0.05290677034266959]
	TIME [epoch: 25.3 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05185978555205303		[learning rate: 0.0045749]
	Learning Rate: 0.00457493
	LOSS [training: 0.05185978555205303 | validation: 0.03545339613283103]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08260251094453		[learning rate: 0.0045588]
	Learning Rate: 0.00455875
	LOSS [training: 0.08260251094453 | validation: 0.05152568099037581]
	TIME [epoch: 25.3 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.09537746601869002		[learning rate: 0.0045426]
	Learning Rate: 0.00454263
	LOSS [training: 0.09537746601869002 | validation: 0.04558498052042411]
	TIME [epoch: 25.3 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07564282580374819		[learning rate: 0.0045266]
	Learning Rate: 0.00452657
	LOSS [training: 0.07564282580374819 | validation: 0.05489308188580151]
	TIME [epoch: 25.3 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08130741569916984		[learning rate: 0.0045106]
	Learning Rate: 0.00451056
	LOSS [training: 0.08130741569916984 | validation: 0.07512903874102973]
	TIME [epoch: 25.3 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.053929780100510895		[learning rate: 0.0044946]
	Learning Rate: 0.00449461
	LOSS [training: 0.053929780100510895 | validation: 0.0482865680872535]
	TIME [epoch: 25.3 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08178453917565424		[learning rate: 0.0044787]
	Learning Rate: 0.00447872
	LOSS [training: 0.08178453917565424 | validation: 0.04119519935358289]
	TIME [epoch: 25.3 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05436887528315605		[learning rate: 0.0044629]
	Learning Rate: 0.00446288
	LOSS [training: 0.05436887528315605 | validation: 0.05848028692635791]
	TIME [epoch: 25.3 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07897255678132534		[learning rate: 0.0044471]
	Learning Rate: 0.0044471
	LOSS [training: 0.07897255678132534 | validation: 0.033970198235865286]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0435573476391714		[learning rate: 0.0044314]
	Learning Rate: 0.00443137
	LOSS [training: 0.0435573476391714 | validation: 0.07454178978281598]
	TIME [epoch: 25.3 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.08517214841111385		[learning rate: 0.0044157]
	Learning Rate: 0.0044157
	LOSS [training: 0.08517214841111385 | validation: 0.04358406612695112]
	TIME [epoch: 25.2 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05230030342786833		[learning rate: 0.0044001]
	Learning Rate: 0.00440009
	LOSS [training: 0.05230030342786833 | validation: 0.05429373436853636]
	TIME [epoch: 25.3 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05864859972240181		[learning rate: 0.0043845]
	Learning Rate: 0.00438453
	LOSS [training: 0.05864859972240181 | validation: 0.04559514617116118]
	TIME [epoch: 25.3 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05719197950763522		[learning rate: 0.004369]
	Learning Rate: 0.00436903
	LOSS [training: 0.05719197950763522 | validation: 0.0812260266256863]
	TIME [epoch: 25.3 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06713872971861977		[learning rate: 0.0043536]
	Learning Rate: 0.00435358
	LOSS [training: 0.06713872971861977 | validation: 0.07171341656023167]
	TIME [epoch: 25.3 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06606011182095332		[learning rate: 0.0043382]
	Learning Rate: 0.00433818
	LOSS [training: 0.06606011182095332 | validation: 0.050230708730495]
	TIME [epoch: 25.3 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07302615206298302		[learning rate: 0.0043228]
	Learning Rate: 0.00432284
	LOSS [training: 0.07302615206298302 | validation: 0.05175843358040519]
	TIME [epoch: 25.3 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04830337989907682		[learning rate: 0.0043076]
	Learning Rate: 0.00430755
	LOSS [training: 0.04830337989907682 | validation: 0.0489657379862109]
	TIME [epoch: 25.3 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.060356951399948705		[learning rate: 0.0042923]
	Learning Rate: 0.00429232
	LOSS [training: 0.060356951399948705 | validation: 0.07356182564546448]
	TIME [epoch: 25.3 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0628526536715989		[learning rate: 0.0042771]
	Learning Rate: 0.00427714
	LOSS [training: 0.0628526536715989 | validation: 0.0339098894726954]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.062300575180482674		[learning rate: 0.004262]
	Learning Rate: 0.00426202
	LOSS [training: 0.062300575180482674 | validation: 0.030380526842241847]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.055811801715727506		[learning rate: 0.0042469]
	Learning Rate: 0.00424695
	LOSS [training: 0.055811801715727506 | validation: 0.06581578382399884]
	TIME [epoch: 25.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04090699635800785		[learning rate: 0.0042319]
	Learning Rate: 0.00423193
	LOSS [training: 0.04090699635800785 | validation: 0.04840229712364929]
	TIME [epoch: 25.3 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07116129822058843		[learning rate: 0.004217]
	Learning Rate: 0.00421696
	LOSS [training: 0.07116129822058843 | validation: 0.03150214602631743]
	TIME [epoch: 25.3 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05895260269617987		[learning rate: 0.0042021]
	Learning Rate: 0.00420205
	LOSS [training: 0.05895260269617987 | validation: 0.05745085771326161]
	TIME [epoch: 25.3 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06384529467328685		[learning rate: 0.0041872]
	Learning Rate: 0.00418719
	LOSS [training: 0.06384529467328685 | validation: 0.03957012135564966]
	TIME [epoch: 25.3 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05113987822444224		[learning rate: 0.0041724]
	Learning Rate: 0.00417239
	LOSS [training: 0.05113987822444224 | validation: 0.0634109473283649]
	TIME [epoch: 25.3 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05796801592262163		[learning rate: 0.0041576]
	Learning Rate: 0.00415763
	LOSS [training: 0.05796801592262163 | validation: 0.04370531063948745]
	TIME [epoch: 25.3 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040453670937980926		[learning rate: 0.0041429]
	Learning Rate: 0.00414293
	LOSS [training: 0.040453670937980926 | validation: 0.07851668349535806]
	TIME [epoch: 25.3 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0585491415090323		[learning rate: 0.0041283]
	Learning Rate: 0.00412828
	LOSS [training: 0.0585491415090323 | validation: 0.07347827271190102]
	TIME [epoch: 25.3 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0509409427428841		[learning rate: 0.0041137]
	Learning Rate: 0.00411368
	LOSS [training: 0.0509409427428841 | validation: 0.03152235272518704]
	TIME [epoch: 25.3 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06356321772313962		[learning rate: 0.0040991]
	Learning Rate: 0.00409914
	LOSS [training: 0.06356321772313962 | validation: 0.06230619189789394]
	TIME [epoch: 25.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06641011012068032		[learning rate: 0.0040846]
	Learning Rate: 0.00408464
	LOSS [training: 0.06641011012068032 | validation: 0.04356009206484534]
	TIME [epoch: 25.3 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038737906770308146		[learning rate: 0.0040702]
	Learning Rate: 0.0040702
	LOSS [training: 0.038737906770308146 | validation: 0.030565746271325883]
	TIME [epoch: 25.3 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054075691369615216		[learning rate: 0.0040558]
	Learning Rate: 0.0040558
	LOSS [training: 0.054075691369615216 | validation: 0.07175852483298137]
	TIME [epoch: 25.3 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05478149604439084		[learning rate: 0.0040415]
	Learning Rate: 0.00404146
	LOSS [training: 0.05478149604439084 | validation: 0.06747822293735523]
	TIME [epoch: 25.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06290421484936838		[learning rate: 0.0040272]
	Learning Rate: 0.00402717
	LOSS [training: 0.06290421484936838 | validation: 0.059178910747327174]
	TIME [epoch: 25.3 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.040944625079753744		[learning rate: 0.0040129]
	Learning Rate: 0.00401293
	LOSS [training: 0.040944625079753744 | validation: 0.029199079716639924]
	TIME [epoch: 25.4 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05011312927870204		[learning rate: 0.0039987]
	Learning Rate: 0.00399874
	LOSS [training: 0.05011312927870204 | validation: 0.03947628858648236]
	TIME [epoch: 25.3 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05803270558541476		[learning rate: 0.0039846]
	Learning Rate: 0.0039846
	LOSS [training: 0.05803270558541476 | validation: 0.027646050107268207]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.054310204569291004		[learning rate: 0.0039705]
	Learning Rate: 0.00397051
	LOSS [training: 0.054310204569291004 | validation: 0.05262048466757942]
	TIME [epoch: 25.2 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.041824436075018935		[learning rate: 0.0039565]
	Learning Rate: 0.00395647
	LOSS [training: 0.041824436075018935 | validation: 0.03339520138090091]
	TIME [epoch: 25.3 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06129128351820863		[learning rate: 0.0039425]
	Learning Rate: 0.00394248
	LOSS [training: 0.06129128351820863 | validation: 0.03603435223406766]
	TIME [epoch: 25.3 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05358315016296564		[learning rate: 0.0039285]
	Learning Rate: 0.00392854
	LOSS [training: 0.05358315016296564 | validation: 0.036782222710867465]
	TIME [epoch: 25.3 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03884170320018775		[learning rate: 0.0039146]
	Learning Rate: 0.00391464
	LOSS [training: 0.03884170320018775 | validation: 0.08436229194293143]
	TIME [epoch: 25.3 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0655115357262469		[learning rate: 0.0039008]
	Learning Rate: 0.0039008
	LOSS [training: 0.0655115357262469 | validation: 0.027686748109970266]
	TIME [epoch: 25.3 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02963806050457378		[learning rate: 0.003887]
	Learning Rate: 0.00388701
	LOSS [training: 0.02963806050457378 | validation: 0.040719163147987125]
	TIME [epoch: 25.3 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06486016332197053		[learning rate: 0.0038733]
	Learning Rate: 0.00387326
	LOSS [training: 0.06486016332197053 | validation: 0.10638612751778793]
	TIME [epoch: 25.3 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07547832519276991		[learning rate: 0.0038596]
	Learning Rate: 0.00385957
	LOSS [training: 0.07547832519276991 | validation: 0.03767349780033001]
	TIME [epoch: 25.2 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03562022002743949		[learning rate: 0.0038459]
	Learning Rate: 0.00384592
	LOSS [training: 0.03562022002743949 | validation: 0.03203170319702286]
	TIME [epoch: 25.3 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028056872818031858		[learning rate: 0.0038323]
	Learning Rate: 0.00383232
	LOSS [training: 0.028056872818031858 | validation: 0.03414087238105847]
	TIME [epoch: 25.3 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.07043003470537659		[learning rate: 0.0038188]
	Learning Rate: 0.00381877
	LOSS [training: 0.07043003470537659 | validation: 0.053616583952858315]
	TIME [epoch: 25.3 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03793001827068953		[learning rate: 0.0038053]
	Learning Rate: 0.00380526
	LOSS [training: 0.03793001827068953 | validation: 0.039527916477932565]
	TIME [epoch: 25.3 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043541691218281386		[learning rate: 0.0037918]
	Learning Rate: 0.00379181
	LOSS [training: 0.043541691218281386 | validation: 0.032844639802147337]
	TIME [epoch: 25.3 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03290845910089071		[learning rate: 0.0037784]
	Learning Rate: 0.0037784
	LOSS [training: 0.03290845910089071 | validation: 0.03967055744910818]
	TIME [epoch: 25.3 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06396699080555192		[learning rate: 0.003765]
	Learning Rate: 0.00376504
	LOSS [training: 0.06396699080555192 | validation: 0.04041479073147808]
	TIME [epoch: 25.3 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06518880372959325		[learning rate: 0.0037517]
	Learning Rate: 0.00375172
	LOSS [training: 0.06518880372959325 | validation: 0.06564769317100325]
	TIME [epoch: 25.3 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.046007351090518595		[learning rate: 0.0037385]
	Learning Rate: 0.00373846
	LOSS [training: 0.046007351090518595 | validation: 0.034288001847750105]
	TIME [epoch: 25.3 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04099877287167896		[learning rate: 0.0037252]
	Learning Rate: 0.00372524
	LOSS [training: 0.04099877287167896 | validation: 0.06394380226529534]
	TIME [epoch: 25.3 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05492745901267149		[learning rate: 0.0037121]
	Learning Rate: 0.00371206
	LOSS [training: 0.05492745901267149 | validation: 0.04610671477289646]
	TIME [epoch: 25.3 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042599267443427596		[learning rate: 0.0036989]
	Learning Rate: 0.00369894
	LOSS [training: 0.042599267443427596 | validation: 0.03149245534490611]
	TIME [epoch: 25.3 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028214290965084023		[learning rate: 0.0036859]
	Learning Rate: 0.00368586
	LOSS [training: 0.028214290965084023 | validation: 0.03160739804960509]
	TIME [epoch: 25.3 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04714636931906613		[learning rate: 0.0036728]
	Learning Rate: 0.00367282
	LOSS [training: 0.04714636931906613 | validation: 0.031108042597597557]
	TIME [epoch: 25.3 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04405124553824569		[learning rate: 0.0036598]
	Learning Rate: 0.00365984
	LOSS [training: 0.04405124553824569 | validation: 0.04213966921364833]
	TIME [epoch: 25.3 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03160049437798791		[learning rate: 0.0036469]
	Learning Rate: 0.00364689
	LOSS [training: 0.03160049437798791 | validation: 0.06007642712853581]
	TIME [epoch: 25.3 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048043417547386086		[learning rate: 0.003634]
	Learning Rate: 0.003634
	LOSS [training: 0.048043417547386086 | validation: 0.04055279419762195]
	TIME [epoch: 25.3 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.044646527851891535		[learning rate: 0.0036211]
	Learning Rate: 0.00362115
	LOSS [training: 0.044646527851891535 | validation: 0.0311760033482226]
	TIME [epoch: 25.3 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028524229156495964		[learning rate: 0.0036083]
	Learning Rate: 0.00360834
	LOSS [training: 0.028524229156495964 | validation: 0.035749496375928946]
	TIME [epoch: 25.3 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05559123790575544		[learning rate: 0.0035956]
	Learning Rate: 0.00359558
	LOSS [training: 0.05559123790575544 | validation: 0.04011965140101982]
	TIME [epoch: 25.3 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03312531030277517		[learning rate: 0.0035829]
	Learning Rate: 0.00358287
	LOSS [training: 0.03312531030277517 | validation: 0.036441389242327975]
	TIME [epoch: 25.3 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04780060091717335		[learning rate: 0.0035702]
	Learning Rate: 0.0035702
	LOSS [training: 0.04780060091717335 | validation: 0.04401582908798983]
	TIME [epoch: 25.3 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03929969120121423		[learning rate: 0.0035576]
	Learning Rate: 0.00355757
	LOSS [training: 0.03929969120121423 | validation: 0.05455114163729459]
	TIME [epoch: 25.3 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03432081315975561		[learning rate: 0.003545]
	Learning Rate: 0.00354499
	LOSS [training: 0.03432081315975561 | validation: 0.0253587257743913]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03996816818871719		[learning rate: 0.0035325]
	Learning Rate: 0.00353246
	LOSS [training: 0.03996816818871719 | validation: 0.04578553736153396]
	TIME [epoch: 25.3 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05369160167132705		[learning rate: 0.00352]
	Learning Rate: 0.00351997
	LOSS [training: 0.05369160167132705 | validation: 0.02485967736569862]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03148301605726683		[learning rate: 0.0035075]
	Learning Rate: 0.00350752
	LOSS [training: 0.03148301605726683 | validation: 0.025476865010801836]
	TIME [epoch: 25.3 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026572323478065298		[learning rate: 0.0034951]
	Learning Rate: 0.00349512
	LOSS [training: 0.026572323478065298 | validation: 0.024027790854126922]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034792264075396956		[learning rate: 0.0034828]
	Learning Rate: 0.00348276
	LOSS [training: 0.034792264075396956 | validation: 0.04475254711554865]
	TIME [epoch: 25.3 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0497970865726892		[learning rate: 0.0034704]
	Learning Rate: 0.00347044
	LOSS [training: 0.0497970865726892 | validation: 0.02268570193866843]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05043506651933222		[learning rate: 0.0034582]
	Learning Rate: 0.00345817
	LOSS [training: 0.05043506651933222 | validation: 0.028364782417076934]
	TIME [epoch: 25.3 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.048744313697001565		[learning rate: 0.0034459]
	Learning Rate: 0.00344594
	LOSS [training: 0.048744313697001565 | validation: 0.029328157142259118]
	TIME [epoch: 25.3 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03290715760457756		[learning rate: 0.0034338]
	Learning Rate: 0.00343375
	LOSS [training: 0.03290715760457756 | validation: 0.05771672949967693]
	TIME [epoch: 25.3 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03988616605541111		[learning rate: 0.0034216]
	Learning Rate: 0.00342161
	LOSS [training: 0.03988616605541111 | validation: 0.02505761695606942]
	TIME [epoch: 25.3 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04144374225107103		[learning rate: 0.0034095]
	Learning Rate: 0.00340951
	LOSS [training: 0.04144374225107103 | validation: 0.0582404824205814]
	TIME [epoch: 25.3 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05091594762202374		[learning rate: 0.0033975]
	Learning Rate: 0.00339746
	LOSS [training: 0.05091594762202374 | validation: 0.022777755773740048]
	TIME [epoch: 25.3 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024831692611635953		[learning rate: 0.0033854]
	Learning Rate: 0.00338544
	LOSS [training: 0.024831692611635953 | validation: 0.023251334068543995]
	TIME [epoch: 25.3 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047905075178921105		[learning rate: 0.0033735]
	Learning Rate: 0.00337347
	LOSS [training: 0.047905075178921105 | validation: 0.056608311669632455]
	TIME [epoch: 25.3 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039357283218199035		[learning rate: 0.0033615]
	Learning Rate: 0.00336154
	LOSS [training: 0.039357283218199035 | validation: 0.030840217027246205]
	TIME [epoch: 25.4 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04002060031778205		[learning rate: 0.0033497]
	Learning Rate: 0.00334965
	LOSS [training: 0.04002060031778205 | validation: 0.02523118071374646]
	TIME [epoch: 25.3 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029418997197056904		[learning rate: 0.0033378]
	Learning Rate: 0.00333781
	LOSS [training: 0.029418997197056904 | validation: 0.059119580651401965]
	TIME [epoch: 25.3 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06203360565345664		[learning rate: 0.003326]
	Learning Rate: 0.00332601
	LOSS [training: 0.06203360565345664 | validation: 0.04315933138629703]
	TIME [epoch: 25.3 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03565438042133333		[learning rate: 0.0033142]
	Learning Rate: 0.00331425
	LOSS [training: 0.03565438042133333 | validation: 0.03783032762931694]
	TIME [epoch: 25.3 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04142769652500805		[learning rate: 0.0033025]
	Learning Rate: 0.00330253
	LOSS [training: 0.04142769652500805 | validation: 0.027312719824250046]
	TIME [epoch: 25.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03488721903520354		[learning rate: 0.0032908]
	Learning Rate: 0.00329085
	LOSS [training: 0.03488721903520354 | validation: 0.07228986100932638]
	TIME [epoch: 25.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.047256898150339295		[learning rate: 0.0032792]
	Learning Rate: 0.00327921
	LOSS [training: 0.047256898150339295 | validation: 0.022859598206731307]
	TIME [epoch: 25.3 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.038434492784001226		[learning rate: 0.0032676]
	Learning Rate: 0.00326761
	LOSS [training: 0.038434492784001226 | validation: 0.041808524891511045]
	TIME [epoch: 25.3 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04490491973128247		[learning rate: 0.0032561]
	Learning Rate: 0.00325606
	LOSS [training: 0.04490491973128247 | validation: 0.045702858530731874]
	TIME [epoch: 25.3 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04670664119805053		[learning rate: 0.0032445]
	Learning Rate: 0.00324455
	LOSS [training: 0.04670664119805053 | validation: 0.02824796338310515]
	TIME [epoch: 25.3 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02602424631479806		[learning rate: 0.0032331]
	Learning Rate: 0.00323307
	LOSS [training: 0.02602424631479806 | validation: 0.0225480689295955]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026680795531018278		[learning rate: 0.0032216]
	Learning Rate: 0.00322164
	LOSS [training: 0.026680795531018278 | validation: 0.05585651456550464]
	TIME [epoch: 25.3 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.058121945724125136		[learning rate: 0.0032102]
	Learning Rate: 0.00321025
	LOSS [training: 0.058121945724125136 | validation: 0.035321024901021555]
	TIME [epoch: 25.3 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02981624551900533		[learning rate: 0.0031989]
	Learning Rate: 0.0031989
	LOSS [training: 0.02981624551900533 | validation: 0.024471790477044033]
	TIME [epoch: 25.3 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03795407294445253		[learning rate: 0.0031876]
	Learning Rate: 0.00318758
	LOSS [training: 0.03795407294445253 | validation: 0.02905405713871307]
	TIME [epoch: 25.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.031866392061243295		[learning rate: 0.0031763]
	Learning Rate: 0.00317631
	LOSS [training: 0.031866392061243295 | validation: 0.021137232147691608]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02417556481056148		[learning rate: 0.0031651]
	Learning Rate: 0.00316508
	LOSS [training: 0.02417556481056148 | validation: 0.021779136954083088]
	TIME [epoch: 25.3 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.05049675591006067		[learning rate: 0.0031539]
	Learning Rate: 0.00315389
	LOSS [training: 0.05049675591006067 | validation: 0.025085626457205547]
	TIME [epoch: 25.3 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025062297053434095		[learning rate: 0.0031427]
	Learning Rate: 0.00314273
	LOSS [training: 0.025062297053434095 | validation: 0.02255416733101644]
	TIME [epoch: 25.3 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03481872657056751		[learning rate: 0.0031316]
	Learning Rate: 0.00313162
	LOSS [training: 0.03481872657056751 | validation: 0.022423749233313677]
	TIME [epoch: 25.3 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03592836010143345		[learning rate: 0.0031205]
	Learning Rate: 0.00312055
	LOSS [training: 0.03592836010143345 | validation: 0.020486371676718806]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.052198060001796345		[learning rate: 0.0031095]
	Learning Rate: 0.00310951
	LOSS [training: 0.052198060001796345 | validation: 0.023444156988239463]
	TIME [epoch: 25.3 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024772594553048047		[learning rate: 0.0030985]
	Learning Rate: 0.00309852
	LOSS [training: 0.024772594553048047 | validation: 0.02138116275436227]
	TIME [epoch: 25.3 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030785384667356825		[learning rate: 0.0030876]
	Learning Rate: 0.00308756
	LOSS [training: 0.030785384667356825 | validation: 0.030848138427056505]
	TIME [epoch: 25.3 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04033848974982141		[learning rate: 0.0030766]
	Learning Rate: 0.00307664
	LOSS [training: 0.04033848974982141 | validation: 0.026844989628986526]
	TIME [epoch: 25.3 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027980808540056933		[learning rate: 0.0030658]
	Learning Rate: 0.00306576
	LOSS [training: 0.027980808540056933 | validation: 0.03648410078010127]
	TIME [epoch: 25.3 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045013618501157135		[learning rate: 0.0030549]
	Learning Rate: 0.00305492
	LOSS [training: 0.045013618501157135 | validation: 0.022392520994742652]
	TIME [epoch: 25.3 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022844089328130757		[learning rate: 0.0030441]
	Learning Rate: 0.00304412
	LOSS [training: 0.022844089328130757 | validation: 0.02472201903875051]
	TIME [epoch: 25.3 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039965136590740434		[learning rate: 0.0030334]
	Learning Rate: 0.00303335
	LOSS [training: 0.039965136590740434 | validation: 0.025950583373852507]
	TIME [epoch: 25.3 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02901438789228386		[learning rate: 0.0030226]
	Learning Rate: 0.00302263
	LOSS [training: 0.02901438789228386 | validation: 0.03458911468273987]
	TIME [epoch: 25.3 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03606129283341794		[learning rate: 0.0030119]
	Learning Rate: 0.00301194
	LOSS [training: 0.03606129283341794 | validation: 0.028012365016916632]
	TIME [epoch: 25.3 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037100253389748025		[learning rate: 0.0030013]
	Learning Rate: 0.00300129
	LOSS [training: 0.037100253389748025 | validation: 0.04096449278295053]
	TIME [epoch: 25.3 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03132304075511235		[learning rate: 0.0029907]
	Learning Rate: 0.00299068
	LOSS [training: 0.03132304075511235 | validation: 0.021266059490231113]
	TIME [epoch: 25.3 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026185799316243846		[learning rate: 0.0029801]
	Learning Rate: 0.0029801
	LOSS [training: 0.026185799316243846 | validation: 0.049799945820135416]
	TIME [epoch: 25.3 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04723282421300611		[learning rate: 0.0029696]
	Learning Rate: 0.00296956
	LOSS [training: 0.04723282421300611 | validation: 0.04058326082231308]
	TIME [epoch: 25.3 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03313778075826502		[learning rate: 0.0029591]
	Learning Rate: 0.00295906
	LOSS [training: 0.03313778075826502 | validation: 0.02195555307900876]
	TIME [epoch: 25.3 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02523876039778966		[learning rate: 0.0029486]
	Learning Rate: 0.0029486
	LOSS [training: 0.02523876039778966 | validation: 0.04907349464863298]
	TIME [epoch: 25.3 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04198391713153146		[learning rate: 0.0029382]
	Learning Rate: 0.00293817
	LOSS [training: 0.04198391713153146 | validation: 0.04725184894618337]
	TIME [epoch: 25.3 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.037305711201859604		[learning rate: 0.0029278]
	Learning Rate: 0.00292778
	LOSS [training: 0.037305711201859604 | validation: 0.030451267601275624]
	TIME [epoch: 25.3 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026757202193543803		[learning rate: 0.0029174]
	Learning Rate: 0.00291743
	LOSS [training: 0.026757202193543803 | validation: 0.031562516878008304]
	TIME [epoch: 25.3 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.043057645195359935		[learning rate: 0.0029071]
	Learning Rate: 0.00290711
	LOSS [training: 0.043057645195359935 | validation: 0.033477135724333085]
	TIME [epoch: 25.3 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04003176451517689		[learning rate: 0.0028968]
	Learning Rate: 0.00289683
	LOSS [training: 0.04003176451517689 | validation: 0.02124787802887823]
	TIME [epoch: 25.3 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021790264891896397		[learning rate: 0.0028866]
	Learning Rate: 0.00288659
	LOSS [training: 0.021790264891896397 | validation: 0.02114723543743508]
	TIME [epoch: 25.3 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034386795514377774		[learning rate: 0.0028764]
	Learning Rate: 0.00287638
	LOSS [training: 0.034386795514377774 | validation: 0.038763745952765785]
	TIME [epoch: 25.3 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028608728586025832		[learning rate: 0.0028662]
	Learning Rate: 0.00286621
	LOSS [training: 0.028608728586025832 | validation: 0.018748556967043084]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.045257782140434494		[learning rate: 0.0028561]
	Learning Rate: 0.00285607
	LOSS [training: 0.045257782140434494 | validation: 0.025968673132772818]
	TIME [epoch: 25.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0271431275168781		[learning rate: 0.002846]
	Learning Rate: 0.00284597
	LOSS [training: 0.0271431275168781 | validation: 0.02744131219467055]
	TIME [epoch: 25.3 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02979589087994232		[learning rate: 0.0028359]
	Learning Rate: 0.00283591
	LOSS [training: 0.02979589087994232 | validation: 0.04169763746547327]
	TIME [epoch: 25.3 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02921596004302636		[learning rate: 0.0028259]
	Learning Rate: 0.00282588
	LOSS [training: 0.02921596004302636 | validation: 0.026180984439745134]
	TIME [epoch: 25.3 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02766571665597742		[learning rate: 0.0028159]
	Learning Rate: 0.00281589
	LOSS [training: 0.02766571665597742 | validation: 0.023166970807528102]
	TIME [epoch: 25.3 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04422180261346811		[learning rate: 0.0028059]
	Learning Rate: 0.00280593
	LOSS [training: 0.04422180261346811 | validation: 0.020924378336064267]
	TIME [epoch: 25.3 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026473847026474616		[learning rate: 0.002796]
	Learning Rate: 0.00279601
	LOSS [training: 0.026473847026474616 | validation: 0.0406133785208463]
	TIME [epoch: 25.3 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0329889931223327		[learning rate: 0.0027861]
	Learning Rate: 0.00278612
	LOSS [training: 0.0329889931223327 | validation: 0.027002274103719297]
	TIME [epoch: 25.3 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028451304282363102		[learning rate: 0.0027763]
	Learning Rate: 0.00277627
	LOSS [training: 0.028451304282363102 | validation: 0.02458162876717556]
	TIME [epoch: 25.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027805884679977882		[learning rate: 0.0027665]
	Learning Rate: 0.00276645
	LOSS [training: 0.027805884679977882 | validation: 0.021095711533249087]
	TIME [epoch: 25.3 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02414033272823801		[learning rate: 0.0027567]
	Learning Rate: 0.00275667
	LOSS [training: 0.02414033272823801 | validation: 0.03249195081419299]
	TIME [epoch: 25.3 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04473347522297933		[learning rate: 0.0027469]
	Learning Rate: 0.00274692
	LOSS [training: 0.04473347522297933 | validation: 0.02322444244469817]
	TIME [epoch: 25.3 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0255304546734488		[learning rate: 0.0027372]
	Learning Rate: 0.00273721
	LOSS [training: 0.0255304546734488 | validation: 0.027190795170869142]
	TIME [epoch: 25.3 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025020841460443408		[learning rate: 0.0027275]
	Learning Rate: 0.00272753
	LOSS [training: 0.025020841460443408 | validation: 0.030116523971561875]
	TIME [epoch: 25.3 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028279947157994978		[learning rate: 0.0027179]
	Learning Rate: 0.00271788
	LOSS [training: 0.028279947157994978 | validation: 0.03600832294532043]
	TIME [epoch: 25.3 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.042020527150624064		[learning rate: 0.0027083]
	Learning Rate: 0.00270827
	LOSS [training: 0.042020527150624064 | validation: 0.021542654697523766]
	TIME [epoch: 25.3 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0237244345216416		[learning rate: 0.0026987]
	Learning Rate: 0.0026987
	LOSS [training: 0.0237244345216416 | validation: 0.01926371043016423]
	TIME [epoch: 25.3 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020950400052111073		[learning rate: 0.0026892]
	Learning Rate: 0.00268915
	LOSS [training: 0.020950400052111073 | validation: 0.02070931842574189]
	TIME [epoch: 25.3 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03267630967884382		[learning rate: 0.0026796]
	Learning Rate: 0.00267964
	LOSS [training: 0.03267630967884382 | validation: 0.024919710546054047]
	TIME [epoch: 25.3 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03394898423972868		[learning rate: 0.0026702]
	Learning Rate: 0.00267017
	LOSS [training: 0.03394898423972868 | validation: 0.029304506767750378]
	TIME [epoch: 25.4 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036297471159027435		[learning rate: 0.0026607]
	Learning Rate: 0.00266073
	LOSS [training: 0.036297471159027435 | validation: 0.017882449047141927]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03173214124500078		[learning rate: 0.0026513]
	Learning Rate: 0.00265132
	LOSS [training: 0.03173214124500078 | validation: 0.034868983848962226]
	TIME [epoch: 25.3 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02432926542946839		[learning rate: 0.0026419]
	Learning Rate: 0.00264194
	LOSS [training: 0.02432926542946839 | validation: 0.019913023037322702]
	TIME [epoch: 25.3 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02094120076896962		[learning rate: 0.0026326]
	Learning Rate: 0.0026326
	LOSS [training: 0.02094120076896962 | validation: 0.018456067007517625]
	TIME [epoch: 25.3 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02861367727942462		[learning rate: 0.0026233]
	Learning Rate: 0.00262329
	LOSS [training: 0.02861367727942462 | validation: 0.0431515707480141]
	TIME [epoch: 25.3 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03577823008462315		[learning rate: 0.002614]
	Learning Rate: 0.00261401
	LOSS [training: 0.03577823008462315 | validation: 0.023672694592336584]
	TIME [epoch: 25.3 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0322853739563964		[learning rate: 0.0026048]
	Learning Rate: 0.00260477
	LOSS [training: 0.0322853739563964 | validation: 0.01770188442376813]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021147778588307506		[learning rate: 0.0025956]
	Learning Rate: 0.00259556
	LOSS [training: 0.021147778588307506 | validation: 0.027147352308399333]
	TIME [epoch: 25.3 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03436339366044701		[learning rate: 0.0025864]
	Learning Rate: 0.00258638
	LOSS [training: 0.03436339366044701 | validation: 0.020488172731342166]
	TIME [epoch: 25.3 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.033562479410060936		[learning rate: 0.0025772]
	Learning Rate: 0.00257723
	LOSS [training: 0.033562479410060936 | validation: 0.021272680262051747]
	TIME [epoch: 25.3 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023192146121025756		[learning rate: 0.0025681]
	Learning Rate: 0.00256812
	LOSS [training: 0.023192146121025756 | validation: 0.022994034226251438]
	TIME [epoch: 25.3 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027157928228473207		[learning rate: 0.002559]
	Learning Rate: 0.00255904
	LOSS [training: 0.027157928228473207 | validation: 0.022477289310005505]
	TIME [epoch: 25.3 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024958607052539765		[learning rate: 0.00255]
	Learning Rate: 0.00254999
	LOSS [training: 0.024958607052539765 | validation: 0.02509961442564815]
	TIME [epoch: 25.3 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03469275675915268		[learning rate: 0.002541]
	Learning Rate: 0.00254097
	LOSS [training: 0.03469275675915268 | validation: 0.02041137339931654]
	TIME [epoch: 25.3 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025384704828529024		[learning rate: 0.002532]
	Learning Rate: 0.00253199
	LOSS [training: 0.025384704828529024 | validation: 0.02063992501217511]
	TIME [epoch: 25.3 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03407200786157356		[learning rate: 0.002523]
	Learning Rate: 0.00252303
	LOSS [training: 0.03407200786157356 | validation: 0.01814632451551915]
	TIME [epoch: 25.3 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020217241232754264		[learning rate: 0.0025141]
	Learning Rate: 0.00251411
	LOSS [training: 0.020217241232754264 | validation: 0.019776936088282075]
	TIME [epoch: 25.3 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026944469231651717		[learning rate: 0.0025052]
	Learning Rate: 0.00250522
	LOSS [training: 0.026944469231651717 | validation: 0.022827198454567613]
	TIME [epoch: 25.3 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026318469834433703		[learning rate: 0.0024964]
	Learning Rate: 0.00249636
	LOSS [training: 0.026318469834433703 | validation: 0.03726546784907051]
	TIME [epoch: 25.2 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.04879954160555959		[learning rate: 0.0024875]
	Learning Rate: 0.00248754
	LOSS [training: 0.04879954160555959 | validation: 0.024089644875528025]
	TIME [epoch: 25.3 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02184732059462701		[learning rate: 0.0024787]
	Learning Rate: 0.00247874
	LOSS [training: 0.02184732059462701 | validation: 0.020307881879138553]
	TIME [epoch: 25.3 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024981732055453384		[learning rate: 0.00247]
	Learning Rate: 0.00246997
	LOSS [training: 0.024981732055453384 | validation: 0.018332439559411766]
	TIME [epoch: 25.3 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032673332710712655		[learning rate: 0.0024612]
	Learning Rate: 0.00246124
	LOSS [training: 0.032673332710712655 | validation: 0.030761260752793774]
	TIME [epoch: 25.3 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029748619268909385		[learning rate: 0.0024525]
	Learning Rate: 0.00245254
	LOSS [training: 0.029748619268909385 | validation: 0.024106692715630672]
	TIME [epoch: 25.3 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024147590299941527		[learning rate: 0.0024439]
	Learning Rate: 0.00244386
	LOSS [training: 0.024147590299941527 | validation: 0.01902568688918343]
	TIME [epoch: 25.3 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023906107197526116		[learning rate: 0.0024352]
	Learning Rate: 0.00243522
	LOSS [training: 0.023906107197526116 | validation: 0.031925485063799816]
	TIME [epoch: 25.3 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034647413140717974		[learning rate: 0.0024266]
	Learning Rate: 0.00242661
	LOSS [training: 0.034647413140717974 | validation: 0.02249515800519391]
	TIME [epoch: 25.3 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020932631035963677		[learning rate: 0.002418]
	Learning Rate: 0.00241803
	LOSS [training: 0.020932631035963677 | validation: 0.01838680081409863]
	TIME [epoch: 25.3 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019782917656654037		[learning rate: 0.0024095]
	Learning Rate: 0.00240948
	LOSS [training: 0.019782917656654037 | validation: 0.017622763922540204]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02806083061677397		[learning rate: 0.002401]
	Learning Rate: 0.00240096
	LOSS [training: 0.02806083061677397 | validation: 0.032392311616967664]
	TIME [epoch: 25.3 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029449514355884405		[learning rate: 0.0023925]
	Learning Rate: 0.00239247
	LOSS [training: 0.029449514355884405 | validation: 0.027432552683715188]
	TIME [epoch: 25.3 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030700578068046828		[learning rate: 0.002384]
	Learning Rate: 0.00238401
	LOSS [training: 0.030700578068046828 | validation: 0.018775766734543137]
	TIME [epoch: 25.3 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02011151432750706		[learning rate: 0.0023756]
	Learning Rate: 0.00237558
	LOSS [training: 0.02011151432750706 | validation: 0.018372908110425072]
	TIME [epoch: 25.3 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021623982098219436		[learning rate: 0.0023672]
	Learning Rate: 0.00236718
	LOSS [training: 0.021623982098219436 | validation: 0.04191205148404191]
	TIME [epoch: 25.3 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.039538689256258586		[learning rate: 0.0023588]
	Learning Rate: 0.00235881
	LOSS [training: 0.039538689256258586 | validation: 0.03466093741028069]
	TIME [epoch: 25.3 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03318960446536665		[learning rate: 0.0023505]
	Learning Rate: 0.00235047
	LOSS [training: 0.03318960446536665 | validation: 0.023250947692607524]
	TIME [epoch: 25.3 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024482128613102706		[learning rate: 0.0023422]
	Learning Rate: 0.00234215
	LOSS [training: 0.024482128613102706 | validation: 0.02705921165713708]
	TIME [epoch: 25.3 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025600102159278246		[learning rate: 0.0023339]
	Learning Rate: 0.00233387
	LOSS [training: 0.025600102159278246 | validation: 0.02189684983153105]
	TIME [epoch: 25.3 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027481283524494544		[learning rate: 0.0023256]
	Learning Rate: 0.00232562
	LOSS [training: 0.027481283524494544 | validation: 0.01586927853666921]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023997260660431535		[learning rate: 0.0023174]
	Learning Rate: 0.00231739
	LOSS [training: 0.023997260660431535 | validation: 0.03192966620499935]
	TIME [epoch: 25.3 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025831802479206228		[learning rate: 0.0023092]
	Learning Rate: 0.0023092
	LOSS [training: 0.025831802479206228 | validation: 0.016860565975535022]
	TIME [epoch: 25.3 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0180566001706148		[learning rate: 0.002301]
	Learning Rate: 0.00230103
	LOSS [training: 0.0180566001706148 | validation: 0.0212166038829257]
	TIME [epoch: 25.3 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025707400941562097		[learning rate: 0.0022929]
	Learning Rate: 0.0022929
	LOSS [training: 0.025707400941562097 | validation: 0.035879354493065695]
	TIME [epoch: 25.3 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030600217094604424		[learning rate: 0.0022848]
	Learning Rate: 0.00228479
	LOSS [training: 0.030600217094604424 | validation: 0.019806532551612865]
	TIME [epoch: 25.3 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.028503494065072462		[learning rate: 0.0022767]
	Learning Rate: 0.00227671
	LOSS [training: 0.028503494065072462 | validation: 0.02588899204184278]
	TIME [epoch: 25.3 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02076708249073545		[learning rate: 0.0022687]
	Learning Rate: 0.00226866
	LOSS [training: 0.02076708249073545 | validation: 0.018361027222634087]
	TIME [epoch: 25.3 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019888656455911095		[learning rate: 0.0022606]
	Learning Rate: 0.00226064
	LOSS [training: 0.019888656455911095 | validation: 0.03208930819355074]
	TIME [epoch: 25.3 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026785225442836562		[learning rate: 0.0022526]
	Learning Rate: 0.00225264
	LOSS [training: 0.026785225442836562 | validation: 0.03829998303214013]
	TIME [epoch: 25.3 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03318749676597578		[learning rate: 0.0022447]
	Learning Rate: 0.00224468
	LOSS [training: 0.03318749676597578 | validation: 0.025663266759000467]
	TIME [epoch: 25.3 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02305766706783724		[learning rate: 0.0022367]
	Learning Rate: 0.00223674
	LOSS [training: 0.02305766706783724 | validation: 0.02081176210450589]
	TIME [epoch: 25.3 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022919957401139557		[learning rate: 0.0022288]
	Learning Rate: 0.00222883
	LOSS [training: 0.022919957401139557 | validation: 0.020111464392617455]
	TIME [epoch: 25.3 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02690937852257026		[learning rate: 0.0022209]
	Learning Rate: 0.00222095
	LOSS [training: 0.02690937852257026 | validation: 0.017580172580605503]
	TIME [epoch: 25.3 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018691129841807146		[learning rate: 0.0022131]
	Learning Rate: 0.00221309
	LOSS [training: 0.018691129841807146 | validation: 0.015209318836153226]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.034937832433723906		[learning rate: 0.0022053]
	Learning Rate: 0.00220527
	LOSS [training: 0.034937832433723906 | validation: 0.022219775652966626]
	TIME [epoch: 25.3 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022944780737009045		[learning rate: 0.0021975]
	Learning Rate: 0.00219747
	LOSS [training: 0.022944780737009045 | validation: 0.01800408466375377]
	TIME [epoch: 25.2 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019953064811441195		[learning rate: 0.0021897]
	Learning Rate: 0.0021897
	LOSS [training: 0.019953064811441195 | validation: 0.016589083927999078]
	TIME [epoch: 25.3 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023808700414648255		[learning rate: 0.002182]
	Learning Rate: 0.00218196
	LOSS [training: 0.023808700414648255 | validation: 0.037186582085588955]
	TIME [epoch: 25.2 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030550823129235873		[learning rate: 0.0021742]
	Learning Rate: 0.00217424
	LOSS [training: 0.030550823129235873 | validation: 0.017811959418166087]
	TIME [epoch: 25.3 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02033996616908155		[learning rate: 0.0021666]
	Learning Rate: 0.00216655
	LOSS [training: 0.02033996616908155 | validation: 0.01729426579276075]
	TIME [epoch: 25.2 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0233477538877085		[learning rate: 0.0021589]
	Learning Rate: 0.00215889
	LOSS [training: 0.0233477538877085 | validation: 0.026174148074621543]
	TIME [epoch: 25.3 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.032776902408745645		[learning rate: 0.0021513]
	Learning Rate: 0.00215126
	LOSS [training: 0.032776902408745645 | validation: 0.023231512074881076]
	TIME [epoch: 25.2 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021405891000793688		[learning rate: 0.0021436]
	Learning Rate: 0.00214365
	LOSS [training: 0.021405891000793688 | validation: 0.015343080088905697]
	TIME [epoch: 25.3 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019715752279240627		[learning rate: 0.0021361]
	Learning Rate: 0.00213607
	LOSS [training: 0.019715752279240627 | validation: 0.02227710879380671]
	TIME [epoch: 25.3 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021930396597197994		[learning rate: 0.0021285]
	Learning Rate: 0.00212852
	LOSS [training: 0.021930396597197994 | validation: 0.019654062183928844]
	TIME [epoch: 25.3 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018598882341293827		[learning rate: 0.002121]
	Learning Rate: 0.00212099
	LOSS [training: 0.018598882341293827 | validation: 0.024290830766733038]
	TIME [epoch: 25.2 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03248610840555469		[learning rate: 0.0021135]
	Learning Rate: 0.00211349
	LOSS [training: 0.03248610840555469 | validation: 0.018152074969232063]
	TIME [epoch: 25.3 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021362765168923013		[learning rate: 0.002106]
	Learning Rate: 0.00210602
	LOSS [training: 0.021362765168923013 | validation: 0.020950320691053306]
	TIME [epoch: 25.2 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022465607686422497		[learning rate: 0.0020986]
	Learning Rate: 0.00209857
	LOSS [training: 0.022465607686422497 | validation: 0.018779917300746633]
	TIME [epoch: 25.3 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018300288616663975		[learning rate: 0.0020911]
	Learning Rate: 0.00209115
	LOSS [training: 0.018300288616663975 | validation: 0.01812409927057466]
	TIME [epoch: 25.3 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017589954644215054		[learning rate: 0.0020838]
	Learning Rate: 0.00208375
	LOSS [training: 0.017589954644215054 | validation: 0.01719266662580334]
	TIME [epoch: 25.3 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03209556905801795		[learning rate: 0.0020764]
	Learning Rate: 0.00207638
	LOSS [training: 0.03209556905801795 | validation: 0.023518375492667228]
	TIME [epoch: 25.3 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020205871446873545		[learning rate: 0.002069]
	Learning Rate: 0.00206904
	LOSS [training: 0.020205871446873545 | validation: 0.015791130451715367]
	TIME [epoch: 25.3 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027160462336197927		[learning rate: 0.0020617]
	Learning Rate: 0.00206173
	LOSS [training: 0.027160462336197927 | validation: 0.015334566881550594]
	TIME [epoch: 25.2 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027326733723926334		[learning rate: 0.0020544]
	Learning Rate: 0.00205443
	LOSS [training: 0.027326733723926334 | validation: 0.014540399714614464]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021629303056935046		[learning rate: 0.0020472]
	Learning Rate: 0.00204717
	LOSS [training: 0.021629303056935046 | validation: 0.015370624791917897]
	TIME [epoch: 25.2 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016739312022013952		[learning rate: 0.0020399]
	Learning Rate: 0.00203993
	LOSS [training: 0.016739312022013952 | validation: 0.015271492898602131]
	TIME [epoch: 25.3 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02002079064734743		[learning rate: 0.0020327]
	Learning Rate: 0.00203272
	LOSS [training: 0.02002079064734743 | validation: 0.03672501948266927]
	TIME [epoch: 25.3 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.036217170900213855		[learning rate: 0.0020255]
	Learning Rate: 0.00202553
	LOSS [training: 0.036217170900213855 | validation: 0.025536421274217562]
	TIME [epoch: 142 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02188941721863498		[learning rate: 0.0020184]
	Learning Rate: 0.00201837
	LOSS [training: 0.02188941721863498 | validation: 0.018074593528454447]
	TIME [epoch: 50.2 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01720684000459401		[learning rate: 0.0020112]
	Learning Rate: 0.00201123
	LOSS [training: 0.01720684000459401 | validation: 0.01941533689047098]
	TIME [epoch: 50.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021969581016228645		[learning rate: 0.0020041]
	Learning Rate: 0.00200412
	LOSS [training: 0.021969581016228645 | validation: 0.021458474549377306]
	TIME [epoch: 50.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022902766346114965		[learning rate: 0.001997]
	Learning Rate: 0.00199703
	LOSS [training: 0.022902766346114965 | validation: 0.018843927791083098]
	TIME [epoch: 50.1 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01740675393493111		[learning rate: 0.00199]
	Learning Rate: 0.00198997
	LOSS [training: 0.01740675393493111 | validation: 0.017577164278883242]
	TIME [epoch: 50.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02601599275134594		[learning rate: 0.0019829]
	Learning Rate: 0.00198293
	LOSS [training: 0.02601599275134594 | validation: 0.050120215415881283]
	TIME [epoch: 50.1 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.03258651998490199		[learning rate: 0.0019759]
	Learning Rate: 0.00197592
	LOSS [training: 0.03258651998490199 | validation: 0.016084865565353962]
	TIME [epoch: 50.1 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017428334710646472		[learning rate: 0.0019689]
	Learning Rate: 0.00196893
	LOSS [training: 0.017428334710646472 | validation: 0.018604177653185082]
	TIME [epoch: 50.1 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02135101187884241		[learning rate: 0.001962]
	Learning Rate: 0.00196197
	LOSS [training: 0.02135101187884241 | validation: 0.015196889814266357]
	TIME [epoch: 50.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01969115207687415		[learning rate: 0.001955]
	Learning Rate: 0.00195503
	LOSS [training: 0.01969115207687415 | validation: 0.023117624403636183]
	TIME [epoch: 50.1 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02166020739289387		[learning rate: 0.0019481]
	Learning Rate: 0.00194812
	LOSS [training: 0.02166020739289387 | validation: 0.017959679554414992]
	TIME [epoch: 50.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.027692798035099372		[learning rate: 0.0019412]
	Learning Rate: 0.00194123
	LOSS [training: 0.027692798035099372 | validation: 0.03438631794750735]
	TIME [epoch: 50.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023370930368029348		[learning rate: 0.0019344]
	Learning Rate: 0.00193437
	LOSS [training: 0.023370930368029348 | validation: 0.015624688004058241]
	TIME [epoch: 50.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017446796881711725		[learning rate: 0.0019275]
	Learning Rate: 0.00192752
	LOSS [training: 0.017446796881711725 | validation: 0.015918581244274824]
	TIME [epoch: 50.2 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016322123477485927		[learning rate: 0.0019207]
	Learning Rate: 0.00192071
	LOSS [training: 0.016322123477485927 | validation: 0.019987040140577377]
	TIME [epoch: 50.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.029859891111221396		[learning rate: 0.0019139]
	Learning Rate: 0.00191392
	LOSS [training: 0.029859891111221396 | validation: 0.017308544407951362]
	TIME [epoch: 50.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022607134198825284		[learning rate: 0.0019071]
	Learning Rate: 0.00190715
	LOSS [training: 0.022607134198825284 | validation: 0.01923154678589293]
	TIME [epoch: 50.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02014193062213835		[learning rate: 0.0019004]
	Learning Rate: 0.0019004
	LOSS [training: 0.02014193062213835 | validation: 0.019842642612565284]
	TIME [epoch: 50.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021608598717166024		[learning rate: 0.0018937]
	Learning Rate: 0.00189368
	LOSS [training: 0.021608598717166024 | validation: 0.015455473829518045]
	TIME [epoch: 50.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020855549470333423		[learning rate: 0.001887]
	Learning Rate: 0.00188699
	LOSS [training: 0.020855549470333423 | validation: 0.013978175767715678]
	TIME [epoch: 50.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02788632714135802		[learning rate: 0.0018803]
	Learning Rate: 0.00188032
	LOSS [training: 0.02788632714135802 | validation: 0.01553015148531298]
	TIME [epoch: 50.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01758171710891816		[learning rate: 0.0018737]
	Learning Rate: 0.00187367
	LOSS [training: 0.01758171710891816 | validation: 0.014928968102744175]
	TIME [epoch: 50.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018102801524955244		[learning rate: 0.001867]
	Learning Rate: 0.00186704
	LOSS [training: 0.018102801524955244 | validation: 0.021687626347086236]
	TIME [epoch: 50.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022822237839136828		[learning rate: 0.0018604]
	Learning Rate: 0.00186044
	LOSS [training: 0.022822237839136828 | validation: 0.015209282939854303]
	TIME [epoch: 50.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019141853851985177		[learning rate: 0.0018539]
	Learning Rate: 0.00185386
	LOSS [training: 0.019141853851985177 | validation: 0.014470753915535253]
	TIME [epoch: 50.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020697567266272747		[learning rate: 0.0018473]
	Learning Rate: 0.0018473
	LOSS [training: 0.020697567266272747 | validation: 0.024055749253121657]
	TIME [epoch: 50.1 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02547652085847977		[learning rate: 0.0018408]
	Learning Rate: 0.00184077
	LOSS [training: 0.02547652085847977 | validation: 0.01428067854800222]
	TIME [epoch: 50.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017534174402488476		[learning rate: 0.0018343]
	Learning Rate: 0.00183426
	LOSS [training: 0.017534174402488476 | validation: 0.016661946875160257]
	TIME [epoch: 50.1 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02062092060038738		[learning rate: 0.0018278]
	Learning Rate: 0.00182778
	LOSS [training: 0.02062092060038738 | validation: 0.016222651675779735]
	TIME [epoch: 50.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026174802378077962		[learning rate: 0.0018213]
	Learning Rate: 0.00182131
	LOSS [training: 0.026174802378077962 | validation: 0.017935918875842054]
	TIME [epoch: 50.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023805617314132995		[learning rate: 0.0018149]
	Learning Rate: 0.00181487
	LOSS [training: 0.023805617314132995 | validation: 0.02424981744108868]
	TIME [epoch: 50.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01993240415553282		[learning rate: 0.0018085]
	Learning Rate: 0.00180845
	LOSS [training: 0.01993240415553282 | validation: 0.015851625459315934]
	TIME [epoch: 50.1 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017630495222233235		[learning rate: 0.0018021]
	Learning Rate: 0.00180206
	LOSS [training: 0.017630495222233235 | validation: 0.013635574928838367]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_534.pth
	Model improved!!!
EPOCH 535/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01926305942658172		[learning rate: 0.0017957]
	Learning Rate: 0.00179569
	LOSS [training: 0.01926305942658172 | validation: 0.016332650635516158]
	TIME [epoch: 50.1 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015926010652656405		[learning rate: 0.0017893]
	Learning Rate: 0.00178934
	LOSS [training: 0.015926010652656405 | validation: 0.01487694330405139]
	TIME [epoch: 50.1 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01786956984801395		[learning rate: 0.001783]
	Learning Rate: 0.00178301
	LOSS [training: 0.01786956984801395 | validation: 0.01596884016853166]
	TIME [epoch: 50.1 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02493128735523836		[learning rate: 0.0017767]
	Learning Rate: 0.00177671
	LOSS [training: 0.02493128735523836 | validation: 0.0200866339397551]
	TIME [epoch: 50.1 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01847172198720569		[learning rate: 0.0017704]
	Learning Rate: 0.00177042
	LOSS [training: 0.01847172198720569 | validation: 0.013801602690096317]
	TIME [epoch: 50.1 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020198601453129832		[learning rate: 0.0017642]
	Learning Rate: 0.00176416
	LOSS [training: 0.020198601453129832 | validation: 0.030314301809728134]
	TIME [epoch: 50.1 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02538411325422855		[learning rate: 0.0017579]
	Learning Rate: 0.00175792
	LOSS [training: 0.02538411325422855 | validation: 0.016515037281385272]
	TIME [epoch: 50.1 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016723475438267098		[learning rate: 0.0017517]
	Learning Rate: 0.00175171
	LOSS [training: 0.016723475438267098 | validation: 0.1615960242160255]
	TIME [epoch: 50.1 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.06769318206555722		[learning rate: 0.0017455]
	Learning Rate: 0.00174551
	LOSS [training: 0.06769318206555722 | validation: 0.03146914831280877]
	TIME [epoch: 50.1 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022524949112060027		[learning rate: 0.0017393]
	Learning Rate: 0.00173934
	LOSS [training: 0.022524949112060027 | validation: 0.018912195172926576]
	TIME [epoch: 50.1 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016801450199759323		[learning rate: 0.0017332]
	Learning Rate: 0.00173319
	LOSS [training: 0.016801450199759323 | validation: 0.014449817494996603]
	TIME [epoch: 50.1 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016179501722998698		[learning rate: 0.0017271]
	Learning Rate: 0.00172706
	LOSS [training: 0.016179501722998698 | validation: 0.015562011213286576]
	TIME [epoch: 50.1 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016224074159705074		[learning rate: 0.001721]
	Learning Rate: 0.00172095
	LOSS [training: 0.016224074159705074 | validation: 0.01973805013528658]
	TIME [epoch: 50.1 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024897205494108003		[learning rate: 0.0017149]
	Learning Rate: 0.00171487
	LOSS [training: 0.024897205494108003 | validation: 0.01545924259366592]
	TIME [epoch: 50.1 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019144784186320794		[learning rate: 0.0017088]
	Learning Rate: 0.0017088
	LOSS [training: 0.019144784186320794 | validation: 0.026113155593549812]
	TIME [epoch: 50.1 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021356950746678944		[learning rate: 0.0017028]
	Learning Rate: 0.00170276
	LOSS [training: 0.021356950746678944 | validation: 0.013738843652027002]
	TIME [epoch: 50.1 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02729260715902694		[learning rate: 0.0016967]
	Learning Rate: 0.00169674
	LOSS [training: 0.02729260715902694 | validation: 0.014073366630383935]
	TIME [epoch: 50.1 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01635432264249417		[learning rate: 0.0016907]
	Learning Rate: 0.00169074
	LOSS [training: 0.01635432264249417 | validation: 0.013565294558739949]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01773970822850114		[learning rate: 0.0016848]
	Learning Rate: 0.00168476
	LOSS [training: 0.01773970822850114 | validation: 0.025144976814933894]
	TIME [epoch: 50.1 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019800971986679367		[learning rate: 0.0016788]
	Learning Rate: 0.0016788
	LOSS [training: 0.019800971986679367 | validation: 0.014614036726593088]
	TIME [epoch: 50.1 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025418282494206076		[learning rate: 0.0016729]
	Learning Rate: 0.00167287
	LOSS [training: 0.025418282494206076 | validation: 0.01504773062191611]
	TIME [epoch: 50.1 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01668163171997089		[learning rate: 0.001667]
	Learning Rate: 0.00166695
	LOSS [training: 0.01668163171997089 | validation: 0.013805498867159462]
	TIME [epoch: 50.1 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017895971203341274		[learning rate: 0.0016611]
	Learning Rate: 0.00166106
	LOSS [training: 0.017895971203341274 | validation: 0.018253267222853246]
	TIME [epoch: 50.1 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01742142674981142		[learning rate: 0.0016552]
	Learning Rate: 0.00165518
	LOSS [training: 0.01742142674981142 | validation: 0.01327230192793467]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02634396295751447		[learning rate: 0.0016493]
	Learning Rate: 0.00164933
	LOSS [training: 0.02634396295751447 | validation: 0.015004932426682528]
	TIME [epoch: 50.1 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01859726456994973		[learning rate: 0.0016435]
	Learning Rate: 0.0016435
	LOSS [training: 0.01859726456994973 | validation: 0.014202844347963463]
	TIME [epoch: 50.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016898796476825403		[learning rate: 0.0016377]
	Learning Rate: 0.00163769
	LOSS [training: 0.016898796476825403 | validation: 0.015177066718156766]
	TIME [epoch: 50.2 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01844265962883363		[learning rate: 0.0016319]
	Learning Rate: 0.0016319
	LOSS [training: 0.01844265962883363 | validation: 0.02019746950446547]
	TIME [epoch: 50.1 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02296172223869633		[learning rate: 0.0016261]
	Learning Rate: 0.00162612
	LOSS [training: 0.02296172223869633 | validation: 0.01750440690824174]
	TIME [epoch: 50.2 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0192098338599134		[learning rate: 0.0016204]
	Learning Rate: 0.00162037
	LOSS [training: 0.0192098338599134 | validation: 0.01163462396101565]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01706552518675846		[learning rate: 0.0016146]
	Learning Rate: 0.00161464
	LOSS [training: 0.01706552518675846 | validation: 0.013672047166014356]
	TIME [epoch: 50.1 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014899738410250024		[learning rate: 0.0016089]
	Learning Rate: 0.00160893
	LOSS [training: 0.014899738410250024 | validation: 0.014352798972675884]
	TIME [epoch: 50.1 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02538293870206444		[learning rate: 0.0016032]
	Learning Rate: 0.00160325
	LOSS [training: 0.02538293870206444 | validation: 0.016968721832834956]
	TIME [epoch: 50.1 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017578087198586195		[learning rate: 0.0015976]
	Learning Rate: 0.00159758
	LOSS [training: 0.017578087198586195 | validation: 0.0160406673111592]
	TIME [epoch: 50.1 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015796188119325337		[learning rate: 0.0015919]
	Learning Rate: 0.00159193
	LOSS [training: 0.015796188119325337 | validation: 0.0179952248746804]
	TIME [epoch: 50.1 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016756816874249365		[learning rate: 0.0015863]
	Learning Rate: 0.0015863
	LOSS [training: 0.016756816874249365 | validation: 0.01524705838760953]
	TIME [epoch: 50.1 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021690046676842947		[learning rate: 0.0015807]
	Learning Rate: 0.00158069
	LOSS [training: 0.021690046676842947 | validation: 0.016064483812923264]
	TIME [epoch: 50.2 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01776443839077878		[learning rate: 0.0015751]
	Learning Rate: 0.0015751
	LOSS [training: 0.01776443839077878 | validation: 0.0172747123010907]
	TIME [epoch: 50.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020624518223253833		[learning rate: 0.0015695]
	Learning Rate: 0.00156953
	LOSS [training: 0.020624518223253833 | validation: 0.020195927084383204]
	TIME [epoch: 50.2 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022356287406946306		[learning rate: 0.001564]
	Learning Rate: 0.00156398
	LOSS [training: 0.022356287406946306 | validation: 0.015506623313145252]
	TIME [epoch: 50.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023486906454409155		[learning rate: 0.0015584]
	Learning Rate: 0.00155845
	LOSS [training: 0.023486906454409155 | validation: 0.012343165020611327]
	TIME [epoch: 50.2 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016177881602040704		[learning rate: 0.0015529]
	Learning Rate: 0.00155294
	LOSS [training: 0.016177881602040704 | validation: 0.014129592952909123]
	TIME [epoch: 50.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014592721484384449		[learning rate: 0.0015474]
	Learning Rate: 0.00154745
	LOSS [training: 0.014592721484384449 | validation: 0.01208995832322658]
	TIME [epoch: 50.1 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015159149720858639		[learning rate: 0.001542]
	Learning Rate: 0.00154197
	LOSS [training: 0.015159149720858639 | validation: 0.014025080597325882]
	TIME [epoch: 50.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017352990129907004		[learning rate: 0.0015365]
	Learning Rate: 0.00153652
	LOSS [training: 0.017352990129907004 | validation: 0.020030512795979907]
	TIME [epoch: 50.2 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022625935242267935		[learning rate: 0.0015311]
	Learning Rate: 0.00153109
	LOSS [training: 0.022625935242267935 | validation: 0.016232709721225855]
	TIME [epoch: 50.1 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017571181461814522		[learning rate: 0.0015257]
	Learning Rate: 0.00152567
	LOSS [training: 0.017571181461814522 | validation: 0.012517232374229921]
	TIME [epoch: 50.2 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017700993838317844		[learning rate: 0.0015203]
	Learning Rate: 0.00152028
	LOSS [training: 0.017700993838317844 | validation: 0.021747334442543148]
	TIME [epoch: 50.1 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019109394464355802		[learning rate: 0.0015149]
	Learning Rate: 0.0015149
	LOSS [training: 0.019109394464355802 | validation: 0.018976061380986335]
	TIME [epoch: 50.1 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02121829962457611		[learning rate: 0.0015095]
	Learning Rate: 0.00150955
	LOSS [training: 0.02121829962457611 | validation: 0.013708244590766052]
	TIME [epoch: 50.1 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016781538041070774		[learning rate: 0.0015042]
	Learning Rate: 0.00150421
	LOSS [training: 0.016781538041070774 | validation: 0.015663862274469368]
	TIME [epoch: 50.1 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015490147068708583		[learning rate: 0.0014989]
	Learning Rate: 0.00149889
	LOSS [training: 0.015490147068708583 | validation: 0.01711042884952091]
	TIME [epoch: 50.1 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017173086393125495		[learning rate: 0.0014936]
	Learning Rate: 0.00149359
	LOSS [training: 0.017173086393125495 | validation: 0.020912551125378338]
	TIME [epoch: 50.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02202922524196275		[learning rate: 0.0014883]
	Learning Rate: 0.00148831
	LOSS [training: 0.02202922524196275 | validation: 0.024567976080523683]
	TIME [epoch: 50.1 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020677614381070738		[learning rate: 0.001483]
	Learning Rate: 0.00148304
	LOSS [training: 0.020677614381070738 | validation: 0.012770424021233854]
	TIME [epoch: 50.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01626695400806168		[learning rate: 0.0014778]
	Learning Rate: 0.0014778
	LOSS [training: 0.01626695400806168 | validation: 0.012588368377508142]
	TIME [epoch: 50.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015892595232008148		[learning rate: 0.0014726]
	Learning Rate: 0.00147257
	LOSS [training: 0.015892595232008148 | validation: 0.013226975578314884]
	TIME [epoch: 50.1 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016367154827186717		[learning rate: 0.0014674]
	Learning Rate: 0.00146737
	LOSS [training: 0.016367154827186717 | validation: 0.013527308008717508]
	TIME [epoch: 50.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01781395161044212		[learning rate: 0.0014622]
	Learning Rate: 0.00146218
	LOSS [training: 0.01781395161044212 | validation: 0.02541106555386508]
	TIME [epoch: 50.1 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020089445300752275		[learning rate: 0.001457]
	Learning Rate: 0.00145701
	LOSS [training: 0.020089445300752275 | validation: 0.013353183177233092]
	TIME [epoch: 50.1 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019623210291837445		[learning rate: 0.0014519]
	Learning Rate: 0.00145185
	LOSS [training: 0.019623210291837445 | validation: 0.020318586767456678]
	TIME [epoch: 50.1 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019498572234811037		[learning rate: 0.0014467]
	Learning Rate: 0.00144672
	LOSS [training: 0.019498572234811037 | validation: 0.01630189574329004]
	TIME [epoch: 50.1 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015753518998196814		[learning rate: 0.0014416]
	Learning Rate: 0.0014416
	LOSS [training: 0.015753518998196814 | validation: 0.012624925540924238]
	TIME [epoch: 50.1 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014167813348899914		[learning rate: 0.0014365]
	Learning Rate: 0.00143651
	LOSS [training: 0.014167813348899914 | validation: 0.014908341320425688]
	TIME [epoch: 50.1 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016791510830446485		[learning rate: 0.0014314]
	Learning Rate: 0.00143143
	LOSS [training: 0.016791510830446485 | validation: 0.014247277877738166]
	TIME [epoch: 50.2 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016673572844122405		[learning rate: 0.0014264]
	Learning Rate: 0.00142637
	LOSS [training: 0.016673572844122405 | validation: 0.012550361556869066]
	TIME [epoch: 50.1 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018332522437375286		[learning rate: 0.0014213]
	Learning Rate: 0.00142132
	LOSS [training: 0.018332522437375286 | validation: 0.013306466088073973]
	TIME [epoch: 50.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015204630950794716		[learning rate: 0.0014163]
	Learning Rate: 0.0014163
	LOSS [training: 0.015204630950794716 | validation: 0.013553508184874085]
	TIME [epoch: 50.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018030338588454608		[learning rate: 0.0014113]
	Learning Rate: 0.00141129
	LOSS [training: 0.018030338588454608 | validation: 0.01763548290406866]
	TIME [epoch: 50.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0218567007106465		[learning rate: 0.0014063]
	Learning Rate: 0.0014063
	LOSS [training: 0.0218567007106465 | validation: 0.03927364736187161]
	TIME [epoch: 50.1 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02716041644115527		[learning rate: 0.0014013]
	Learning Rate: 0.00140132
	LOSS [training: 0.02716041644115527 | validation: 0.02069830434690116]
	TIME [epoch: 50.2 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02284883278550966		[learning rate: 0.0013964]
	Learning Rate: 0.00139637
	LOSS [training: 0.02284883278550966 | validation: 0.015793414062408306]
	TIME [epoch: 50.2 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017585310116826673		[learning rate: 0.0013914]
	Learning Rate: 0.00139143
	LOSS [training: 0.017585310116826673 | validation: 0.013309561151330161]
	TIME [epoch: 50.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014653995090871463		[learning rate: 0.0013865]
	Learning Rate: 0.00138651
	LOSS [training: 0.014653995090871463 | validation: 0.01406830846885173]
	TIME [epoch: 50.2 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017695919993239172		[learning rate: 0.0013816]
	Learning Rate: 0.00138161
	LOSS [training: 0.017695919993239172 | validation: 0.014785758409044598]
	TIME [epoch: 50.2 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01763177861502139		[learning rate: 0.0013767]
	Learning Rate: 0.00137672
	LOSS [training: 0.01763177861502139 | validation: 0.021795123958698058]
	TIME [epoch: 50.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017193746046784315		[learning rate: 0.0013719]
	Learning Rate: 0.00137185
	LOSS [training: 0.017193746046784315 | validation: 0.013717789456528488]
	TIME [epoch: 50.2 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015785873875090655		[learning rate: 0.001367]
	Learning Rate: 0.001367
	LOSS [training: 0.015785873875090655 | validation: 0.0224741884188361]
	TIME [epoch: 50.1 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021437779236998425		[learning rate: 0.0013622]
	Learning Rate: 0.00136217
	LOSS [training: 0.021437779236998425 | validation: 0.012131705386622163]
	TIME [epoch: 50.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0154153039445663		[learning rate: 0.0013574]
	Learning Rate: 0.00135735
	LOSS [training: 0.0154153039445663 | validation: 0.01323771837010582]
	TIME [epoch: 50.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014865736292729739		[learning rate: 0.0013526]
	Learning Rate: 0.00135255
	LOSS [training: 0.014865736292729739 | validation: 0.01245627735438599]
	TIME [epoch: 50.2 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01574933485534198		[learning rate: 0.0013478]
	Learning Rate: 0.00134777
	LOSS [training: 0.01574933485534198 | validation: 0.017065262944297374]
	TIME [epoch: 50.2 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019160861501372313		[learning rate: 0.001343]
	Learning Rate: 0.001343
	LOSS [training: 0.019160861501372313 | validation: 0.012515766259941866]
	TIME [epoch: 50.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014332982200589244		[learning rate: 0.0013383]
	Learning Rate: 0.00133825
	LOSS [training: 0.014332982200589244 | validation: 0.012710621200382239]
	TIME [epoch: 50.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016445856541227034		[learning rate: 0.0013335]
	Learning Rate: 0.00133352
	LOSS [training: 0.016445856541227034 | validation: 0.01329077858478985]
	TIME [epoch: 50.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014715230271331765		[learning rate: 0.0013288]
	Learning Rate: 0.00132881
	LOSS [training: 0.014715230271331765 | validation: 0.011639115773968966]
	TIME [epoch: 50.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015923352149181932		[learning rate: 0.0013241]
	Learning Rate: 0.00132411
	LOSS [training: 0.015923352149181932 | validation: 0.016880922792908273]
	TIME [epoch: 50.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01774704396018107		[learning rate: 0.0013194]
	Learning Rate: 0.00131942
	LOSS [training: 0.01774704396018107 | validation: 0.012207307097463956]
	TIME [epoch: 50.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013950494419911655		[learning rate: 0.0013148]
	Learning Rate: 0.00131476
	LOSS [training: 0.013950494419911655 | validation: 0.015375759985128055]
	TIME [epoch: 50.1 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020733498883171733		[learning rate: 0.0013101]
	Learning Rate: 0.00131011
	LOSS [training: 0.020733498883171733 | validation: 0.014442289381434879]
	TIME [epoch: 50.2 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015361433746569604		[learning rate: 0.0013055]
	Learning Rate: 0.00130548
	LOSS [training: 0.015361433746569604 | validation: 0.014055208736353002]
	TIME [epoch: 50.2 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01971275567001898		[learning rate: 0.0013009]
	Learning Rate: 0.00130086
	LOSS [training: 0.01971275567001898 | validation: 0.014690188074811948]
	TIME [epoch: 50.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014201624988421396		[learning rate: 0.0012963]
	Learning Rate: 0.00129626
	LOSS [training: 0.014201624988421396 | validation: 0.02551880337516034]
	TIME [epoch: 50.1 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.024907325398875756		[learning rate: 0.0012917]
	Learning Rate: 0.00129168
	LOSS [training: 0.024907325398875756 | validation: 0.01822017084407212]
	TIME [epoch: 50.2 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01638869274625718		[learning rate: 0.0012871]
	Learning Rate: 0.00128711
	LOSS [training: 0.01638869274625718 | validation: 0.012122132229811266]
	TIME [epoch: 50.1 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013136669834734521		[learning rate: 0.0012826]
	Learning Rate: 0.00128256
	LOSS [training: 0.013136669834734521 | validation: 0.010749529789804578]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_630.pth
	Model improved!!!
EPOCH 631/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01789251621907968		[learning rate: 0.001278]
	Learning Rate: 0.00127802
	LOSS [training: 0.01789251621907968 | validation: 0.022005482052719377]
	TIME [epoch: 50.2 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019538674913241023		[learning rate: 0.0012735]
	Learning Rate: 0.0012735
	LOSS [training: 0.019538674913241023 | validation: 0.011308550408349017]
	TIME [epoch: 50.1 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014222903578337134		[learning rate: 0.001269]
	Learning Rate: 0.001269
	LOSS [training: 0.014222903578337134 | validation: 0.012337336592238606]
	TIME [epoch: 50.5 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012871066268461535		[learning rate: 0.0012645]
	Learning Rate: 0.00126451
	LOSS [training: 0.012871066268461535 | validation: 0.01333115611196077]
	TIME [epoch: 50.2 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015487674767045934		[learning rate: 0.00126]
	Learning Rate: 0.00126004
	LOSS [training: 0.015487674767045934 | validation: 0.014628466904737398]
	TIME [epoch: 50.2 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018950361809004126		[learning rate: 0.0012556]
	Learning Rate: 0.00125559
	LOSS [training: 0.018950361809004126 | validation: 0.012421402898280417]
	TIME [epoch: 50.2 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02024336755187242		[learning rate: 0.0012511]
	Learning Rate: 0.00125115
	LOSS [training: 0.02024336755187242 | validation: 0.012977650835168742]
	TIME [epoch: 50.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013655120164414877		[learning rate: 0.0012467]
	Learning Rate: 0.00124672
	LOSS [training: 0.013655120164414877 | validation: 0.012760508771253483]
	TIME [epoch: 50.2 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017893748761776178		[learning rate: 0.0012423]
	Learning Rate: 0.00124231
	LOSS [training: 0.017893748761776178 | validation: 0.014803864926358759]
	TIME [epoch: 50.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014516514482308434		[learning rate: 0.0012379]
	Learning Rate: 0.00123792
	LOSS [training: 0.014516514482308434 | validation: 0.011916177473531391]
	TIME [epoch: 50.2 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013479137091344929		[learning rate: 0.0012335]
	Learning Rate: 0.00123354
	LOSS [training: 0.013479137091344929 | validation: 0.013971893893516597]
	TIME [epoch: 50.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015334117897796568		[learning rate: 0.0012292]
	Learning Rate: 0.00122918
	LOSS [training: 0.015334117897796568 | validation: 0.013786233129842977]
	TIME [epoch: 50.2 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016450718959490983		[learning rate: 0.0012248]
	Learning Rate: 0.00122483
	LOSS [training: 0.016450718959490983 | validation: 0.01392472301427482]
	TIME [epoch: 50.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014159321838189118		[learning rate: 0.0012205]
	Learning Rate: 0.0012205
	LOSS [training: 0.014159321838189118 | validation: 0.013261464204197592]
	TIME [epoch: 50.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014914999173820538		[learning rate: 0.0012162]
	Learning Rate: 0.00121619
	LOSS [training: 0.014914999173820538 | validation: 0.014479933895148348]
	TIME [epoch: 50.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01750256176862483		[learning rate: 0.0012119]
	Learning Rate: 0.00121189
	LOSS [training: 0.01750256176862483 | validation: 0.01688534293035454]
	TIME [epoch: 50.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015260763267325023		[learning rate: 0.0012076]
	Learning Rate: 0.0012076
	LOSS [training: 0.015260763267325023 | validation: 0.013235693672674449]
	TIME [epoch: 50.2 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01702691811578167		[learning rate: 0.0012033]
	Learning Rate: 0.00120333
	LOSS [training: 0.01702691811578167 | validation: 0.017960390078865886]
	TIME [epoch: 50.2 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016848474174792058		[learning rate: 0.0011991]
	Learning Rate: 0.00119907
	LOSS [training: 0.016848474174792058 | validation: 0.013027348880511292]
	TIME [epoch: 50.2 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014723352577617952		[learning rate: 0.0011948]
	Learning Rate: 0.00119483
	LOSS [training: 0.014723352577617952 | validation: 0.012885630612124964]
	TIME [epoch: 50.2 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01595148793834348		[learning rate: 0.0011906]
	Learning Rate: 0.00119061
	LOSS [training: 0.01595148793834348 | validation: 0.012551176501197036]
	TIME [epoch: 50.2 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01835314174035426		[learning rate: 0.0011864]
	Learning Rate: 0.0011864
	LOSS [training: 0.01835314174035426 | validation: 0.013071690343714863]
	TIME [epoch: 50.2 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013701804477230487		[learning rate: 0.0011822]
	Learning Rate: 0.0011822
	LOSS [training: 0.013701804477230487 | validation: 0.012686794207672616]
	TIME [epoch: 50.2 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01422053831021359		[learning rate: 0.001178]
	Learning Rate: 0.00117802
	LOSS [training: 0.01422053831021359 | validation: 0.026357309716706426]
	TIME [epoch: 50.2 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020022973681771715		[learning rate: 0.0011739]
	Learning Rate: 0.00117386
	LOSS [training: 0.020022973681771715 | validation: 0.01233826783194383]
	TIME [epoch: 50.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0157100311089944		[learning rate: 0.0011697]
	Learning Rate: 0.00116971
	LOSS [training: 0.0157100311089944 | validation: 0.011809697390528411]
	TIME [epoch: 50.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014605714730348048		[learning rate: 0.0011656]
	Learning Rate: 0.00116557
	LOSS [training: 0.014605714730348048 | validation: 0.015426536653340617]
	TIME [epoch: 50.2 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015315355546295125		[learning rate: 0.0011614]
	Learning Rate: 0.00116145
	LOSS [training: 0.015315355546295125 | validation: 0.026208719294342077]
	TIME [epoch: 50.2 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023410315199796014		[learning rate: 0.0011573]
	Learning Rate: 0.00115734
	LOSS [training: 0.023410315199796014 | validation: 0.012720310670439312]
	TIME [epoch: 50.2 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014851775109103805		[learning rate: 0.0011532]
	Learning Rate: 0.00115325
	LOSS [training: 0.014851775109103805 | validation: 0.011603613603599353]
	TIME [epoch: 50.2 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015209508019751143		[learning rate: 0.0011492]
	Learning Rate: 0.00114917
	LOSS [training: 0.015209508019751143 | validation: 0.011343205733502656]
	TIME [epoch: 50.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01406418140560655		[learning rate: 0.0011451]
	Learning Rate: 0.00114511
	LOSS [training: 0.01406418140560655 | validation: 0.016489064832481085]
	TIME [epoch: 50.2 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014678028725276723		[learning rate: 0.0011411]
	Learning Rate: 0.00114106
	LOSS [training: 0.014678028725276723 | validation: 0.01545051258046923]
	TIME [epoch: 50.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022963310023819346		[learning rate: 0.001137]
	Learning Rate: 0.00113702
	LOSS [training: 0.022963310023819346 | validation: 0.012101211469242274]
	TIME [epoch: 50.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01812508164707101		[learning rate: 0.001133]
	Learning Rate: 0.001133
	LOSS [training: 0.01812508164707101 | validation: 0.01371868056866478]
	TIME [epoch: 50.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015555430288141157		[learning rate: 0.001129]
	Learning Rate: 0.001129
	LOSS [training: 0.015555430288141157 | validation: 0.013947327408997501]
	TIME [epoch: 50.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01586422247661846		[learning rate: 0.001125]
	Learning Rate: 0.001125
	LOSS [training: 0.01586422247661846 | validation: 0.013924903560626934]
	TIME [epoch: 50.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014688519870161452		[learning rate: 0.001121]
	Learning Rate: 0.00112103
	LOSS [training: 0.014688519870161452 | validation: 0.012471239869170168]
	TIME [epoch: 50.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013268851870288416		[learning rate: 0.0011171]
	Learning Rate: 0.00111706
	LOSS [training: 0.013268851870288416 | validation: 0.016122648509682137]
	TIME [epoch: 50.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.020622006558037954		[learning rate: 0.0011131]
	Learning Rate: 0.00111311
	LOSS [training: 0.020622006558037954 | validation: 0.01401133599876814]
	TIME [epoch: 50.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02101055697282278		[learning rate: 0.0011092]
	Learning Rate: 0.00110917
	LOSS [training: 0.02101055697282278 | validation: 0.01662593214927808]
	TIME [epoch: 50.2 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014919861237398122		[learning rate: 0.0011053]
	Learning Rate: 0.00110525
	LOSS [training: 0.014919861237398122 | validation: 0.01260006352647367]
	TIME [epoch: 50.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013212666567884236		[learning rate: 0.0011013]
	Learning Rate: 0.00110134
	LOSS [training: 0.013212666567884236 | validation: 0.013006108924767801]
	TIME [epoch: 50.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01272051380281954		[learning rate: 0.0010974]
	Learning Rate: 0.00109745
	LOSS [training: 0.01272051380281954 | validation: 0.01205705106479881]
	TIME [epoch: 50.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013879225928232302		[learning rate: 0.0010936]
	Learning Rate: 0.00109357
	LOSS [training: 0.013879225928232302 | validation: 0.017432063288680785]
	TIME [epoch: 50.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01922056439442914		[learning rate: 0.0010897]
	Learning Rate: 0.0010897
	LOSS [training: 0.01922056439442914 | validation: 0.012203497559935451]
	TIME [epoch: 50.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014747655134674376		[learning rate: 0.0010858]
	Learning Rate: 0.00108585
	LOSS [training: 0.014747655134674376 | validation: 0.011516809156409645]
	TIME [epoch: 50.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014175482464140236		[learning rate: 0.001082]
	Learning Rate: 0.00108201
	LOSS [training: 0.014175482464140236 | validation: 0.011169253807317411]
	TIME [epoch: 50.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013161275979358279		[learning rate: 0.0010782]
	Learning Rate: 0.00107818
	LOSS [training: 0.013161275979358279 | validation: 0.012834399260163743]
	TIME [epoch: 50.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018973337329404837		[learning rate: 0.0010744]
	Learning Rate: 0.00107437
	LOSS [training: 0.018973337329404837 | validation: 0.015605457700796303]
	TIME [epoch: 50.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01921739419040319		[learning rate: 0.0010706]
	Learning Rate: 0.00107057
	LOSS [training: 0.01921739419040319 | validation: 0.013631081083401542]
	TIME [epoch: 50.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012726803387180245		[learning rate: 0.0010668]
	Learning Rate: 0.00106679
	LOSS [training: 0.012726803387180245 | validation: 0.01451304374224534]
	TIME [epoch: 50.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01447664668739249		[learning rate: 0.001063]
	Learning Rate: 0.00106301
	LOSS [training: 0.01447664668739249 | validation: 0.011474080533582268]
	TIME [epoch: 50.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0134782558191245		[learning rate: 0.0010593]
	Learning Rate: 0.00105925
	LOSS [training: 0.0134782558191245 | validation: 0.013410877572716723]
	TIME [epoch: 50.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01442476784374425		[learning rate: 0.0010555]
	Learning Rate: 0.00105551
	LOSS [training: 0.01442476784374425 | validation: 0.011516675491799511]
	TIME [epoch: 50.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013605478262229855		[learning rate: 0.0010518]
	Learning Rate: 0.00105178
	LOSS [training: 0.013605478262229855 | validation: 0.015592372118322019]
	TIME [epoch: 50.2 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.018950838712139154		[learning rate: 0.0010481]
	Learning Rate: 0.00104806
	LOSS [training: 0.018950838712139154 | validation: 0.012379762600505155]
	TIME [epoch: 50.2 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014606614861206017		[learning rate: 0.0010444]
	Learning Rate: 0.00104435
	LOSS [training: 0.014606614861206017 | validation: 0.01349705449495061]
	TIME [epoch: 50.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01542855342154014		[learning rate: 0.0010407]
	Learning Rate: 0.00104066
	LOSS [training: 0.01542855342154014 | validation: 0.013791379910648894]
	TIME [epoch: 50.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013400426714573324		[learning rate: 0.001037]
	Learning Rate: 0.00103698
	LOSS [training: 0.013400426714573324 | validation: 0.0114799152685666]
	TIME [epoch: 50.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015646673694624993		[learning rate: 0.0010333]
	Learning Rate: 0.00103331
	LOSS [training: 0.015646673694624993 | validation: 0.0129560754822628]
	TIME [epoch: 50.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015006422623725223		[learning rate: 0.0010297]
	Learning Rate: 0.00102966
	LOSS [training: 0.015006422623725223 | validation: 0.012026210694287644]
	TIME [epoch: 50.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015860037699414103		[learning rate: 0.001026]
	Learning Rate: 0.00102602
	LOSS [training: 0.015860037699414103 | validation: 0.012572078055841701]
	TIME [epoch: 50.3 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.025403657650507026		[learning rate: 0.0010224]
	Learning Rate: 0.00102239
	LOSS [training: 0.025403657650507026 | validation: 0.016098636629305697]
	TIME [epoch: 50.3 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.030456240648408475		[learning rate: 0.0010188]
	Learning Rate: 0.00101877
	LOSS [training: 0.030456240648408475 | validation: 0.036215645242628054]
	TIME [epoch: 50.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017765660916943107		[learning rate: 0.0010152]
	Learning Rate: 0.00101517
	LOSS [training: 0.017765660916943107 | validation: 0.015611986960069232]
	TIME [epoch: 50.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014523117560489116		[learning rate: 0.0010116]
	Learning Rate: 0.00101158
	LOSS [training: 0.014523117560489116 | validation: 0.012377608565879522]
	TIME [epoch: 50.3 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014472364502689268		[learning rate: 0.001008]
	Learning Rate: 0.001008
	LOSS [training: 0.014472364502689268 | validation: 0.01345091258429124]
	TIME [epoch: 50.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017033118927148823		[learning rate: 0.0010044]
	Learning Rate: 0.00100444
	LOSS [training: 0.017033118927148823 | validation: 0.02621181607110999]
	TIME [epoch: 50.3 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.021930021957823218		[learning rate: 0.0010009]
	Learning Rate: 0.00100089
	LOSS [training: 0.021930021957823218 | validation: 0.018266212627634573]
	TIME [epoch: 50.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01428470280243994		[learning rate: 0.00099735]
	Learning Rate: 0.000997347
	LOSS [training: 0.01428470280243994 | validation: 0.012774854887426334]
	TIME [epoch: 50.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014296551044563864		[learning rate: 0.00099382]
	Learning Rate: 0.00099382
	LOSS [training: 0.014296551044563864 | validation: 0.015048486310825051]
	TIME [epoch: 50.2 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015718876212769385		[learning rate: 0.00099031]
	Learning Rate: 0.000990306
	LOSS [training: 0.015718876212769385 | validation: 0.014688756711556162]
	TIME [epoch: 50.2 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014732355993357812		[learning rate: 0.0009868]
	Learning Rate: 0.000986804
	LOSS [training: 0.014732355993357812 | validation: 0.009852914296280702]
	TIME [epoch: 50.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014864580735954199		[learning rate: 0.00098331]
	Learning Rate: 0.000983314
	LOSS [training: 0.014864580735954199 | validation: 0.012727177619304985]
	TIME [epoch: 50.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013677249000912151		[learning rate: 0.00097984]
	Learning Rate: 0.000979837
	LOSS [training: 0.013677249000912151 | validation: 0.00966137225898024]
	TIME [epoch: 50.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_706.pth
	Model improved!!!
EPOCH 707/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013640431698007535		[learning rate: 0.00097637]
	Learning Rate: 0.000976372
	LOSS [training: 0.013640431698007535 | validation: 0.012388272744895767]
	TIME [epoch: 50.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013240486901224164		[learning rate: 0.00097292]
	Learning Rate: 0.00097292
	LOSS [training: 0.013240486901224164 | validation: 0.014852229308981339]
	TIME [epoch: 50.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01603509859939496		[learning rate: 0.00096948]
	Learning Rate: 0.000969479
	LOSS [training: 0.01603509859939496 | validation: 0.016476007143933463]
	TIME [epoch: 50.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01485543735903187		[learning rate: 0.00096605]
	Learning Rate: 0.000966051
	LOSS [training: 0.01485543735903187 | validation: 0.01119022976553087]
	TIME [epoch: 50.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013498456194844882		[learning rate: 0.00096263]
	Learning Rate: 0.000962635
	LOSS [training: 0.013498456194844882 | validation: 0.01189166686983394]
	TIME [epoch: 50.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014218424867650474		[learning rate: 0.00095923]
	Learning Rate: 0.000959231
	LOSS [training: 0.014218424867650474 | validation: 0.017584285154648945]
	TIME [epoch: 50.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014714699882125808		[learning rate: 0.00095584]
	Learning Rate: 0.000955839
	LOSS [training: 0.014714699882125808 | validation: 0.011864296536890279]
	TIME [epoch: 50.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014321086109549849		[learning rate: 0.00095246]
	Learning Rate: 0.000952459
	LOSS [training: 0.014321086109549849 | validation: 0.014281591240197539]
	TIME [epoch: 50.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014588875797504253		[learning rate: 0.00094909]
	Learning Rate: 0.000949091
	LOSS [training: 0.014588875797504253 | validation: 0.04198306001684124]
	TIME [epoch: 50.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022974535624522986		[learning rate: 0.00094573]
	Learning Rate: 0.000945735
	LOSS [training: 0.022974535624522986 | validation: 0.016692333820531077]
	TIME [epoch: 50.2 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013880671204494186		[learning rate: 0.00094239]
	Learning Rate: 0.00094239
	LOSS [training: 0.013880671204494186 | validation: 0.011577651343145033]
	TIME [epoch: 50.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013890608244141229		[learning rate: 0.00093906]
	Learning Rate: 0.000939058
	LOSS [training: 0.013890608244141229 | validation: 0.012231917823396908]
	TIME [epoch: 50.2 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01288145031606409		[learning rate: 0.00093574]
	Learning Rate: 0.000935737
	LOSS [training: 0.01288145031606409 | validation: 0.013027894473312944]
	TIME [epoch: 50.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01518112052113871		[learning rate: 0.00093243]
	Learning Rate: 0.000932428
	LOSS [training: 0.01518112052113871 | validation: 0.010975940151968448]
	TIME [epoch: 50.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013076148122648108		[learning rate: 0.00092913]
	Learning Rate: 0.000929131
	LOSS [training: 0.013076148122648108 | validation: 0.013106648436248397]
	TIME [epoch: 50.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013067476943094736		[learning rate: 0.00092585]
	Learning Rate: 0.000925845
	LOSS [training: 0.013067476943094736 | validation: 0.012133511050077553]
	TIME [epoch: 50.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014837248818388446		[learning rate: 0.00092257]
	Learning Rate: 0.000922571
	LOSS [training: 0.014837248818388446 | validation: 0.016514379737020097]
	TIME [epoch: 50.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015363152805718122		[learning rate: 0.00091931]
	Learning Rate: 0.000919309
	LOSS [training: 0.015363152805718122 | validation: 0.010381727219944164]
	TIME [epoch: 50.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.022080536562015567		[learning rate: 0.00091606]
	Learning Rate: 0.000916058
	LOSS [training: 0.022080536562015567 | validation: 0.022323717459174282]
	TIME [epoch: 50.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02062696921430661		[learning rate: 0.00091282]
	Learning Rate: 0.000912819
	LOSS [training: 0.02062696921430661 | validation: 0.014454895815080691]
	TIME [epoch: 50.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014167836310143199		[learning rate: 0.00090959]
	Learning Rate: 0.000909591
	LOSS [training: 0.014167836310143199 | validation: 0.0103633830760428]
	TIME [epoch: 50.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014090580852362353		[learning rate: 0.00090637]
	Learning Rate: 0.000906374
	LOSS [training: 0.014090580852362353 | validation: 0.010754584662715722]
	TIME [epoch: 50.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014018568510022958		[learning rate: 0.00090317]
	Learning Rate: 0.00090317
	LOSS [training: 0.014018568510022958 | validation: 0.012940994021508069]
	TIME [epoch: 50.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012241858303783828		[learning rate: 0.00089998]
	Learning Rate: 0.000899976
	LOSS [training: 0.012241858303783828 | validation: 0.013076948478943176]
	TIME [epoch: 50.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013323332981122371		[learning rate: 0.00089679]
	Learning Rate: 0.000896793
	LOSS [training: 0.013323332981122371 | validation: 0.012648290971077244]
	TIME [epoch: 50.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014862877596448991		[learning rate: 0.00089362]
	Learning Rate: 0.000893622
	LOSS [training: 0.014862877596448991 | validation: 0.04980082842226538]
	TIME [epoch: 50.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.026043369784353343		[learning rate: 0.00089046]
	Learning Rate: 0.000890462
	LOSS [training: 0.026043369784353343 | validation: 0.010966125951773183]
	TIME [epoch: 50.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012278477895382929		[learning rate: 0.00088731]
	Learning Rate: 0.000887313
	LOSS [training: 0.012278477895382929 | validation: 0.013357871692972567]
	TIME [epoch: 50.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013038114005991029		[learning rate: 0.00088418]
	Learning Rate: 0.000884176
	LOSS [training: 0.013038114005991029 | validation: 0.010165096430543395]
	TIME [epoch: 50.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013119150638102785		[learning rate: 0.00088105]
	Learning Rate: 0.000881049
	LOSS [training: 0.013119150638102785 | validation: 0.0118374011168254]
	TIME [epoch: 50.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016017597469103292		[learning rate: 0.00087793]
	Learning Rate: 0.000877933
	LOSS [training: 0.016017597469103292 | validation: 0.012797660836890747]
	TIME [epoch: 50.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01409495656964667		[learning rate: 0.00087483]
	Learning Rate: 0.000874829
	LOSS [training: 0.01409495656964667 | validation: 0.01158363124113233]
	TIME [epoch: 50.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014156479203982296		[learning rate: 0.00087174]
	Learning Rate: 0.000871735
	LOSS [training: 0.014156479203982296 | validation: 0.014574795006705731]
	TIME [epoch: 50.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014182881982569284		[learning rate: 0.00086865]
	Learning Rate: 0.000868653
	LOSS [training: 0.014182881982569284 | validation: 0.01830589944176102]
	TIME [epoch: 50.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013191577577806207		[learning rate: 0.00086558]
	Learning Rate: 0.000865581
	LOSS [training: 0.013191577577806207 | validation: 0.01174407897558188]
	TIME [epoch: 50.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017413547291135333		[learning rate: 0.00086252]
	Learning Rate: 0.00086252
	LOSS [training: 0.017413547291135333 | validation: 0.01141684247602032]
	TIME [epoch: 50.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01247045590792952		[learning rate: 0.00085947]
	Learning Rate: 0.00085947
	LOSS [training: 0.01247045590792952 | validation: 0.011568647443740556]
	TIME [epoch: 50.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013341416237009072		[learning rate: 0.00085643]
	Learning Rate: 0.000856431
	LOSS [training: 0.013341416237009072 | validation: 0.013984669693635626]
	TIME [epoch: 50.2 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01400509704519735		[learning rate: 0.0008534]
	Learning Rate: 0.000853403
	LOSS [training: 0.01400509704519735 | validation: 0.010663491372575344]
	TIME [epoch: 50.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011748474261265364		[learning rate: 0.00085038]
	Learning Rate: 0.000850385
	LOSS [training: 0.011748474261265364 | validation: 0.011967044440355868]
	TIME [epoch: 50.2 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015661341915010472		[learning rate: 0.00084738]
	Learning Rate: 0.000847378
	LOSS [training: 0.015661341915010472 | validation: 0.011358855300872902]
	TIME [epoch: 50.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013098609825388333		[learning rate: 0.00084438]
	Learning Rate: 0.000844381
	LOSS [training: 0.013098609825388333 | validation: 0.00955269018031996]
	TIME [epoch: 50.2 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014454684338343533		[learning rate: 0.0008414]
	Learning Rate: 0.000841395
	LOSS [training: 0.014454684338343533 | validation: 0.012113289700999341]
	TIME [epoch: 50.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013981576110728531		[learning rate: 0.00083842]
	Learning Rate: 0.00083842
	LOSS [training: 0.013981576110728531 | validation: 0.011795065891087432]
	TIME [epoch: 50.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.035121187046148986		[learning rate: 0.00083546]
	Learning Rate: 0.000835455
	LOSS [training: 0.035121187046148986 | validation: 0.013917982422791225]
	TIME [epoch: 50.2 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013879008682033667		[learning rate: 0.0008325]
	Learning Rate: 0.000832501
	LOSS [training: 0.013879008682033667 | validation: 0.010552921897152104]
	TIME [epoch: 50.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013823962100466717		[learning rate: 0.00082956]
	Learning Rate: 0.000829557
	LOSS [training: 0.013823962100466717 | validation: 0.011518098711772232]
	TIME [epoch: 50.2 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01253201242790265		[learning rate: 0.00082662]
	Learning Rate: 0.000826623
	LOSS [training: 0.01253201242790265 | validation: 0.011261732283076949]
	TIME [epoch: 50.1 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01318253821670698		[learning rate: 0.0008237]
	Learning Rate: 0.000823701
	LOSS [training: 0.01318253821670698 | validation: 0.011565724180548773]
	TIME [epoch: 50.2 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011843765683232366		[learning rate: 0.00082079]
	Learning Rate: 0.000820788
	LOSS [training: 0.011843765683232366 | validation: 0.010342840761240703]
	TIME [epoch: 50.2 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012050938787350267		[learning rate: 0.00081789]
	Learning Rate: 0.000817885
	LOSS [training: 0.012050938787350267 | validation: 0.010950564522377734]
	TIME [epoch: 50.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014500359893071767		[learning rate: 0.00081499]
	Learning Rate: 0.000814993
	LOSS [training: 0.014500359893071767 | validation: 0.012270526484319021]
	TIME [epoch: 50.1 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012269835679843546		[learning rate: 0.00081211]
	Learning Rate: 0.000812111
	LOSS [training: 0.012269835679843546 | validation: 0.013548559526415907]
	TIME [epoch: 50.2 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017834563874010355		[learning rate: 0.00080924]
	Learning Rate: 0.000809239
	LOSS [training: 0.017834563874010355 | validation: 0.012806241174621293]
	TIME [epoch: 50.2 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013877106223250146		[learning rate: 0.00080638]
	Learning Rate: 0.000806378
	LOSS [training: 0.013877106223250146 | validation: 0.012898504070225802]
	TIME [epoch: 50.2 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01258391534130211		[learning rate: 0.00080353]
	Learning Rate: 0.000803526
	LOSS [training: 0.01258391534130211 | validation: 0.011261083100891085]
	TIME [epoch: 50.1 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013644359350466808		[learning rate: 0.00080068]
	Learning Rate: 0.000800685
	LOSS [training: 0.013644359350466808 | validation: 0.01642302602489868]
	TIME [epoch: 50.1 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.02068764117384768		[learning rate: 0.00079785]
	Learning Rate: 0.000797853
	LOSS [training: 0.02068764117384768 | validation: 0.013233911067188915]
	TIME [epoch: 50.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013343179487049283		[learning rate: 0.00079503]
	Learning Rate: 0.000795032
	LOSS [training: 0.013343179487049283 | validation: 0.014276420975078336]
	TIME [epoch: 50.1 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012688754918215285		[learning rate: 0.00079222]
	Learning Rate: 0.000792221
	LOSS [training: 0.012688754918215285 | validation: 0.012571683761161143]
	TIME [epoch: 50.2 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01211584245745659		[learning rate: 0.00078942]
	Learning Rate: 0.000789419
	LOSS [training: 0.01211584245745659 | validation: 0.011332492861105872]
	TIME [epoch: 50.1 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01333478504794073		[learning rate: 0.00078663]
	Learning Rate: 0.000786628
	LOSS [training: 0.01333478504794073 | validation: 0.011689554763510069]
	TIME [epoch: 50.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015208362599972713		[learning rate: 0.00078385]
	Learning Rate: 0.000783846
	LOSS [training: 0.015208362599972713 | validation: 0.01366376312415085]
	TIME [epoch: 50.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014084193895526561		[learning rate: 0.00078107]
	Learning Rate: 0.000781074
	LOSS [training: 0.014084193895526561 | validation: 0.011139176049126955]
	TIME [epoch: 50.2 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013977455278000676		[learning rate: 0.00077831]
	Learning Rate: 0.000778312
	LOSS [training: 0.013977455278000676 | validation: 0.013774293178389395]
	TIME [epoch: 50.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012804988950608946		[learning rate: 0.00077556]
	Learning Rate: 0.00077556
	LOSS [training: 0.012804988950608946 | validation: 0.011399353221321334]
	TIME [epoch: 50.2 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015349500773575112		[learning rate: 0.00077282]
	Learning Rate: 0.000772817
	LOSS [training: 0.015349500773575112 | validation: 0.013244368689214935]
	TIME [epoch: 50.1 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013923439709656043		[learning rate: 0.00077008]
	Learning Rate: 0.000770085
	LOSS [training: 0.013923439709656043 | validation: 0.010006343551197389]
	TIME [epoch: 50.2 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01436628108934943		[learning rate: 0.00076736]
	Learning Rate: 0.000767362
	LOSS [training: 0.01436628108934943 | validation: 0.013432123864729773]
	TIME [epoch: 50.2 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01489962394802806		[learning rate: 0.00076465]
	Learning Rate: 0.000764648
	LOSS [training: 0.01489962394802806 | validation: 0.013487423452087479]
	TIME [epoch: 50.2 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013864273150633903		[learning rate: 0.00076194]
	Learning Rate: 0.000761944
	LOSS [training: 0.013864273150633903 | validation: 0.011246936033836103]
	TIME [epoch: 50.2 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012054495373045985		[learning rate: 0.00075925]
	Learning Rate: 0.00075925
	LOSS [training: 0.012054495373045985 | validation: 0.011403558254221072]
	TIME [epoch: 50.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012571545806382353		[learning rate: 0.00075656]
	Learning Rate: 0.000756565
	LOSS [training: 0.012571545806382353 | validation: 0.01093448368771936]
	TIME [epoch: 50.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01308018857261217		[learning rate: 0.00075389]
	Learning Rate: 0.00075389
	LOSS [training: 0.01308018857261217 | validation: 0.01380233992270385]
	TIME [epoch: 50.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012666800532944701		[learning rate: 0.00075122]
	Learning Rate: 0.000751224
	LOSS [training: 0.012666800532944701 | validation: 0.012685510895665504]
	TIME [epoch: 50.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01397196647849691		[learning rate: 0.00074857]
	Learning Rate: 0.000748567
	LOSS [training: 0.01397196647849691 | validation: 0.010824894750449897]
	TIME [epoch: 50.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01841026161792288		[learning rate: 0.00074592]
	Learning Rate: 0.00074592
	LOSS [training: 0.01841026161792288 | validation: 0.01418490613771693]
	TIME [epoch: 50.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014721896724974285		[learning rate: 0.00074328]
	Learning Rate: 0.000743282
	LOSS [training: 0.014721896724974285 | validation: 0.011165034045178176]
	TIME [epoch: 50.2 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014796789895412204		[learning rate: 0.00074065]
	Learning Rate: 0.000740654
	LOSS [training: 0.014796789895412204 | validation: 0.011509357198989503]
	TIME [epoch: 50.2 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01473307889164751		[learning rate: 0.00073803]
	Learning Rate: 0.000738035
	LOSS [training: 0.01473307889164751 | validation: 0.011233047503879497]
	TIME [epoch: 50.2 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01239133656772465		[learning rate: 0.00073543]
	Learning Rate: 0.000735425
	LOSS [training: 0.01239133656772465 | validation: 0.011569593370936944]
	TIME [epoch: 50.2 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011958078597916125		[learning rate: 0.00073282]
	Learning Rate: 0.000732824
	LOSS [training: 0.011958078597916125 | validation: 0.01194778912483182]
	TIME [epoch: 50.2 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01744311300975869		[learning rate: 0.00073023]
	Learning Rate: 0.000730233
	LOSS [training: 0.01744311300975869 | validation: 0.01244803283608526]
	TIME [epoch: 50.2 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012864572092561522		[learning rate: 0.00072765]
	Learning Rate: 0.000727651
	LOSS [training: 0.012864572092561522 | validation: 0.01107411279178589]
	TIME [epoch: 50.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014336743039761886		[learning rate: 0.00072508]
	Learning Rate: 0.000725078
	LOSS [training: 0.014336743039761886 | validation: 0.011179864612539376]
	TIME [epoch: 50.2 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012773848569542116		[learning rate: 0.00072251]
	Learning Rate: 0.000722514
	LOSS [training: 0.012773848569542116 | validation: 0.011633973466912305]
	TIME [epoch: 50.1 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01286221620871628		[learning rate: 0.00071996]
	Learning Rate: 0.000719959
	LOSS [training: 0.01286221620871628 | validation: 0.010502830993753811]
	TIME [epoch: 50.1 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011830284923754753		[learning rate: 0.00071741]
	Learning Rate: 0.000717413
	LOSS [training: 0.011830284923754753 | validation: 0.011583313439126268]
	TIME [epoch: 50.1 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014303271833179972		[learning rate: 0.00071488]
	Learning Rate: 0.000714876
	LOSS [training: 0.014303271833179972 | validation: 0.014612330004688433]
	TIME [epoch: 50.2 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013422286021172847		[learning rate: 0.00071235]
	Learning Rate: 0.000712348
	LOSS [training: 0.013422286021172847 | validation: 0.011899203291207677]
	TIME [epoch: 50.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012713570846044409		[learning rate: 0.00070983]
	Learning Rate: 0.000709829
	LOSS [training: 0.012713570846044409 | validation: 0.024005787522305977]
	TIME [epoch: 50.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01935821954092657		[learning rate: 0.00070732]
	Learning Rate: 0.000707319
	LOSS [training: 0.01935821954092657 | validation: 0.01206876334644871]
	TIME [epoch: 50.1 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01311949805708136		[learning rate: 0.00070482]
	Learning Rate: 0.000704818
	LOSS [training: 0.01311949805708136 | validation: 0.011807110861617936]
	TIME [epoch: 50.2 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013105276333153719		[learning rate: 0.00070233]
	Learning Rate: 0.000702326
	LOSS [training: 0.013105276333153719 | validation: 0.010457227339735385]
	TIME [epoch: 50.1 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01604784773562297		[learning rate: 0.00069984]
	Learning Rate: 0.000699842
	LOSS [training: 0.01604784773562297 | validation: 0.014296224100607671]
	TIME [epoch: 50.2 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014237500340083059		[learning rate: 0.00069737]
	Learning Rate: 0.000697367
	LOSS [training: 0.014237500340083059 | validation: 0.011678216752725571]
	TIME [epoch: 50.1 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01199690915558073		[learning rate: 0.0006949]
	Learning Rate: 0.000694901
	LOSS [training: 0.01199690915558073 | validation: 0.011445428985612845]
	TIME [epoch: 50.1 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013601210692721018		[learning rate: 0.00069244]
	Learning Rate: 0.000692444
	LOSS [training: 0.013601210692721018 | validation: 0.011030597595634826]
	TIME [epoch: 50.1 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013709191330962248		[learning rate: 0.00069]
	Learning Rate: 0.000689995
	LOSS [training: 0.013709191330962248 | validation: 0.017005036310543495]
	TIME [epoch: 50.1 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013637674556704307		[learning rate: 0.00068756]
	Learning Rate: 0.000687555
	LOSS [training: 0.013637674556704307 | validation: 0.013046462121577462]
	TIME [epoch: 50.1 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01266698842618521		[learning rate: 0.00068512]
	Learning Rate: 0.000685124
	LOSS [training: 0.01266698842618521 | validation: 0.011286584022880894]
	TIME [epoch: 50.1 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013440558565760732		[learning rate: 0.0006827]
	Learning Rate: 0.000682701
	LOSS [training: 0.013440558565760732 | validation: 0.011293293490764264]
	TIME [epoch: 50.2 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014419475085562445		[learning rate: 0.00068029]
	Learning Rate: 0.000680287
	LOSS [training: 0.014419475085562445 | validation: 0.010593735257034333]
	TIME [epoch: 50.1 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011106355144599712		[learning rate: 0.00067788]
	Learning Rate: 0.000677882
	LOSS [training: 0.011106355144599712 | validation: 0.010184119037450038]
	TIME [epoch: 50.2 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012266378161183417		[learning rate: 0.00067548]
	Learning Rate: 0.000675485
	LOSS [training: 0.012266378161183417 | validation: 0.013561164764965647]
	TIME [epoch: 50.2 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013488857445113368		[learning rate: 0.0006731]
	Learning Rate: 0.000673096
	LOSS [training: 0.013488857445113368 | validation: 0.00998418854890952]
	TIME [epoch: 50.1 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012454439054467723		[learning rate: 0.00067072]
	Learning Rate: 0.000670716
	LOSS [training: 0.012454439054467723 | validation: 0.010485762111898281]
	TIME [epoch: 50.1 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012550574319732308		[learning rate: 0.00066834]
	Learning Rate: 0.000668344
	LOSS [training: 0.012550574319732308 | validation: 0.009687971709218337]
	TIME [epoch: 50.1 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01189009289356507		[learning rate: 0.00066598]
	Learning Rate: 0.000665981
	LOSS [training: 0.01189009289356507 | validation: 0.009941534390016623]
	TIME [epoch: 50.1 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015773765455045545		[learning rate: 0.00066363]
	Learning Rate: 0.000663626
	LOSS [training: 0.015773765455045545 | validation: 0.01394529649091314]
	TIME [epoch: 50.2 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015101595389569348		[learning rate: 0.00066128]
	Learning Rate: 0.000661279
	LOSS [training: 0.015101595389569348 | validation: 0.01231781967448435]
	TIME [epoch: 50.1 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012945316869137812		[learning rate: 0.00065894]
	Learning Rate: 0.000658941
	LOSS [training: 0.012945316869137812 | validation: 0.011124901035589605]
	TIME [epoch: 50.1 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011145119164454183		[learning rate: 0.00065661]
	Learning Rate: 0.00065661
	LOSS [training: 0.011145119164454183 | validation: 0.011291402626876085]
	TIME [epoch: 50.1 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012832383502392467		[learning rate: 0.00065429]
	Learning Rate: 0.000654289
	LOSS [training: 0.012832383502392467 | validation: 0.02095228973161254]
	TIME [epoch: 50.1 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.019760104149833188		[learning rate: 0.00065197]
	Learning Rate: 0.000651975
	LOSS [training: 0.019760104149833188 | validation: 0.010302881409679129]
	TIME [epoch: 50.1 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.023666060423578276		[learning rate: 0.00064967]
	Learning Rate: 0.000649669
	LOSS [training: 0.023666060423578276 | validation: 0.014024265147017923]
	TIME [epoch: 50.2 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014284288310082946		[learning rate: 0.00064737]
	Learning Rate: 0.000647372
	LOSS [training: 0.014284288310082946 | validation: 0.0113994712520675]
	TIME [epoch: 50.2 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011840738641704766		[learning rate: 0.00064508]
	Learning Rate: 0.000645083
	LOSS [training: 0.011840738641704766 | validation: 0.010033657004442625]
	TIME [epoch: 50.2 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011196444147509511		[learning rate: 0.0006428]
	Learning Rate: 0.000642802
	LOSS [training: 0.011196444147509511 | validation: 0.00955599967403757]
	TIME [epoch: 50.1 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012015690722745125		[learning rate: 0.00064053]
	Learning Rate: 0.000640529
	LOSS [training: 0.012015690722745125 | validation: 0.009852595748064137]
	TIME [epoch: 50.2 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012207252788616662		[learning rate: 0.00063826]
	Learning Rate: 0.000638264
	LOSS [training: 0.012207252788616662 | validation: 0.009675689163153197]
	TIME [epoch: 50.2 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011322994685068392		[learning rate: 0.00063601]
	Learning Rate: 0.000636006
	LOSS [training: 0.011322994685068392 | validation: 0.010953638122871695]
	TIME [epoch: 50.1 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011843868921282606		[learning rate: 0.00063376]
	Learning Rate: 0.000633757
	LOSS [training: 0.011843868921282606 | validation: 0.013777324065547434]
	TIME [epoch: 50.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01381929253238686		[learning rate: 0.00063152]
	Learning Rate: 0.000631517
	LOSS [training: 0.01381929253238686 | validation: 0.01167076634613507]
	TIME [epoch: 50.1 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012182560073978244		[learning rate: 0.00062928]
	Learning Rate: 0.000629283
	LOSS [training: 0.012182560073978244 | validation: 0.010006540753297754]
	TIME [epoch: 50.2 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012519167389809181		[learning rate: 0.00062706]
	Learning Rate: 0.000627058
	LOSS [training: 0.012519167389809181 | validation: 0.01793792681589809]
	TIME [epoch: 50.2 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014945598257541201		[learning rate: 0.00062484]
	Learning Rate: 0.000624841
	LOSS [training: 0.014945598257541201 | validation: 0.010853928198295284]
	TIME [epoch: 50.2 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012060590474265557		[learning rate: 0.00062263]
	Learning Rate: 0.000622631
	LOSS [training: 0.012060590474265557 | validation: 0.00932045135805524]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01182619030871476		[learning rate: 0.00062043]
	Learning Rate: 0.000620429
	LOSS [training: 0.01182619030871476 | validation: 0.009130968469746533]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012854352507769083		[learning rate: 0.00061824]
	Learning Rate: 0.000618235
	LOSS [training: 0.012854352507769083 | validation: 0.013234712515734794]
	TIME [epoch: 50.2 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012667370480223285		[learning rate: 0.00061605]
	Learning Rate: 0.000616049
	LOSS [training: 0.012667370480223285 | validation: 0.011559295828842221]
	TIME [epoch: 50 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01214545539202978		[learning rate: 0.00061387]
	Learning Rate: 0.000613871
	LOSS [training: 0.01214545539202978 | validation: 0.012441595902521016]
	TIME [epoch: 50.1 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015002467128057785		[learning rate: 0.0006117]
	Learning Rate: 0.0006117
	LOSS [training: 0.015002467128057785 | validation: 0.010441590120768491]
	TIME [epoch: 50 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.013033334282528371		[learning rate: 0.00060954]
	Learning Rate: 0.000609537
	LOSS [training: 0.013033334282528371 | validation: 0.011245666019581872]
	TIME [epoch: 50.1 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01216213769542563		[learning rate: 0.00060738]
	Learning Rate: 0.000607381
	LOSS [training: 0.01216213769542563 | validation: 0.012161517947413446]
	TIME [epoch: 50.1 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01263640251197601		[learning rate: 0.00060523]
	Learning Rate: 0.000605234
	LOSS [training: 0.01263640251197601 | validation: 0.009192722460888663]
	TIME [epoch: 50.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.017400707670692704		[learning rate: 0.00060309]
	Learning Rate: 0.000603093
	LOSS [training: 0.017400707670692704 | validation: 0.01281243561068022]
	TIME [epoch: 50 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0128759939223362		[learning rate: 0.00060096]
	Learning Rate: 0.000600961
	LOSS [training: 0.0128759939223362 | validation: 0.013434791289021636]
	TIME [epoch: 50.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011707119392972655		[learning rate: 0.00059884]
	Learning Rate: 0.000598836
	LOSS [training: 0.011707119392972655 | validation: 0.009504337269571813]
	TIME [epoch: 50 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011183334728230208		[learning rate: 0.00059672]
	Learning Rate: 0.000596718
	LOSS [training: 0.011183334728230208 | validation: 0.01131673145792121]
	TIME [epoch: 50.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011627722471386638		[learning rate: 0.00059461]
	Learning Rate: 0.000594608
	LOSS [training: 0.011627722471386638 | validation: 0.01022370361292944]
	TIME [epoch: 50 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011268642157067514		[learning rate: 0.00059251]
	Learning Rate: 0.000592505
	LOSS [training: 0.011268642157067514 | validation: 0.021782088569290124]
	TIME [epoch: 50.1 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.016753160510017927		[learning rate: 0.00059041]
	Learning Rate: 0.00059041
	LOSS [training: 0.016753160510017927 | validation: 0.011728488420750577]
	TIME [epoch: 50 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012768594994464035		[learning rate: 0.00058832]
	Learning Rate: 0.000588323
	LOSS [training: 0.012768594994464035 | validation: 0.010004078375110277]
	TIME [epoch: 50 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01225596250988591		[learning rate: 0.00058624]
	Learning Rate: 0.000586242
	LOSS [training: 0.01225596250988591 | validation: 0.010663236336316474]
	TIME [epoch: 50.1 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011454331494841492		[learning rate: 0.00058417]
	Learning Rate: 0.000584169
	LOSS [training: 0.011454331494841492 | validation: 0.011232709899665435]
	TIME [epoch: 50.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011995717946927414		[learning rate: 0.0005821]
	Learning Rate: 0.000582103
	LOSS [training: 0.011995717946927414 | validation: 0.0185221064328691]
	TIME [epoch: 50.1 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.015399406801822352		[learning rate: 0.00058004]
	Learning Rate: 0.000580045
	LOSS [training: 0.015399406801822352 | validation: 0.011832881906098328]
	TIME [epoch: 50.1 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012237425974690295		[learning rate: 0.00057799]
	Learning Rate: 0.000577994
	LOSS [training: 0.012237425974690295 | validation: 0.009976819186088911]
	TIME [epoch: 50.1 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01262001407437799		[learning rate: 0.00057595]
	Learning Rate: 0.00057595
	LOSS [training: 0.01262001407437799 | validation: 0.012047660357541883]
	TIME [epoch: 50.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012381139399320657		[learning rate: 0.00057391]
	Learning Rate: 0.000573913
	LOSS [training: 0.012381139399320657 | validation: 0.010771181189728632]
	TIME [epoch: 50.2 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012615335170803873		[learning rate: 0.00057188]
	Learning Rate: 0.000571884
	LOSS [training: 0.012615335170803873 | validation: 0.010097530697889212]
	TIME [epoch: 50.1 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.014769671840627389		[learning rate: 0.00056986]
	Learning Rate: 0.000569861
	LOSS [training: 0.014769671840627389 | validation: 0.00929770490286111]
	TIME [epoch: 50.1 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012141311318087662		[learning rate: 0.00056785]
	Learning Rate: 0.000567846
	LOSS [training: 0.012141311318087662 | validation: 0.011067123139986574]
	TIME [epoch: 50.1 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011613491193424018		[learning rate: 0.00056584]
	Learning Rate: 0.000565838
	LOSS [training: 0.011613491193424018 | validation: 0.01693074986246372]
	TIME [epoch: 50.1 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01735462641798066		[learning rate: 0.00056384]
	Learning Rate: 0.000563837
	LOSS [training: 0.01735462641798066 | validation: 0.01599454651939612]
	TIME [epoch: 50.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.0141990097848929		[learning rate: 0.00056184]
	Learning Rate: 0.000561844
	LOSS [training: 0.0141990097848929 | validation: 0.01149553521247898]
	TIME [epoch: 50.1 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011015822110097195		[learning rate: 0.00055986]
	Learning Rate: 0.000559857
	LOSS [training: 0.011015822110097195 | validation: 0.009731358165660387]
	TIME [epoch: 50.1 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012231343221524582		[learning rate: 0.00055788]
	Learning Rate: 0.000557877
	LOSS [training: 0.012231343221524582 | validation: 0.011163652595436816]
	TIME [epoch: 50.1 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.01265468605207527		[learning rate: 0.0005559]
	Learning Rate: 0.000555904
	LOSS [training: 0.01265468605207527 | validation: 0.0091238001663908]
	TIME [epoch: 50.1 sec]
	Saving model to: out/model_training/model_phi2_1a_v_mmd1_20241012_114606/states/model_phi2_1a_v_mmd1_866.pth
	Model improved!!!
EPOCH 867/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.011735674613647697		[learning rate: 0.00055394]
	Learning Rate: 0.000553939
	LOSS [training: 0.011735674613647697 | validation: 0.010918661836183861]
	TIME [epoch: 50.1 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 4/4] avg loss: 0.012093696463194373		[learning rate: 0.00055198]
	Learning Rate: 0.00055198
	LOSS [training: 0.012093696463194373 | validation: 0.012717275835772632]
	TIME [epoch: 50.1 sec]
EPOCH 869/2000:
	Training over batches...
