Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3273554503

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.644456401078663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.644456401078663 | validation: 3.985274444157814]
	TIME [epoch: 155 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.065066289848174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.065066289848174 | validation: 4.2589486583811]
	TIME [epoch: 0.703 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.3472358180546715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.3472358180546715 | validation: 3.185176252478049]
	TIME [epoch: 0.679 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.905546355432091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.905546355432091 | validation: 4.095364989642603]
	TIME [epoch: 0.677 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.916180074305529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.916180074305529 | validation: 3.951707183147713]
	TIME [epoch: 0.676 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.830100525149149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.830100525149149 | validation: 3.357049609098124]
	TIME [epoch: 0.675 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.6448430769258895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.6448430769258895 | validation: 2.8747678429469445]
	TIME [epoch: 0.678 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.461818063011269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.461818063011269 | validation: 2.6494647782167506]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.477583554588509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.477583554588509 | validation: 2.6525910424873014]
	TIME [epoch: 0.681 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.333876894342722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.333876894342722 | validation: 2.6253443287278206]
	TIME [epoch: 0.677 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.265691994651108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.265691994651108 | validation: 2.381465279499442]
	TIME [epoch: 0.679 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.164364736177377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.164364736177377 | validation: 2.3252042042753787]
	TIME [epoch: 0.679 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.072534272639134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.072534272639134 | validation: 2.3524927775486164]
	TIME [epoch: 0.68 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.055470305538322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.055470305538322 | validation: 2.999022887349603]
	TIME [epoch: 0.677 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.267147704611856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.267147704611856 | validation: 2.254517671864971]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.877796632467854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.877796632467854 | validation: 2.6487754900223446]
	TIME [epoch: 0.68 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.377431876033565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.377431876033565 | validation: 2.3834662477333537]
	TIME [epoch: 0.676 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.729627665882508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.729627665882508 | validation: 2.1721018382259425]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.693381757077097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.693381757077097 | validation: 1.7709981378353856]
	TIME [epoch: 0.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4429896699923694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4429896699923694 | validation: 1.9174847421173618]
	TIME [epoch: 0.677 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.149338879659906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.149338879659906 | validation: 1.7960063127182513]
	TIME [epoch: 0.676 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.045365901756542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.045365901756542 | validation: 6.871442892142696]
	TIME [epoch: 0.676 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.217177152392142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.217177152392142 | validation: 6.939218843938436]
	TIME [epoch: 0.673 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.34323807182776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.34323807182776 | validation: 6.049507779135841]
	TIME [epoch: 0.674 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.626796590641826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.626796590641826 | validation: 3.3045606531421523]
	TIME [epoch: 0.674 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.590903134341362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.590903134341362 | validation: 2.341546608152826]
	TIME [epoch: 0.675 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2420977049930344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2420977049930344 | validation: 2.4795772541375296]
	TIME [epoch: 0.674 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.351287376701237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.351287376701237 | validation: 2.2123365170594362]
	TIME [epoch: 0.676 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.260097448245166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.260097448245166 | validation: 1.639622119191416]
	TIME [epoch: 0.677 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7809568706495267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7809568706495267 | validation: 1.9589736879763668]
	TIME [epoch: 0.677 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0057184105370705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0057184105370705 | validation: 1.0284526637956974]
	TIME [epoch: 0.674 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4765401823419944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4765401823419944 | validation: 0.8797909551789908]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2722404643108787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2722404643108787 | validation: 1.2268063263797082]
	TIME [epoch: 0.679 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3738660636499804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3738660636499804 | validation: 1.0449235282216849]
	TIME [epoch: 0.678 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4898067460069444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4898067460069444 | validation: 1.0050755221838201]
	TIME [epoch: 0.676 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2133787074460887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2133787074460887 | validation: 0.7257692282684248]
	TIME [epoch: 0.674 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1079874425426122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1079874425426122 | validation: 0.7367778820309678]
	TIME [epoch: 0.678 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0736371367761397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0736371367761397 | validation: 0.8214170453696223]
	TIME [epoch: 0.676 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0471007351882882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0471007351882882 | validation: 0.6981760705507494]
	TIME [epoch: 0.677 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0339070996077169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0339070996077169 | validation: 0.6975000579148148]
	TIME [epoch: 0.679 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0150950556072187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0150950556072187 | validation: 0.6638321366335016]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0328066931842341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0328066931842341 | validation: 0.8728927349404515]
	TIME [epoch: 0.681 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0964086306588947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0964086306588947 | validation: 0.6886062226902215]
	TIME [epoch: 0.678 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26342541128765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.26342541128765 | validation: 1.0452163161935957]
	TIME [epoch: 0.678 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1060588456966767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1060588456966767 | validation: 0.5795794878998122]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0518778717101267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0518778717101267 | validation: 0.7786010145911474]
	TIME [epoch: 0.676 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.973364420892506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.973364420892506 | validation: 0.7144792696566132]
	TIME [epoch: 0.676 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9652266465168494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9652266465168494 | validation: 0.6909608929214746]
	TIME [epoch: 0.677 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9533144645184636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9533144645184636 | validation: 0.6612100679948768]
	TIME [epoch: 0.676 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9620064702973973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9620064702973973 | validation: 0.7763506883755031]
	TIME [epoch: 0.676 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9498756384560412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9498756384560412 | validation: 0.5753025274152366]
	TIME [epoch: 0.681 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9981915727229651		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.9981915727229651 | validation: 1.0024251316342088]
	TIME [epoch: 0.676 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0223539037162517		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.0223539037162517 | validation: 0.588678469002823]
	TIME [epoch: 0.675 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1667771098787336		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.1667771098787336 | validation: 1.2050526197044236]
	TIME [epoch: 0.677 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0579292337820312		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0579292337820312 | validation: 0.5280889149571587]
	TIME [epoch: 0.674 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9199131866317378		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.9199131866317378 | validation: 0.6243957072343047]
	TIME [epoch: 0.676 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704784396049147		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.8704784396049147 | validation: 0.8344192295664966]
	TIME [epoch: 0.675 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912636039520693		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.8912636039520693 | validation: 0.598173081123388]
	TIME [epoch: 0.674 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8921887420880574		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.8921887420880574 | validation: 0.8212193737270813]
	TIME [epoch: 0.674 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869675537497383		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.8869675537497383 | validation: 0.597250305014918]
	TIME [epoch: 0.675 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741320138769115		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.8741320138769115 | validation: 0.6977829515989264]
	TIME [epoch: 0.675 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733106935552866		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.8733106935552866 | validation: 0.7321676121982947]
	TIME [epoch: 0.674 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9232047599097376		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9232047599097376 | validation: 0.7180095346817015]
	TIME [epoch: 0.675 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0209700093725949		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.0209700093725949 | validation: 0.7397299655679479]
	TIME [epoch: 0.675 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678998186751522		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.8678998186751522 | validation: 0.5984414342222136]
	TIME [epoch: 0.672 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418510092716807		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.8418510092716807 | validation: 0.8343177137046219]
	TIME [epoch: 0.673 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8704567590493886		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.8704567590493886 | validation: 0.5702254799042576]
	TIME [epoch: 0.674 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0221872166878598		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.0221872166878598 | validation: 1.4892658316206497]
	TIME [epoch: 0.673 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2236784176068927		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.2236784176068927 | validation: 0.5429172694114117]
	TIME [epoch: 0.673 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9964735125049607		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.9964735125049607 | validation: 0.6230921890501862]
	TIME [epoch: 0.673 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8559844903271393		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.8559844903271393 | validation: 0.9571590563884755]
	TIME [epoch: 0.674 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8978059746668595		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8978059746668595 | validation: 0.5599175424285524]
	TIME [epoch: 0.674 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8687514661795973		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.8687514661795973 | validation: 0.7157708955540527]
	TIME [epoch: 0.673 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8223738043388307		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.8223738043388307 | validation: 0.6724152827456731]
	TIME [epoch: 0.674 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810561045811937		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.810561045811937 | validation: 0.6474922217764612]
	TIME [epoch: 0.674 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236772490374352		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.8236772490374352 | validation: 0.8485690000494487]
	TIME [epoch: 0.672 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8559784041196651		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8559784041196651 | validation: 0.6347465697425365]
	TIME [epoch: 0.673 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636910669276929		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8636910669276929 | validation: 0.9291705664430222]
	TIME [epoch: 0.673 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9119637575576051		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9119637575576051 | validation: 0.6550159054168303]
	TIME [epoch: 0.674 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8438948471521434		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.8438948471521434 | validation: 0.6460140582438547]
	TIME [epoch: 0.673 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8660316907842787		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8660316907842787 | validation: 0.8325584298144201]
	TIME [epoch: 0.675 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9106540419970963		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.9106540419970963 | validation: 0.5397975568263467]
	TIME [epoch: 0.673 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0076949517695954		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.0076949517695954 | validation: 0.8804873314960056]
	TIME [epoch: 0.673 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448533183371327		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8448533183371327 | validation: 0.65110851816256]
	TIME [epoch: 0.672 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8653760447474611		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8653760447474611 | validation: 0.8762362760302372]
	TIME [epoch: 0.674 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953099009263017		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8953099009263017 | validation: 0.6262902514424662]
	TIME [epoch: 0.673 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815040952920306		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.815040952920306 | validation: 0.6423441117758517]
	TIME [epoch: 0.672 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869535371894203		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.7869535371894203 | validation: 0.6989378295280873]
	TIME [epoch: 0.671 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973613309751008		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.7973613309751008 | validation: 0.601182828557]
	TIME [epoch: 0.674 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059912872352433		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8059912872352433 | validation: 0.8473959038866856]
	TIME [epoch: 0.672 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.844064758850401		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.844064758850401 | validation: 0.5636722882741961]
	TIME [epoch: 0.672 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.042195694685284		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.042195694685284 | validation: 1.147310646502202]
	TIME [epoch: 0.674 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.989604600619593		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.989604600619593 | validation: 0.5676278450933682]
	TIME [epoch: 0.674 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8281911250516457		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.8281911250516457 | validation: 0.6729576262302932]
	TIME [epoch: 0.672 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913603989011597		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7913603989011597 | validation: 0.6934218238759265]
	TIME [epoch: 0.674 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823010522638956		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.7823010522638956 | validation: 0.6404771221152896]
	TIME [epoch: 0.674 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896070982178117		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.7896070982178117 | validation: 0.7439389168905882]
	TIME [epoch: 0.675 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347591732055816		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8347591732055816 | validation: 0.7987219943717156]
	TIME [epoch: 0.673 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9495814071909087		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9495814071909087 | validation: 0.9474968028398467]
	TIME [epoch: 0.673 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038066894026068		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 1.038066894026068 | validation: 0.652669304016769]
	TIME [epoch: 0.677 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7737051034776836		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7737051034776836 | validation: 0.7092233208990455]
	TIME [epoch: 0.68 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861670681812098		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.861670681812098 | validation: 0.8854015347043863]
	TIME [epoch: 0.678 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9359884601017433		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9359884601017433 | validation: 0.6555138814118666]
	TIME [epoch: 0.676 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840603026162143		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.7840603026162143 | validation: 0.6315710448468193]
	TIME [epoch: 0.678 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870936219223983		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7870936219223983 | validation: 0.7994478737434184]
	TIME [epoch: 0.674 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8134572693747114		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8134572693747114 | validation: 0.6100693725941438]
	TIME [epoch: 0.674 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235193068311295		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8235193068311295 | validation: 0.9054515020430458]
	TIME [epoch: 0.674 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489258038953398		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8489258038953398 | validation: 0.5399505772130183]
	TIME [epoch: 0.673 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997732231595259		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8997732231595259 | validation: 0.9865800268562368]
	TIME [epoch: 0.673 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9740183755585625		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.9740183755585625 | validation: 0.5565847192499676]
	TIME [epoch: 0.673 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9561957973088607		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.9561957973088607 | validation: 0.6356352612827049]
	TIME [epoch: 0.673 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680940843572057		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.7680940843572057 | validation: 0.7675526761083882]
	TIME [epoch: 0.673 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8318839697406007		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8318839697406007 | validation: 0.57559086975729]
	TIME [epoch: 0.675 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725201544661182		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.8725201544661182 | validation: 0.6711220748115516]
	TIME [epoch: 0.672 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770677308208038		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.770677308208038 | validation: 0.7252716972364752]
	TIME [epoch: 0.673 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784766147982833		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7784766147982833 | validation: 0.5751322314356628]
	TIME [epoch: 0.672 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065529598508161		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8065529598508161 | validation: 0.7773315745092353]
	TIME [epoch: 0.673 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7899154681828879		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.7899154681828879 | validation: 0.5907352494128979]
	TIME [epoch: 0.673 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218708731006581		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8218708731006581 | validation: 0.9803459736898253]
	TIME [epoch: 0.673 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876213507797645		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.876213507797645 | validation: 0.6346481460929252]
	TIME [epoch: 0.674 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346471940861251		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8346471940861251 | validation: 0.8374435117261161]
	TIME [epoch: 0.673 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296593791514983		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8296593791514983 | validation: 0.6865733332177861]
	TIME [epoch: 0.673 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251229832596704		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8251229832596704 | validation: 0.6749812821954004]
	TIME [epoch: 0.676 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8794925155112463		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8794925155112463 | validation: 0.7055539073153579]
	TIME [epoch: 0.675 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867832086433538		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.7867832086433538 | validation: 0.6463420027319553]
	TIME [epoch: 0.675 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581851431623637		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7581851431623637 | validation: 0.636430069476289]
	TIME [epoch: 0.674 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7560400877093737		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.7560400877093737 | validation: 0.6437403790091839]
	TIME [epoch: 0.675 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7558109170625323		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.7558109170625323 | validation: 0.6496858027367582]
	TIME [epoch: 0.675 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643447417876965		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.7643447417876965 | validation: 0.7374431376826603]
	TIME [epoch: 0.673 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7988998137786496		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7988998137786496 | validation: 0.7666895404327905]
	TIME [epoch: 0.673 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8894759430063521		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8894759430063521 | validation: 1.0258012052155914]
	TIME [epoch: 0.673 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646563762127788		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9646563762127788 | validation: 0.6379195183691826]
	TIME [epoch: 0.673 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601562565317712		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7601562565317712 | validation: 0.5759329705270431]
	TIME [epoch: 0.674 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7926488590596277		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.7926488590596277 | validation: 0.9782910216279284]
	TIME [epoch: 0.677 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8725377420157119		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8725377420157119 | validation: 0.5358484179567352]
	TIME [epoch: 0.677 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8457422549021891		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8457422549021891 | validation: 0.7656099853392286]
	TIME [epoch: 0.675 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839150133381506		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.839150133381506 | validation: 0.5867651887978902]
	TIME [epoch: 0.674 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688971122766469		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.8688971122766469 | validation: 0.6496004882326651]
	TIME [epoch: 0.677 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.763115402011123		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.763115402011123 | validation: 0.6911567857148597]
	TIME [epoch: 0.678 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7585507637113378		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.7585507637113378 | validation: 0.5760776501037357]
	TIME [epoch: 0.676 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741291504300424		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7741291504300424 | validation: 0.7414510541064567]
	TIME [epoch: 0.677 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7751542240796997		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7751542240796997 | validation: 0.5690486463141364]
	TIME [epoch: 0.676 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871458162546899		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.7871458162546899 | validation: 0.7316779553222373]
	TIME [epoch: 0.677 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813029142300004		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.7813029142300004 | validation: 0.5882388320552773]
	TIME [epoch: 0.676 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891763727011586		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7891763727011586 | validation: 0.7160698044312221]
	TIME [epoch: 0.676 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7687807755891057		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7687807755891057 | validation: 0.5648418668047589]
	TIME [epoch: 0.678 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765400563657296		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.765400563657296 | validation: 0.8096966616285176]
	TIME [epoch: 0.676 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862069901930422		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.7862069901930422 | validation: 0.5984041910861316]
	TIME [epoch: 0.676 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345344529343632		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8345344529343632 | validation: 1.0406083838766726]
	TIME [epoch: 0.677 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8904492570901406		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8904492570901406 | validation: 0.6447368469732258]
	TIME [epoch: 0.678 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808534559692448		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7808534559692448 | validation: 0.6291105916411196]
	TIME [epoch: 0.675 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291825297756037		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8291825297756037 | validation: 0.7122904026422522]
	TIME [epoch: 0.676 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909743562327244		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7909743562327244 | validation: 0.6462035408638847]
	TIME [epoch: 0.676 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7798920309096248		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7798920309096248 | validation: 0.6322547232433958]
	TIME [epoch: 0.676 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563427696478331		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7563427696478331 | validation: 0.7133022966856762]
	TIME [epoch: 0.675 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557731902804935		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7557731902804935 | validation: 0.6134010134586829]
	TIME [epoch: 0.677 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732361315174282		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7732361315174282 | validation: 0.765934487421108]
	TIME [epoch: 0.677 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7812323105514395		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7812323105514395 | validation: 0.6102795976539678]
	TIME [epoch: 0.676 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7696279367627373		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7696279367627373 | validation: 0.6996165915889256]
	TIME [epoch: 0.675 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7779035397574217		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7779035397574217 | validation: 0.6845025365192486]
	TIME [epoch: 0.678 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7847812173511607		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.7847812173511607 | validation: 0.6368709153141273]
	TIME [epoch: 0.677 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270870015959044		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8270870015959044 | validation: 0.7008662664456017]
	TIME [epoch: 0.677 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602441485721133		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7602441485721133 | validation: 0.5603232271363983]
	TIME [epoch: 0.677 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484659286041324		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7484659286041324 | validation: 0.7334797472177144]
	TIME [epoch: 0.678 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576507162726059		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7576507162726059 | validation: 0.5807801384929778]
	TIME [epoch: 0.676 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633116821972713		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7633116821972713 | validation: 0.821295369737038]
	TIME [epoch: 0.677 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776282452804929		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7776282452804929 | validation: 0.5668723052139286]
	TIME [epoch: 0.676 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593579523194433		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7593579523194433 | validation: 0.7119336944745118]
	TIME [epoch: 0.677 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745492455540465		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.745492455540465 | validation: 0.5749391864988256]
	TIME [epoch: 0.676 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365476371665213		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7365476371665213 | validation: 0.6715282204872661]
	TIME [epoch: 0.677 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728265991275118		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.728265991275118 | validation: 0.569613650024495]
	TIME [epoch: 0.677 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7293510075352697		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7293510075352697 | validation: 0.6937635910965994]
	TIME [epoch: 0.677 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415322879525631		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7415322879525631 | validation: 0.5605061884650667]
	TIME [epoch: 0.675 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7935715971320738		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7935715971320738 | validation: 0.7400965589475754]
	TIME [epoch: 0.676 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459200402352619		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.8459200402352619 | validation: 0.6552126543926718]
	TIME [epoch: 0.677 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685814002159639		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.8685814002159639 | validation: 0.6104661202611623]
	TIME [epoch: 0.677 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654762446378471		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7654762446378471 | validation: 0.9201118322059707]
	TIME [epoch: 0.675 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306371489512586		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8306371489512586 | validation: 0.5506780180906922]
	TIME [epoch: 0.677 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7320945117827335		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7320945117827335 | validation: 0.6207336602647594]
	TIME [epoch: 0.674 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715768321402082		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.715768321402082 | validation: 0.6673823881926266]
	TIME [epoch: 0.673 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265288639475626		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7265288639475626 | validation: 0.5819310548293174]
	TIME [epoch: 0.674 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327134189931852		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7327134189931852 | validation: 0.7101578581218266]
	TIME [epoch: 0.673 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589557111644558		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7589557111644558 | validation: 0.639177888857168]
	TIME [epoch: 0.674 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619924527217729		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7619924527217729 | validation: 0.632923932720124]
	TIME [epoch: 0.673 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7781288445593554		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.7781288445593554 | validation: 0.6414330629020865]
	TIME [epoch: 0.675 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7413617109006997		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7413617109006997 | validation: 0.5852669495379401]
	TIME [epoch: 0.674 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245563884179487		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7245563884179487 | validation: 0.6188968363823486]
	TIME [epoch: 0.674 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203715700629724		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.7203715700629724 | validation: 0.5964867078113655]
	TIME [epoch: 0.674 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7104291199585948		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7104291199585948 | validation: 0.6199874332215294]
	TIME [epoch: 0.675 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223291387653782		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7223291387653782 | validation: 0.6254009112898358]
	TIME [epoch: 0.672 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735813264058526		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.735813264058526 | validation: 0.6269856803245312]
	TIME [epoch: 0.674 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7565699300352671		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7565699300352671 | validation: 0.7932953324412005]
	TIME [epoch: 0.677 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059986505242364		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8059986505242364 | validation: 0.5908634186691176]
	TIME [epoch: 0.68 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530908549403188		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7530908549403188 | validation: 0.6725209191762488]
	TIME [epoch: 0.675 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713930703203363		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.713930703203363 | validation: 0.5659202433215683]
	TIME [epoch: 0.674 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715501206866473		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.715501206866473 | validation: 0.6052147444772223]
	TIME [epoch: 0.677 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692472626259775		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.692472626259775 | validation: 0.5561477626830806]
	TIME [epoch: 0.676 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999492054241541		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6999492054241541 | validation: 0.6568262062816337]
	TIME [epoch: 0.676 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205391671329824		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7205391671329824 | validation: 0.5684254515201959]
	TIME [epoch: 0.677 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7735641319011913		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7735641319011913 | validation: 0.6771838110473859]
	TIME [epoch: 0.676 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532360760590529		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.7532360760590529 | validation: 0.5478239174303677]
	TIME [epoch: 161 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250867303055694		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7250867303055694 | validation: 0.6187431749716092]
	TIME [epoch: 1.34 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6978197876844567		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.6978197876844567 | validation: 0.6020908686538609]
	TIME [epoch: 1.33 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692407580411073		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.692407580411073 | validation: 0.5801198046754646]
	TIME [epoch: 1.33 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6896453201529843		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6896453201529843 | validation: 0.6703398377951351]
	TIME [epoch: 1.33 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118865040616131		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7118865040616131 | validation: 0.6123981459219714]
	TIME [epoch: 1.33 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833877990702961		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.7833877990702961 | validation: 0.8659042366181655]
	TIME [epoch: 1.33 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156841422442906		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.8156841422442906 | validation: 0.6091816891710351]
	TIME [epoch: 1.33 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105147699594574		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7105147699594574 | validation: 0.5011136650223139]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7360497584846262		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7360497584846262 | validation: 0.662593768783855]
	TIME [epoch: 1.33 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7033779997571948		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7033779997571948 | validation: 0.5604814897647382]
	TIME [epoch: 1.33 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830582037285798		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6830582037285798 | validation: 0.5916542873027575]
	TIME [epoch: 1.33 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763612517277586		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6763612517277586 | validation: 0.5842184826937543]
	TIME [epoch: 1.33 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6720227983737502		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.6720227983737502 | validation: 0.5442301481042943]
	TIME [epoch: 1.33 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678098762663094		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.678098762663094 | validation: 0.5789085236412507]
	TIME [epoch: 1.33 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676112415726167		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.676112415726167 | validation: 0.5676365681941763]
	TIME [epoch: 1.33 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726828263337634		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6726828263337634 | validation: 0.5716568252717159]
	TIME [epoch: 1.33 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643986153223529		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6643986153223529 | validation: 0.5925343917755838]
	TIME [epoch: 1.33 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969043065007796		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6969043065007796 | validation: 0.6311670452202528]
	TIME [epoch: 1.33 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224051433588093		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.8224051433588093 | validation: 0.7473995638773645]
	TIME [epoch: 1.33 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858478520221911		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.858478520221911 | validation: 0.5485704531138031]
	TIME [epoch: 1.33 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6811802635025148		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6811802635025148 | validation: 0.6264164605399394]
	TIME [epoch: 1.33 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788783692247173		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.6788783692247173 | validation: 0.5587132985942549]
	TIME [epoch: 1.33 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946855820769469		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6946855820769469 | validation: 0.5414204625154396]
	TIME [epoch: 1.33 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859858928706729		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6859858928706729 | validation: 0.6263145703013805]
	TIME [epoch: 1.33 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894944078876659		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6894944078876659 | validation: 0.5468960183738816]
	TIME [epoch: 1.33 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789136262158306		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6789136262158306 | validation: 0.6421430422553803]
	TIME [epoch: 1.33 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6849796891770277		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6849796891770277 | validation: 0.5445512303218742]
	TIME [epoch: 1.33 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613248546122239		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6613248546122239 | validation: 0.5556625090780173]
	TIME [epoch: 1.33 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646925835656262		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6646925835656262 | validation: 0.595041947305609]
	TIME [epoch: 1.33 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6818456173615097		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.6818456173615097 | validation: 0.5390221519706826]
	TIME [epoch: 1.33 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016044530827423		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7016044530827423 | validation: 0.5816670099771131]
	TIME [epoch: 1.33 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6997312947912269		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6997312947912269 | validation: 0.5744255063358927]
	TIME [epoch: 1.33 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861372242776682		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6861372242776682 | validation: 0.5393234124895618]
	TIME [epoch: 1.33 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730405036341489		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.6730405036341489 | validation: 0.6591032095807496]
	TIME [epoch: 1.33 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764051165599895		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6764051165599895 | validation: 0.5025225594320019]
	TIME [epoch: 1.33 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663063194401129		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6663063194401129 | validation: 0.6047370114997306]
	TIME [epoch: 1.33 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6582417671715698		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.6582417671715698 | validation: 0.5271039614213847]
	TIME [epoch: 1.33 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541332922312542		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6541332922312542 | validation: 0.5476238560055722]
	TIME [epoch: 1.33 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471754541564373		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6471754541564373 | validation: 0.516263145513641]
	TIME [epoch: 1.33 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446806624344756		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6446806624344756 | validation: 0.5533809729861326]
	TIME [epoch: 1.33 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408461731168498		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6408461731168498 | validation: 0.5568441428014063]
	TIME [epoch: 1.33 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730929371534632		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6730929371534632 | validation: 0.5816166210746084]
	TIME [epoch: 1.33 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7471529425216542		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7471529425216542 | validation: 0.5655129000198741]
	TIME [epoch: 1.33 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6971735916938289		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.6971735916938289 | validation: 0.6617167585505288]
	TIME [epoch: 1.33 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587411381851394		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.6587411381851394 | validation: 0.47549996881527323]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6544227601904489		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.6544227601904489 | validation: 0.5696054500822794]
	TIME [epoch: 1.33 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354450891072281		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.6354450891072281 | validation: 0.5624552729053759]
	TIME [epoch: 1.33 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314729274321675		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.6314729274321675 | validation: 0.49808926265475933]
	TIME [epoch: 1.33 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337662827677416		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6337662827677416 | validation: 0.5354186532072026]
	TIME [epoch: 1.33 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276060686427423		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6276060686427423 | validation: 0.5222155172682567]
	TIME [epoch: 1.33 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6364389590360078		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.6364389590360078 | validation: 0.5975814680163488]
	TIME [epoch: 1.33 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6560496829459205		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6560496829459205 | validation: 0.5353987278646681]
	TIME [epoch: 1.33 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710361738212012		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6710361738212012 | validation: 0.5728645745492494]
	TIME [epoch: 1.33 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6638253253410786		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6638253253410786 | validation: 0.5419098063246768]
	TIME [epoch: 1.33 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63791742834146		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.63791742834146 | validation: 0.5045206884018454]
	TIME [epoch: 1.33 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6192903666235116		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.6192903666235116 | validation: 0.5464906792419593]
	TIME [epoch: 1.33 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6118286691078731		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.6118286691078731 | validation: 0.47593293987301594]
	TIME [epoch: 1.33 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143395647503773		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.6143395647503773 | validation: 0.5752805044094335]
	TIME [epoch: 1.33 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6211374155400202		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.6211374155400202 | validation: 0.47975927486483916]
	TIME [epoch: 1.33 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222720999040668		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6222720999040668 | validation: 0.5645937205275869]
	TIME [epoch: 1.33 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6262939705014569		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6262939705014569 | validation: 0.49191946534679004]
	TIME [epoch: 1.33 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332047138328288		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6332047138328288 | validation: 0.5751368575530256]
	TIME [epoch: 1.33 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346013897218747		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6346013897218747 | validation: 0.4835633021817956]
	TIME [epoch: 1.33 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321102528276851		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6321102528276851 | validation: 0.5269126485743943]
	TIME [epoch: 1.33 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022915159846127		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.6022915159846127 | validation: 0.5403695777128911]
	TIME [epoch: 1.33 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5952892626674798		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.5952892626674798 | validation: 0.44824282052421466]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605391840716067		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.605391840716067 | validation: 0.5538668540092111]
	TIME [epoch: 1.33 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6051539850029625		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6051539850029625 | validation: 0.4613006740694939]
	TIME [epoch: 1.33 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5957122062765939		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.5957122062765939 | validation: 0.5532688174405529]
	TIME [epoch: 1.33 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.579841846762118		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.579841846762118 | validation: 0.45777350866056205]
	TIME [epoch: 1.33 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5906269333993426		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5906269333993426 | validation: 0.5434201024224741]
	TIME [epoch: 1.33 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941291246825875		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.5941291246825875 | validation: 0.4662837964240413]
	TIME [epoch: 1.33 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6109771172449594		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6109771172449594 | validation: 0.5856310772372585]
	TIME [epoch: 1.33 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001915257054014		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.6001915257054014 | validation: 0.4373241083971464]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5863986770646727		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.5863986770646727 | validation: 0.5217239517087358]
	TIME [epoch: 1.33 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5638199375820786		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.5638199375820786 | validation: 0.44987159735104376]
	TIME [epoch: 1.33 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5558330842025663		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.5558330842025663 | validation: 0.4968107972988158]
	TIME [epoch: 1.33 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467845611699086		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.5467845611699086 | validation: 0.4437165494869059]
	TIME [epoch: 1.33 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5481149116770913		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5481149116770913 | validation: 0.534389140310575]
	TIME [epoch: 1.33 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5560090601478738		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.5560090601478738 | validation: 0.41153907783898636]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973254719420633		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5973254719420633 | validation: 0.5302911140424195]
	TIME [epoch: 1.33 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5356165988676749		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5356165988676749 | validation: 0.4434475864391562]
	TIME [epoch: 1.33 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5553200274105257		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.5553200274105257 | validation: 0.557405958865418]
	TIME [epoch: 1.33 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5909025976874381		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5909025976874381 | validation: 0.4128010988915349]
	TIME [epoch: 1.33 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6387405958046511		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6387405958046511 | validation: 0.4188280823913779]
	TIME [epoch: 1.33 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5497631920692748		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.5497631920692748 | validation: 0.642512751740441]
	TIME [epoch: 1.33 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6135088436072649		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6135088436072649 | validation: 0.4627211259258324]
	TIME [epoch: 1.33 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6012572613295283		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6012572613295283 | validation: 0.4123650501077835]
	TIME [epoch: 1.33 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543570042191327		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.543570042191327 | validation: 0.5396322703388482]
	TIME [epoch: 1.33 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5344402204831822		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.5344402204831822 | validation: 0.42936418874610816]
	TIME [epoch: 1.33 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5129467057048334		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.5129467057048334 | validation: 0.4381579626336925]
	TIME [epoch: 1.33 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5010386110609061		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5010386110609061 | validation: 0.46718571328308756]
	TIME [epoch: 1.33 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49767963113834823		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.49767963113834823 | validation: 0.4068177994971398]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49833473440758275		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.49833473440758275 | validation: 0.5095440884436598]
	TIME [epoch: 1.33 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.498601008823884		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.498601008823884 | validation: 0.39262626521764343]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5382322781109317		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.5382322781109317 | validation: 0.45018692673369076]
	TIME [epoch: 1.33 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46815902563768397		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.46815902563768397 | validation: 0.4202446432173546]
	TIME [epoch: 1.33 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4658085399449881		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.4658085399449881 | validation: 0.4188056894483653]
	TIME [epoch: 1.33 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.450295447369123		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.450295447369123 | validation: 0.3780807167928173]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46597353435529293		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.46597353435529293 | validation: 0.5652911058403173]
	TIME [epoch: 1.33 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470366500836766		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.5470366500836766 | validation: 0.46361090376116376]
	TIME [epoch: 1.33 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631703572280191		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6631703572280191 | validation: 0.4157064918210418]
	TIME [epoch: 1.33 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6146614708492004		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6146614708492004 | validation: 0.4507844628646729]
	TIME [epoch: 1.33 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4547078273564077		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.4547078273564077 | validation: 0.4639599617394291]
	TIME [epoch: 1.33 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4639081073957869		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.4639081073957869 | validation: 0.36666775765403]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48581085661376167		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.48581085661376167 | validation: 0.42620968114950886]
	TIME [epoch: 1.33 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46118688055037255		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.46118688055037255 | validation: 0.3671615373666286]
	TIME [epoch: 1.33 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.457360864977139		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.457360864977139 | validation: 0.43024313552196036]
	TIME [epoch: 1.33 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4320239100481762		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.4320239100481762 | validation: 0.3479664600856822]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414503437500274		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.4414503437500274 | validation: 0.43104954210479945]
	TIME [epoch: 1.33 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43233764422305854		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.43233764422305854 | validation: 0.3328271023159167]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4487668904917392		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.4487668904917392 | validation: 0.4261927678458638]
	TIME [epoch: 1.33 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43451723610484894		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.43451723610484894 | validation: 0.34703965555976835]
	TIME [epoch: 1.33 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45295336910035405		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.45295336910035405 | validation: 0.42326523795413185]
	TIME [epoch: 1.33 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40215402605113165		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.40215402605113165 | validation: 0.33317209535595876]
	TIME [epoch: 1.33 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40627144401814563		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.40627144401814563 | validation: 0.42830957345478915]
	TIME [epoch: 1.33 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4193232597126504		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.4193232597126504 | validation: 0.35426864805910663]
	TIME [epoch: 1.33 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4723869021159757		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.4723869021159757 | validation: 0.3582789505526968]
	TIME [epoch: 1.33 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698304718982922		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.3698304718982922 | validation: 0.3517356739547259]
	TIME [epoch: 1.33 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36155934660289063		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.36155934660289063 | validation: 0.3305811421823073]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38037284789917336		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.38037284789917336 | validation: 0.43432085658852454]
	TIME [epoch: 1.33 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44625511962312675		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.44625511962312675 | validation: 0.35674269601255676]
	TIME [epoch: 1.33 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5142086834984044		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.5142086834984044 | validation: 0.3081276181596926]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3441019582986106		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.3441019582986106 | validation: 0.5173148079305163]
	TIME [epoch: 1.33 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111878968193492		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.5111878968193492 | validation: 0.3421757228220343]
	TIME [epoch: 1.34 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4383856199734325		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.4383856199734325 | validation: 0.3147444722369362]
	TIME [epoch: 1.33 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32613423418673565		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.32613423418673565 | validation: 0.40899562881688045]
	TIME [epoch: 1.33 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.383021791986268		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.383021791986268 | validation: 0.3175913779518184]
	TIME [epoch: 1.32 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40514797576375017		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.40514797576375017 | validation: 0.3401712814305769]
	TIME [epoch: 1.33 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229679589450488		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.3229679589450488 | validation: 0.32987861595394175]
	TIME [epoch: 1.33 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31594319725813824		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.31594319725813824 | validation: 0.3120579216862225]
	TIME [epoch: 1.33 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3060693245438045		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.3060693245438045 | validation: 0.3502492559435866]
	TIME [epoch: 1.32 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32827588620553444		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.32827588620553444 | validation: 0.30214693713368107]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39418218748914235		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.39418218748914235 | validation: 0.43151791742479323]
	TIME [epoch: 1.33 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974553601147925		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.3974553601147925 | validation: 0.3096049980876127]
	TIME [epoch: 1.33 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36191204782669983		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.36191204782669983 | validation: 0.3313472807684427]
	TIME [epoch: 1.33 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29972274324897114		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.29972274324897114 | validation: 0.29961168583162545]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2752684011705619		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.2752684011705619 | validation: 0.30995342772755397]
	TIME [epoch: 1.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2690446733943958		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.2690446733943958 | validation: 0.27100739993070294]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260838810879835		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.260838810879835 | validation: 0.32767766740774495]
	TIME [epoch: 1.33 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28761372394205753		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.28761372394205753 | validation: 0.32476588708152615]
	TIME [epoch: 1.33 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4347325663721483		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.4347325663721483 | validation: 0.39267947722419216]
	TIME [epoch: 1.33 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3532137807176058		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.3532137807176058 | validation: 0.3067849643161383]
	TIME [epoch: 1.33 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002624716526283		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.3002624716526283 | validation: 0.3287618278647155]
	TIME [epoch: 1.33 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2894841119141806		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.2894841119141806 | validation: 0.29312147239122616]
	TIME [epoch: 1.33 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29900979425062946		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.29900979425062946 | validation: 0.3205982465895493]
	TIME [epoch: 1.33 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27089845303421656		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.27089845303421656 | validation: 0.27196626852779027]
	TIME [epoch: 1.33 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25903227681258933		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.25903227681258933 | validation: 0.2985655486372357]
	TIME [epoch: 1.32 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2457223818922403		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.2457223818922403 | validation: 0.2703653397591141]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780391050921333		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.2780391050921333 | validation: 0.33545474377631473]
	TIME [epoch: 1.33 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3015806128800222		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.3015806128800222 | validation: 0.2718459701475671]
	TIME [epoch: 1.33 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2964587204647975		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.2964587204647975 | validation: 0.31151687617240076]
	TIME [epoch: 1.33 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2514521346812804		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.2514521346812804 | validation: 0.26304942905260464]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26655793509860015		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.26655793509860015 | validation: 0.32247801480576926]
	TIME [epoch: 1.33 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2829812012449114		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.2829812012449114 | validation: 0.2702106240316335]
	TIME [epoch: 1.33 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25953277611471287		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.25953277611471287 | validation: 0.3090441021254135]
	TIME [epoch: 1.33 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24616831298487973		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.24616831298487973 | validation: 0.25339750534633304]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2120990616944443		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.2120990616944443 | validation: 0.2731374885856063]
	TIME [epoch: 1.32 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21398564904199144		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.21398564904199144 | validation: 0.2506766578876203]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22877267095603196		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.22877267095603196 | validation: 0.31019754391050486]
	TIME [epoch: 1.33 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2549528851097324		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.2549528851097324 | validation: 0.2654145471050759]
	TIME [epoch: 1.33 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836076789661682		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.2836076789661682 | validation: 0.306291078715599]
	TIME [epoch: 1.33 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24831636421143632		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.24831636421143632 | validation: 0.23268280678494302]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21618703628123523		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.21618703628123523 | validation: 0.22921965621087279]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17924616158105114		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.17924616158105114 | validation: 0.2313861981664525]
	TIME [epoch: 1.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17000068223696718		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.17000068223696718 | validation: 0.2968902610144361]
	TIME [epoch: 1.33 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19357159132144788		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.19357159132144788 | validation: 0.227920851504885]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1879574531248764		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.1879574531248764 | validation: 0.26700020845646694]
	TIME [epoch: 1.33 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22238378421163765		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.22238378421163765 | validation: 0.28603895533482293]
	TIME [epoch: 1.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36729383004798516		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.36729383004798516 | validation: 0.28303508326448495]
	TIME [epoch: 1.33 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2295943723313264		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.2295943723313264 | validation: 0.20329607280045306]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17476015203432396		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.17476015203432396 | validation: 0.2939775626770805]
	TIME [epoch: 1.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20171621998831427		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.20171621998831427 | validation: 0.23418528991178078]
	TIME [epoch: 1.33 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2215393394988943		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.2215393394988943 | validation: 0.26828124139256876]
	TIME [epoch: 1.33 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2121852211520446		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.2121852211520446 | validation: 0.232967728843574]
	TIME [epoch: 1.33 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17184066258235178		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.17184066258235178 | validation: 0.26259288570230666]
	TIME [epoch: 1.33 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018516890678102		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.2018516890678102 | validation: 0.21475660165635732]
	TIME [epoch: 1.33 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.182339827265767		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.182339827265767 | validation: 0.23772997893596012]
	TIME [epoch: 1.33 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742658278883247		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.1742658278883247 | validation: 0.22286744480160367]
	TIME [epoch: 1.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18109236717878333		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.18109236717878333 | validation: 0.3355125835185322]
	TIME [epoch: 1.33 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29788856221937854		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.29788856221937854 | validation: 0.22462110052629658]
	TIME [epoch: 1.33 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14535122118877733		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.14535122118877733 | validation: 0.20028255740325154]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335655427307504		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1335655427307504 | validation: 0.20406659052276016]
	TIME [epoch: 1.33 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13122243385982402		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.13122243385982402 | validation: 0.19917240371406275]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1550312055321131		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.1550312055321131 | validation: 0.2548584042366974]
	TIME [epoch: 1.33 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23761738006586608		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.23761738006586608 | validation: 0.24051029902258778]
	TIME [epoch: 1.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2819474957932258		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.2819474957932258 | validation: 0.2833854465143057]
	TIME [epoch: 1.33 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1885570789734108		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.1885570789734108 | validation: 0.2106271516478484]
	TIME [epoch: 1.34 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12847571512097047		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.12847571512097047 | validation: 0.184349774466278]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11611547701695862		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.11611547701695862 | validation: 0.1811656465497775]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11463845669311618		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.11463845669311618 | validation: 0.183924343797346]
	TIME [epoch: 1.33 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11205506810678834		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.11205506810678834 | validation: 0.1961190117553486]
	TIME [epoch: 1.33 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11896767974366562		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.11896767974366562 | validation: 0.199190941833826]
	TIME [epoch: 1.34 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16487882055847114		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.16487882055847114 | validation: 0.3881184810298897]
	TIME [epoch: 1.33 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3666931593399811		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.3666931593399811 | validation: 0.17633738501387364]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13754650169093446		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.13754650169093446 | validation: 0.1626546819989542]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10580446775131395		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.10580446775131395 | validation: 0.18122111261552795]
	TIME [epoch: 1.33 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11363256937809166		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.11363256937809166 | validation: 0.17148096030686763]
	TIME [epoch: 1.33 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12648595698487583		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.12648595698487583 | validation: 0.2556031878814313]
	TIME [epoch: 1.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.196682023717089		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.196682023717089 | validation: 0.19640845667720103]
	TIME [epoch: 1.34 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21627394318313045		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.21627394318313045 | validation: 0.19464858284993625]
	TIME [epoch: 1.33 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17683338484931388		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.17683338484931388 | validation: 0.17073088172439407]
	TIME [epoch: 1.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12090365589714841		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.12090365589714841 | validation: 0.18803507755690546]
	TIME [epoch: 1.33 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10949181456596169		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.10949181456596169 | validation: 0.1656746732809669]
	TIME [epoch: 1.33 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12588582231506124		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.12588582231506124 | validation: 0.2525548429748442]
	TIME [epoch: 1.33 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1712169884933377		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.1712169884933377 | validation: 0.19524005685647883]
	TIME [epoch: 1.33 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1917876641997475		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.1917876641997475 | validation: 0.23351425344086546]
	TIME [epoch: 1.33 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16637748337377478		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.16637748337377478 | validation: 0.1763914058593667]
	TIME [epoch: 1.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10378591712984556		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.10378591712984556 | validation: 0.14998313733442847]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09513316488366752		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.09513316488366752 | validation: 0.16222786536784675]
	TIME [epoch: 1.33 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0929878780645242		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.0929878780645242 | validation: 0.15131938119149912]
	TIME [epoch: 1.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796285457547889		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.09796285457547889 | validation: 0.17236264192793774]
	TIME [epoch: 1.33 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306807788200706		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.1306807788200706 | validation: 0.19862509462123254]
	TIME [epoch: 1.33 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651459817543982		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.2651459817543982 | validation: 0.24233654935875273]
	TIME [epoch: 1.33 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1888651249216746		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.1888651249216746 | validation: 0.15498748424067776]
	TIME [epoch: 1.33 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10603075795776143		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.10603075795776143 | validation: 0.1838075052186865]
	TIME [epoch: 1.33 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09079211815957884		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.09079211815957884 | validation: 0.13430619879163924]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0994306072192549		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.0994306072192549 | validation: 0.18604483175259431]
	TIME [epoch: 1.33 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12492702094360283		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.12492702094360283 | validation: 0.1714337787436095]
	TIME [epoch: 1.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16545045569684314		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.16545045569684314 | validation: 0.41369156024746384]
	TIME [epoch: 1.33 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34560677136686485		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.34560677136686485 | validation: 0.20737002262926782]
	TIME [epoch: 1.33 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788829190087249		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.09788829190087249 | validation: 0.19114727738111625]
	TIME [epoch: 1.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19805201340950185		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.19805201340950185 | validation: 0.2160173259873307]
	TIME [epoch: 1.33 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15083531348493512		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.15083531348493512 | validation: 0.18703683732538395]
	TIME [epoch: 1.33 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09225535771164896		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.09225535771164896 | validation: 0.1486533794326745]
	TIME [epoch: 1.33 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11932089925653266		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.11932089925653266 | validation: 0.18548873917823772]
	TIME [epoch: 1.33 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13744792480504772		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.13744792480504772 | validation: 0.1707731795106467]
	TIME [epoch: 1.33 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175903644504947		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.10175903644504947 | validation: 0.14869899985759424]
	TIME [epoch: 1.33 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464687144197408		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.09464687144197408 | validation: 0.14781854132777447]
	TIME [epoch: 1.33 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0864310866503518		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.0864310866503518 | validation: 0.22987868560759353]
	TIME [epoch: 1.33 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12175603002285204		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.12175603002285204 | validation: 0.15287290637691198]
	TIME [epoch: 1.33 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0999104568553545		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.0999104568553545 | validation: 0.1501111512648825]
	TIME [epoch: 1.33 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671730365240899		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.09671730365240899 | validation: 0.1620623270217809]
	TIME [epoch: 1.33 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12529154255598254		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.12529154255598254 | validation: 0.174447341365716]
	TIME [epoch: 1.33 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13468777501374823		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.13468777501374823 | validation: 0.14402034664128213]
	TIME [epoch: 1.33 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10789104142560958		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.10789104142560958 | validation: 0.15747505102020604]
	TIME [epoch: 1.33 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08574494895288601		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.08574494895288601 | validation: 0.15186185109665212]
	TIME [epoch: 1.33 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08933730509454942		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.08933730509454942 | validation: 0.2021849978536658]
	TIME [epoch: 1.33 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11277208594187528		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.11277208594187528 | validation: 0.14404024618846747]
	TIME [epoch: 1.33 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12937360427412192		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.12937360427412192 | validation: 0.17430329591142898]
	TIME [epoch: 1.33 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259412129511423		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.1259412129511423 | validation: 0.14513773528854115]
	TIME [epoch: 1.33 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.108829891633696		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.108829891633696 | validation: 0.17833238883180189]
	TIME [epoch: 1.33 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11967968293032465		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.11967968293032465 | validation: 0.1422436645357486]
	TIME [epoch: 1.33 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08374568861831776		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.08374568861831776 | validation: 0.1388010169945186]
	TIME [epoch: 1.33 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07315746999517175		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.07315746999517175 | validation: 0.17367989975792353]
	TIME [epoch: 1.33 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515212577517552		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.07515212577517552 | validation: 0.11236801914435945]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_447.pth
	Model improved!!!
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07825934220480907		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.07825934220480907 | validation: 0.1976742496106851]
	TIME [epoch: 1.34 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022566992125077		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.1022566992125077 | validation: 0.148363007045585]
	TIME [epoch: 1.33 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20428758842972675		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.20428758842972675 | validation: 0.2223046681906042]
	TIME [epoch: 1.33 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17431442541283013		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.17431442541283013 | validation: 0.15195575902765565]
	TIME [epoch: 1.33 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07027039559129042		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.07027039559129042 | validation: 0.13355590234293815]
	TIME [epoch: 1.35 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13172346747213007		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.13172346747213007 | validation: 0.17365143698121766]
	TIME [epoch: 1.33 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1387551482267053		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.1387551482267053 | validation: 0.14349764632651554]
	TIME [epoch: 1.34 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065226316418695		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.07065226316418695 | validation: 0.12198882778496403]
	TIME [epoch: 1.33 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07343688041341792		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.07343688041341792 | validation: 0.14615005460467376]
	TIME [epoch: 1.33 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08746682420824169		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.08746682420824169 | validation: 0.13961344101016315]
	TIME [epoch: 1.33 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15547957268503332		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.15547957268503332 | validation: 0.21935888352885838]
	TIME [epoch: 1.33 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14797477055201466		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.14797477055201466 | validation: 0.1417246132410502]
	TIME [epoch: 1.33 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0643397249380259		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.0643397249380259 | validation: 0.13631827688847428]
	TIME [epoch: 1.33 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07407920030020834		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.07407920030020834 | validation: 0.15773416076217595]
	TIME [epoch: 1.33 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08237064191148402		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.08237064191148402 | validation: 0.14631016188674]
	TIME [epoch: 1.33 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14068295819048832		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.14068295819048832 | validation: 0.1732075480722557]
	TIME [epoch: 1.33 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11877162986496614		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.11877162986496614 | validation: 0.15225980931639946]
	TIME [epoch: 1.33 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738572801784031		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.0738572801784031 | validation: 0.12673048327189393]
	TIME [epoch: 1.33 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058002940571536696		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.058002940571536696 | validation: 0.12693929170965804]
	TIME [epoch: 1.33 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05863220821207308		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.05863220821207308 | validation: 0.11941066211590355]
	TIME [epoch: 1.33 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05684283470563119		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.05684283470563119 | validation: 0.29997202063387524]
	TIME [epoch: 1.33 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17317419155031794		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.17317419155031794 | validation: 0.1468073871420502]
	TIME [epoch: 1.33 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10801273116945449		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.10801273116945449 | validation: 0.11324639505722174]
	TIME [epoch: 1.33 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654928902805055		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.0654928902805055 | validation: 0.140749184915865]
	TIME [epoch: 1.33 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061682958221884125		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.061682958221884125 | validation: 0.09311926660783554]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_472.pth
	Model improved!!!
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738935206430206		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.0738935206430206 | validation: 0.14871509190909307]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10425339308648225		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.10425339308648225 | validation: 0.12594658131158015]
	TIME [epoch: 1.33 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08034116501751974		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.08034116501751974 | validation: 0.1627484429563376]
	TIME [epoch: 1.33 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10290090914726413		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.10290090914726413 | validation: 0.14454305726845057]
	TIME [epoch: 1.33 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08399121558636254		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.08399121558636254 | validation: 0.13256371844519532]
	TIME [epoch: 1.33 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08003842893269973		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.08003842893269973 | validation: 0.1450587580111561]
	TIME [epoch: 1.33 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12545977226008928		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.12545977226008928 | validation: 0.17180114768804056]
	TIME [epoch: 1.33 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11627386948271781		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.11627386948271781 | validation: 0.12481538500843203]
	TIME [epoch: 1.33 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05592390503854628		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.05592390503854628 | validation: 0.1086599790000522]
	TIME [epoch: 1.33 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05613643530294861		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.05613643530294861 | validation: 0.17272249849477742]
	TIME [epoch: 1.33 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08107720446671547		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.08107720446671547 | validation: 0.11636143224039058]
	TIME [epoch: 1.33 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770289579563604		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.07770289579563604 | validation: 0.16882600147780016]
	TIME [epoch: 1.33 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11024420824375525		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.11024420824375525 | validation: 0.12635160565876055]
	TIME [epoch: 1.33 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10077981647471919		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.10077981647471919 | validation: 0.12936958142826888]
	TIME [epoch: 1.33 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08500535787171082		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.08500535787171082 | validation: 0.12283074809088981]
	TIME [epoch: 1.33 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05716868635711427		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.05716868635711427 | validation: 0.10274150762612724]
	TIME [epoch: 1.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051504130738185536		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.051504130738185536 | validation: 0.11782248135306053]
	TIME [epoch: 1.33 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05761532340166032		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.05761532340166032 | validation: 0.11282982406888509]
	TIME [epoch: 1.33 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06650386735685299		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.06650386735685299 | validation: 0.13969237403804138]
	TIME [epoch: 1.33 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08518189431777025		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.08518189431777025 | validation: 0.11539159246931222]
	TIME [epoch: 1.33 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08113685268822948		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.08113685268822948 | validation: 0.26330722480514296]
	TIME [epoch: 1.33 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15596440526739394		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.15596440526739394 | validation: 0.14082846034920532]
	TIME [epoch: 1.34 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07143585023060699		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.07143585023060699 | validation: 0.10777333927990429]
	TIME [epoch: 1.33 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05621940592174788		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.05621940592174788 | validation: 0.12258517689252929]
	TIME [epoch: 1.33 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05785016922653202		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.05785016922653202 | validation: 0.11483280455045963]
	TIME [epoch: 1.33 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06175124899238174		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.06175124899238174 | validation: 0.10702126205424088]
	TIME [epoch: 1.34 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06619143778562724		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.06619143778562724 | validation: 0.11486322314326639]
	TIME [epoch: 1.33 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776183258726712		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.0776183258726712 | validation: 0.20544235712638598]
	TIME [epoch: 1.34 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15169639724712408		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.15169639724712408 | validation: 0.11403771446720284]
	TIME [epoch: 164 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0517591715379927		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.0517591715379927 | validation: 0.1099127897239723]
	TIME [epoch: 2.65 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045246616787074964		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.045246616787074964 | validation: 0.11325366873512199]
	TIME [epoch: 2.63 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048787349775069655		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.048787349775069655 | validation: 0.0984051183777747]
	TIME [epoch: 2.63 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04939791121621283		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.04939791121621283 | validation: 0.10681502787235911]
	TIME [epoch: 2.63 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726226568283904		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.04726226568283904 | validation: 0.08719852082418225]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051923571512917495		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.051923571512917495 | validation: 0.16122301033950137]
	TIME [epoch: 2.63 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09442702545288637		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.09442702545288637 | validation: 0.11637743276637823]
	TIME [epoch: 2.63 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477318707232857		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.1477318707232857 | validation: 0.17664549015373016]
	TIME [epoch: 2.63 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913728988891711		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.11913728988891711 | validation: 0.10059246032750534]
	TIME [epoch: 2.63 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042945729195813326		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.042945729195813326 | validation: 0.08992947948918005]
	TIME [epoch: 2.63 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06213855014374605		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.06213855014374605 | validation: 0.13532484875969655]
	TIME [epoch: 2.63 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392664305212255		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.06392664305212255 | validation: 0.09425602189762437]
	TIME [epoch: 2.63 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05693966814943175		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.05693966814943175 | validation: 0.11360074244883125]
	TIME [epoch: 2.63 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05909878199441843		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.05909878199441843 | validation: 0.09948019819725777]
	TIME [epoch: 2.63 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379657255779865		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.06379657255779865 | validation: 0.12307961272718966]
	TIME [epoch: 2.63 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08590804687329234		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.08590804687329234 | validation: 0.10000202306780502]
	TIME [epoch: 2.63 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540749428548773		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.06540749428548773 | validation: 0.1541264032959807]
	TIME [epoch: 2.63 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004986419307473		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.07004986419307473 | validation: 0.09737345495239752]
	TIME [epoch: 2.63 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0473454050202554		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.0473454050202554 | validation: 0.12051165997049901]
	TIME [epoch: 2.63 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046784575963805766		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.046784575963805766 | validation: 0.09002149844657535]
	TIME [epoch: 2.63 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04811778887587059		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.04811778887587059 | validation: 0.1311474230312289]
	TIME [epoch: 2.63 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06081126707552606		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.06081126707552606 | validation: 0.10425060760727548]
	TIME [epoch: 2.63 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08318033030905374		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.08318033030905374 | validation: 0.1473250685829504]
	TIME [epoch: 2.63 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11488482384157261		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.11488482384157261 | validation: 0.10347700507825026]
	TIME [epoch: 2.63 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047743376614785484		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.047743376614785484 | validation: 0.08686189800680841]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_526.pth
	Model improved!!!
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04364503394743435		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.04364503394743435 | validation: 0.11852387745975566]
	TIME [epoch: 2.63 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860005202216625		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.05860005202216625 | validation: 0.10044939450116033]
	TIME [epoch: 2.63 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060412604343272246		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.060412604343272246 | validation: 0.11631225612168171]
	TIME [epoch: 2.63 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196462779347396		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.06196462779347396 | validation: 0.08987189922241601]
	TIME [epoch: 2.63 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048078393497946834		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.048078393497946834 | validation: 0.10730962873202197]
	TIME [epoch: 2.63 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042239802344720676		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.042239802344720676 | validation: 0.0803183956245011]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05818224555109954		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.05818224555109954 | validation: 0.1336089281682063]
	TIME [epoch: 2.63 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09640812654254105		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.09640812654254105 | validation: 0.08689505206204526]
	TIME [epoch: 2.63 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058932007647961554		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.058932007647961554 | validation: 0.11680772826111233]
	TIME [epoch: 2.63 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048825260844112955		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.048825260844112955 | validation: 0.0877389554326536]
	TIME [epoch: 2.62 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04483392147750368		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.04483392147750368 | validation: 0.14090781230269142]
	TIME [epoch: 2.63 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05071571516287126		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.05071571516287126 | validation: 0.08228071188309896]
	TIME [epoch: 2.63 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05333200676694001		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.05333200676694001 | validation: 0.11083566944342405]
	TIME [epoch: 2.63 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057442696182719626		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.057442696182719626 | validation: 0.09975367534120416]
	TIME [epoch: 2.63 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06075209578193708		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.06075209578193708 | validation: 0.12591378886077353]
	TIME [epoch: 2.63 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08393363222392906		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.08393363222392906 | validation: 0.09424769052411815]
	TIME [epoch: 2.63 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06156462280592732		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.06156462280592732 | validation: 0.10513008155903734]
	TIME [epoch: 2.63 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06404410464195252		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.06404410464195252 | validation: 0.10747151125308313]
	TIME [epoch: 2.63 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206616825235169		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.04206616825235169 | validation: 0.08446125271121391]
	TIME [epoch: 2.63 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721447763125772		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.03721447763125772 | validation: 0.0975761196429929]
	TIME [epoch: 2.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03264446960506252		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.03264446960506252 | validation: 0.08117691000945698]
	TIME [epoch: 2.63 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034912151783433085		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.034912151783433085 | validation: 0.11245733664206954]
	TIME [epoch: 2.63 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04194840843864306		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.04194840843864306 | validation: 0.08505134736226982]
	TIME [epoch: 2.62 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07367189429279465		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.07367189429279465 | validation: 0.2170759001459114]
	TIME [epoch: 2.63 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15207801924675088		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.15207801924675088 | validation: 0.09468623005912881]
	TIME [epoch: 2.62 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03544020584540804		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.03544020584540804 | validation: 0.0863873932593042]
	TIME [epoch: 2.63 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06893305551537608		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.06893305551537608 | validation: 0.20715626084622368]
	TIME [epoch: 2.63 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14013997155239166		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.14013997155239166 | validation: 0.10964743022338422]
	TIME [epoch: 2.62 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038541482406061464		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.038541482406061464 | validation: 0.07821599182991323]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_555.pth
	Model improved!!!
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06534816901356681		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.06534816901356681 | validation: 0.13535578900785372]
	TIME [epoch: 2.62 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770542769438434		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.07770542769438434 | validation: 0.09920187746090874]
	TIME [epoch: 2.63 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0369115595348465		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.0369115595348465 | validation: 0.07641835442190152]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03815172812893647		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.03815172812893647 | validation: 0.09670708160826542]
	TIME [epoch: 2.63 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046832245169882505		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.046832245169882505 | validation: 0.09535681679391766]
	TIME [epoch: 2.63 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048040260135489725		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.048040260135489725 | validation: 0.09136509461414236]
	TIME [epoch: 2.63 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922214779096721		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.04922214779096721 | validation: 0.09569031519823082]
	TIME [epoch: 2.63 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04033699721235113		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.04033699721235113 | validation: 0.08633585761646771]
	TIME [epoch: 2.63 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032494401655712005		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.032494401655712005 | validation: 0.08087688811934418]
	TIME [epoch: 2.63 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033420296709179204		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.033420296709179204 | validation: 0.09267478202029515]
	TIME [epoch: 2.63 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142555633373942		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.03142555633373942 | validation: 0.0764041931807151]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_566.pth
	Model improved!!!
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0322965835416599		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.0322965835416599 | validation: 0.09685555059773814]
	TIME [epoch: 2.62 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040860856453653276		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.040860856453653276 | validation: 0.07946988745783995]
	TIME [epoch: 2.63 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05569663159698728		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.05569663159698728 | validation: 0.13361166481858727]
	TIME [epoch: 2.63 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09031424205877757		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.09031424205877757 | validation: 0.07469525459710524]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741109548822923		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.06741109548822923 | validation: 0.13531304455823273]
	TIME [epoch: 2.62 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06013909208955737		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.06013909208955737 | validation: 0.08645290796400058]
	TIME [epoch: 2.62 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531549204196205		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.03531549204196205 | validation: 0.08336283816067086]
	TIME [epoch: 2.63 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030443907219360966		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.030443907219360966 | validation: 0.08787865081808]
	TIME [epoch: 2.62 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03549837324839384		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.03549837324839384 | validation: 0.0754511299324731]
	TIME [epoch: 2.63 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041566547995147814		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.041566547995147814 | validation: 0.08592409649362773]
	TIME [epoch: 2.63 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04806573492937678		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.04806573492937678 | validation: 0.11414304128269381]
	TIME [epoch: 2.63 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07421638127443302		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.07421638127443302 | validation: 0.07238215325452052]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06205358432222503		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.06205358432222503 | validation: 0.10367695261751075]
	TIME [epoch: 2.63 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051797518593392315		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.051797518593392315 | validation: 0.07145159997362419]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346125953001722		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.05346125953001722 | validation: 0.12497484668807249]
	TIME [epoch: 2.63 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04947559888459079		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.04947559888459079 | validation: 0.06963681393652092]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036283837230713295		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.036283837230713295 | validation: 0.0747208893615343]
	TIME [epoch: 2.63 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032650652220864926		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.032650652220864926 | validation: 0.07520718515888077]
	TIME [epoch: 2.63 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036311539708932465		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.036311539708932465 | validation: 0.0869586992356218]
	TIME [epoch: 2.63 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03752511716928502		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.03752511716928502 | validation: 0.07752037339131927]
	TIME [epoch: 2.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03425083937523578		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.03425083937523578 | validation: 0.11897148300966152]
	TIME [epoch: 2.62 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04776286381653531		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.04776286381653531 | validation: 0.0717513500444125]
	TIME [epoch: 2.63 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282861406666754		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.05282861406666754 | validation: 0.09606109805191876]
	TIME [epoch: 2.62 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056403514968395775		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.056403514968395775 | validation: 0.08642198304257974]
	TIME [epoch: 2.62 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04671197106998361		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.04671197106998361 | validation: 0.09287282941198627]
	TIME [epoch: 2.63 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044563124176997634		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.044563124176997634 | validation: 0.07504196418058476]
	TIME [epoch: 2.63 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0423669681242631		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.0423669681242631 | validation: 0.08901524249963291]
	TIME [epoch: 2.62 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04162359024332895		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.04162359024332895 | validation: 0.06449586555396816]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042045151682879256		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.042045151682879256 | validation: 0.11036163433002355]
	TIME [epoch: 2.64 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05330300741785282		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.05330300741785282 | validation: 0.07764321626120463]
	TIME [epoch: 2.64 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03800559228448832		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.03800559228448832 | validation: 0.06793546041379216]
	TIME [epoch: 2.63 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03115810237693343		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.03115810237693343 | validation: 0.08001219224491188]
	TIME [epoch: 2.64 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031484544597289936		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.031484544597289936 | validation: 0.06771948649596927]
	TIME [epoch: 2.63 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030870124441021274		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.030870124441021274 | validation: 0.07326583171038249]
	TIME [epoch: 2.63 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03430446966735111		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.03430446966735111 | validation: 0.20012649074748656]
	TIME [epoch: 2.63 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12247035169276319		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.12247035169276319 | validation: 0.10581032403921747]
	TIME [epoch: 2.64 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03760879322663005		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.03760879322663005 | validation: 0.07762167918801605]
	TIME [epoch: 2.63 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038274432948789586		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.038274432948789586 | validation: 0.09724725122247956]
	TIME [epoch: 2.63 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05619519622905486		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.05619519622905486 | validation: 0.08468993709611859]
	TIME [epoch: 2.64 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04891944069182695		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.04891944069182695 | validation: 0.08779978788157339]
	TIME [epoch: 2.63 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601271218222971		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.03601271218222971 | validation: 0.07493858177980975]
	TIME [epoch: 2.63 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030116291700813976		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.030116291700813976 | validation: 0.0830716249222118]
	TIME [epoch: 2.63 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026002021468963306		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.026002021468963306 | validation: 0.07693401909205395]
	TIME [epoch: 2.63 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026077288788227678		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.026077288788227678 | validation: 0.060222060627653284]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024217540716584867		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.024217540716584867 | validation: 0.07304137051788434]
	TIME [epoch: 2.63 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024289809131542262		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.024289809131542262 | validation: 0.07477564713379045]
	TIME [epoch: 2.63 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02605130504177602		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.02605130504177602 | validation: 0.06690558034925263]
	TIME [epoch: 2.63 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026266151660750036		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.026266151660750036 | validation: 0.07158768318987752]
	TIME [epoch: 2.63 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02904970500669668		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.02904970500669668 | validation: 0.0918339070240874]
	TIME [epoch: 2.63 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732945635298298		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.04732945635298298 | validation: 0.08041117547455703]
	TIME [epoch: 2.63 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10456880310444928		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.10456880310444928 | validation: 0.144711351191746]
	TIME [epoch: 2.63 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11560710540404376		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.11560710540404376 | validation: 0.08931557778344912]
	TIME [epoch: 2.63 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04571898211048339		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.04571898211048339 | validation: 0.08501708809814636]
	TIME [epoch: 2.64 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10480476183677967		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10480476183677967 | validation: 0.08786597438352613]
	TIME [epoch: 2.63 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973794977574788		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.03973794977574788 | validation: 0.09493000791909868]
	TIME [epoch: 2.63 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035823986682283086		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.035823986682283086 | validation: 0.07417896161058625]
	TIME [epoch: 2.63 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039199736435607944		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.039199736435607944 | validation: 0.062311811411329646]
	TIME [epoch: 2.63 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0268463803239123		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.0268463803239123 | validation: 0.07947299577335881]
	TIME [epoch: 2.63 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026750341575926515		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.026750341575926515 | validation: 0.06942714067891523]
	TIME [epoch: 2.63 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026966622477409212		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.026966622477409212 | validation: 0.07405156777126627]
	TIME [epoch: 2.63 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026721440093427293		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.026721440093427293 | validation: 0.07557285983872765]
	TIME [epoch: 2.63 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025312257857363143		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.025312257857363143 | validation: 0.06691696546226467]
	TIME [epoch: 2.63 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02542188605085941		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.02542188605085941 | validation: 0.06724864958322282]
	TIME [epoch: 2.63 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026929626903218696		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.026929626903218696 | validation: 0.06951249475183495]
	TIME [epoch: 2.63 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0304163511552202		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.0304163511552202 | validation: 0.06277186099390031]
	TIME [epoch: 2.63 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03959250837273018		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.03959250837273018 | validation: 0.1355681923646381]
	TIME [epoch: 2.63 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07827364268373949		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.07827364268373949 | validation: 0.08667468066026646]
	TIME [epoch: 2.63 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053098330861053776		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.053098330861053776 | validation: 0.07691614599234292]
	TIME [epoch: 2.63 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03405335557678463		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.03405335557678463 | validation: 0.060461276809741706]
	TIME [epoch: 2.63 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024488753795567746		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.024488753795567746 | validation: 0.0588737660553899]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025886228101107477		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.025886228101107477 | validation: 0.08278745863817333]
	TIME [epoch: 2.63 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03243515542240129		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.03243515542240129 | validation: 0.06193699572954582]
	TIME [epoch: 2.64 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032357705299940115		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.032357705299940115 | validation: 0.08317334664615957]
	TIME [epoch: 2.63 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03213009529726829		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.03213009529726829 | validation: 0.05411943407963324]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_640.pth
	Model improved!!!
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036883326205746046		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.036883326205746046 | validation: 0.08795662022739002]
	TIME [epoch: 2.63 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04174924387971651		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.04174924387971651 | validation: 0.051721201327928706]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02873924527973748		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.02873924527973748 | validation: 0.06061030074515314]
	TIME [epoch: 2.62 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022645641370292097		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.022645641370292097 | validation: 0.06546639103609182]
	TIME [epoch: 2.62 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023034758431732884		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.023034758431732884 | validation: 0.06051863923014812]
	TIME [epoch: 2.62 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024690010703702303		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.024690010703702303 | validation: 0.05361321609525929]
	TIME [epoch: 2.62 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02662567963062096		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.02662567963062096 | validation: 0.07851489677671275]
	TIME [epoch: 2.62 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03424792087185043		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.03424792087185043 | validation: 0.06191471875746775]
	TIME [epoch: 2.62 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04918439479672564		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.04918439479672564 | validation: 0.1112519266220427]
	TIME [epoch: 2.62 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848472381215205		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.06848472381215205 | validation: 0.06635088856173037]
	TIME [epoch: 2.62 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410137650387353		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.03410137650387353 | validation: 0.05329252623045432]
	TIME [epoch: 2.63 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023605000253175804		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.023605000253175804 | validation: 0.0704402417075009]
	TIME [epoch: 2.63 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02457717321290372		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.02457717321290372 | validation: 0.04914606530573245]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0354320768588267		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.0354320768588267 | validation: 0.09435685895704642]
	TIME [epoch: 2.63 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05442632514583344		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.05442632514583344 | validation: 0.06734985893668503]
	TIME [epoch: 2.63 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03727602721903158		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.03727602721903158 | validation: 0.07240709717635546]
	TIME [epoch: 2.63 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03029428739395744		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.03029428739395744 | validation: 0.05712053206299688]
	TIME [epoch: 2.62 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02221708042319035		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.02221708042319035 | validation: 0.06274415089506295]
	TIME [epoch: 2.63 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024508587652114677		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.024508587652114677 | validation: 0.053029735873305076]
	TIME [epoch: 2.63 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02165874688807959		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.02165874688807959 | validation: 0.06267145225355704]
	TIME [epoch: 2.63 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020692661653151284		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.020692661653151284 | validation: 0.05809790745606082]
	TIME [epoch: 2.63 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021345863639457657		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.021345863639457657 | validation: 0.04822763635056026]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022045157834658796		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.022045157834658796 | validation: 0.051175370355486416]
	TIME [epoch: 2.63 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021458159925902935		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.021458159925902935 | validation: 0.0493194734625708]
	TIME [epoch: 2.62 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021340429113682814		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.021340429113682814 | validation: 0.057704022284191414]
	TIME [epoch: 2.62 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021148531011928123		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.021148531011928123 | validation: 0.05468079827423236]
	TIME [epoch: 2.62 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022750873542789377		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.022750873542789377 | validation: 0.05814811956093158]
	TIME [epoch: 2.62 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02558126181773436		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.02558126181773436 | validation: 0.06686520836131431]
	TIME [epoch: 2.62 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08287765487610865		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.08287765487610865 | validation: 0.13776806491056695]
	TIME [epoch: 2.62 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.109225345425863		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.109225345425863 | validation: 0.06353272530520175]
	TIME [epoch: 2.62 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02720063788051639		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.02720063788051639 | validation: 0.06817916711199319]
	TIME [epoch: 2.62 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783720277472184		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.0783720277472184 | validation: 0.07226721067351154]
	TIME [epoch: 2.62 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886753780783738		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.03886753780783738 | validation: 0.06202186353244409]
	TIME [epoch: 2.62 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02399176916563252		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.02399176916563252 | validation: 0.058876784264388075]
	TIME [epoch: 2.62 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03014524426420131		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.03014524426420131 | validation: 0.05347845007809542]
	TIME [epoch: 2.62 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02624825149175317		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.02624825149175317 | validation: 0.06235410276399037]
	TIME [epoch: 2.62 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02133003706158332		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.02133003706158332 | validation: 0.05113142695525821]
	TIME [epoch: 2.62 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02066747142970647		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.02066747142970647 | validation: 0.0619890998857619]
	TIME [epoch: 2.62 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020306024092923047		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.020306024092923047 | validation: 0.06082848236230264]
	TIME [epoch: 2.62 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021012325133317803		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.021012325133317803 | validation: 0.05865187755857579]
	TIME [epoch: 2.62 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01963326192869146		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.01963326192869146 | validation: 0.04902737252902978]
	TIME [epoch: 2.62 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01991560482636323		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.01991560482636323 | validation: 0.0539559627027827]
	TIME [epoch: 2.62 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018539690990769183		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.018539690990769183 | validation: 0.06352821782110143]
	TIME [epoch: 2.62 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020056505886079334		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.020056505886079334 | validation: 0.04964259492837184]
	TIME [epoch: 2.62 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02032052235914859		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.02032052235914859 | validation: 0.06199868172020998]
	TIME [epoch: 2.62 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021096128437633137		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.021096128437633137 | validation: 0.04848073931062641]
	TIME [epoch: 2.62 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01989860113008964		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.01989860113008964 | validation: 0.05074646503391697]
	TIME [epoch: 2.62 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02102627021377319		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.02102627021377319 | validation: 0.07290737068450769]
	TIME [epoch: 2.62 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03127232966339784		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.03127232966339784 | validation: 0.05841228620041797]
	TIME [epoch: 2.62 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06465831625709535		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.06465831625709535 | validation: 0.10874576167594763]
	TIME [epoch: 2.62 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08158770324271775		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.08158770324271775 | validation: 0.053746088425103804]
	TIME [epoch: 2.62 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019589545589451914		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.019589545589451914 | validation: 0.05241049714628984]
	TIME [epoch: 2.62 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03235422143874901		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.03235422143874901 | validation: 0.08877022205587615]
	TIME [epoch: 2.62 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0465067479116934		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0465067479116934 | validation: 0.06653999189353059]
	TIME [epoch: 2.62 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02197249271176771		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.02197249271176771 | validation: 0.04210632996872639]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_695.pth
	Model improved!!!
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021345949492507607		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.021345949492507607 | validation: 0.05374908631850135]
	TIME [epoch: 2.62 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02277323099498883		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.02277323099498883 | validation: 0.055433617155816156]
	TIME [epoch: 2.62 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01982390324668509		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.01982390324668509 | validation: 0.05082796718192756]
	TIME [epoch: 2.62 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017924094766082987		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.017924094766082987 | validation: 0.061174021974157326]
	TIME [epoch: 2.63 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01964292696951993		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.01964292696951993 | validation: 0.05035143163752967]
	TIME [epoch: 2.62 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018766654084623184		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.018766654084623184 | validation: 0.06392920728938145]
	TIME [epoch: 2.62 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024426516419084658		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.024426516419084658 | validation: 0.048150270135713616]
	TIME [epoch: 2.62 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03034990807174888		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.03034990807174888 | validation: 0.07614380218155209]
	TIME [epoch: 2.62 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04319003242081235		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.04319003242081235 | validation: 0.060956133922706816]
	TIME [epoch: 2.62 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046898654150266665		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.046898654150266665 | validation: 0.07083272434475243]
	TIME [epoch: 2.62 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026575077228810196		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.026575077228810196 | validation: 0.048079784545733376]
	TIME [epoch: 2.62 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01841129790436986		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.01841129790436986 | validation: 0.051232073557496476]
	TIME [epoch: 2.63 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027676126409911853		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.027676126409911853 | validation: 0.0679693290227471]
	TIME [epoch: 2.62 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030843603606247393		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.030843603606247393 | validation: 0.046598081152513404]
	TIME [epoch: 2.62 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022797648814884823		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.022797648814884823 | validation: 0.05859397100773711]
	TIME [epoch: 2.62 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018354004393680796		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.018354004393680796 | validation: 0.05646221128567275]
	TIME [epoch: 2.62 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019057627054636272		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.019057627054636272 | validation: 0.03694135189836634]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020443082323599097		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.020443082323599097 | validation: 0.06644587171648805]
	TIME [epoch: 2.62 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02393072605894309		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.02393072605894309 | validation: 0.051708492925384875]
	TIME [epoch: 2.62 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022228196599006596		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.022228196599006596 | validation: 0.06174434680487359]
	TIME [epoch: 2.63 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023638254232195673		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.023638254232195673 | validation: 0.0462537963751343]
	TIME [epoch: 2.62 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031297632161822235		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.031297632161822235 | validation: 0.061344064819964864]
	TIME [epoch: 2.62 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352201457173255		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.0352201457173255 | validation: 0.043558937205806884]
	TIME [epoch: 2.62 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024561749279875478		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.024561749279875478 | validation: 0.05298494016141042]
	TIME [epoch: 2.62 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01788864716457337		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.01788864716457337 | validation: 0.04816361075409148]
	TIME [epoch: 2.62 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017705363096320172		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.017705363096320172 | validation: 0.03867758736125221]
	TIME [epoch: 2.62 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015972127073071166		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.015972127073071166 | validation: 0.05263000276908544]
	TIME [epoch: 2.62 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019046012680458295		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.019046012680458295 | validation: 0.04960624084684422]
	TIME [epoch: 2.62 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023003512497378448		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.023003512497378448 | validation: 0.07817538501489206]
	TIME [epoch: 2.62 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04150604026147207		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.04150604026147207 | validation: 0.05611172612316238]
	TIME [epoch: 2.62 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04983696933712529		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.04983696933712529 | validation: 0.0683637282231409]
	TIME [epoch: 2.62 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032641023510333496		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.032641023510333496 | validation: 0.04623786449220571]
	TIME [epoch: 2.62 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017805001850362857		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.017805001850362857 | validation: 0.05240302015098313]
	TIME [epoch: 2.62 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017068270221922935		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.017068270221922935 | validation: 0.04895526710928225]
	TIME [epoch: 2.62 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016505447020430445		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.016505447020430445 | validation: 0.05554785924557918]
	TIME [epoch: 2.62 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016457386176972552		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.016457386176972552 | validation: 0.04726553217774532]
	TIME [epoch: 2.62 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016894479734795735		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.016894479734795735 | validation: 0.0417375432804871]
	TIME [epoch: 2.62 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02008970699129667		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.02008970699129667 | validation: 0.055628163137450864]
	TIME [epoch: 2.62 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024228007781831016		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.024228007781831016 | validation: 0.04448456332309328]
	TIME [epoch: 2.62 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023671510925710113		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.023671510925710113 | validation: 0.06136056251384273]
	TIME [epoch: 2.62 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02487356859517014		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.02487356859517014 | validation: 0.04570958261114444]
	TIME [epoch: 2.62 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0210563241588976		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.0210563241588976 | validation: 0.038757790369516314]
	TIME [epoch: 2.62 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020341321033064408		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.020341321033064408 | validation: 0.04193324441844453]
	TIME [epoch: 2.62 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01905272500655633		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.01905272500655633 | validation: 0.07294253525790838]
	TIME [epoch: 2.62 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032320816114766775		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.032320816114766775 | validation: 0.03733192507481192]
	TIME [epoch: 2.62 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02496002114051736		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.02496002114051736 | validation: 0.05216144418845095]
	TIME [epoch: 2.62 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021865745340915125		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.021865745340915125 | validation: 0.045064187925378967]
	TIME [epoch: 2.62 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0195290475365063		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.0195290475365063 | validation: 0.044771752018538724]
	TIME [epoch: 2.62 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016898307361294745		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.016898307361294745 | validation: 0.04598756084771121]
	TIME [epoch: 2.62 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016059961214624323		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.016059961214624323 | validation: 0.03763891907419801]
	TIME [epoch: 2.62 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01551637770840972		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.01551637770840972 | validation: 0.04784129684618388]
	TIME [epoch: 2.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016683791402391214		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.016683791402391214 | validation: 0.04323384292975185]
	TIME [epoch: 2.62 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015670347160066513		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.015670347160066513 | validation: 0.03554021462362427]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01680332443867019		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.01680332443867019 | validation: 0.042856460826818044]
	TIME [epoch: 2.62 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017559744496475002		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.017559744496475002 | validation: 0.05191723702206855]
	TIME [epoch: 2.62 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021874524057274733		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.021874524057274733 | validation: 0.049548787251815676]
	TIME [epoch: 2.63 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202394180713405		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.04202394180713405 | validation: 0.08785901699179614]
	TIME [epoch: 2.62 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045617124297793715		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.045617124297793715 | validation: 0.04209729969408095]
	TIME [epoch: 2.63 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016628676034645507		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.016628676034645507 | validation: 0.03971466201728292]
	TIME [epoch: 2.62 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01756980767209818		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.01756980767209818 | validation: 0.060699455625049095]
	TIME [epoch: 2.62 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022906359234860584		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.022906359234860584 | validation: 0.04110584120429209]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020471025122151695		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.020471025122151695 | validation: 0.046770911089654324]
	TIME [epoch: 2.62 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01892744318251167		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.01892744318251167 | validation: 0.04770513825463886]
	TIME [epoch: 2.62 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01595732712546299		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.01595732712546299 | validation: 0.04418222403175325]
	TIME [epoch: 2.62 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015395273937702458		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.015395273937702458 | validation: 0.033118626056330945]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015224011779132622		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.015224011779132622 | validation: 0.05162596584211132]
	TIME [epoch: 2.62 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01848638621584528		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.01848638621584528 | validation: 0.04119926724954337]
	TIME [epoch: 2.62 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020154823376265868		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.020154823376265868 | validation: 0.059957118771497286]
	TIME [epoch: 2.62 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025382702766661103		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.025382702766661103 | validation: 0.039413049549684644]
	TIME [epoch: 2.62 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025620990500012875		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.025620990500012875 | validation: 0.07100367221986043]
	TIME [epoch: 2.62 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03304138069151207		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.03304138069151207 | validation: 0.04814518583426254]
	TIME [epoch: 2.62 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021240873526332492		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.021240873526332492 | validation: 0.04748796389631898]
	TIME [epoch: 2.62 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018713824089044293		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.018713824089044293 | validation: 0.04242068579755263]
	TIME [epoch: 2.62 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016672513987902194		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.016672513987902194 | validation: 0.04697745885390391]
	TIME [epoch: 2.62 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015107286808006206		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.015107286808006206 | validation: 0.038850350916525084]
	TIME [epoch: 2.62 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014508487566293016		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.014508487566293016 | validation: 0.04642825671767126]
	TIME [epoch: 2.62 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01775844326096628		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.01775844326096628 | validation: 0.03848938060464921]
	TIME [epoch: 2.62 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019776598942424978		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.019776598942424978 | validation: 0.05931836646172877]
	TIME [epoch: 2.62 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027133694478716292		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.027133694478716292 | validation: 0.036909861350360444]
	TIME [epoch: 2.62 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032587944671618345		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.032587944671618345 | validation: 0.06406947261306044]
	TIME [epoch: 2.62 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037245576214070056		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.037245576214070056 | validation: 0.04393793713686054]
	TIME [epoch: 2.62 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015797706270268486		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.015797706270268486 | validation: 0.03551455676588235]
	TIME [epoch: 2.62 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018645265229681608		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.018645265229681608 | validation: 0.060039269565733536]
	TIME [epoch: 2.62 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02376053826552247		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.02376053826552247 | validation: 0.04405523778554424]
	TIME [epoch: 2.62 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018547677785177543		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.018547677785177543 | validation: 0.03977700731837343]
	TIME [epoch: 2.62 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013912788279801483		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.013912788279801483 | validation: 0.05061256490050671]
	TIME [epoch: 2.62 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015856670688142227		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.015856670688142227 | validation: 0.03981221150000189]
	TIME [epoch: 2.62 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014723927815519322		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.014723927815519322 | validation: 0.034215213018862545]
	TIME [epoch: 2.62 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015380195231554013		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.015380195231554013 | validation: 0.042745835085981815]
	TIME [epoch: 2.62 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014812665924284093		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.014812665924284093 | validation: 0.043143586210926656]
	TIME [epoch: 2.62 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014100297470994156		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.014100297470994156 | validation: 0.04009368099243604]
	TIME [epoch: 2.62 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014321607909818715		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.014321607909818715 | validation: 0.04623311236125776]
	TIME [epoch: 2.62 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013582557141758942		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.013582557141758942 | validation: 0.045730698634386516]
	TIME [epoch: 2.62 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015077716925391319		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.015077716925391319 | validation: 0.03696087602587962]
	TIME [epoch: 2.62 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021938750990080785		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.021938750990080785 | validation: 0.07019260485780358]
	TIME [epoch: 2.62 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0431650727735149		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.0431650727735149 | validation: 0.042115406000913784]
	TIME [epoch: 2.62 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032479798646803545		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.032479798646803545 | validation: 0.04465041440469045]
	TIME [epoch: 2.62 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018284357040728227		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.018284357040728227 | validation: 0.04137577110160626]
	TIME [epoch: 2.62 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01569098679341269		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.01569098679341269 | validation: 0.032682944828978835]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01681969346161019		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.01681969346161019 | validation: 0.03497407173511794]
	TIME [epoch: 2.62 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016145283574743426		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.016145283574743426 | validation: 0.037647937129671266]
	TIME [epoch: 2.62 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013807228908997113		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.013807228908997113 | validation: 0.03799899185497836]
	TIME [epoch: 2.62 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013900896251408562		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.013900896251408562 | validation: 0.03755876776958951]
	TIME [epoch: 2.62 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01616741820117508		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.01616741820117508 | validation: 0.03919450502492292]
	TIME [epoch: 2.62 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014956748697196294		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.014956748697196294 | validation: 0.03331925490664236]
	TIME [epoch: 2.62 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015290060213714422		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.015290060213714422 | validation: 0.032910189675639814]
	TIME [epoch: 2.62 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0172783184412819		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.0172783184412819 | validation: 0.04517783759566845]
	TIME [epoch: 2.62 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015620449080961782		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.015620449080961782 | validation: 0.0467475094672304]
	TIME [epoch: 2.62 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015468280259185474		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.015468280259185474 | validation: 0.03817281206979675]
	TIME [epoch: 2.62 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018936220753009877		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.018936220753009877 | validation: 0.05554094556915495]
	TIME [epoch: 2.62 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025705929182386057		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.025705929182386057 | validation: 0.03846562068511359]
	TIME [epoch: 2.62 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02460259977173446		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.02460259977173446 | validation: 0.0447317143802932]
	TIME [epoch: 2.62 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024769191317806413		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.024769191317806413 | validation: 0.0386719599419931]
	TIME [epoch: 2.62 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014883016077540253		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.014883016077540253 | validation: 0.03919112626949023]
	TIME [epoch: 2.62 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013167682370824375		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.013167682370824375 | validation: 0.042973658126375484]
	TIME [epoch: 2.62 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013996157178320258		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.013996157178320258 | validation: 0.029802977923086595]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014747140489227477		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.014747140489227477 | validation: 0.037940230488710106]
	TIME [epoch: 2.62 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015390825655575218		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.015390825655575218 | validation: 0.041751102309329206]
	TIME [epoch: 2.62 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01884238978441731		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.01884238978441731 | validation: 0.05482729474435392]
	TIME [epoch: 2.62 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021897816304320653		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.021897816304320653 | validation: 0.0364659625903619]
	TIME [epoch: 2.62 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02423926215238644		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.02423926215238644 | validation: 0.05525913628901178]
	TIME [epoch: 2.62 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019163989265407452		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.019163989265407452 | validation: 0.031744217538041734]
	TIME [epoch: 2.62 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01397268883038894		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.01397268883038894 | validation: 0.03475635331607246]
	TIME [epoch: 2.62 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013584702031107697		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.013584702031107697 | validation: 0.034675887206895344]
	TIME [epoch: 2.62 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012359563341233795		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.012359563341233795 | validation: 0.04113311735445827]
	TIME [epoch: 2.62 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013238575646458157		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.013238575646458157 | validation: 0.03437936018400808]
	TIME [epoch: 2.62 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015113188580708705		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.015113188580708705 | validation: 0.03848622636017588]
	TIME [epoch: 2.62 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01579012077982631		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.01579012077982631 | validation: 0.031477562526011596]
	TIME [epoch: 2.62 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021042191513903387		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.021042191513903387 | validation: 0.062351023847369326]
	TIME [epoch: 2.62 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029031939639526654		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.029031939639526654 | validation: 0.03868829044287567]
	TIME [epoch: 2.62 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017398336181874835		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.017398336181874835 | validation: 0.02931321055629811]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013164627474542799		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.013164627474542799 | validation: 0.039946778643948466]
	TIME [epoch: 2.62 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014873000866554133		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.014873000866554133 | validation: 0.030847336307291875]
	TIME [epoch: 2.62 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014488185180922984		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.014488185180922984 | validation: 0.037466796309737115]
	TIME [epoch: 2.63 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015096151589203657		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.015096151589203657 | validation: 0.038077768067371]
	TIME [epoch: 2.62 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013735793548283852		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.013735793548283852 | validation: 0.03888943886761195]
	TIME [epoch: 2.62 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012982615714411914		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.012982615714411914 | validation: 0.03457447076313913]
	TIME [epoch: 2.62 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013804414120378389		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.013804414120378389 | validation: 0.036643057577948425]
	TIME [epoch: 2.62 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012523108792396837		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.012523108792396837 | validation: 0.032898342772633996]
	TIME [epoch: 2.62 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01511799420684239		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.01511799420684239 | validation: 0.04337605511655973]
	TIME [epoch: 2.62 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018858466532154398		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.018858466532154398 | validation: 0.030256201555975717]
	TIME [epoch: 2.62 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024962821946213687		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.024962821946213687 | validation: 0.04953316344459622]
	TIME [epoch: 2.62 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026757763469585655		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.026757763469585655 | validation: 0.03061380859347712]
	TIME [epoch: 2.62 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013869713874321244		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.013869713874321244 | validation: 0.028755028713367214]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_839.pth
	Model improved!!!
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012318923082315665		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.012318923082315665 | validation: 0.033911732672887156]
	TIME [epoch: 2.62 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011965374152742592		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.011965374152742592 | validation: 0.03335988600710994]
	TIME [epoch: 2.63 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012198931864481715		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.012198931864481715 | validation: 0.03204480697768785]
	TIME [epoch: 2.62 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012198775325668416		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.012198775325668416 | validation: 0.028211228355607677]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01309419187689713		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.01309419187689713 | validation: 0.03712329539896632]
	TIME [epoch: 2.62 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01421118892793098		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.01421118892793098 | validation: 0.027637374177974707]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023723701664177296		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.023723701664177296 | validation: 0.060944528733235775]
	TIME [epoch: 2.62 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531155474594337		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.03531155474594337 | validation: 0.03160820786354671]
	TIME [epoch: 2.62 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01436491958281309		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.01436491958281309 | validation: 0.032541653508103896]
	TIME [epoch: 2.62 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012327767337361423		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.012327767337361423 | validation: 0.03542647052622157]
	TIME [epoch: 2.62 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015027756259720613		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.015027756259720613 | validation: 0.03919232438831969]
	TIME [epoch: 2.62 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012636876550877696		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.012636876550877696 | validation: 0.02810177194260212]
	TIME [epoch: 2.63 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013634988429446505		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.013634988429446505 | validation: 0.04106666095372006]
	TIME [epoch: 2.62 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020227173861262857		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.020227173861262857 | validation: 0.03810886662200295]
	TIME [epoch: 2.62 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018561162088811114		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.018561162088811114 | validation: 0.04130648733552174]
	TIME [epoch: 2.62 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01639427376401111		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.01639427376401111 | validation: 0.028596851336936225]
	TIME [epoch: 2.62 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01233084886086324		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.01233084886086324 | validation: 0.04260418812192288]
	TIME [epoch: 2.62 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013594894299764878		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.013594894299764878 | validation: 0.03891798079385303]
	TIME [epoch: 2.62 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014075686657332071		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.014075686657332071 | validation: 0.02848893426148215]
	TIME [epoch: 2.62 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012233135614884936		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.012233135614884936 | validation: 0.029548173207781126]
	TIME [epoch: 2.62 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010913979748561177		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.010913979748561177 | validation: 0.03036964363202388]
	TIME [epoch: 2.62 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012253232402875193		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.012253232402875193 | validation: 0.028695858652747798]
	TIME [epoch: 2.62 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012172228342485382		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.012172228342485382 | validation: 0.038701205245142846]
	TIME [epoch: 2.62 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015080592313494021		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.015080592313494021 | validation: 0.035020672649758236]
	TIME [epoch: 2.62 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017135089453932474		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.017135089453932474 | validation: 0.04047760540028888]
	TIME [epoch: 2.62 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019447638950033076		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.019447638950033076 | validation: 0.03627935803951448]
	TIME [epoch: 2.62 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016669841601063325		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.016669841601063325 | validation: 0.029799942621604927]
	TIME [epoch: 2.62 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013010848597077001		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.013010848597077001 | validation: 0.03317988851925774]
	TIME [epoch: 2.62 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012020617651569553		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.012020617651569553 | validation: 0.03697154212319771]
	TIME [epoch: 2.62 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012124735617565987		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.012124735617565987 | validation: 0.04250711921206421]
	TIME [epoch: 2.62 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015233943867135134		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.015233943867135134 | validation: 0.032778727049981804]
	TIME [epoch: 2.62 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015497632414433662		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.015497632414433662 | validation: 0.03743213278278436]
	TIME [epoch: 2.62 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01475585705791702		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.01475585705791702 | validation: 0.03383409604259615]
	TIME [epoch: 2.62 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012261089604463123		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.012261089604463123 | validation: 0.02953163469887604]
	TIME [epoch: 2.62 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011334490743943671		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.011334490743943671 | validation: 0.030994220260139482]
	TIME [epoch: 2.62 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012228002096603975		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.012228002096603975 | validation: 0.03411656765573785]
	TIME [epoch: 2.62 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012394844959019836		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.012394844959019836 | validation: 0.03003566559084413]
	TIME [epoch: 2.63 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013153499007895673		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.013153499007895673 | validation: 0.03247840926605977]
	TIME [epoch: 2.62 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014665018285420809		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.014665018285420809 | validation: 0.03534254520216612]
	TIME [epoch: 2.62 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014893877082886791		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.014893877082886791 | validation: 0.0383705360186225]
	TIME [epoch: 2.62 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016899598768795324		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.016899598768795324 | validation: 0.027475497195327804]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016912709713654273		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.016912709713654273 | validation: 0.03750081080756366]
	TIME [epoch: 2.62 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017324890321763122		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.017324890321763122 | validation: 0.03146452283148399]
	TIME [epoch: 2.62 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014485890980136305		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.014485890980136305 | validation: 0.03399193621823221]
	TIME [epoch: 2.63 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014940126216223561		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.014940126216223561 | validation: 0.025597174868269082]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011746310462510191		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.011746310462510191 | validation: 0.02688418656868348]
	TIME [epoch: 2.64 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01192933168538712		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.01192933168538712 | validation: 0.031521117767273135]
	TIME [epoch: 2.64 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011864244162410888		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.011864244162410888 | validation: 0.0268590598467075]
	TIME [epoch: 2.64 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012751558386495259		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.012751558386495259 | validation: 0.04111050967203287]
	TIME [epoch: 2.64 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013626701139108093		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.013626701139108093 | validation: 0.030489412963521825]
	TIME [epoch: 2.64 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014164955752059552		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.014164955752059552 | validation: 0.030345620669110052]
	TIME [epoch: 2.64 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014261225598325837		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.014261225598325837 | validation: 0.032068543068361745]
	TIME [epoch: 2.64 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012470748628723556		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.012470748628723556 | validation: 0.029706757984134104]
	TIME [epoch: 2.64 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011219690453615762		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.011219690453615762 | validation: 0.03191495641545729]
	TIME [epoch: 2.64 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011518423626246142		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.011518423626246142 | validation: 0.02860901677651676]
	TIME [epoch: 2.64 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01292269851407958		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.01292269851407958 | validation: 0.03110113533960056]
	TIME [epoch: 2.64 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014341176243892965		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.014341176243892965 | validation: 0.034102948950363]
	TIME [epoch: 2.64 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014713021663114391		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.014713021663114391 | validation: 0.0389850959213665]
	TIME [epoch: 2.64 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013971099506940237		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.013971099506940237 | validation: 0.03086055913591389]
	TIME [epoch: 2.64 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014183694431125498		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.014183694431125498 | validation: 0.031947577574013196]
	TIME [epoch: 2.64 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014491382393273163		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.014491382393273163 | validation: 0.029001942701826267]
	TIME [epoch: 2.64 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012306307245341593		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.012306307245341593 | validation: 0.03271376853285161]
	TIME [epoch: 2.63 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010796199899218277		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.010796199899218277 | validation: 0.027617212537041372]
	TIME [epoch: 2.63 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011702662251491925		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.011702662251491925 | validation: 0.03742190439686406]
	TIME [epoch: 2.63 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011284416920332625		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.011284416920332625 | validation: 0.030605425872937932]
	TIME [epoch: 2.64 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01252769598076082		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.01252769598076082 | validation: 0.029453113330990044]
	TIME [epoch: 2.64 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012759066217748924		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.012759066217748924 | validation: 0.026081511818920614]
	TIME [epoch: 2.63 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014539718480282167		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.014539718480282167 | validation: 0.030688777249327583]
	TIME [epoch: 2.64 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019104183780162442		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.019104183780162442 | validation: 0.02958357322951274]
	TIME [epoch: 2.64 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01590435039104436		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.01590435039104436 | validation: 0.02969188965726063]
	TIME [epoch: 2.64 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011053469718607678		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.011053469718607678 | validation: 0.03309165734033327]
	TIME [epoch: 2.64 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010502196092976469		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.010502196092976469 | validation: 0.027938659995130545]
	TIME [epoch: 2.64 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012652334541686093		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.012652334541686093 | validation: 0.028417114913845666]
	TIME [epoch: 2.64 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014292141245295582		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.014292141245295582 | validation: 0.027936330823518984]
	TIME [epoch: 2.63 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013103496109287936		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.013103496109287936 | validation: 0.034410539600057015]
	TIME [epoch: 2.64 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015249779186805963		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.015249779186805963 | validation: 0.030454608961901542]
	TIME [epoch: 2.63 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011125124311985344		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.011125124311985344 | validation: 0.0323277391942568]
	TIME [epoch: 2.64 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011376859519968705		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.011376859519968705 | validation: 0.026036818151451025]
	TIME [epoch: 2.63 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010947916316145017		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.010947916316145017 | validation: 0.029585745622620886]
	TIME [epoch: 2.64 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01022319965875255		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.01022319965875255 | validation: 0.03438409439208299]
	TIME [epoch: 2.63 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011553181752282593		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.011553181752282593 | validation: 0.02896530792156631]
	TIME [epoch: 2.64 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011674320887986272		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.011674320887986272 | validation: 0.02596698673201238]
	TIME [epoch: 2.64 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014002566354533428		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.014002566354533428 | validation: 0.0347552312486634]
	TIME [epoch: 2.64 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015560600109682732		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.015560600109682732 | validation: 0.02730977582082056]
	TIME [epoch: 2.63 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016248691589021768		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.016248691589021768 | validation: 0.042085989193725915]
	TIME [epoch: 2.63 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017065958065293776		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.017065958065293776 | validation: 0.033864456203475134]
	TIME [epoch: 2.64 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012536126712690393		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.012536126712690393 | validation: 0.028132299697760543]
	TIME [epoch: 2.63 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010413112325701468		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.010413112325701468 | validation: 0.02697600453175533]
	TIME [epoch: 2.64 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010251409429186212		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.010251409429186212 | validation: 0.028486000631593678]
	TIME [epoch: 2.64 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010828141721664799		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.010828141721664799 | validation: 0.026215474575919828]
	TIME [epoch: 2.64 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010448729040291425		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.010448729040291425 | validation: 0.02706401197815892]
	TIME [epoch: 2.64 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010605703891732262		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.010605703891732262 | validation: 0.03255392643246929]
	TIME [epoch: 2.64 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013407587115653517		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.013407587115653517 | validation: 0.03411505359916405]
	TIME [epoch: 2.64 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017797133157395843		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.017797133157395843 | validation: 0.025057432081230947]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014272377835734665		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.014272377835734665 | validation: 0.029902708689463223]
	TIME [epoch: 2.63 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009825594497662724		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.009825594497662724 | validation: 0.03327768828954858]
	TIME [epoch: 2.62 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010776536931206358		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.010776536931206358 | validation: 0.02973543030431446]
	TIME [epoch: 2.62 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01102067516551553		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.01102067516551553 | validation: 0.031089896110557846]
	TIME [epoch: 2.62 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01061787019308801		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.01061787019308801 | validation: 0.031756532474123966]
	TIME [epoch: 2.62 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01069916482738868		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.01069916482738868 | validation: 0.024320060232605335]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_939.pth
	Model improved!!!
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011080042388190188		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.011080042388190188 | validation: 0.030732716577477195]
	TIME [epoch: 2.63 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01121231877183246		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.01121231877183246 | validation: 0.026612686051124858]
	TIME [epoch: 2.62 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015804483820736678		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.015804483820736678 | validation: 0.03304246218056796]
	TIME [epoch: 2.63 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01828697005638557		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.01828697005638557 | validation: 0.026923643757299588]
	TIME [epoch: 2.62 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0125746260550727		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.0125746260550727 | validation: 0.032243857055274654]
	TIME [epoch: 2.63 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00973118856442137		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.00973118856442137 | validation: 0.024556025141631656]
	TIME [epoch: 2.62 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01047460421651493		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.01047460421651493 | validation: 0.02454854658148238]
	TIME [epoch: 2.63 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009796867576057348		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.009796867576057348 | validation: 0.024042404552307373]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_947.pth
	Model improved!!!
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01088833030787865		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.01088833030787865 | validation: 0.020840614714990737]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_948.pth
	Model improved!!!
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01030372042420241		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.01030372042420241 | validation: 0.02735257789021659]
	TIME [epoch: 2.63 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00959153284135675		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.00959153284135675 | validation: 0.02323390779772342]
	TIME [epoch: 2.64 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00933286254119854		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.00933286254119854 | validation: 0.028449585546305934]
	TIME [epoch: 2.63 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011045089508839033		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.011045089508839033 | validation: 0.021965483998218894]
	TIME [epoch: 2.63 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01454886717850032		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.01454886717850032 | validation: 0.029926327075229176]
	TIME [epoch: 2.63 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018790321113245412		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.018790321113245412 | validation: 0.030127105019586098]
	TIME [epoch: 2.63 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014772040326468169		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.014772040326468169 | validation: 0.026163521172781614]
	TIME [epoch: 2.63 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010761298945872685		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.010761298945872685 | validation: 0.023008938470313378]
	TIME [epoch: 2.64 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009283228774980614		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.009283228774980614 | validation: 0.02590922874476438]
	TIME [epoch: 2.64 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009968787800156862		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.009968787800156862 | validation: 0.02139790832923797]
	TIME [epoch: 2.63 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00974016684416061		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.00974016684416061 | validation: 0.022019503224495685]
	TIME [epoch: 2.64 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009423307610275617		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.009423307610275617 | validation: 0.02617431197263065]
	TIME [epoch: 2.63 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0096644695877455		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0096644695877455 | validation: 0.033087483671563576]
	TIME [epoch: 2.64 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010893533132051462		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.010893533132051462 | validation: 0.02327330766189363]
	TIME [epoch: 2.63 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016175770306993926		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.016175770306993926 | validation: 0.03326114422901261]
	TIME [epoch: 2.63 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016306722440627922		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.016306722440627922 | validation: 0.027856940611206738]
	TIME [epoch: 2.63 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00933098777549778		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.00933098777549778 | validation: 0.029023067813770554]
	TIME [epoch: 2.64 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010151521288472668		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.010151521288472668 | validation: 0.02747959672542273]
	TIME [epoch: 2.63 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01295482088481943		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.01295482088481943 | validation: 0.026918455305200196]
	TIME [epoch: 2.64 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011324788488126245		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.011324788488126245 | validation: 0.029111338503647544]
	TIME [epoch: 2.63 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010787272905972657		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.010787272905972657 | validation: 0.025577390596400074]
	TIME [epoch: 2.64 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009690160094355851		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.009690160094355851 | validation: 0.025722602382650873]
	TIME [epoch: 2.63 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009101001297354996		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.009101001297354996 | validation: 0.029136550282551577]
	TIME [epoch: 2.64 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008517281010002327		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.008517281010002327 | validation: 0.02387443744118617]
	TIME [epoch: 2.63 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010128151972850347		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.010128151972850347 | validation: 0.027127640004726575]
	TIME [epoch: 2.63 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010363261576726695		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.010363261576726695 | validation: 0.032345514731592195]
	TIME [epoch: 2.63 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01031923755094205		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.01031923755094205 | validation: 0.029700622227453455]
	TIME [epoch: 2.63 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013805515045125718		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.013805515045125718 | validation: 0.02690503435684446]
	TIME [epoch: 2.63 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015988465733819943		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.015988465733819943 | validation: 0.03134852285160431]
	TIME [epoch: 2.64 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011795410231868614		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.011795410231868614 | validation: 0.021256518161615934]
	TIME [epoch: 2.63 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009778423529015775		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.009778423529015775 | validation: 0.024081853976028213]
	TIME [epoch: 2.63 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009718837916965522		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.009718837916965522 | validation: 0.036568572890726625]
	TIME [epoch: 2.63 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013221996226508744		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.013221996226508744 | validation: 0.021814122095517297]
	TIME [epoch: 2.63 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011024368873543218		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.011024368873543218 | validation: 0.02219504525293702]
	TIME [epoch: 2.63 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01032255249790012		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.01032255249790012 | validation: 0.02751805186096501]
	TIME [epoch: 2.64 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008734110410842244		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.008734110410842244 | validation: 0.026972768483622036]
	TIME [epoch: 2.64 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009923468203421078		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.009923468203421078 | validation: 0.02417329283752965]
	TIME [epoch: 2.64 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010414257026338006		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.010414257026338006 | validation: 0.02731445983456541]
	TIME [epoch: 2.64 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009179768640133864		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.009179768640133864 | validation: 0.02753712388407672]
	TIME [epoch: 2.64 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009545576300366688		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.009545576300366688 | validation: 0.024274206968011237]
	TIME [epoch: 2.63 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011046493000497875		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.011046493000497875 | validation: 0.022351499989454383]
	TIME [epoch: 2.64 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013426813935549666		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.013426813935549666 | validation: 0.027275135288605225]
	TIME [epoch: 2.63 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017896441328380787		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.017896441328380787 | validation: 0.03373952833604134]
	TIME [epoch: 2.64 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012047075284221793		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.012047075284221793 | validation: 0.025456370461683166]
	TIME [epoch: 2.63 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009436132830059929		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.009436132830059929 | validation: 0.026780043763518304]
	TIME [epoch: 2.64 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009796219022737602		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.009796219022737602 | validation: 0.022105538673942073]
	TIME [epoch: 2.63 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00923872003770683		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.00923872003770683 | validation: 0.02192604720634135]
	TIME [epoch: 2.64 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009951927890787995		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.009951927890787995 | validation: 0.02319360366623843]
	TIME [epoch: 2.63 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011962076073956492		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.011962076073956492 | validation: 0.028551755902762932]
	TIME [epoch: 2.63 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011602436991417336		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.011602436991417336 | validation: 0.025611645212962955]
	TIME [epoch: 2.63 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010409043118188634		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.010409043118188634 | validation: 0.020488753635558465]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_999.pth
	Model improved!!!
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00886865356983553		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.00886865356983553 | validation: 0.02135580423088166]
	TIME [epoch: 2.63 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008948575649243265		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.008948575649243265 | validation: 0.027563305735212974]
	TIME [epoch: 169 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009137479621249977		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.009137479621249977 | validation: 0.024389864684854535]
	TIME [epoch: 5.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009303471955459915		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.009303471955459915 | validation: 0.024788713073586266]
	TIME [epoch: 5.69 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010171720976881729		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.010171720976881729 | validation: 0.025084159374478388]
	TIME [epoch: 5.68 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01279438016538618		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.01279438016538618 | validation: 0.025329517812644]
	TIME [epoch: 5.69 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011578951135097486		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.011578951135097486 | validation: 0.02174768621798877]
	TIME [epoch: 5.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0096939873228053		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.0096939873228053 | validation: 0.0219592008057255]
	TIME [epoch: 5.68 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010336404095477477		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.010336404095477477 | validation: 0.020690506323073544]
	TIME [epoch: 5.69 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00997001807764994		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.00997001807764994 | validation: 0.025537008750792026]
	TIME [epoch: 5.69 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009166347797680656		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.009166347797680656 | validation: 0.022157825758497774]
	TIME [epoch: 5.69 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008902363425728092		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.008902363425728092 | validation: 0.02380120609190474]
	TIME [epoch: 5.69 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011035008791456333		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.011035008791456333 | validation: 0.027245369203045412]
	TIME [epoch: 5.69 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013520764575279318		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.013520764575279318 | validation: 0.025432041368101222]
	TIME [epoch: 5.69 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013840914055600645		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.013840914055600645 | validation: 0.02537318359830646]
	TIME [epoch: 5.68 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008987124587893476		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.008987124587893476 | validation: 0.024681783053050334]
	TIME [epoch: 5.69 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00947081386456425		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.00947081386456425 | validation: 0.02377150781228803]
	TIME [epoch: 5.68 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008961048501914412		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.008961048501914412 | validation: 0.025462160661463087]
	TIME [epoch: 5.69 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008745694187548017		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.008745694187548017 | validation: 0.022891162220422513]
	TIME [epoch: 5.68 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009043455021811751		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.009043455021811751 | validation: 0.019632009856773748]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1019.pth
	Model improved!!!
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008555442610938333		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.008555442610938333 | validation: 0.024631144334553358]
	TIME [epoch: 5.68 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009779318884561017		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.009779318884561017 | validation: 0.023419320117809297]
	TIME [epoch: 5.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01484746012819909		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.01484746012819909 | validation: 0.03143077698674762]
	TIME [epoch: 5.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014830544623418459		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.014830544623418459 | validation: 0.01970583622263211]
	TIME [epoch: 5.68 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009726234688603853		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.009726234688603853 | validation: 0.026207092680438072]
	TIME [epoch: 5.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008295324817624744		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.008295324817624744 | validation: 0.020578115000667387]
	TIME [epoch: 5.68 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007972847109504704		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.007972847109504704 | validation: 0.023219936321196878]
	TIME [epoch: 5.69 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00844806003773682		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.00844806003773682 | validation: 0.02530235749095995]
	TIME [epoch: 5.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009626550646895443		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.009626550646895443 | validation: 0.027052726510182847]
	TIME [epoch: 5.69 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010610116291094648		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.010610116291094648 | validation: 0.02604486668575572]
	TIME [epoch: 5.69 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011851159257556665		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.011851159257556665 | validation: 0.024107682879764225]
	TIME [epoch: 5.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010756034803943883		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.010756034803943883 | validation: 0.02223582623290027]
	TIME [epoch: 5.69 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008625063074026688		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.008625063074026688 | validation: 0.019748426195585356]
	TIME [epoch: 5.69 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009232380138728256		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.009232380138728256 | validation: 0.020332364802766845]
	TIME [epoch: 5.69 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00881535661551515		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.00881535661551515 | validation: 0.02444657945745539]
	TIME [epoch: 5.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008557152368851314		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.008557152368851314 | validation: 0.02491061020467541]
	TIME [epoch: 5.69 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008692342592378645		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.008692342592378645 | validation: 0.02527250239059573]
	TIME [epoch: 5.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009072668053214416		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.009072668053214416 | validation: 0.025371910490154182]
	TIME [epoch: 5.69 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008691091674839068		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.008691091674839068 | validation: 0.019969873108456906]
	TIME [epoch: 5.71 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008944653380320659		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.008944653380320659 | validation: 0.03054488870377923]
	TIME [epoch: 5.69 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009484955163541305		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.009484955163541305 | validation: 0.020210913848143355]
	TIME [epoch: 5.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010192200049866087		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.010192200049866087 | validation: 0.03128887703238288]
	TIME [epoch: 5.69 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01701559160388835		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.01701559160388835 | validation: 0.028689783387385162]
	TIME [epoch: 5.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01062128415719372		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.01062128415719372 | validation: 0.02051259354196108]
	TIME [epoch: 5.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008520859277350091		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.008520859277350091 | validation: 0.02403826656039154]
	TIME [epoch: 5.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00924983950788726		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.00924983950788726 | validation: 0.024232934791132935]
	TIME [epoch: 5.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008974399068709692		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.008974399068709692 | validation: 0.01973296079387329]
	TIME [epoch: 5.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008321793101286824		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.008321793101286824 | validation: 0.02422755928200765]
	TIME [epoch: 5.69 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008228766111271659		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.008228766111271659 | validation: 0.01897352646128807]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1048.pth
	Model improved!!!
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009879531402446252		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.009879531402446252 | validation: 0.028189978964242347]
	TIME [epoch: 5.69 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012466337474851331		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.012466337474851331 | validation: 0.022988754836771964]
	TIME [epoch: 5.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012445988833282462		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.012445988833282462 | validation: 0.023662296134134175]
	TIME [epoch: 5.69 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008666534570391632		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.008666534570391632 | validation: 0.02063607229485194]
	TIME [epoch: 5.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008199169282194834		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.008199169282194834 | validation: 0.02634070966139578]
	TIME [epoch: 5.69 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008888878453211674		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.008888878453211674 | validation: 0.020127871617470386]
	TIME [epoch: 5.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008866944631761124		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.008866944631761124 | validation: 0.024987020710259645]
	TIME [epoch: 5.68 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009838056407209759		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.009838056407209759 | validation: 0.02266161046148285]
	TIME [epoch: 5.72 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010994676603408622		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.010994676603408622 | validation: 0.02377889007598917]
	TIME [epoch: 5.69 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009146192465332194		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.009146192465332194 | validation: 0.0198707290057494]
	TIME [epoch: 5.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00867172239662045		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.00867172239662045 | validation: 0.02155317791765217]
	TIME [epoch: 5.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008566457794550807		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.008566457794550807 | validation: 0.02479479181734623]
	TIME [epoch: 5.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009534446110798516		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.009534446110798516 | validation: 0.029912171910575804]
	TIME [epoch: 5.69 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010988558148041394		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.010988558148041394 | validation: 0.020965750857463897]
	TIME [epoch: 5.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010012659329004345		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.010012659329004345 | validation: 0.022065468137140522]
	TIME [epoch: 5.69 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0075447946004352385		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.0075447946004352385 | validation: 0.02433617215253149]
	TIME [epoch: 5.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007495615949481951		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.007495615949481951 | validation: 0.019878801478446975]
	TIME [epoch: 5.69 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008280010718512061		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.008280010718512061 | validation: 0.022980109433661824]
	TIME [epoch: 5.69 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010521939256930322		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.010521939256930322 | validation: 0.02585005929823885]
	TIME [epoch: 5.69 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00985894206808087		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.00985894206808087 | validation: 0.021496353068262586]
	TIME [epoch: 5.68 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010620581356585541		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.010620581356585541 | validation: 0.015226078095376884]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1069.pth
	Model improved!!!
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009006648327511125		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.009006648327511125 | validation: 0.022435369317150934]
	TIME [epoch: 5.69 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008159351474168158		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.008159351474168158 | validation: 0.02476746550537643]
	TIME [epoch: 5.69 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0113351641024166		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.0113351641024166 | validation: 0.023947692092284992]
	TIME [epoch: 5.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012105333021204895		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.012105333021204895 | validation: 0.02669769682430341]
	TIME [epoch: 5.69 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009909429107271174		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.009909429107271174 | validation: 0.024324512805949328]
	TIME [epoch: 5.71 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007752644301396851		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.007752644301396851 | validation: 0.019533695902176764]
	TIME [epoch: 5.69 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008791102909043103		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.008791102909043103 | validation: 0.018732864785224113]
	TIME [epoch: 5.69 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009168549058728893		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.009168549058728893 | validation: 0.017248291870729193]
	TIME [epoch: 5.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010826069286121105		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.010826069286121105 | validation: 0.02414415707229385]
	TIME [epoch: 5.69 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008867253424049728		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.008867253424049728 | validation: 0.01733681500103276]
	TIME [epoch: 5.69 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00816290317739004		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.00816290317739004 | validation: 0.01899997114623571]
	TIME [epoch: 5.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009056221495663345		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.009056221495663345 | validation: 0.02211448878643968]
	TIME [epoch: 5.69 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008203939176955449		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.008203939176955449 | validation: 0.02134460299048282]
	TIME [epoch: 5.68 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008411714496971706		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.008411714496971706 | validation: 0.02382133587985169]
	TIME [epoch: 5.69 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00897945124934512		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.00897945124934512 | validation: 0.020802959825031133]
	TIME [epoch: 5.69 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007367265299593656		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.007367265299593656 | validation: 0.018286308926654206]
	TIME [epoch: 5.69 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008652483925251548		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.008652483925251548 | validation: 0.02196649991619124]
	TIME [epoch: 5.69 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010536486043004747		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.010536486043004747 | validation: 0.024910965475345062]
	TIME [epoch: 5.69 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009582336983564976		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.009582336983564976 | validation: 0.02309240866265241]
	TIME [epoch: 5.69 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009837414383461252		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.009837414383461252 | validation: 0.020177650769513025]
	TIME [epoch: 5.69 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008757059149657952		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.008757059149657952 | validation: 0.021587460820163495]
	TIME [epoch: 5.69 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007943691569936904		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.007943691569936904 | validation: 0.01906905137478611]
	TIME [epoch: 5.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008407500166320216		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.008407500166320216 | validation: 0.021223342576480897]
	TIME [epoch: 5.69 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00842270345082323		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.00842270345082323 | validation: 0.026712403309902145]
	TIME [epoch: 5.69 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008907615658375324		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.008907615658375324 | validation: 0.025053544088171932]
	TIME [epoch: 5.69 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008555977236398673		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.008555977236398673 | validation: 0.023208342903014424]
	TIME [epoch: 5.69 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009864915496435377		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.009864915496435377 | validation: 0.022346449337296282]
	TIME [epoch: 5.69 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00937324308127337		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.00937324308127337 | validation: 0.022951401230615422]
	TIME [epoch: 5.69 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00922592554286372		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.00922592554286372 | validation: 0.02338075635403976]
	TIME [epoch: 5.69 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00831025282135006		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.00831025282135006 | validation: 0.02229654323666406]
	TIME [epoch: 5.69 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008474155093788491		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.008474155093788491 | validation: 0.019765782631565088]
	TIME [epoch: 5.69 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009967424890254584		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.009967424890254584 | validation: 0.026300532763107867]
	TIME [epoch: 5.69 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011357358277659553		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.011357358277659553 | validation: 0.018926669413297792]
	TIME [epoch: 5.69 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00929096433314845		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.00929096433314845 | validation: 0.018275473208340255]
	TIME [epoch: 5.69 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00845311607379109		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.00845311607379109 | validation: 0.018720773717286966]
	TIME [epoch: 5.68 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008965937385658795		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.008965937385658795 | validation: 0.020037434065046256]
	TIME [epoch: 5.69 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009031866424876618		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.009031866424876618 | validation: 0.020667800774909373]
	TIME [epoch: 5.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007885663789205449		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.007885663789205449 | validation: 0.020654605043349275]
	TIME [epoch: 5.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009937494304705474		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.009937494304705474 | validation: 0.023614219852590813]
	TIME [epoch: 5.69 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010467709520027171		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.010467709520027171 | validation: 0.020879835556795526]
	TIME [epoch: 5.69 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00881088696067446		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.00881088696067446 | validation: 0.02028962669286735]
	TIME [epoch: 5.69 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00827689245880858		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.00827689245880858 | validation: 0.021598944171627367]
	TIME [epoch: 5.69 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008084741656524083		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.008084741656524083 | validation: 0.02186382692899427]
	TIME [epoch: 5.68 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008196570614604659		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.008196570614604659 | validation: 0.017014168597340673]
	TIME [epoch: 5.69 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007194761046991938		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.007194761046991938 | validation: 0.023484951250684217]
	TIME [epoch: 5.68 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008218101457603544		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.008218101457603544 | validation: 0.018545200400659247]
	TIME [epoch: 5.69 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008645393855550444		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.008645393855550444 | validation: 0.019194896575739386]
	TIME [epoch: 5.69 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009484475658928647		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.009484475658928647 | validation: 0.020215017149583715]
	TIME [epoch: 5.68 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009585054427947944		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.009585054427947944 | validation: 0.019083837007332086]
	TIME [epoch: 5.69 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007912727834313113		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.007912727834313113 | validation: 0.022804859873408825]
	TIME [epoch: 5.69 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007640050297268948		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.007640050297268948 | validation: 0.023799963992581587]
	TIME [epoch: 5.68 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008429732967495536		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.008429732967495536 | validation: 0.022215076448888538]
	TIME [epoch: 5.68 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009765753322456433		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.009765753322456433 | validation: 0.020103760962491682]
	TIME [epoch: 5.69 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01003823368859238		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.01003823368859238 | validation: 0.02207267854301892]
	TIME [epoch: 5.69 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008556360415825809		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.008556360415825809 | validation: 0.025989676718393386]
	TIME [epoch: 5.69 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007637759059666145		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.007637759059666145 | validation: 0.01870706683680068]
	TIME [epoch: 5.68 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008903319972418808		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.008903319972418808 | validation: 0.02469908980370794]
	TIME [epoch: 5.69 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008609585698037272		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.008609585698037272 | validation: 0.01922644194783887]
	TIME [epoch: 5.69 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00869121599484456		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.00869121599484456 | validation: 0.025210593598849165]
	TIME [epoch: 5.69 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00800779945106594		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.00800779945106594 | validation: 0.022758553497714162]
	TIME [epoch: 5.69 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008996461235364149		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.008996461235364149 | validation: 0.02118836122272725]
	TIME [epoch: 5.69 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010486110949666725		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.010486110949666725 | validation: 0.02489201591459962]
	TIME [epoch: 5.69 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0116686501298406		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.0116686501298406 | validation: 0.018595701955521262]
	TIME [epoch: 5.69 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007209617059141117		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.007209617059141117 | validation: 0.021113829350435667]
	TIME [epoch: 5.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008083776346104257		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.008083776346104257 | validation: 0.01556798931530965]
	TIME [epoch: 5.69 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008027869405059587		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.008027869405059587 | validation: 0.024424514291031388]
	TIME [epoch: 5.69 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00803408892252547		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.00803408892252547 | validation: 0.01778642965372834]
	TIME [epoch: 5.68 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00845813438435242		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.00845813438435242 | validation: 0.018938107574218724]
	TIME [epoch: 5.69 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00873470851075197		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.00873470851075197 | validation: 0.015647278047863766]
	TIME [epoch: 5.68 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0074570694212672956		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.0074570694212672956 | validation: 0.019932758799985895]
	TIME [epoch: 5.69 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008196893343453417		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.008196893343453417 | validation: 0.020240554342994134]
	TIME [epoch: 5.69 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009020561501573168		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.009020561501573168 | validation: 0.025865221813860684]
	TIME [epoch: 5.69 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008750557496440513		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.008750557496440513 | validation: 0.018324102447516987]
	TIME [epoch: 5.69 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008269574545514712		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.008269574545514712 | validation: 0.017744335173353]
	TIME [epoch: 5.68 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009557534424648548		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.009557534424648548 | validation: 0.02319473962175882]
	TIME [epoch: 5.69 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007965155558522183		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.007965155558522183 | validation: 0.021698513813858112]
	TIME [epoch: 5.68 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008000307938961514		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.008000307938961514 | validation: 0.018962003981541933]
	TIME [epoch: 5.69 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008432549094435528		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.008432549094435528 | validation: 0.01950998998463013]
	TIME [epoch: 5.68 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008099102675677695		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.008099102675677695 | validation: 0.022457067170771894]
	TIME [epoch: 5.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008055776006785047		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.008055776006785047 | validation: 0.01878531499863524]
	TIME [epoch: 5.69 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0077785830848030625		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.0077785830848030625 | validation: 0.019905653624469978]
	TIME [epoch: 5.69 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007346476172425472		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.007346476172425472 | validation: 0.021984190113586433]
	TIME [epoch: 5.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010423959350664647		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.010423959350664647 | validation: 0.020451757861139633]
	TIME [epoch: 5.69 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009754433216848483		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.009754433216848483 | validation: 0.019945045666677288]
	TIME [epoch: 5.69 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008197607137585283		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.008197607137585283 | validation: 0.015031196070077857]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1154.pth
	Model improved!!!
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007642862951247889		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.007642862951247889 | validation: 0.018750579999942465]
	TIME [epoch: 5.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008529370324116853		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.008529370324116853 | validation: 0.015367882574867054]
	TIME [epoch: 5.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007985683447519546		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.007985683447519546 | validation: 0.020904209695967735]
	TIME [epoch: 5.69 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007414301686748777		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.007414301686748777 | validation: 0.012900487945078021]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1158.pth
	Model improved!!!
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009817361360221984		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.009817361360221984 | validation: 0.022088057118465145]
	TIME [epoch: 5.69 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008848932032086872		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.008848932032086872 | validation: 0.015392638555564775]
	TIME [epoch: 5.69 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007937008982600148		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.007937008982600148 | validation: 0.018664950155254535]
	TIME [epoch: 5.69 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008610850405315403		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.008610850405315403 | validation: 0.022794979347938338]
	TIME [epoch: 5.69 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008274893766627895		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.008274893766627895 | validation: 0.019002886970827217]
	TIME [epoch: 5.69 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007304547588489028		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.007304547588489028 | validation: 0.02094377721663239]
	TIME [epoch: 5.69 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011251138488164742		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.011251138488164742 | validation: 0.020682568706088256]
	TIME [epoch: 5.69 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009909792796105002		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.009909792796105002 | validation: 0.02015448417304777]
	TIME [epoch: 5.69 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0070137752358213545		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.0070137752358213545 | validation: 0.02074245992406524]
	TIME [epoch: 5.69 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009139300145073304		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.009139300145073304 | validation: 0.022221442325636442]
	TIME [epoch: 5.69 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008844234223116806		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.008844234223116806 | validation: 0.02000677923590798]
	TIME [epoch: 5.69 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007199578026906942		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.007199578026906942 | validation: 0.02179157195132696]
	TIME [epoch: 5.68 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007605996649881961		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.007605996649881961 | validation: 0.021048897969700088]
	TIME [epoch: 5.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008088289598520542		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.008088289598520542 | validation: 0.018329796572440382]
	TIME [epoch: 5.68 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007900933540667931		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.007900933540667931 | validation: 0.018462163633955076]
	TIME [epoch: 5.7 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007773955307112446		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.007773955307112446 | validation: 0.020153822036779814]
	TIME [epoch: 5.69 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008281710794965666		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.008281710794965666 | validation: 0.017311468355441152]
	TIME [epoch: 5.7 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007689677055917771		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.007689677055917771 | validation: 0.01664802553620731]
	TIME [epoch: 5.68 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007674902606907578		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.007674902606907578 | validation: 0.019606254209418295]
	TIME [epoch: 5.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007518977483993787		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.007518977483993787 | validation: 0.020463549892125545]
	TIME [epoch: 5.69 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008124341711623883		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.008124341711623883 | validation: 0.020301033533664595]
	TIME [epoch: 5.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007042910339783496		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.007042910339783496 | validation: 0.017073633536785194]
	TIME [epoch: 5.69 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007405635460694241		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.007405635460694241 | validation: 0.018996455768100363]
	TIME [epoch: 5.7 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00770150819621599		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.00770150819621599 | validation: 0.01622560690861068]
	TIME [epoch: 5.69 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008055327047090659		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.008055327047090659 | validation: 0.02427201191100873]
	TIME [epoch: 5.7 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007562928927979031		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.007562928927979031 | validation: 0.02168877064418009]
	TIME [epoch: 5.69 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007965056294182095		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.007965056294182095 | validation: 0.019555963370938168]
	TIME [epoch: 5.7 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009925384813572871		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.009925384813572871 | validation: 0.018326303532660262]
	TIME [epoch: 5.69 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007597230511501152		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.007597230511501152 | validation: 0.01896273192944563]
	TIME [epoch: 5.69 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006706686955731059		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.006706686955731059 | validation: 0.02036149366208565]
	TIME [epoch: 5.69 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007286147808728058		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.007286147808728058 | validation: 0.02043216344543175]
	TIME [epoch: 5.69 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00881133306594388		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.00881133306594388 | validation: 0.02911350279188084]
	TIME [epoch: 5.69 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014131006990903828		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.014131006990903828 | validation: 0.021697102389353053]
	TIME [epoch: 5.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008937427534692488		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.008937427534692488 | validation: 0.024298207275909624]
	TIME [epoch: 5.69 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008524646666291158		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.008524646666291158 | validation: 0.022031613732730493]
	TIME [epoch: 5.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008161281185723014		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.008161281185723014 | validation: 0.01714349342085856]
	TIME [epoch: 5.69 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00785300076505199		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.00785300076505199 | validation: 0.020536367717112494]
	TIME [epoch: 5.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008427521941316417		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.008427521941316417 | validation: 0.016411595907711563]
	TIME [epoch: 5.69 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00782326113068682		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.00782326113068682 | validation: 0.018202114950570447]
	TIME [epoch: 5.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008287419615195824		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.008287419615195824 | validation: 0.02194770171265935]
	TIME [epoch: 5.69 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006354935033506968		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.006354935033506968 | validation: 0.019794544894255174]
	TIME [epoch: 5.7 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007350965386659441		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.007350965386659441 | validation: 0.018423273504291883]
	TIME [epoch: 5.68 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0077375098547929335		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.0077375098547929335 | validation: 0.01607798042312102]
	TIME [epoch: 5.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008789838166119663		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.008789838166119663 | validation: 0.01779569094435537]
	TIME [epoch: 5.68 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00783440199696328		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.00783440199696328 | validation: 0.015272215928352706]
	TIME [epoch: 5.69 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00785461864794117		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.00785461864794117 | validation: 0.020373432012710826]
	TIME [epoch: 5.68 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007257480342204768		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.007257480342204768 | validation: 0.023612250867152074]
	TIME [epoch: 5.68 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008197124604340435		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.008197124604340435 | validation: 0.01722198323641081]
	TIME [epoch: 5.68 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008024946457824537		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.008024946457824537 | validation: 0.015483592856362262]
	TIME [epoch: 5.69 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007776898065186456		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.007776898065186456 | validation: 0.018577932609219062]
	TIME [epoch: 5.68 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008166390332113365		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.008166390332113365 | validation: 0.018707791973456435]
	TIME [epoch: 5.69 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007478703716011915		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.007478703716011915 | validation: 0.015393717933270747]
	TIME [epoch: 5.68 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007255004801718486		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.007255004801718486 | validation: 0.01617855225032947]
	TIME [epoch: 5.69 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006888977460832902		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.006888977460832902 | validation: 0.02138755119820688]
	TIME [epoch: 5.68 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006362442889678242		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.006362442889678242 | validation: 0.018971147431629688]
	TIME [epoch: 5.69 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0076703762238768515		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.0076703762238768515 | validation: 0.01818773800786483]
	TIME [epoch: 5.68 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007487874354640501		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.007487874354640501 | validation: 0.01751868789283998]
	TIME [epoch: 5.7 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00788070878682053		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.00788070878682053 | validation: 0.01903142005552504]
	TIME [epoch: 5.68 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007464590987253089		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.007464590987253089 | validation: 0.019180692472858852]
	TIME [epoch: 5.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007979625102194307		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.007979625102194307 | validation: 0.016126636160585527]
	TIME [epoch: 5.69 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00804513722175096		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.00804513722175096 | validation: 0.014863280172013783]
	TIME [epoch: 5.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008089703824981592		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.008089703824981592 | validation: 0.01946825013040736]
	TIME [epoch: 5.69 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008454701125759825		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.008454701125759825 | validation: 0.01838301538153182]
	TIME [epoch: 5.69 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00793567173614949		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.00793567173614949 | validation: 0.019746578706486353]
	TIME [epoch: 5.69 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00856012661098893		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.00856012661098893 | validation: 0.01746784064827708]
	TIME [epoch: 5.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008900465712198108		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.008900465712198108 | validation: 0.020783771754934344]
	TIME [epoch: 5.68 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007441904972595155		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.007441904972595155 | validation: 0.022785306553241538]
	TIME [epoch: 5.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007811413317399893		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.007811413317399893 | validation: 0.01801150662891552]
	TIME [epoch: 5.69 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007552647460943136		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.007552647460943136 | validation: 0.01713795003393234]
	TIME [epoch: 5.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0071715419745831625		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.0071715419745831625 | validation: 0.018059007569774698]
	TIME [epoch: 5.69 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007172575233801551		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.007172575233801551 | validation: 0.017452024088866702]
	TIME [epoch: 5.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006826665202931818		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.006826665202931818 | validation: 0.017335317488601953]
	TIME [epoch: 5.68 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0072818417576109885		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.0072818417576109885 | validation: 0.02053197550617728]
	TIME [epoch: 5.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007244425913371419		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.007244425913371419 | validation: 0.018718413531368595]
	TIME [epoch: 5.69 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006769769969576841		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.006769769969576841 | validation: 0.01673371940873687]
	TIME [epoch: 5.69 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009263543760068462		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.009263543760068462 | validation: 0.023760770305146808]
	TIME [epoch: 5.68 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009673333513028088		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.009673333513028088 | validation: 0.01815672254310672]
	TIME [epoch: 5.69 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006624347515853077		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.006624347515853077 | validation: 0.018111057114349193]
	TIME [epoch: 5.69 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007703631923889021		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.007703631923889021 | validation: 0.02097210008266989]
	TIME [epoch: 5.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00864054707403105		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.00864054707403105 | validation: 0.014382700532621218]
	TIME [epoch: 5.69 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008147280276597932		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.008147280276597932 | validation: 0.01413641975937572]
	TIME [epoch: 5.7 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007578894466502891		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.007578894466502891 | validation: 0.02218266244330317]
	TIME [epoch: 5.69 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008146296346825998		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.008146296346825998 | validation: 0.01730166836059174]
	TIME [epoch: 5.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0074650324193724915		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.0074650324193724915 | validation: 0.0162282942437727]
	TIME [epoch: 5.69 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007602044151811962		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.007602044151811962 | validation: 0.017371946939497406]
	TIME [epoch: 5.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0068836908706738555		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.0068836908706738555 | validation: 0.019004651817232578]
	TIME [epoch: 5.69 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007612070184261528		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.007612070184261528 | validation: 0.016682737150466143]
	TIME [epoch: 5.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0073888019283922905		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.0073888019283922905 | validation: 0.018094528213736593]
	TIME [epoch: 5.68 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008907573207754267		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.008907573207754267 | validation: 0.018912590038601476]
	TIME [epoch: 5.69 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008437140172098605		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.008437140172098605 | validation: 0.023669452168700634]
	TIME [epoch: 5.69 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007705958564790664		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.007705958564790664 | validation: 0.016694198390867398]
	TIME [epoch: 5.7 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00682930484290465		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.00682930484290465 | validation: 0.01573971193194287]
	TIME [epoch: 5.68 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006840038826357297		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.006840038826357297 | validation: 0.017875785971938143]
	TIME [epoch: 5.69 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007027858845533397		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.007027858845533397 | validation: 0.017550909395093874]
	TIME [epoch: 5.69 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008113898833787046		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.008113898833787046 | validation: 0.015693883244650698]
	TIME [epoch: 5.7 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007779359317285024		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.007779359317285024 | validation: 0.019944990295973034]
	TIME [epoch: 5.69 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006910957319607696		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.006910957319607696 | validation: 0.02209000204791105]
	TIME [epoch: 5.69 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007202181932378127		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.007202181932378127 | validation: 0.01761270730017882]
	TIME [epoch: 5.69 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007257497775871716		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.007257497775871716 | validation: 0.015345129925211077]
	TIME [epoch: 5.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007698464263515618		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.007698464263515618 | validation: 0.02034248577797796]
	TIME [epoch: 5.69 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0070636039732433		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.0070636039732433 | validation: 0.013123063965537519]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_133424/states/model_phi1_4a_v_mmd1_1259.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 4057.801 seconds.
