Args:
Namespace(name='model_phi1_3a_v_mmd1', outdir='out/model_training/model_phi1_3a_v_mmd1', training_data='data/training_data/basic/data_phi1_3a/training', validation_data='data/training_data/basic/data_phi1_3a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2164694306

Training model...

Saving initial model state to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.78711023885212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.78711023885212 | validation: 3.4069661853607616]
	TIME [epoch: 253 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.350402589134805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.350402589134805 | validation: 3.235083512320513]
	TIME [epoch: 0.812 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.277870414045333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.277870414045333 | validation: 3.10134590275541]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.078473610699611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.078473610699611 | validation: 3.21802935399254]
	TIME [epoch: 0.703 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9696029471868304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9696029471868304 | validation: 3.0426172115663443]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.697167612404532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.697167612404532 | validation: 3.140992400410994]
	TIME [epoch: 0.702 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.447542379874186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.447542379874186 | validation: 2.2716557299088733]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0912975574972283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0912975574972283 | validation: 6.191984938256019]
	TIME [epoch: 0.699 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.388654519415151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.388654519415151 | validation: 5.047499322228038]
	TIME [epoch: 0.698 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.517061213374819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.517061213374819 | validation: 2.6702237459858944]
	TIME [epoch: 0.701 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5282101126869048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5282101126869048 | validation: 2.5065801025397305]
	TIME [epoch: 0.705 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1534413982579834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1534413982579834 | validation: 2.3808913301064085]
	TIME [epoch: 0.702 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9887877254213648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9887877254213648 | validation: 2.8182646219656196]
	TIME [epoch: 0.701 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8280143577550225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8280143577550225 | validation: 2.496620095497904]
	TIME [epoch: 0.702 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0897077281702026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0897077281702026 | validation: 2.1642967845879446]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0251774235276043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0251774235276043 | validation: 1.8952301744796003]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6064646048176388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6064646048176388 | validation: 1.7932463470728899]
	TIME [epoch: 0.698 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8838907906730544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8838907906730544 | validation: 1.7245196345066844]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3749511186273333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3749511186273333 | validation: 1.829768307812184]
	TIME [epoch: 0.757 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4937014434498355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4937014434498355 | validation: 1.636068793220658]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3632351692364197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3632351692364197 | validation: 1.6591167527310429]
	TIME [epoch: 0.703 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2893555013868696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2893555013868696 | validation: 1.6716423459509961]
	TIME [epoch: 0.7 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2788292977088724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2788292977088724 | validation: 1.6462749498023925]
	TIME [epoch: 0.699 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.27471508092975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.27471508092975 | validation: 1.6456909474612296]
	TIME [epoch: 0.699 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2614390274353047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2614390274353047 | validation: 1.634212590028354]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2475812131158157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2475812131158157 | validation: 1.620684536667832]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2380377202570692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2380377202570692 | validation: 1.6102631922225528]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2245451059851367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2245451059851367 | validation: 1.5989279098686766]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2192616058133054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2192616058133054 | validation: 1.5759033827589517]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2093851323012277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2093851323012277 | validation: 1.564650657728155]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.210806338413357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.210806338413357 | validation: 1.5486675570635668]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2162623766993301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2162623766993301 | validation: 1.583847423260461]
	TIME [epoch: 0.7 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2567654731576334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2567654731576334 | validation: 1.5456570364438054]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.299066323897377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.299066323897377 | validation: 1.590248113957356]
	TIME [epoch: 0.706 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.24710879866379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.24710879866379 | validation: 1.5012890467242075]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1784951235771106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1784951235771106 | validation: 1.4880846316580858]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1470959932467082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1470959932467082 | validation: 1.4563580513332348]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.134104644236025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.134104644236025 | validation: 1.4497473336950606]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120424084463372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.120424084463372 | validation: 1.4176916448159966]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.10932195632195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.10932195632195 | validation: 1.427710590906213]
	TIME [epoch: 0.704 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093900427956174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1093900427956174 | validation: 1.4034337206872756]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1185037500429449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1185037500429449 | validation: 1.4150355395174525]
	TIME [epoch: 0.703 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1480752394068092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1480752394068092 | validation: 1.41078440608227]
	TIME [epoch: 0.701 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1746819543978209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1746819543978209 | validation: 1.4027977038723964]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1509545675903716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1509545675903716 | validation: 1.4042010568287508]
	TIME [epoch: 0.701 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0988549623779027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0988549623779027 | validation: 1.3372117167222208]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0974573425236593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0974573425236593 | validation: 1.4126590898716107]
	TIME [epoch: 0.703 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.109501765486167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.109501765486167 | validation: 1.3011048280513713]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0527997306020778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0527997306020778 | validation: 1.3197431408885891]
	TIME [epoch: 0.703 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0391249973183387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0391249973183387 | validation: 1.261731548694805]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0081670900089044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0081670900089044 | validation: 1.2316507756129942]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.995459617433691		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 0.995459617433691 | validation: 1.2299105824192043]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9826825631335468		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9826825631335468 | validation: 1.200727700588017]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9841826949120644		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.9841826949120644 | validation: 1.3095188779669344]
	TIME [epoch: 0.698 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0421735199000646		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0421735199000646 | validation: 1.2631248512911324]
	TIME [epoch: 0.703 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.127255081435458		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.127255081435458 | validation: 1.2608352421401354]
	TIME [epoch: 0.702 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9864844746140289		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9864844746140289 | validation: 1.2080791053837083]
	TIME [epoch: 0.7 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9824862986479836		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.9824862986479836 | validation: 1.1566549002278321]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9832293143096651		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.9832293143096651 | validation: 1.1291965881859327]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9506502975637201		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.9506502975637201 | validation: 1.1288316513793728]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9359575210837552		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.9359575210837552 | validation: 1.101483191079372]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293608670901415		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.9293608670901415 | validation: 1.211467013799761]
	TIME [epoch: 0.704 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9898647055077858		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9898647055077858 | validation: 1.164389367628809]
	TIME [epoch: 0.703 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0307597905365546		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 1.0307597905365546 | validation: 1.1462407324484407]
	TIME [epoch: 0.702 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9366504487875557		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9366504487875557 | validation: 1.055231657169765]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8911587895356625		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.8911587895356625 | validation: 0.9941991892016132]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799055402954877		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.8799055402954877 | validation: 1.036621181920991]
	TIME [epoch: 0.703 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795678835148354		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.8795678835148354 | validation: 1.0285221295668119]
	TIME [epoch: 0.702 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9207571138441557		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9207571138441557 | validation: 1.1457224805011028]
	TIME [epoch: 0.701 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9686857579938508		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.9686857579938508 | validation: 1.1091377242396947]
	TIME [epoch: 0.7 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9981956296454186		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.9981956296454186 | validation: 1.0271774953278847]
	TIME [epoch: 0.701 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750134173571366		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8750134173571366 | validation: 1.0108691344556433]
	TIME [epoch: 0.702 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006259180050284		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9006259180050284 | validation: 1.0081661688808627]
	TIME [epoch: 0.701 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9089390503686064		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9089390503686064 | validation: 0.9670488247282709]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688716164450668		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8688716164450668 | validation: 0.949267158911228]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506456272488027		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.8506456272488027 | validation: 0.9022712323270838]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506578075652858		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8506578075652858 | validation: 0.9942815834844412]
	TIME [epoch: 0.702 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8977881765921976		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8977881765921976 | validation: 1.012891828545994]
	TIME [epoch: 0.704 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422846898201778		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9422846898201778 | validation: 1.0542164912795418]
	TIME [epoch: 0.698 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212920309914472		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.9212920309914472 | validation: 0.933267759113045]
	TIME [epoch: 0.696 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8357324001289947		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8357324001289947 | validation: 0.8886870047641026]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276724758227105		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.8276724758227105 | validation: 0.8799562894255062]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8208304516277996		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.8208304516277996 | validation: 0.8628157897254612]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174421429804906		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8174421429804906 | validation: 0.8831766721419833]
	TIME [epoch: 0.718 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133242258820436		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8133242258820436 | validation: 0.9005174408057788]
	TIME [epoch: 0.705 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831443513350743		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.831443513350743 | validation: 0.9650148915715552]
	TIME [epoch: 0.699 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9021789877501978		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.9021789877501978 | validation: 0.9788697010110218]
	TIME [epoch: 0.702 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8913611691039249		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.8913611691039249 | validation: 0.91878879959516]
	TIME [epoch: 0.7 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713151199509003		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.8713151199509003 | validation: 0.9308787129037708]
	TIME [epoch: 0.699 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828745643321279		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8828745643321279 | validation: 0.8278546993243886]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8343085788548106		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8343085788548106 | validation: 0.8667371900105567]
	TIME [epoch: 0.705 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8128974812176404		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8128974812176404 | validation: 0.8337294092726989]
	TIME [epoch: 0.703 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872805588516729		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.7872805588516729 | validation: 0.8222073582193903]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78567799568769		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.78567799568769 | validation: 0.8395100249852345]
	TIME [epoch: 0.702 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957227021914782		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.7957227021914782 | validation: 0.8228886084385769]
	TIME [epoch: 0.702 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038862679308298		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.8038862679308298 | validation: 0.8608275084425672]
	TIME [epoch: 0.703 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241665505812259		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8241665505812259 | validation: 0.9241699887016019]
	TIME [epoch: 0.701 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858053150025006		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.858053150025006 | validation: 0.9972898444308007]
	TIME [epoch: 0.701 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9165002624929313		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9165002624929313 | validation: 0.9381804909011667]
	TIME [epoch: 0.699 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823419561439656		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8823419561439656 | validation: 0.8047606602476144]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827716168819923		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.7827716168819923 | validation: 0.8299212231028483]
	TIME [epoch: 0.703 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862353469176984		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.7862353469176984 | validation: 0.812894769498063]
	TIME [epoch: 0.702 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867400843162523		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.7867400843162523 | validation: 0.8015672107957714]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764335325957426		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.764335325957426 | validation: 0.7906251299509384]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524113632732093		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.7524113632732093 | validation: 0.7939342023201622]
	TIME [epoch: 0.703 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571143492752953		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.7571143492752953 | validation: 0.7804595439342533]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7594917670494511		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.7594917670494511 | validation: 0.822574641139799]
	TIME [epoch: 0.705 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7835439284740037		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.7835439284740037 | validation: 0.79466519509708]
	TIME [epoch: 0.701 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168502549204223		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8168502549204223 | validation: 0.8713139513627189]
	TIME [epoch: 0.7 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467321768459606		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8467321768459606 | validation: 0.7948304478771502]
	TIME [epoch: 0.699 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484487856503321		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.7484487856503321 | validation: 0.7744105909475817]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770286802844009		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.770286802844009 | validation: 0.8439932589645232]
	TIME [epoch: 0.702 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861969380356177		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.7861969380356177 | validation: 0.8012334258822341]
	TIME [epoch: 0.701 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7702450494322404		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7702450494322404 | validation: 0.8261864450917792]
	TIME [epoch: 0.703 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827112400679317		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.7827112400679317 | validation: 0.8248235212610183]
	TIME [epoch: 0.702 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922863959765466		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7922863959765466 | validation: 0.7971999227260874]
	TIME [epoch: 0.699 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638278587603999		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7638278587603999 | validation: 0.7667989119175787]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371959038453081		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.7371959038453081 | validation: 0.7416126205874405]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7223045715115195		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7223045715115195 | validation: 0.7313020251589221]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7116759632403578		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.7116759632403578 | validation: 0.7251171646757832]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153543301815198		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.7153543301815198 | validation: 0.7376929161536698]
	TIME [epoch: 0.701 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263359510875919		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7263359510875919 | validation: 0.8131340428800509]
	TIME [epoch: 0.703 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7947780025981357		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7947780025981357 | validation: 0.8698928235171846]
	TIME [epoch: 0.705 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829752074900138		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.829752074900138 | validation: 0.7392973403590559]
	TIME [epoch: 0.699 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380728328897971		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.7380728328897971 | validation: 0.7203932622638467]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7453179933086199		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.7453179933086199 | validation: 0.7869755035031943]
	TIME [epoch: 0.703 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7279852140691915		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.7279852140691915 | validation: 0.6921094706513891]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7011790077805117		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.7011790077805117 | validation: 0.6837426479601951]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6920516156763756		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.6920516156763756 | validation: 0.704101504875961]
	TIME [epoch: 0.706 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6914660697103184		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.6914660697103184 | validation: 0.6903288541588029]
	TIME [epoch: 0.707 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6991730597930118		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.6991730597930118 | validation: 0.7322155053826015]
	TIME [epoch: 0.704 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162562550050775		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.7162562550050775 | validation: 0.7956259483077104]
	TIME [epoch: 0.705 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743970344847133		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7743970344847133 | validation: 0.8466778664888346]
	TIME [epoch: 0.701 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237579302033204		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8237579302033204 | validation: 0.7151805251398691]
	TIME [epoch: 0.704 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6837649856778579		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.6837649856778579 | validation: 0.6990334926332874]
	TIME [epoch: 0.703 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7062200600808413		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7062200600808413 | validation: 0.8072656897568318]
	TIME [epoch: 0.705 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463490117768116		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7463490117768116 | validation: 0.6702895360466787]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_137.pth
	Model improved!!!
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649741232881333		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.6649741232881333 | validation: 0.6545891040220471]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637062278055826		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.6637062278055826 | validation: 0.6767114420232361]
	TIME [epoch: 0.704 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6633148226700094		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6633148226700094 | validation: 0.6526321026066133]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6636017418061146		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.6636017418061146 | validation: 0.6574051867316134]
	TIME [epoch: 0.703 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6528666659057782		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.6528666659057782 | validation: 0.6380938848614557]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612485116017723		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.6612485116017723 | validation: 0.7059129648209669]
	TIME [epoch: 0.702 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6926382966975442		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.6926382966975442 | validation: 0.6774499443144422]
	TIME [epoch: 0.702 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7191548591019865		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7191548591019865 | validation: 0.6825322601325625]
	TIME [epoch: 0.703 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709577630926471		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.709577630926471 | validation: 0.762558514277424]
	TIME [epoch: 0.701 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214707944723352		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.7214707944723352 | validation: 0.6534313196528218]
	TIME [epoch: 0.701 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903524656908033		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6903524656908033 | validation: 0.6644678688445318]
	TIME [epoch: 0.701 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6497550070069223		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.6497550070069223 | validation: 0.6016947472360776]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332434241327143		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6332434241327143 | validation: 0.6375278157055296]
	TIME [epoch: 0.706 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340082076000929		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6340082076000929 | validation: 0.6324266527326139]
	TIME [epoch: 0.702 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6614464002378572		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6614464002378572 | validation: 0.6980975595124458]
	TIME [epoch: 0.706 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894312942886596		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.6894312942886596 | validation: 0.6115052281720078]
	TIME [epoch: 0.704 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470657330590792		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.6470657330590792 | validation: 0.6070766488523512]
	TIME [epoch: 0.704 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6186036347534056		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6186036347534056 | validation: 0.5732063795077611]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6040550032520713		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6040550032520713 | validation: 0.6220243992261616]
	TIME [epoch: 0.704 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6060772407471141		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6060772407471141 | validation: 0.5846765736745964]
	TIME [epoch: 0.707 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6306914873085259		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6306914873085259 | validation: 0.7469596894804629]
	TIME [epoch: 0.705 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7161407110078463		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7161407110078463 | validation: 0.6035469901805822]
	TIME [epoch: 0.702 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6225188153854259		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6225188153854259 | validation: 0.5489884221888922]
	TIME [epoch: 0.702 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5959707007738134		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.5959707007738134 | validation: 0.5741010384113034]
	TIME [epoch: 0.704 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5956084176741314		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.5956084176741314 | validation: 0.538489124013588]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941805304874713		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.5941805304874713 | validation: 0.5766928660667173]
	TIME [epoch: 0.705 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5854666446494993		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.5854666446494993 | validation: 0.5585706200709409]
	TIME [epoch: 0.705 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5925139248234913		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.5925139248234913 | validation: 0.6689863928629701]
	TIME [epoch: 0.706 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704155913478644		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6704155913478644 | validation: 0.6198849239689475]
	TIME [epoch: 0.701 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6597158967354784		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.6597158967354784 | validation: 0.5325799171041941]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5747891339154421		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.5747891339154421 | validation: 0.49531079020510727]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469672516699963		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.5469672516699963 | validation: 0.5274029103379846]
	TIME [epoch: 0.703 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5559857552129079		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.5559857552129079 | validation: 0.4771974523539953]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5473319118897584		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.5473319118897584 | validation: 0.4884568496352623]
	TIME [epoch: 0.704 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316730166552577		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.5316730166552577 | validation: 0.4525511912269631]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5195428530856664		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.5195428530856664 | validation: 0.5496628012725177]
	TIME [epoch: 0.703 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5732347958433228		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.5732347958433228 | validation: 0.6690477125757371]
	TIME [epoch: 0.702 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216321706292126		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7216321706292126 | validation: 0.6618892517623401]
	TIME [epoch: 0.701 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541505083594501		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6541505083594501 | validation: 0.4500534208826631]
	TIME [epoch: 0.701 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49600808059841267		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.49600808059841267 | validation: 0.5264870500924]
	TIME [epoch: 0.698 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5702309401456985		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.5702309401456985 | validation: 0.5318172077893818]
	TIME [epoch: 0.7 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5407805757530926		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.5407805757530926 | validation: 0.45814473862836186]
	TIME [epoch: 0.698 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.510089361594509		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.510089361594509 | validation: 0.42562675514918374]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47762225960827775		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.47762225960827775 | validation: 0.4350577394260146]
	TIME [epoch: 0.704 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4819234814513941		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.4819234814513941 | validation: 0.4274814573751753]
	TIME [epoch: 0.704 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4931539379688185		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.4931539379688185 | validation: 0.45779686566193406]
	TIME [epoch: 0.705 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4944785484902802		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.4944785484902802 | validation: 0.4565994646535586]
	TIME [epoch: 0.703 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111595919799428		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.5111595919799428 | validation: 0.5275135509954153]
	TIME [epoch: 0.702 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401065133483164		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5401065133483164 | validation: 0.4517392979089538]
	TIME [epoch: 0.701 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49877037769940613		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.49877037769940613 | validation: 0.43565618644021]
	TIME [epoch: 0.703 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4444429814815557		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.4444429814815557 | validation: 0.3715954459156605]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41688282783737307		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.41688282783737307 | validation: 0.36955566954587044]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4121925738425628		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.4121925738425628 | validation: 0.37492727468914533]
	TIME [epoch: 0.706 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41985778607478835		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.41985778607478835 | validation: 0.3953744488272589]
	TIME [epoch: 0.708 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44647197040015657		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.44647197040015657 | validation: 0.6641513707538136]
	TIME [epoch: 0.704 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616007805267797		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.616007805267797 | validation: 0.4625433605284899]
	TIME [epoch: 0.703 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5184493580103811		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.5184493580103811 | validation: 0.3899330996984525]
	TIME [epoch: 0.704 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39148010542072165		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.39148010542072165 | validation: 0.3528634861519442]
	TIME [epoch: 0.703 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37540199874897606		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.37540199874897606 | validation: 0.3547954056955374]
	TIME [epoch: 0.705 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36721347883678596		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.36721347883678596 | validation: 0.3683578376039498]
	TIME [epoch: 0.705 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38026092464080175		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.38026092464080175 | validation: 0.3834811601514936]
	TIME [epoch: 0.707 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4119397908827245		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.4119397908827245 | validation: 0.507638100015758]
	TIME [epoch: 0.705 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.489727881263202		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.489727881263202 | validation: 0.4003969764367364]
	TIME [epoch: 0.702 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4362242997622211		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.4362242997622211 | validation: 0.3809063838125509]
	TIME [epoch: 265 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37345455148922285		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.37345455148922285 | validation: 0.32651422118604806]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.332171450229571		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.332171450229571 | validation: 0.3051600750590948]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3127771419818114		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.3127771419818114 | validation: 0.31964515116041126]
	TIME [epoch: 1.38 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3011512126220189		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.3011512126220189 | validation: 0.3031103148350189]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2956310190733373		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.2956310190733373 | validation: 0.3253087099004844]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30378811836809183		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.30378811836809183 | validation: 0.37841543370600617]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3744066923517885		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.3744066923517885 | validation: 0.645301316058516]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5878379588458295		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.5878379588458295 | validation: 0.6538077263057365]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509133297096219		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7509133297096219 | validation: 0.3270448396955935]
	TIME [epoch: 1.38 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2773282578438794		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.2773282578438794 | validation: 0.4350342211913443]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4168425448027736		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.4168425448027736 | validation: 0.37497792167222754]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3442817012878402		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.3442817012878402 | validation: 0.3315220246731949]
	TIME [epoch: 1.38 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28309271604491965		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.28309271604491965 | validation: 0.31519160696769]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2732276757748408		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.2732276757748408 | validation: 0.3221549697162656]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742053473861076		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.2742053473861076 | validation: 0.27737239554630605]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24570201130836034		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.24570201130836034 | validation: 0.27790883576518216]
	TIME [epoch: 1.39 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2262793728044022		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.2262793728044022 | validation: 0.25791485547609716]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21749653913212427		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.21749653913212427 | validation: 0.28233696795629726]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21110053949585028		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.21110053949585028 | validation: 0.2577714082210494]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20942169034462527		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.20942169034462527 | validation: 0.30142891432419955]
	TIME [epoch: 1.37 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2313698470835538		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.2313698470835538 | validation: 0.4560438175896314]
	TIME [epoch: 1.37 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42568847096396284		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.42568847096396284 | validation: 0.557964912717144]
	TIME [epoch: 1.37 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337428662508383		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7337428662508383 | validation: 0.3052066713184767]
	TIME [epoch: 1.37 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21529267344131353		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.21529267344131353 | validation: 0.3997454717760154]
	TIME [epoch: 1.38 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37407473432643523		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.37407473432643523 | validation: 0.36601405151956967]
	TIME [epoch: 1.38 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2844338483041488		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.2844338483041488 | validation: 0.2941132536877136]
	TIME [epoch: 1.38 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20917607423805593		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.20917607423805593 | validation: 0.2628445814363515]
	TIME [epoch: 1.38 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22510508783647176		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.22510508783647176 | validation: 0.28357259979589355]
	TIME [epoch: 1.38 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19965679751192178		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.19965679751192178 | validation: 0.25964263154805434]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18681177338550778		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.18681177338550778 | validation: 0.24138200560944828]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18029377395331814		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.18029377395331814 | validation: 0.2504593380734836]
	TIME [epoch: 1.37 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17837173118578442		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.17837173118578442 | validation: 0.23170857675817036]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17317086603424778		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.17317086603424778 | validation: 0.22195614737221528]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635564507814572		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.1635564507814572 | validation: 0.2160181110381908]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16484409172541803		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.16484409172541803 | validation: 0.2444984675230696]
	TIME [epoch: 1.39 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1819257719563144		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.1819257719563144 | validation: 0.2843009389014591]
	TIME [epoch: 1.39 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22061619922645392		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.22061619922645392 | validation: 0.3199840285428128]
	TIME [epoch: 1.38 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26387354571128635		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.26387354571128635 | validation: 0.37552997658152504]
	TIME [epoch: 1.38 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.335101884365821		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.335101884365821 | validation: 0.5842153522405685]
	TIME [epoch: 1.38 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5297197884077252		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.5297197884077252 | validation: 0.37614788084126616]
	TIME [epoch: 1.38 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2870504307255192		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.2870504307255192 | validation: 0.2834516186973657]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1868956455516653		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.1868956455516653 | validation: 0.2323263575464999]
	TIME [epoch: 1.38 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20524663261249299		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.20524663261249299 | validation: 0.24979723180116184]
	TIME [epoch: 1.38 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17560772471236724		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.17560772471236724 | validation: 0.22287086247374283]
	TIME [epoch: 1.38 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15017736683665245		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.15017736683665245 | validation: 0.19560363248318988]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15692438203597917		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.15692438203597917 | validation: 0.20779813396607089]
	TIME [epoch: 1.38 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697598545645638		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.14697598545645638 | validation: 0.1901829009239486]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13589973717722864		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.13589973717722864 | validation: 0.18750821949502153]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13536161266624586		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.13536161266624586 | validation: 0.18443548697007195]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12916853639463616		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.12916853639463616 | validation: 0.1612510148292553]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12189012029452034		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.12189012029452034 | validation: 0.18887693940276717]
	TIME [epoch: 1.38 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12557467851359003		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.12557467851359003 | validation: 0.15599444065379348]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260000668844114		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.1260000668844114 | validation: 0.2911454929396573]
	TIME [epoch: 1.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19968277904819626		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.19968277904819626 | validation: 0.6128269494464598]
	TIME [epoch: 1.39 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.566257288799938		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.566257288799938 | validation: 0.4558213033744465]
	TIME [epoch: 1.38 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4173083317942824		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.4173083317942824 | validation: 0.25283575985540835]
	TIME [epoch: 1.38 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16402798142389863		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.16402798142389863 | validation: 0.19955826281732292]
	TIME [epoch: 1.38 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1960811141031004		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.1960811141031004 | validation: 0.24584819620078885]
	TIME [epoch: 1.38 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1724718131995592		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.1724718131995592 | validation: 0.20347007705542222]
	TIME [epoch: 1.38 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12446637406872826		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.12446637406872826 | validation: 0.1803833875053883]
	TIME [epoch: 1.38 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13476185674877098		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.13476185674877098 | validation: 0.16749246792728634]
	TIME [epoch: 1.38 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1198550432827469		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.1198550432827469 | validation: 0.1558748838671724]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11594690101810391		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.11594690101810391 | validation: 0.15315336957377962]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11256218231260956		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.11256218231260956 | validation: 0.15097092216395536]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072262053365763		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.1072262053365763 | validation: 0.16087444474435647]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10834612630215461		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.10834612630215461 | validation: 0.16787272042505733]
	TIME [epoch: 1.38 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12460964653182627		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.12460964653182627 | validation: 0.19917550871448936]
	TIME [epoch: 1.38 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615272826208478		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.1615272826208478 | validation: 0.22250544991611215]
	TIME [epoch: 1.39 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16335768325050962		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.16335768325050962 | validation: 0.1515443374565473]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12505759781443235		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.12505759781443235 | validation: 0.20704711246110233]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1332820895384961		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.1332820895384961 | validation: 0.2511558762303997]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27114628425923687		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.27114628425923687 | validation: 0.43111074562037754]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021367734506227		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5021367734506227 | validation: 0.1888788875656892]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11679422358347145		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.11679422358347145 | validation: 0.2045158014060114]
	TIME [epoch: 1.38 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1981965245410359		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.1981965245410359 | validation: 0.23785360662967028]
	TIME [epoch: 1.38 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16447951299968203		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.16447951299968203 | validation: 0.15454176226720687]
	TIME [epoch: 1.38 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10887133543969142		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.10887133543969142 | validation: 0.1363378644022692]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11638003194772925		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.11638003194772925 | validation: 0.17309812839916905]
	TIME [epoch: 1.38 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10870634900713783		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.10870634900713783 | validation: 0.12830833753660562]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09564252727238387		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.09564252727238387 | validation: 0.12588398148508528]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09058537906160168		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.09058537906160168 | validation: 0.1304382526655315]
	TIME [epoch: 1.38 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0864933939116702		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.0864933939116702 | validation: 0.11692083189006248]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555995569439234		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.08555995569439234 | validation: 0.12459988196754385]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08580491563775489		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.08580491563775489 | validation: 0.12385426783225827]
	TIME [epoch: 1.38 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08916043808752122		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.08916043808752122 | validation: 0.15110439271639164]
	TIME [epoch: 1.38 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969302227033996		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.0969302227033996 | validation: 0.14069429570041433]
	TIME [epoch: 1.38 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15223079807845763		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.15223079807845763 | validation: 0.3284953219126565]
	TIME [epoch: 1.38 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2887663859169088		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.2887663859169088 | validation: 0.265118333027109]
	TIME [epoch: 1.38 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24343165841498518		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.24343165841498518 | validation: 0.2235538964719692]
	TIME [epoch: 1.38 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19807687436631136		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.19807687436631136 | validation: 0.12549831385233487]
	TIME [epoch: 1.38 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530714489192668		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.09530714489192668 | validation: 0.12250339031932578]
	TIME [epoch: 1.38 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09843751538639285		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.09843751538639285 | validation: 0.1493892247313354]
	TIME [epoch: 1.38 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10703480620988223		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.10703480620988223 | validation: 0.10440604843899502]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08666548843821702		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.08666548843821702 | validation: 0.1114300529138395]
	TIME [epoch: 1.38 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07595573303795869		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.07595573303795869 | validation: 0.09741822237866515]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569027243268499		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.07569027243268499 | validation: 0.0990210755921358]
	TIME [epoch: 1.38 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07586905033167268		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.07586905033167268 | validation: 0.09635814313422736]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07312514122371715		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.07312514122371715 | validation: 0.09370854364171338]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08191721127381385		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.08191721127381385 | validation: 0.15506336889446992]
	TIME [epoch: 1.38 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10996596941984446		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.10996596941984446 | validation: 0.21476555245737902]
	TIME [epoch: 1.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19766765380740217		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.19766765380740217 | validation: 0.32093452345278983]
	TIME [epoch: 1.38 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33586155315834476		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.33586155315834476 | validation: 0.16532537098984837]
	TIME [epoch: 1.39 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13547778378170422		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.13547778378170422 | validation: 0.13108402299662478]
	TIME [epoch: 1.39 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12467868358767203		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.12467868358767203 | validation: 0.16282763465726416]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11182907892576932		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.11182907892576932 | validation: 0.11254126604515778]
	TIME [epoch: 1.38 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966434412418837		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.0966434412418837 | validation: 0.08242648928470431]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08671409043759386		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.08671409043759386 | validation: 0.10957465672517835]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07433285646684408		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.07433285646684408 | validation: 0.09350828365686165]
	TIME [epoch: 1.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07043322795835645		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.07043322795835645 | validation: 0.08039260995409557]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728677446966959		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.06728677446966959 | validation: 0.08711312627320528]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06586784387616142		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.06586784387616142 | validation: 0.093635328259959]
	TIME [epoch: 1.38 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06338068047537178		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.06338068047537178 | validation: 0.08065385046219392]
	TIME [epoch: 1.38 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07032776311725147		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.07032776311725147 | validation: 0.1291570980311237]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10354336801220304		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.10354336801220304 | validation: 0.22556317788237423]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2316074332289554		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.2316074332289554 | validation: 0.3464692220613015]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32118606554890333		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.32118606554890333 | validation: 0.12061011633796581]
	TIME [epoch: 1.38 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1012285989229504		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.1012285989229504 | validation: 0.09105183932740696]
	TIME [epoch: 1.38 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08528948908921902		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.08528948908921902 | validation: 0.1339509466728396]
	TIME [epoch: 1.38 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09099406342096138		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.09099406342096138 | validation: 0.08390673030282536]
	TIME [epoch: 1.38 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06626063193474739		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.06626063193474739 | validation: 0.07284942116199855]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06042488684804064		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.06042488684804064 | validation: 0.09538161862465414]
	TIME [epoch: 1.37 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06123306460805646		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.06123306460805646 | validation: 0.07355385025383136]
	TIME [epoch: 1.38 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06174380394044467		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.06174380394044467 | validation: 0.08717863531979449]
	TIME [epoch: 1.37 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062006019171144545		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.062006019171144545 | validation: 0.08105725846807894]
	TIME [epoch: 1.37 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317812398642744		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.06317812398642744 | validation: 0.092385832348103]
	TIME [epoch: 1.37 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07251872337187591		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.07251872337187591 | validation: 0.1067223122563955]
	TIME [epoch: 1.37 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08479890915141455		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.08479890915141455 | validation: 0.09447707437325421]
	TIME [epoch: 1.38 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08796375133335889		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.08796375133335889 | validation: 0.1015937909394365]
	TIME [epoch: 1.38 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034879704367318		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.1034879704367318 | validation: 0.18368823115572674]
	TIME [epoch: 1.38 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15650675363135416		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.15650675363135416 | validation: 0.17183251819155354]
	TIME [epoch: 1.38 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19496524097892864		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.19496524097892864 | validation: 0.16387119244409268]
	TIME [epoch: 1.38 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268160010012053		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.1268160010012053 | validation: 0.07830003544554585]
	TIME [epoch: 1.38 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05552986373854548		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.05552986373854548 | validation: 0.0675604013891386]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06963811620505128		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.06963811620505128 | validation: 0.09823399897459273]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103814779972424		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.07103814779972424 | validation: 0.07100545365395808]
	TIME [epoch: 1.38 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05956547471228224		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.05956547471228224 | validation: 0.072921705211528]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05305629350422694		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.05305629350422694 | validation: 0.07223154627275045]
	TIME [epoch: 1.38 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05153469398519712		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.05153469398519712 | validation: 0.05757294835810292]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048848015350378325		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.048848015350378325 | validation: 0.07068288009772015]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04833109698871273		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.04833109698871273 | validation: 0.06777547949089148]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05800129789367494		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.05800129789367494 | validation: 0.10478183436321967]
	TIME [epoch: 1.38 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09691048945107525		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.09691048945107525 | validation: 0.18344070506615015]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19353313055830978		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.19353313055830978 | validation: 0.14538538338637366]
	TIME [epoch: 1.38 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15414167862310305		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.15414167862310305 | validation: 0.15090813023032745]
	TIME [epoch: 1.38 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623523249228115		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.09623523249228115 | validation: 0.09839578615578662]
	TIME [epoch: 1.38 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07924467388619513		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.07924467388619513 | validation: 0.06603226050330006]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05843475571103106		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.05843475571103106 | validation: 0.0733105465442411]
	TIME [epoch: 1.38 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05479439662044523		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.05479439662044523 | validation: 0.06509042843740771]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05419336830621256		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.05419336830621256 | validation: 0.06908315950391736]
	TIME [epoch: 1.38 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04900536318670721		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.04900536318670721 | validation: 0.05143352484835964]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820155525292503		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.04820155525292503 | validation: 0.06484359929415638]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0470224099236931		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.0470224099236931 | validation: 0.05159189685222714]
	TIME [epoch: 1.38 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052984187834476726		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.052984187834476726 | validation: 0.10846387455893224]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08944194896999486		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.08944194896999486 | validation: 0.1403462801021232]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15352145616048404		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.15352145616048404 | validation: 0.18659656492446194]
	TIME [epoch: 1.38 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16532588676469254		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.16532588676469254 | validation: 0.058111067672745445]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04809649255882987		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.04809649255882987 | validation: 0.051728824318864476]
	TIME [epoch: 1.39 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356341546000655		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.05356341546000655 | validation: 0.08920193817507521]
	TIME [epoch: 1.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06567513289844641		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.06567513289844641 | validation: 0.05040759443968488]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05512793336771716		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.05512793336771716 | validation: 0.07093362064310271]
	TIME [epoch: 1.38 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424924898465486		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.0424924898465486 | validation: 0.04402825143741749]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03994689183918074		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.03994689183918074 | validation: 0.0561666660980881]
	TIME [epoch: 1.38 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048518494059172745		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.048518494059172745 | validation: 0.08873291804951744]
	TIME [epoch: 1.38 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507137839775008		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.07507137839775008 | validation: 0.12468224952263901]
	TIME [epoch: 1.38 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546824213134901		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.11546824213134901 | validation: 0.10779169989710113]
	TIME [epoch: 1.39 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09824553484600916		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.09824553484600916 | validation: 0.07547575374984987]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07845044176746012		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.07845044176746012 | validation: 0.09101790568498164]
	TIME [epoch: 1.38 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705639608467961		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.06705639608467961 | validation: 0.05738782719498924]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054341261017422884		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.054341261017422884 | validation: 0.05417966264503796]
	TIME [epoch: 1.38 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03971850607444918		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.03971850607444918 | validation: 0.03939789271367153]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03709274995532791		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.03709274995532791 | validation: 0.0476363974029604]
	TIME [epoch: 1.38 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03851031721719362		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.03851031721719362 | validation: 0.052965804389649845]
	TIME [epoch: 1.38 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03993032795680151		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.03993032795680151 | validation: 0.04703925668621062]
	TIME [epoch: 1.38 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045790395198568506		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.045790395198568506 | validation: 0.08452094927043935]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06647917771948324		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.06647917771948324 | validation: 0.1136050859298158]
	TIME [epoch: 1.38 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304012753088046		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.1304012753088046 | validation: 0.16875514471266514]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17678992538041569		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.17678992538041569 | validation: 0.06477936874619591]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05496706173975698		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.05496706173975698 | validation: 0.04790729551214752]
	TIME [epoch: 1.38 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04665700011573575		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.04665700011573575 | validation: 0.07573083492441633]
	TIME [epoch: 1.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054871798192348935		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.054871798192348935 | validation: 0.03781159741365531]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03979938802709974		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.03979938802709974 | validation: 0.04316294327477619]
	TIME [epoch: 1.38 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03191246883223867		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.03191246883223867 | validation: 0.04776827156997992]
	TIME [epoch: 1.38 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033001412453441556		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.033001412453441556 | validation: 0.04204415061151956]
	TIME [epoch: 1.38 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03487396928074579		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.03487396928074579 | validation: 0.0475905193079261]
	TIME [epoch: 1.38 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0363855886118586		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.0363855886118586 | validation: 0.04953850586662299]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04409167105413259		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.04409167105413259 | validation: 0.06275459924112614]
	TIME [epoch: 1.38 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055823019266907005		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.055823019266907005 | validation: 0.07496816653281517]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07226754783422225		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.07226754783422225 | validation: 0.10783423322894946]
	TIME [epoch: 1.38 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08532062457695419		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.08532062457695419 | validation: 0.11180572797855062]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1269803957721806		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.1269803957721806 | validation: 0.11522226488459936]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10784301895041778		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.10784301895041778 | validation: 0.07227330690205735]
	TIME [epoch: 1.38 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05894428580845844		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.05894428580845844 | validation: 0.044042574765792486]
	TIME [epoch: 1.38 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038244432056176995		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.038244432056176995 | validation: 0.05429871456513449]
	TIME [epoch: 1.38 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034597597964344945		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.034597597964344945 | validation: 0.03759869146039617]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034881865213712344		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.034881865213712344 | validation: 0.041551859213520274]
	TIME [epoch: 1.38 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03384412985484155		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.03384412985484155 | validation: 0.04133044597983754]
	TIME [epoch: 1.38 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034388850222792186		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.034388850222792186 | validation: 0.056601354692790484]
	TIME [epoch: 1.38 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03679577288490462		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.03679577288490462 | validation: 0.04511044367793144]
	TIME [epoch: 1.38 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05422472608615081		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.05422472608615081 | validation: 0.09743512622179709]
	TIME [epoch: 1.38 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08386537103167661		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.08386537103167661 | validation: 0.0928516033086815]
	TIME [epoch: 1.38 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09346546368584133		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.09346546368584133 | validation: 0.0780044291145978]
	TIME [epoch: 1.38 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05898442744239634		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.05898442744239634 | validation: 0.04253841432068814]
	TIME [epoch: 1.38 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03380552690954753		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.03380552690954753 | validation: 0.0366814311219537]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03371629904202862		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.03371629904202862 | validation: 0.05747910897615647]
	TIME [epoch: 1.37 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03330475046553983		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.03330475046553983 | validation: 0.027977308892582167]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032568483361065076		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.032568483361065076 | validation: 0.04357102576132038]
	TIME [epoch: 1.37 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03193912120539832		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.03193912120539832 | validation: 0.03842187568899855]
	TIME [epoch: 1.37 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03456818521352953		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.03456818521352953 | validation: 0.05165812337682068]
	TIME [epoch: 1.37 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04304893165842965		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.04304893165842965 | validation: 0.052466135956293306]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05322637678733788		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.05322637678733788 | validation: 0.053594856842434015]
	TIME [epoch: 1.37 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326932527970549		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.05326932527970549 | validation: 0.06197024144534398]
	TIME [epoch: 1.38 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0535361194182658		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.0535361194182658 | validation: 0.09195646148667165]
	TIME [epoch: 1.38 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09005890321377862		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.09005890321377862 | validation: 0.13866635196010865]
	TIME [epoch: 1.38 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13188497827787315		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.13188497827787315 | validation: 0.046226542934563804]
	TIME [epoch: 1.38 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04658515302925851		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.04658515302925851 | validation: 0.03137894489149767]
	TIME [epoch: 1.38 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024359711704398705		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.024359711704398705 | validation: 0.035121807586394604]
	TIME [epoch: 1.38 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026190817846732467		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.026190817846732467 | validation: 0.028838961770828175]
	TIME [epoch: 1.38 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028164823248049257		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.028164823248049257 | validation: 0.044379714455269864]
	TIME [epoch: 1.38 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028664529620910876		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.028664529620910876 | validation: 0.03151788521876722]
	TIME [epoch: 1.38 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03352487053884133		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.03352487053884133 | validation: 0.050774673004013726]
	TIME [epoch: 1.38 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038693690282763364		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.038693690282763364 | validation: 0.037807748555521714]
	TIME [epoch: 1.38 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04093834011961499		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.04093834011961499 | validation: 0.06102493831365571]
	TIME [epoch: 1.38 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044032623273693706		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.044032623273693706 | validation: 0.03650982376770209]
	TIME [epoch: 1.38 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044067795635000434		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.044067795635000434 | validation: 0.057090503957267474]
	TIME [epoch: 1.38 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04341032586759691		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.04341032586759691 | validation: 0.028682767725519268]
	TIME [epoch: 1.38 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757099356748921		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.03757099356748921 | validation: 0.043659846492451145]
	TIME [epoch: 1.38 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653075733971572		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.03653075733971572 | validation: 0.0431888536576204]
	TIME [epoch: 1.38 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04525512125420814		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.04525512125420814 | validation: 0.07430600615392037]
	TIME [epoch: 1.38 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06819130412214071		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.06819130412214071 | validation: 0.08809830981346845]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08294651455209674		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.08294651455209674 | validation: 0.05778256769495942]
	TIME [epoch: 1.38 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046330308836713345		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.046330308836713345 | validation: 0.02753841691337023]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021792346361639153		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.021792346361639153 | validation: 0.022490885183245043]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021565638187331242		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.021565638187331242 | validation: 0.02666028820478207]
	TIME [epoch: 1.37 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022501712747489928		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.022501712747489928 | validation: 0.02710264928475506]
	TIME [epoch: 1.38 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023404571312984707		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.023404571312984707 | validation: 0.023159588971483337]
	TIME [epoch: 1.37 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023282117148181874		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.023282117148181874 | validation: 0.030275151045144683]
	TIME [epoch: 1.37 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025175246349161517		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.025175246349161517 | validation: 0.03413404334663852]
	TIME [epoch: 1.37 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031347411867913		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.031347411867913 | validation: 0.05558803030361612]
	TIME [epoch: 1.37 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056367256033342555		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.056367256033342555 | validation: 0.10366385583015324]
	TIME [epoch: 1.37 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09768476798345184		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.09768476798345184 | validation: 0.08415537159247756]
	TIME [epoch: 1.38 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960092538394639		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.0960092538394639 | validation: 0.07168309778496165]
	TIME [epoch: 1.37 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05179805831294112		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.05179805831294112 | validation: 0.02827444415702773]
	TIME [epoch: 1.37 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025081351847896807		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.025081351847896807 | validation: 0.03611407197449115]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03249342227190193		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.03249342227190193 | validation: 0.0344438604616964]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03207619832811612		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.03207619832811612 | validation: 0.02923725438162952]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025466268561321507		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.025466268561321507 | validation: 0.0271905016879158]
	TIME [epoch: 1.38 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02304037771133758		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.02304037771133758 | validation: 0.028097730839524278]
	TIME [epoch: 1.37 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022658017220136314		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.022658017220136314 | validation: 0.029329875050538065]
	TIME [epoch: 1.37 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028363653903080283		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.028363653903080283 | validation: 0.03978548852196489]
	TIME [epoch: 1.37 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464625627797398		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.03464625627797398 | validation: 0.032055715827942886]
	TIME [epoch: 1.38 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03291382205800437		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.03291382205800437 | validation: 0.03389778230657603]
	TIME [epoch: 1.38 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03126418717099012		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.03126418717099012 | validation: 0.03642973033114049]
	TIME [epoch: 1.38 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03511675063333735		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.03511675063333735 | validation: 0.044661751138842326]
	TIME [epoch: 1.38 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048650433112290184		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.048650433112290184 | validation: 0.07865800809227304]
	TIME [epoch: 1.38 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739727185998098		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.0739727185998098 | validation: 0.03649005503757957]
	TIME [epoch: 1.38 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05023639792899474		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.05023639792899474 | validation: 0.04202599404827622]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02959111346749272		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.02959111346749272 | validation: 0.019898376723777197]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020980237128227268		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.020980237128227268 | validation: 0.024929049378290182]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02084067435068053		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.02084067435068053 | validation: 0.019518021138240044]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_460.pth
	Model improved!!!
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0197400299656101		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.0197400299656101 | validation: 0.029905056309749135]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021121648240088896		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.021121648240088896 | validation: 0.02349850153900719]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021323603656635392		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.021323603656635392 | validation: 0.026192715415682313]
	TIME [epoch: 1.38 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02587788931505794		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.02587788931505794 | validation: 0.049535745045661383]
	TIME [epoch: 1.38 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05238981420037919		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.05238981420037919 | validation: 0.07608657275789567]
	TIME [epoch: 1.38 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09251314259273766		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.09251314259273766 | validation: 0.07627710458153125]
	TIME [epoch: 1.38 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05972348423983413		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.05972348423983413 | validation: 0.02077798566554145]
	TIME [epoch: 1.38 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020846252636664855		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.020846252636664855 | validation: 0.02523542185153084]
	TIME [epoch: 1.38 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019856035357284967		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.019856035357284967 | validation: 0.02804887694604362]
	TIME [epoch: 1.38 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025725919943966495		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.025725919943966495 | validation: 0.023656214728208932]
	TIME [epoch: 1.38 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027417666358710104		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.027417666358710104 | validation: 0.025126229472313216]
	TIME [epoch: 1.38 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022367624024498538		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.022367624024498538 | validation: 0.031819599254611655]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023268874508461837		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.023268874508461837 | validation: 0.026224373262932393]
	TIME [epoch: 1.38 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02861280901201595		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.02861280901201595 | validation: 0.038678838677265265]
	TIME [epoch: 1.38 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024674738005296445		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.024674738005296445 | validation: 0.015439879497303334]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_475.pth
	Model improved!!!
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022745264005607974		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.022745264005607974 | validation: 0.035215674172127114]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03235166372587438		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.03235166372587438 | validation: 0.05200571400835672]
	TIME [epoch: 1.38 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059709039979595556		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.059709039979595556 | validation: 0.07801420336862067]
	TIME [epoch: 1.38 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768380888285685		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.07768380888285685 | validation: 0.033539285785852656]
	TIME [epoch: 1.38 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03415901519090832		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.03415901519090832 | validation: 0.017416825222531487]
	TIME [epoch: 1.38 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016206319414270115		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.016206319414270115 | validation: 0.020161137063537185]
	TIME [epoch: 1.38 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01563855726135975		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.01563855726135975 | validation: 0.015556267199583057]
	TIME [epoch: 1.38 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018923448387017004		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.018923448387017004 | validation: 0.02074868211487206]
	TIME [epoch: 1.38 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020141577573326023		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.020141577573326023 | validation: 0.01614025637887554]
	TIME [epoch: 1.38 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02059622517330684		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.02059622517330684 | validation: 0.03509616623736463]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025129700857242535		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.025129700857242535 | validation: 0.02571932275776332]
	TIME [epoch: 1.38 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030536478502071213		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.030536478502071213 | validation: 0.03457776685428734]
	TIME [epoch: 1.38 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03112720306193038		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.03112720306193038 | validation: 0.028336746113768248]
	TIME [epoch: 1.38 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02852359106869329		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.02852359106869329 | validation: 0.040783554007610745]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03779984406596126		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.03779984406596126 | validation: 0.052589465827370896]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05936969723947689		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.05936969723947689 | validation: 0.03961104846623634]
	TIME [epoch: 1.38 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034028144989467386		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.034028144989467386 | validation: 0.021683268295888476]
	TIME [epoch: 1.38 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01888521516209586		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.01888521516209586 | validation: 0.018357857384222614]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01942656957809607		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.01942656957809607 | validation: 0.031840512179101575]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020218437067945856		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.020218437067945856 | validation: 0.013209432070103017]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015239179436767081		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.015239179436767081 | validation: 0.019399859482612542]
	TIME [epoch: 1.38 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01285764633894829		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.01285764633894829 | validation: 0.014362973542823239]
	TIME [epoch: 1.38 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013880720629686883		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.013880720629686883 | validation: 0.014947931213466421]
	TIME [epoch: 1.38 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017436038923658526		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.017436038923658526 | validation: 0.0363178415103658]
	TIME [epoch: 1.38 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027121991699699564		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.027121991699699564 | validation: 0.055538783111077454]
	TIME [epoch: 1.38 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06337121396770937		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.06337121396770937 | validation: 0.07366816826274387]
	TIME [epoch: 269 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714412004902598		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.0714412004902598 | validation: 0.030521349240196262]
	TIME [epoch: 2.77 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035063737992856496		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.035063737992856496 | validation: 0.023225561757488827]
	TIME [epoch: 2.75 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021328381696598023		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.021328381696598023 | validation: 0.015639735406924378]
	TIME [epoch: 2.74 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017509731954029367		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.017509731954029367 | validation: 0.019885291887767182]
	TIME [epoch: 2.75 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017113974027410387		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.017113974027410387 | validation: 0.01230133201163236]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014752896657983103		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.014752896657983103 | validation: 0.021034274100239926]
	TIME [epoch: 2.74 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014673455196004137		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.014673455196004137 | validation: 0.009928742911381433]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014513221819113232		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.014513221819113232 | validation: 0.025056805808119184]
	TIME [epoch: 2.74 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020694543137520317		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.020694543137520317 | validation: 0.03923047426596256]
	TIME [epoch: 2.75 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045456198706635205		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.045456198706635205 | validation: 0.07025338691833563]
	TIME [epoch: 2.75 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06543961231754127		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.06543961231754127 | validation: 0.023938630986610948]
	TIME [epoch: 2.74 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030959845090903534		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.030959845090903534 | validation: 0.017141898983964673]
	TIME [epoch: 2.74 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012896920548708772		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.012896920548708772 | validation: 0.01852512354922648]
	TIME [epoch: 2.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013572972142516432		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.013572972142516432 | validation: 0.01677781355617732]
	TIME [epoch: 2.74 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01674976898200168		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.01674976898200168 | validation: 0.017440208642958]
	TIME [epoch: 2.74 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018070992639206005		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.018070992639206005 | validation: 0.02589355058823284]
	TIME [epoch: 2.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020232873920548837		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.020232873920548837 | validation: 0.0267602018977742]
	TIME [epoch: 2.74 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026716619582801526		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.026716619582801526 | validation: 0.03122544586503035]
	TIME [epoch: 2.75 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025755949001804588		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.025755949001804588 | validation: 0.023400876301034153]
	TIME [epoch: 2.75 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026086853129604626		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.026086853129604626 | validation: 0.01849908445694613]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02529717174104456		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.02529717174104456 | validation: 0.026584715801884547]
	TIME [epoch: 2.74 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021541682687200258		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.021541682687200258 | validation: 0.01734059599077824]
	TIME [epoch: 2.75 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022101761805036126		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.022101761805036126 | validation: 0.028845497227883383]
	TIME [epoch: 2.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02415757110223564		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.02415757110223564 | validation: 0.02849773972094992]
	TIME [epoch: 2.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03395512787703026		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.03395512787703026 | validation: 0.042876393033901755]
	TIME [epoch: 2.75 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008017219325748		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.04008017219325748 | validation: 0.019129195368779602]
	TIME [epoch: 2.75 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022000541244502313		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.022000541244502313 | validation: 0.015803203748001815]
	TIME [epoch: 2.74 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012496408321180201		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.012496408321180201 | validation: 0.012293714744945679]
	TIME [epoch: 2.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012054948572702685		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.012054948572702685 | validation: 0.010895207004740538]
	TIME [epoch: 2.75 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013230133229926966		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.013230133229926966 | validation: 0.018952517340058508]
	TIME [epoch: 2.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01608984251024189		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.01608984251024189 | validation: 0.00966810604368722]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01435620661615388		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.01435620661615388 | validation: 0.01710218666829267]
	TIME [epoch: 2.74 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015247175005443796		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.015247175005443796 | validation: 0.018012857678986]
	TIME [epoch: 2.74 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020185711025227412		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.020185711025227412 | validation: 0.036129915119755544]
	TIME [epoch: 2.74 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0301142947863779		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.0301142947863779 | validation: 0.033165373777084]
	TIME [epoch: 2.75 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03966547284578978		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.03966547284578978 | validation: 0.03130273519332986]
	TIME [epoch: 2.75 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400001458721762		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.03400001458721762 | validation: 0.027909042226480687]
	TIME [epoch: 2.74 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027318937880684464		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.027318937880684464 | validation: 0.026370797587149875]
	TIME [epoch: 2.75 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02280506547839906		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.02280506547839906 | validation: 0.012151732539080418]
	TIME [epoch: 2.75 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013509652085557683		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.013509652085557683 | validation: 0.010929949975420317]
	TIME [epoch: 2.74 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009552448280951009		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.009552448280951009 | validation: 0.00871792868393519]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_542.pth
	Model improved!!!
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011402281825773946		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.011402281825773946 | validation: 0.017077910022265552]
	TIME [epoch: 2.74 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0138864681573275		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.0138864681573275 | validation: 0.01800519237664566]
	TIME [epoch: 2.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015472493173070308		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.015472493173070308 | validation: 0.026437345800990365]
	TIME [epoch: 2.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023545996059526397		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.023545996059526397 | validation: 0.03100225474330147]
	TIME [epoch: 2.74 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030925418752271457		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.030925418752271457 | validation: 0.025702427276143792]
	TIME [epoch: 2.75 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025084950219838344		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.025084950219838344 | validation: 0.027133295921844703]
	TIME [epoch: 2.74 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023143022346056034		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.023143022346056034 | validation: 0.020511216286969797]
	TIME [epoch: 2.74 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03287716837179831		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.03287716837179831 | validation: 0.04524475929345088]
	TIME [epoch: 2.74 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040452844900899045		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.040452844900899045 | validation: 0.013819652998570759]
	TIME [epoch: 2.74 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01727501627451335		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.01727501627451335 | validation: 0.006434473910840655]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_552.pth
	Model improved!!!
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00962409767931042		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.00962409767931042 | validation: 0.014395295757583816]
	TIME [epoch: 2.72 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011321148781932777		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.011321148781932777 | validation: 0.011158559616836445]
	TIME [epoch: 2.72 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013216991901599393		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.013216991901599393 | validation: 0.01541396797650349]
	TIME [epoch: 2.73 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01464302963195006		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.01464302963195006 | validation: 0.009939336750496264]
	TIME [epoch: 2.73 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015673156552548643		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.015673156552548643 | validation: 0.022796738086723323]
	TIME [epoch: 2.73 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020657470608571874		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.020657470608571874 | validation: 0.02596552963695404]
	TIME [epoch: 2.72 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029265609592946405		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.029265609592946405 | validation: 0.024676463429401288]
	TIME [epoch: 2.73 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022312188281318566		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.022312188281318566 | validation: 0.02024157619280863]
	TIME [epoch: 2.73 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01805385167566136		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.01805385167566136 | validation: 0.015461057469886087]
	TIME [epoch: 2.74 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016675744671093246		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.016675744671093246 | validation: 0.02076747741393046]
	TIME [epoch: 2.75 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015480362438840331		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.015480362438840331 | validation: 0.013741868605425162]
	TIME [epoch: 2.74 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013653422001474702		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.013653422001474702 | validation: 0.01003068002586925]
	TIME [epoch: 2.75 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011509551427150395		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.011509551427150395 | validation: 0.01307141862864249]
	TIME [epoch: 2.74 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01184205816845262		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.01184205816845262 | validation: 0.00781898648396952]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014238007120860874		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.014238007120860874 | validation: 0.026422258603307075]
	TIME [epoch: 2.74 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01779391762600396		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.01779391762600396 | validation: 0.01682649626716795]
	TIME [epoch: 2.74 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021824322528950946		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.021824322528950946 | validation: 0.026856959649985065]
	TIME [epoch: 2.74 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023848502261890298		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.023848502261890298 | validation: 0.025857458282226953]
	TIME [epoch: 2.74 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663373205053232		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.03663373205053232 | validation: 0.039102632374763725]
	TIME [epoch: 2.74 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04021876788174823		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.04021876788174823 | validation: 0.017354421543823884]
	TIME [epoch: 2.74 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021780962793350428		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.021780962793350428 | validation: 0.009747591168546111]
	TIME [epoch: 2.74 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009876377352373785		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.009876377352373785 | validation: 0.008710213687877178]
	TIME [epoch: 2.74 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009119547530450139		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.009119547530450139 | validation: 0.005936655886364911]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_575.pth
	Model improved!!!
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01077642569291735		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.01077642569291735 | validation: 0.012637738028634894]
	TIME [epoch: 2.72 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012810320335943395		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.012810320335943395 | validation: 0.01239816834923862]
	TIME [epoch: 2.73 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01248100159390514		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.01248100159390514 | validation: 0.01260232664458706]
	TIME [epoch: 2.72 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01416906008276875		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.01416906008276875 | validation: 0.010256514689909475]
	TIME [epoch: 2.72 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015508358680276668		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.015508358680276668 | validation: 0.023998816558390493]
	TIME [epoch: 2.72 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019673044181891055		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.019673044181891055 | validation: 0.014034656655898115]
	TIME [epoch: 2.73 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023053338376025304		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.023053338376025304 | validation: 0.02358043957376695]
	TIME [epoch: 2.73 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02182929630418716		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.02182929630418716 | validation: 0.01539518858058061]
	TIME [epoch: 2.73 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017395391817211083		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.017395391817211083 | validation: 0.012976965215078913]
	TIME [epoch: 2.73 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01266674230065588		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.01266674230065588 | validation: 0.018968557650717796]
	TIME [epoch: 2.73 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01427720699934417		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.01427720699934417 | validation: 0.013397314648261145]
	TIME [epoch: 2.73 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014160766847459713		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.014160766847459713 | validation: 0.013995083971795864]
	TIME [epoch: 2.74 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013108344535452612		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.013108344535452612 | validation: 0.0124040569826109]
	TIME [epoch: 2.73 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014218390350967733		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.014218390350967733 | validation: 0.015528716868217564]
	TIME [epoch: 2.74 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017563810927000222		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.017563810927000222 | validation: 0.013444876310008026]
	TIME [epoch: 2.73 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016662964699239734		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.016662964699239734 | validation: 0.013879670068911705]
	TIME [epoch: 2.73 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016562757995782006		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.016562757995782006 | validation: 0.016122029164265167]
	TIME [epoch: 2.73 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016548353099213525		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.016548353099213525 | validation: 0.009310206464101568]
	TIME [epoch: 2.73 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015360990946317156		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.015360990946317156 | validation: 0.020830036849806756]
	TIME [epoch: 2.73 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951546417108617		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.01951546417108617 | validation: 0.013226038618735121]
	TIME [epoch: 2.73 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020371770975009754		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.020371770975009754 | validation: 0.021827601997527812]
	TIME [epoch: 2.73 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018615070072796853		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.018615070072796853 | validation: 0.010694011099370439]
	TIME [epoch: 2.74 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012219189566596574		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.012219189566596574 | validation: 0.00879916171769476]
	TIME [epoch: 2.74 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00796093578422971		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.00796093578422971 | validation: 0.007812244579741024]
	TIME [epoch: 2.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007384153006046127		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.007384153006046127 | validation: 0.008408151991249169]
	TIME [epoch: 2.74 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008545381013305499		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.008545381013305499 | validation: 0.007829953869075696]
	TIME [epoch: 2.73 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00999835535050719		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.00999835535050719 | validation: 0.013351213736221385]
	TIME [epoch: 2.72 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01290727345685429		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.01290727345685429 | validation: 0.021515219078731485]
	TIME [epoch: 2.72 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022164657142355226		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.022164657142355226 | validation: 0.03223181780233117]
	TIME [epoch: 2.72 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034627338438957794		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.034627338438957794 | validation: 0.0219767929267844]
	TIME [epoch: 2.73 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022966021788854273		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.022966021788854273 | validation: 0.005339717555436641]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012059057137936336		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.012059057137936336 | validation: 0.011869406821624141]
	TIME [epoch: 2.74 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01167292537105828		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.01167292537105828 | validation: 0.005862390299752107]
	TIME [epoch: 2.74 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010491896999854365		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.010491896999854365 | validation: 0.009882059980137033]
	TIME [epoch: 2.74 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008080061197466123		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.008080061197466123 | validation: 0.007127460102318734]
	TIME [epoch: 2.73 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007825822287807972		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.007825822287807972 | validation: 0.005998198291436153]
	TIME [epoch: 2.74 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0073589552845915755		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.0073589552845915755 | validation: 0.009006248599809159]
	TIME [epoch: 2.73 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011657651263499404		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.011657651263499404 | validation: 0.023695227012958793]
	TIME [epoch: 2.73 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020969985531171506		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.020969985531171506 | validation: 0.02133132491722481]
	TIME [epoch: 2.73 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02764971195544727		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.02764971195544727 | validation: 0.02222946232617705]
	TIME [epoch: 2.73 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022555930123526743		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.022555930123526743 | validation: 0.013054344669777375]
	TIME [epoch: 2.74 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01645379487257218		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.01645379487257218 | validation: 0.009383426972189541]
	TIME [epoch: 2.73 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011427284825235839		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.011427284825235839 | validation: 0.013143065017018174]
	TIME [epoch: 2.74 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011096025518226589		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.011096025518226589 | validation: 0.007892421273672923]
	TIME [epoch: 2.73 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01150758259412724		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.01150758259412724 | validation: 0.009771302566742225]
	TIME [epoch: 2.74 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008011441372166394		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.008011441372166394 | validation: 0.005122925381012722]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_621.pth
	Model improved!!!
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008289018915469407		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.008289018915469407 | validation: 0.008058990666309829]
	TIME [epoch: 2.74 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009822814300880885		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.009822814300880885 | validation: 0.008680556377165161]
	TIME [epoch: 2.74 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011148026801732967		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.011148026801732967 | validation: 0.019673635348292685]
	TIME [epoch: 2.74 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014297781511310795		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.014297781511310795 | validation: 0.016810955686792285]
	TIME [epoch: 2.74 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02171667847225116		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.02171667847225116 | validation: 0.022446254175881305]
	TIME [epoch: 2.74 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02445268598935508		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.02445268598935508 | validation: 0.013085894157666639]
	TIME [epoch: 2.74 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018834118334529734		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.018834118334529734 | validation: 0.00986232414079389]
	TIME [epoch: 2.73 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010493498162911398		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.010493498162911398 | validation: 0.006573138126916067]
	TIME [epoch: 2.73 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006531508085606365		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.006531508085606365 | validation: 0.006228729935053851]
	TIME [epoch: 2.74 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008042937071889312		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.008042937071889312 | validation: 0.007007657972645626]
	TIME [epoch: 2.73 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008677702191748366		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.008677702191748366 | validation: 0.010871992490075717]
	TIME [epoch: 2.74 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00874291802074621		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.00874291802074621 | validation: 0.011293162076014652]
	TIME [epoch: 2.74 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012329671806912726		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.012329671806912726 | validation: 0.012107460065669363]
	TIME [epoch: 2.74 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010568061086354814		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.010568061086354814 | validation: 0.0060347495496724975]
	TIME [epoch: 2.74 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010023629141351994		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.010023629141351994 | validation: 0.007239762562267377]
	TIME [epoch: 2.74 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010210718599151262		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.010210718599151262 | validation: 0.015232187672251563]
	TIME [epoch: 2.74 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015934047421806718		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.015934047421806718 | validation: 0.019631076392070696]
	TIME [epoch: 2.74 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025912642282300384		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.025912642282300384 | validation: 0.023084893632189477]
	TIME [epoch: 2.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019703965760799504		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.019703965760799504 | validation: 0.01160875959828031]
	TIME [epoch: 2.74 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01368098737753475		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.01368098737753475 | validation: 0.01312781682211438]
	TIME [epoch: 2.74 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011337550171513859		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.011337550171513859 | validation: 0.005375396894506973]
	TIME [epoch: 2.74 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008434186042220747		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.008434186042220747 | validation: 0.0027909814039287443]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_643.pth
	Model improved!!!
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007581536079489288		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.007581536079489288 | validation: 0.00554115212944436]
	TIME [epoch: 2.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006755709644657072		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.006755709644657072 | validation: 0.004369825111703219]
	TIME [epoch: 2.74 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006842016294846744		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.006842016294846744 | validation: 0.003959188224760596]
	TIME [epoch: 2.74 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006519226185171885		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.006519226185171885 | validation: 0.006615109196441565]
	TIME [epoch: 2.74 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007311879559508495		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.007311879559508495 | validation: 0.00790949055290528]
	TIME [epoch: 2.74 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007318992622028343		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.007318992622028343 | validation: 0.009676466064807139]
	TIME [epoch: 2.74 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008849375369292261		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.008849375369292261 | validation: 0.004662408882286639]
	TIME [epoch: 2.74 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011724728490086198		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.011724728490086198 | validation: 0.016156957848458716]
	TIME [epoch: 2.73 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01761523266864494		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.01761523266864494 | validation: 0.014193837352804984]
	TIME [epoch: 2.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021281848024964335		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.021281848024964335 | validation: 0.017586281787499287]
	TIME [epoch: 2.74 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020499941541040036		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.020499941541040036 | validation: 0.015238857388645173]
	TIME [epoch: 2.74 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019941156882452703		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.019941156882452703 | validation: 0.009387612346604169]
	TIME [epoch: 2.74 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01046249363114061		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.01046249363114061 | validation: 0.006537881331823237]
	TIME [epoch: 2.74 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006217987173033583		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.006217987173033583 | validation: 0.0038425544456510475]
	TIME [epoch: 2.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006817432853283051		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.006817432853283051 | validation: 0.008733322565834223]
	TIME [epoch: 2.74 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007402825957979319		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.007402825957979319 | validation: 0.005379306451737348]
	TIME [epoch: 2.74 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008465910451788653		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.008465910451788653 | validation: 0.007802200447277941]
	TIME [epoch: 2.74 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008050052490399556		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.008050052490399556 | validation: 0.00908252664020891]
	TIME [epoch: 2.74 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011083642207213943		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.011083642207213943 | validation: 0.006611472746782732]
	TIME [epoch: 2.74 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011247418787536002		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.011247418787536002 | validation: 0.0107177240337835]
	TIME [epoch: 2.74 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014243933857376531		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.014243933857376531 | validation: 0.007521215877194509]
	TIME [epoch: 2.74 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013130798204991096		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.013130798204991096 | validation: 0.007121781560251939]
	TIME [epoch: 2.74 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008295206460106475		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.008295206460106475 | validation: 0.003942744108838165]
	TIME [epoch: 2.74 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007239825384258468		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.007239825384258468 | validation: 0.007293076389863685]
	TIME [epoch: 2.73 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007576455982748681		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.007576455982748681 | validation: 0.00393627365973942]
	TIME [epoch: 2.74 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007666941367746784		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.007666941367746784 | validation: 0.005029162474453375]
	TIME [epoch: 2.74 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007363481315975999		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.007363481315975999 | validation: 0.011312802760765996]
	TIME [epoch: 2.74 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011715238683344622		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.011715238683344622 | validation: 0.014383353186926401]
	TIME [epoch: 2.74 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013878414847096714		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.013878414847096714 | validation: 0.007591221926106906]
	TIME [epoch: 2.74 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013690182919382614		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.013690182919382614 | validation: 0.01560707926010453]
	TIME [epoch: 2.74 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011675911672670831		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.011675911672670831 | validation: 0.0017591519705503489]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_674.pth
	Model improved!!!
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00892532127743465		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.00892532127743465 | validation: 0.006725865845047519]
	TIME [epoch: 2.74 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006459257851089192		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.006459257851089192 | validation: 0.00672466895726297]
	TIME [epoch: 2.74 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006307120995883471		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.006307120995883471 | validation: 0.006263149459193019]
	TIME [epoch: 2.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008744225380720701		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.008744225380720701 | validation: 0.01145203016313916]
	TIME [epoch: 2.73 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010431670724531026		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.010431670724531026 | validation: 0.0060360988880264425]
	TIME [epoch: 2.73 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010283722752897115		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.010283722752897115 | validation: 0.010320415269384231]
	TIME [epoch: 2.73 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01069783527281422		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.01069783527281422 | validation: 0.012276752056129653]
	TIME [epoch: 2.73 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013458781964322611		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.013458781964322611 | validation: 0.011311636152009764]
	TIME [epoch: 2.73 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014938580069470105		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.014938580069470105 | validation: 0.01218579625121975]
	TIME [epoch: 2.73 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010656530222063308		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.010656530222063308 | validation: 0.005335592449922339]
	TIME [epoch: 2.73 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00791958131989818		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.00791958131989818 | validation: 0.005477609868887812]
	TIME [epoch: 2.73 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007007202672076378		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.007007202672076378 | validation: 0.007238714466662761]
	TIME [epoch: 2.74 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006735483556771641		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.006735483556771641 | validation: 0.006089254387004139]
	TIME [epoch: 2.73 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008523048823497158		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.008523048823497158 | validation: 0.011391187364987254]
	TIME [epoch: 2.73 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012003795330942731		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.012003795330942731 | validation: 0.008962986744469431]
	TIME [epoch: 2.73 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01400565481612603		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.01400565481612603 | validation: 0.007863846397404983]
	TIME [epoch: 2.73 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011375548765774148		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.011375548765774148 | validation: 0.008853790614630347]
	TIME [epoch: 2.73 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008023346476896672		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.008023346476896672 | validation: 0.00378399099636072]
	TIME [epoch: 2.73 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0073595004319413895		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.0073595004319413895 | validation: 0.004185172732339942]
	TIME [epoch: 2.73 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0071113127677388125		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.0071113127677388125 | validation: 0.0022816222711559754]
	TIME [epoch: 2.73 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0061117893377854796		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.0061117893377854796 | validation: 0.003477973616781982]
	TIME [epoch: 2.74 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005929701820678296		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.005929701820678296 | validation: 0.00437802037140414]
	TIME [epoch: 2.73 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005151214518640534		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.005151214518640534 | validation: 0.008004386023449833]
	TIME [epoch: 2.74 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004552489167859825		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.004552489167859825 | validation: 0.0011855711911762546]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004790982729312673		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.004790982729312673 | validation: 0.007059357787964982]
	TIME [epoch: 2.74 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005376312311886166		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.005376312311886166 | validation: 0.001751523025181534]
	TIME [epoch: 2.74 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008109330251682362		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.008109330251682362 | validation: 0.012582214901478046]
	TIME [epoch: 2.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011618235820802562		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.011618235820802562 | validation: 0.008223741950553571]
	TIME [epoch: 2.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012692844379882745		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.012692844379882745 | validation: 0.010700348673510302]
	TIME [epoch: 2.73 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013734800719248446		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.013734800719248446 | validation: 0.02081597821129329]
	TIME [epoch: 2.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02735677844658808		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.02735677844658808 | validation: 0.01772534392634037]
	TIME [epoch: 2.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022428843674576694		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.022428843674576694 | validation: 0.0035057760980575225]
	TIME [epoch: 2.74 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006198984683250029		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.006198984683250029 | validation: 0.004028950254634478]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006690116836676469		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.006690116836676469 | validation: 0.003135083125640592]
	TIME [epoch: 2.73 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008082872215486637		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.008082872215486637 | validation: 0.006526563581935186]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006049147114426174		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.006049147114426174 | validation: 0.004910524438406828]
	TIME [epoch: 2.74 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005723076913893687		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.005723076913893687 | validation: 0.0016562379599168121]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005199986141865817		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.005199986141865817 | validation: 0.0036970024858560047]
	TIME [epoch: 2.75 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004432319346937981		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.004432319346937981 | validation: 0.002477151431821173]
	TIME [epoch: 2.74 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005165043462010205		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.005165043462010205 | validation: 0.0030789927292517063]
	TIME [epoch: 2.74 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0050195156723101845		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.0050195156723101845 | validation: 0.0026075349681743743]
	TIME [epoch: 2.74 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005377880283657393		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.005377880283657393 | validation: 0.003550815346686487]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006172126505425534		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.006172126505425534 | validation: 0.0029058334840016023]
	TIME [epoch: 2.74 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008439979380101176		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.008439979380101176 | validation: 0.006212788086105903]
	TIME [epoch: 2.74 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01223293926708589		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.01223293926708589 | validation: 0.015288654699516586]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014057624089341018		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.014057624089341018 | validation: 0.005234184821794486]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012241184655043098		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.012241184655043098 | validation: 0.01357435739936379]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010833608776824209		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.010833608776824209 | validation: 0.005231741285780528]
	TIME [epoch: 2.73 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008370452238190641		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.008370452238190641 | validation: 0.005003365380595626]
	TIME [epoch: 2.73 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006490834427180418		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.006490834427180418 | validation: 0.008728364812584299]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007506184713074181		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.007506184713074181 | validation: 0.003110046975143316]
	TIME [epoch: 2.75 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009215734908236737		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.009215734908236737 | validation: 0.007375861988684629]
	TIME [epoch: 2.73 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007550959476331893		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.007550959476331893 | validation: 0.003340373206662589]
	TIME [epoch: 2.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007304748696579616		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.007304748696579616 | validation: 0.004130450180433543]
	TIME [epoch: 2.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0061649679345688446		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.0061649679345688446 | validation: 0.004260157197814002]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0044035180029841844		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.0044035180029841844 | validation: 0.003260816465269001]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004522888338930914		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.004522888338930914 | validation: 0.0038176894141936147]
	TIME [epoch: 2.74 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004579563141636199		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.004579563141636199 | validation: 0.0030242219181326082]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004247687292759959		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.004247687292759959 | validation: 0.003368101387414446]
	TIME [epoch: 2.73 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004163394750504003		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.004163394750504003 | validation: 0.0021384822982469854]
	TIME [epoch: 2.73 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005265158423848194		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.005265158423848194 | validation: 0.006669139810286368]
	TIME [epoch: 2.73 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00815918331403068		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.00815918331403068 | validation: 0.012426349450787556]
	TIME [epoch: 2.72 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016843006897252676		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.016843006897252676 | validation: 0.010449286036073752]
	TIME [epoch: 2.73 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021177726392607208		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.021177726392607208 | validation: 0.003972934651011051]
	TIME [epoch: 2.72 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006857089829675921		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.006857089829675921 | validation: 0.003857662807635848]
	TIME [epoch: 2.72 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007143400082085855		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.007143400082085855 | validation: 0.005531814366671595]
	TIME [epoch: 2.72 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00678605580868581		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.00678605580868581 | validation: 0.0018878068816234883]
	TIME [epoch: 2.73 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00482582507145957		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.00482582507145957 | validation: -9.695492572968645e-05]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002949021777910429		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.002949021777910429 | validation: 0.004181738072661395]
	TIME [epoch: 2.72 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004075634234735471		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.004075634234735471 | validation: 0.003300476021421989]
	TIME [epoch: 2.73 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005085016746292906		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.005085016746292906 | validation: 0.0024658358925572057]
	TIME [epoch: 2.72 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0044033553857973865		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.0044033553857973865 | validation: 0.0059401743948336305]
	TIME [epoch: 2.73 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005466075427369003		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.005466075427369003 | validation: 0.0030502855871033852]
	TIME [epoch: 2.72 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006234762256635538		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.006234762256635538 | validation: 0.008729947809421523]
	TIME [epoch: 2.73 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008641048810960202		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.008641048810960202 | validation: 0.006221254437648566]
	TIME [epoch: 2.73 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014364413232843853		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.014364413232843853 | validation: 0.014674153375723909]
	TIME [epoch: 2.72 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013328918497275773		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.013328918497275773 | validation: 0.004519283091280119]
	TIME [epoch: 2.73 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007272341660539249		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.007272341660539249 | validation: 0.00021981847877969307]
	TIME [epoch: 2.73 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005105499311991032		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.005105499311991032 | validation: 0.0023330318800811214]
	TIME [epoch: 2.73 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00312068185104459		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.00312068185104459 | validation: 0.0022708420869727575]
	TIME [epoch: 2.73 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005127975107490373		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.005127975107490373 | validation: 0.002620665557438634]
	TIME [epoch: 2.73 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007312924175229306		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.007312924175229306 | validation: 0.006050441191533313]
	TIME [epoch: 2.72 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008228127302493645		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.008228127302493645 | validation: 0.008805384234739866]
	TIME [epoch: 2.72 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009029767512970547		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.009029767512970547 | validation: 0.005541290821597722]
	TIME [epoch: 2.73 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006681909714026914		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.006681909714026914 | validation: 0.0006572277803025607]
	TIME [epoch: 2.73 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004450220458603877		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.004450220458603877 | validation: 0.002395285913299428]
	TIME [epoch: 2.72 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003889492562927098		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.003889492562927098 | validation: 0.0006768560054481731]
	TIME [epoch: 2.73 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006197130025266285		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.006197130025266285 | validation: 0.006386429515276293]
	TIME [epoch: 2.73 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007817287810239618		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.007817287810239618 | validation: 0.0043588312196298]
	TIME [epoch: 2.72 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009795552413335353		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.009795552413335353 | validation: 0.0035592964322266555]
	TIME [epoch: 2.72 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0061606496311034774		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.0061606496311034774 | validation: 0.0017483577903935788]
	TIME [epoch: 2.73 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0058420299362182265		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.0058420299362182265 | validation: 0.004720752998936373]
	TIME [epoch: 2.72 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005762324060534639		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.005762324060534639 | validation: 0.004129745638752719]
	TIME [epoch: 2.72 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005001770450977812		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.005001770450977812 | validation: 0.0015406781176954265]
	TIME [epoch: 2.73 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0060899300261937775		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.0060899300261937775 | validation: 0.003354342305647207]
	TIME [epoch: 2.72 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0067120157590327		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.0067120157590327 | validation: 0.0071045831996964905]
	TIME [epoch: 2.73 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0096268258319296		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.0096268258319296 | validation: 0.003938585991884097]
	TIME [epoch: 2.72 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009435234227145928		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.009435234227145928 | validation: 0.008820490663959351]
	TIME [epoch: 2.72 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00853125727911957		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.00853125727911957 | validation: 0.0051548574733934205]
	TIME [epoch: 2.73 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007621380344035381		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.007621380344035381 | validation: 0.004392822637428101]
	TIME [epoch: 2.73 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005065398873578128		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.005065398873578128 | validation: 0.00305188596374838]
	TIME [epoch: 2.72 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003526522779484166		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.003526522779484166 | validation: 0.0018511157425786684]
	TIME [epoch: 2.72 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004602386861009652		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.004602386861009652 | validation: 0.0008165447900988498]
	TIME [epoch: 2.73 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002992518674857747		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.002992518674857747 | validation: 0.001457409853013214]
	TIME [epoch: 2.73 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003962917676470975		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.003962917676470975 | validation: 0.0069915783431001854]
	TIME [epoch: 2.72 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006225147266915473		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.006225147266915473 | validation: 0.0022274588004293273]
	TIME [epoch: 2.73 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008515384206540676		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.008515384206540676 | validation: 0.012716045315869473]
	TIME [epoch: 2.73 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010649414887920736		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.010649414887920736 | validation: 0.004616231724265069]
	TIME [epoch: 2.72 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00785248102207061		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.00785248102207061 | validation: 0.008471766796160974]
	TIME [epoch: 2.73 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00765233296204599		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.00765233296204599 | validation: 0.002804013195736932]
	TIME [epoch: 2.73 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007250651683439519		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.007250651683439519 | validation: 0.001228256400315886]
	TIME [epoch: 2.72 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003506255648413702		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.003506255648413702 | validation: 0.003196370193002368]
	TIME [epoch: 2.72 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0032346707548495724		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.0032346707548495724 | validation: 0.00022358523792905262]
	TIME [epoch: 2.72 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004354174767864142		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.004354174767864142 | validation: 0.002723721828396042]
	TIME [epoch: 2.72 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004654346423238095		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.004654346423238095 | validation: 0.0061891597531448855]
	TIME [epoch: 2.72 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005414878484656648		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.005414878484656648 | validation: 0.003127223338622126]
	TIME [epoch: 2.73 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006321103755682648		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.006321103755682648 | validation: 0.00822284458967693]
	TIME [epoch: 2.72 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005983062788913941		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.005983062788913941 | validation: 0.004649544568654191]
	TIME [epoch: 2.73 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005705326192597604		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.005705326192597604 | validation: 0.0008846058882685138]
	TIME [epoch: 2.72 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004578900869442443		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.004578900869442443 | validation: 0.004438898920303059]
	TIME [epoch: 2.73 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005330476648624637		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.005330476648624637 | validation: 0.0013234108743150064]
	TIME [epoch: 2.72 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005894441125186146		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.005894441125186146 | validation: 0.005382996063151974]
	TIME [epoch: 2.72 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007458729530223174		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.007458729530223174 | validation: 0.0035297371277662917]
	TIME [epoch: 2.72 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008584623197779697		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.008584623197779697 | validation: 0.008656385867654427]
	TIME [epoch: 2.72 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00928804821715623		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.00928804821715623 | validation: 0.0020402367117779985]
	TIME [epoch: 2.72 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00807226779258261		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.00807226779258261 | validation: 0.0005293854307641566]
	TIME [epoch: 2.72 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004545331680522159		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.004545331680522159 | validation: 0.004388020435909168]
	TIME [epoch: 2.72 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003851469181331242		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.003851469181331242 | validation: 0.0017422786026194737]
	TIME [epoch: 2.72 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004379702448027745		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.004379702448027745 | validation: 0.0022253918572495922]
	TIME [epoch: 2.73 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005273417988581003		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.005273417988581003 | validation: 0.0007872881778582475]
	TIME [epoch: 2.72 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00482369493946267		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.00482369493946267 | validation: 0.0003715783969743425]
	TIME [epoch: 2.72 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004053026404196448		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.004053026404196448 | validation: 0.003258468157728084]
	TIME [epoch: 2.73 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00466027814589845		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.00466027814589845 | validation: 0.0014540923467720125]
	TIME [epoch: 2.73 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005181516975782617		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.005181516975782617 | validation: 0.0038279695902959654]
	TIME [epoch: 2.72 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005035791133771875		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.005035791133771875 | validation: 0.0007077717086683811]
	TIME [epoch: 2.72 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038921523649315492		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.0038921523649315492 | validation: 0.0016940973935687333]
	TIME [epoch: 2.73 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003648110976873853		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.003648110976873853 | validation: 0.002683208824757566]
	TIME [epoch: 2.72 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004112525997851105		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.004112525997851105 | validation: 0.005347488023946606]
	TIME [epoch: 2.73 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006269089425654122		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.006269089425654122 | validation: 0.0006036786751583145]
	TIME [epoch: 2.73 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006588661250873416		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.006588661250873416 | validation: 0.0056584256304870574]
	TIME [epoch: 2.73 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005857866786367788		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.005857866786367788 | validation: 0.0023527075149393784]
	TIME [epoch: 2.72 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006003540371523718		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.006003540371523718 | validation: 0.0020368232230663307]
	TIME [epoch: 2.73 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0051471045388719995		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.0051471045388719995 | validation: -0.0002484522668773803]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_817.pth
	Model improved!!!
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003210386542378104		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.003210386542378104 | validation: 0.001404013631002571]
	TIME [epoch: 2.72 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004734033269874211		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.004734033269874211 | validation: 0.0023290742727488213]
	TIME [epoch: 2.73 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005837473568676897		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.005837473568676897 | validation: 0.003486989791385109]
	TIME [epoch: 2.72 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005249479414959115		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.005249479414959115 | validation: 0.0025637114950673723]
	TIME [epoch: 2.72 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005590190087033485		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.005590190087033485 | validation: 0.00479982468859384]
	TIME [epoch: 2.73 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004755562929585783		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.004755562929585783 | validation: 0.00220682959529257]
	TIME [epoch: 2.72 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006235026581531379		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.006235026581531379 | validation: 0.003693843596035468]
	TIME [epoch: 2.72 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005529791719009577		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.005529791719009577 | validation: 0.001707716145705085]
	TIME [epoch: 2.72 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038849121181385783		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.0038849121181385783 | validation: 0.0017769235827104159]
	TIME [epoch: 2.73 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0048207912609727335		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.0048207912609727335 | validation: 0.0017370246893457565]
	TIME [epoch: 2.72 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003324046941710398		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.003324046941710398 | validation: 0.0007038504443430349]
	TIME [epoch: 2.72 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003231802774228284		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.003231802774228284 | validation: 0.001225374209394281]
	TIME [epoch: 2.73 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00290029793331548		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.00290029793331548 | validation: 0.0009723782341341414]
	TIME [epoch: 2.73 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0036105414382647926		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.0036105414382647926 | validation: 0.00016334154214914798]
	TIME [epoch: 2.72 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004160916185387668		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.004160916185387668 | validation: 0.0034997922705485244]
	TIME [epoch: 2.73 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004610017433241233		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.004610017433241233 | validation: 0.005096673593642371]
	TIME [epoch: 2.73 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004880603838994974		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.004880603838994974 | validation: 0.0025487332032447575]
	TIME [epoch: 2.73 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005119555973030808		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.005119555973030808 | validation: 0.002242297953213007]
	TIME [epoch: 2.72 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00507051157595399		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.00507051157595399 | validation: 0.005387076510144318]
	TIME [epoch: 2.73 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008604400456266512		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.008604400456266512 | validation: 0.009933115350547146]
	TIME [epoch: 2.72 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010398540754382903		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.010398540754382903 | validation: 0.0024821150626312682]
	TIME [epoch: 2.72 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004242301595702289		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.004242301595702289 | validation: 0.004281470665512277]
	TIME [epoch: 2.73 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003911030417354252		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.003911030417354252 | validation: 0.0006305929364586516]
	TIME [epoch: 2.72 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004796245781205272		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.004796245781205272 | validation: 0.00496524078499579]
	TIME [epoch: 2.72 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0037477760026788367		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.0037477760026788367 | validation: 0.0022365503838758572]
	TIME [epoch: 2.73 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004170213960219985		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.004170213960219985 | validation: 0.0030324884288096157]
	TIME [epoch: 2.73 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004044192668404415		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.004044192668404415 | validation: 0.0016197101544294101]
	TIME [epoch: 2.72 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006112138845405105		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.006112138845405105 | validation: 0.004632885561750744]
	TIME [epoch: 2.72 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0046929322316382124		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.0046929322316382124 | validation: 0.0005085967281890203]
	TIME [epoch: 2.73 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0031928908596279616		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.0031928908596279616 | validation: -0.0007219097203805258]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_847.pth
	Model improved!!!
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0030041629204140773		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.0030041629204140773 | validation: -0.0013955052651753398]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0027708207202435976		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.0027708207202435976 | validation: 0.00200065009716357]
	TIME [epoch: 2.74 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0033358926837451243		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.0033358926837451243 | validation: 0.0010413858196418047]
	TIME [epoch: 2.73 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004125902657366044		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.004125902657366044 | validation: 0.0010685755264787434]
	TIME [epoch: 2.72 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005050396088586785		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.005050396088586785 | validation: 0.0023888321891522737]
	TIME [epoch: 2.73 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007305091006153618		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.007305091006153618 | validation: 0.01161016517469859]
	TIME [epoch: 2.73 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010201025656083652		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.010201025656083652 | validation: -0.0012763489305148814]
	TIME [epoch: 2.72 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005987164934252156		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.005987164934252156 | validation: 0.003103603689473239]
	TIME [epoch: 2.72 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0029034747301843423		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.0029034747301843423 | validation: -1.6154764577602786e-05]
	TIME [epoch: 2.73 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003231043915540273		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.003231043915540273 | validation: 0.0015852003939828053]
	TIME [epoch: 2.72 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003228110739285629		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.003228110739285629 | validation: -0.00016867185919289818]
	TIME [epoch: 2.72 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0026607442589396315		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.0026607442589396315 | validation: 0.004397009800769547]
	TIME [epoch: 2.72 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0027046376596589375		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.0027046376596589375 | validation: -0.0002484507449131757]
	TIME [epoch: 2.73 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0035202369564874135		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.0035202369564874135 | validation: 0.0005833405572537809]
	TIME [epoch: 2.72 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003687162721671956		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.003687162721671956 | validation: 0.0029196510359204286]
	TIME [epoch: 2.72 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003939224954300327		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.003939224954300327 | validation: 0.00702421092312221]
	TIME [epoch: 2.73 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005609306661533119		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.005609306661533119 | validation: 0.0001727790636530613]
	TIME [epoch: 2.73 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005811657099620321		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.005811657099620321 | validation: 0.0041657419097478115]
	TIME [epoch: 2.72 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004393929339762176		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.004393929339762176 | validation: -0.0011093626182750727]
	TIME [epoch: 2.73 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006027113579064265		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.006027113579064265 | validation: 0.0020192888475206896]
	TIME [epoch: 2.72 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0028141706255497513		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.0028141706255497513 | validation: 0.003333111222238988]
	TIME [epoch: 2.73 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003273544305780113		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.003273544305780113 | validation: -0.001328590133833385]
	TIME [epoch: 2.73 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003944422951685479		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.003944422951685479 | validation: -0.0012999996088728706]
	TIME [epoch: 2.73 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0032147015063174666		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.0032147015063174666 | validation: 0.0015195500112713256]
	TIME [epoch: 2.72 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0037917670567732065		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.0037917670567732065 | validation: -0.00041667798678754475]
	TIME [epoch: 2.73 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003974407246385047		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.003974407246385047 | validation: 0.0043005518998978365]
	TIME [epoch: 2.73 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005655969277392378		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.005655969277392378 | validation: 0.0032358608193610583]
	TIME [epoch: 2.72 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004660385476779949		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.004660385476779949 | validation: 0.003651855279470345]
	TIME [epoch: 2.72 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005857786121041381		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.005857786121041381 | validation: -0.000544116163423225]
	TIME [epoch: 2.73 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0047426931822236815		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.0047426931822236815 | validation: -0.00047568159821355653]
	TIME [epoch: 2.72 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0036889447495626515		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.0036889447495626515 | validation: 0.0008085802460793435]
	TIME [epoch: 2.72 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0035605846536868346		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.0035605846536868346 | validation: 0.0006430923449695481]
	TIME [epoch: 2.72 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002544606712108175		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.002544606712108175 | validation: 0.0002941978386185096]
	TIME [epoch: 2.72 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0024476044141309027		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.0024476044141309027 | validation: 0.0013371046320551862]
	TIME [epoch: 2.73 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003100295183448455		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.003100295183448455 | validation: 0.0032069323142066286]
	TIME [epoch: 2.72 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004110214276604661		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.004110214276604661 | validation: 0.0012779810189602837]
	TIME [epoch: 2.72 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038712206722610588		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.0038712206722610588 | validation: 0.001803725858397204]
	TIME [epoch: 2.72 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004693463351377832		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.004693463351377832 | validation: 0.00048620205103886983]
	TIME [epoch: 2.73 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00269450899731852		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.00269450899731852 | validation: 0.0026181067161214103]
	TIME [epoch: 2.72 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0036142920678189573		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.0036142920678189573 | validation: 0.0005892344825038976]
	TIME [epoch: 2.72 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006667841417015511		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.006667841417015511 | validation: 0.008164598271838703]
	TIME [epoch: 2.73 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.007870534110347938		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.007870534110347938 | validation: 0.0025251520281901977]
	TIME [epoch: 2.73 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004237462882016269		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.004237462882016269 | validation: 2.856296513488177e-05]
	TIME [epoch: 2.72 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0019155699523246627		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.0019155699523246627 | validation: -0.0004501433221458018]
	TIME [epoch: 2.72 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038574880062883375		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.0038574880062883375 | validation: 0.002746610633677699]
	TIME [epoch: 2.72 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0035101908871997033		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.0035101908871997033 | validation: 0.0021586209564635806]
	TIME [epoch: 2.72 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003833506418205891		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.003833506418205891 | validation: 0.001511893860226027]
	TIME [epoch: 2.72 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0026281661478206797		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.0026281661478206797 | validation: -0.0006901846538730328]
	TIME [epoch: 2.73 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038526190052879175		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.0038526190052879175 | validation: 0.001650628751449812]
	TIME [epoch: 2.72 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0035842815821380825		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.0035842815821380825 | validation: 0.0033952974035645735]
	TIME [epoch: 2.72 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003777988865479568		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.003777988865479568 | validation: 0.0007015116859718174]
	TIME [epoch: 2.73 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0032052286519768046		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.0032052286519768046 | validation: -0.0008299775406864773]
	TIME [epoch: 2.72 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002928437032042679		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.002928437032042679 | validation: -0.003029350152141003]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_900.pth
	Model improved!!!
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0041390144362288385		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.0041390144362288385 | validation: 0.0013349895972094962]
	TIME [epoch: 2.72 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0036209598412297837		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.0036209598412297837 | validation: 0.005365939644201923]
	TIME [epoch: 2.72 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005130691623555088		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.005130691623555088 | validation: 0.003722640389176357]
	TIME [epoch: 2.72 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005689769078443715		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.005689769078443715 | validation: 0.0009949675724038153]
	TIME [epoch: 2.72 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005315372384451871		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.005315372384451871 | validation: -0.000988065398614585]
	TIME [epoch: 2.73 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004072725621523668		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.004072725621523668 | validation: -0.0013238093253169659]
	TIME [epoch: 2.72 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0025338015723435615		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.0025338015723435615 | validation: 0.00208900924428706]
	TIME [epoch: 2.72 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005082690652646037		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.005082690652646037 | validation: 0.0033070229186157652]
	TIME [epoch: 2.73 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.005391938023612416		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.005391938023612416 | validation: -1.441619927045057e-06]
	TIME [epoch: 2.72 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004608396141594412		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.004608396141594412 | validation: 0.001711861866701303]
	TIME [epoch: 2.72 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003796263598805216		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.003796263598805216 | validation: 0.002105504505562944]
	TIME [epoch: 2.73 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0017544269800183487		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.0017544269800183487 | validation: 0.0007274574242584431]
	TIME [epoch: 2.72 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0029683039027206984		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.0029683039027206984 | validation: 0.004941611799640183]
	TIME [epoch: 2.72 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004062791329445743		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.004062791329445743 | validation: 0.0006699113127704549]
	TIME [epoch: 2.73 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003603275564741719		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.003603275564741719 | validation: 0.00322094917132007]
	TIME [epoch: 2.73 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0021344855271357645		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.0021344855271357645 | validation: -0.0010623361289411604]
	TIME [epoch: 2.72 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0023670961951863913		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.0023670961951863913 | validation: 0.0022347447931583666]
	TIME [epoch: 2.72 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0020232719419367674		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.0020232719419367674 | validation: -2.6912342266199875e-05]
	TIME [epoch: 2.72 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002678603486471718		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.002678603486471718 | validation: 7.015822144018636e-07]
	TIME [epoch: 2.72 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003377354007871666		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.003377354007871666 | validation: 0.004086764605003935]
	TIME [epoch: 2.72 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0021171232218648774		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.0021171232218648774 | validation: -0.0014492479272909975]
	TIME [epoch: 2.72 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0026426309511383608		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.0026426309511383608 | validation: 0.00020034718479911562]
	TIME [epoch: 2.72 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0030831407919854415		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.0030831407919854415 | validation: 0.002214424479061844]
	TIME [epoch: 2.72 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004779387417114073		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.004779387417114073 | validation: 0.0009250719107091022]
	TIME [epoch: 2.73 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004942745316230305		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.004942745316230305 | validation: 0.005697692162465773]
	TIME [epoch: 2.72 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004907196951105764		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.004907196951105764 | validation: 0.0050778739377006395]
	TIME [epoch: 2.72 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0046933437897289		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.0046933437897289 | validation: 0.0004464238344358329]
	TIME [epoch: 2.73 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0037681303188884475		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.0037681303188884475 | validation: 0.0016269121689346468]
	TIME [epoch: 2.73 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00248410206429201		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.00248410206429201 | validation: 0.0027916688671134173]
	TIME [epoch: 2.73 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0034224244425051643		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.0034224244425051643 | validation: -0.00037544304174947366]
	TIME [epoch: 2.73 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0035072512390952873		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.0035072512390952873 | validation: 0.0012317331648599552]
	TIME [epoch: 2.73 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0024918546933947773		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.0024918546933947773 | validation: -0.001065130602791342]
	TIME [epoch: 2.72 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0032994173547756744		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.0032994173547756744 | validation: 0.0015958699080511507]
	TIME [epoch: 2.73 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004063804432886714		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.004063804432886714 | validation: 0.004408981728627826]
	TIME [epoch: 2.73 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0054642673317796256		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.0054642673317796256 | validation: 0.000739258081690336]
	TIME [epoch: 2.72 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00615654743800223		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.00615654743800223 | validation: 0.0010871527462225695]
	TIME [epoch: 2.72 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004154954205445752		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.004154954205445752 | validation: -0.0009661629910684256]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0024055834314516134		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.0024055834314516134 | validation: -0.0016040613513690295]
	TIME [epoch: 2.72 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002131535952952124		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.002131535952952124 | validation: 0.00045174030266129095]
	TIME [epoch: 2.72 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003293252740314946		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.003293252740314946 | validation: 0.001588731789483955]
	TIME [epoch: 2.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003182024023010732		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.003182024023010732 | validation: 0.0028880199402902154]
	TIME [epoch: 2.72 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0033309068918598653		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.0033309068918598653 | validation: -0.0011814713640746046]
	TIME [epoch: 2.72 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00270944952462031		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.00270944952462031 | validation: -0.0018619145977095243]
	TIME [epoch: 2.73 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003501633959777685		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.003501633959777685 | validation: 0.0011436533556669626]
	TIME [epoch: 2.72 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0023140675076512253		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.0023140675076512253 | validation: -0.002335736608106809]
	TIME [epoch: 2.72 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0034229281889761897		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.0034229281889761897 | validation: -0.0003406226924415279]
	TIME [epoch: 2.72 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002340878193473589		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.002340878193473589 | validation: -0.0005258476161908776]
	TIME [epoch: 2.73 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0027350244119114433		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.0027350244119114433 | validation: 0.0007843715187856037]
	TIME [epoch: 2.72 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0016599139510319917		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.0016599139510319917 | validation: 0.0001705386190962721]
	TIME [epoch: 2.73 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.001991955369230102		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.001991955369230102 | validation: -0.0026479243138873344]
	TIME [epoch: 2.73 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0026862611823908022		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.0026862611823908022 | validation: -0.0015507057909027512]
	TIME [epoch: 2.73 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00231909383919103		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.00231909383919103 | validation: -0.0019752407438689223]
	TIME [epoch: 2.73 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0031463545487793233		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.0031463545487793233 | validation: 0.000895853102129135]
	TIME [epoch: 2.73 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0034501988452235012		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.0034501988452235012 | validation: 0.00334664423856888]
	TIME [epoch: 2.72 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003815358465084915		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.003815358465084915 | validation: 0.0003723345973040415]
	TIME [epoch: 2.72 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004696201745990728		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.004696201745990728 | validation: 0.002325328901394591]
	TIME [epoch: 2.73 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0038330445364636047		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.0038330445364636047 | validation: 0.0017590371248678628]
	TIME [epoch: 2.72 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004104339071516451		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.004104339071516451 | validation: -0.0018950459087471228]
	TIME [epoch: 2.72 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003430139132667328		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.003430139132667328 | validation: 5.874357230298833e-05]
	TIME [epoch: 2.73 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0030439538765056483		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.0030439538765056483 | validation: -0.0010974939232383685]
	TIME [epoch: 2.72 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0032587997838820484		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0032587997838820484 | validation: 0.00341908262092564]
	TIME [epoch: 2.72 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004347356366821834		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.004347356366821834 | validation: -0.0006165103881383138]
	TIME [epoch: 2.72 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0029909324725230302		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.0029909324725230302 | validation: -0.0018911721269357586]
	TIME [epoch: 2.73 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0018447353494510277		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.0018447353494510277 | validation: -0.0003565549953773062]
	TIME [epoch: 2.72 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0019884966105427004		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.0019884966105427004 | validation: -0.0020637708475247265]
	TIME [epoch: 2.73 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0019957838278376085		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.0019957838278376085 | validation: -0.00018120748488993367]
	TIME [epoch: 2.72 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00128766070178019		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.00128766070178019 | validation: -0.0009330589461844286]
	TIME [epoch: 2.72 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0018529281008106068		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.0018529281008106068 | validation: -0.0009132830341620657]
	TIME [epoch: 2.73 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0019493367158668663		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.0019493367158668663 | validation: -0.0012525577914732355]
	TIME [epoch: 2.73 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0021746023475726415		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.0021746023475726415 | validation: -1.774164904098097e-05]
	TIME [epoch: 2.72 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0021867275946258116		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.0021867275946258116 | validation: -0.002885321701903182]
	TIME [epoch: 2.73 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002218664887965226		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.002218664887965226 | validation: 0.0024244681339299027]
	TIME [epoch: 2.73 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004350575658374256		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.004350575658374256 | validation: 0.00211242621070562]
	TIME [epoch: 2.72 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.006982439773713609		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.006982439773713609 | validation: 0.0007775488516597873]
	TIME [epoch: 2.72 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002246888453030057		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.002246888453030057 | validation: 0.002063429728067268]
	TIME [epoch: 2.72 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0029076121349077143		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.0029076121349077143 | validation: 0.0016719996034410267]
	TIME [epoch: 2.72 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00352285286201505		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.00352285286201505 | validation: -0.0006656810774394728]
	TIME [epoch: 2.72 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002489433603309469		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.002489433603309469 | validation: 0.0029393177106552927]
	TIME [epoch: 2.72 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0024212320496912964		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.0024212320496912964 | validation: -0.002439837138091794]
	TIME [epoch: 2.76 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0030683795040825877		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.0030683795040825877 | validation: -0.001490040254790387]
	TIME [epoch: 2.75 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00216657077652587		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.00216657077652587 | validation: -0.0015469805095917333]
	TIME [epoch: 2.75 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0017068941096661928		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.0017068941096661928 | validation: -0.0006570912209555414]
	TIME [epoch: 2.75 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0015028322393678228		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.0015028322393678228 | validation: -0.002017170112708039]
	TIME [epoch: 2.75 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0022384480674060613		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.0022384480674060613 | validation: -5.023779048042299e-05]
	TIME [epoch: 2.75 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003782033126902714		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.003782033126902714 | validation: 0.00133077074120444]
	TIME [epoch: 2.76 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.004665211943021274		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.004665211943021274 | validation: 0.0012092893356261238]
	TIME [epoch: 2.75 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00404001470136473		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.00404001470136473 | validation: -0.0008654968607888669]
	TIME [epoch: 2.75 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0024892176457774894		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.0024892176457774894 | validation: 0.00026382540245476486]
	TIME [epoch: 2.76 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0015570148025182517		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.0015570148025182517 | validation: 0.002489819147982464]
	TIME [epoch: 2.75 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0019604896714233576		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.0019604896714233576 | validation: 0.0016461690160289367]
	TIME [epoch: 2.75 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002303497962293595		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.002303497962293595 | validation: 2.793714126070324e-06]
	TIME [epoch: 2.75 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002640907584349893		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.002640907584349893 | validation: -0.0019133204993976806]
	TIME [epoch: 2.76 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0020997378907839214		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.0020997378907839214 | validation: -0.0012638110414346705]
	TIME [epoch: 2.75 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0028741971959627024		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.0028741971959627024 | validation: -0.0010180664214723134]
	TIME [epoch: 2.75 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002037022125503708		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.002037022125503708 | validation: -0.002483001108402622]
	TIME [epoch: 2.76 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0013983361673990536		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.0013983361673990536 | validation: 0.0029701443599764177]
	TIME [epoch: 2.76 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003026059516801873		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.003026059516801873 | validation: -0.0027666259124452253]
	TIME [epoch: 2.75 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0024970671993563786		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.0024970671993563786 | validation: 0.0008024856943574577]
	TIME [epoch: 2.76 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.002307404447962361		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.002307404447962361 | validation: -0.0012066584153231253]
	TIME [epoch: 2.75 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0018068059386724623		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.0018068059386724623 | validation: 0.00031212078698564797]
	TIME [epoch: 2.75 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.003020536099099821		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.003020536099099821 | validation: -0.0024213237183715376]
	TIME [epoch: 273 sec]
	Saving model to: out/model_training/model_phi1_3a_v_mmd1_20241105_144128/states/model_phi1_3a_v_mmd1_1001.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3083.161 seconds.
