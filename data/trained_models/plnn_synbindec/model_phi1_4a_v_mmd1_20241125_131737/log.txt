Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2719035216

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.26994402769294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.26994402769294 | validation: 4.523629870506762]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.971124690826366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.971124690826366 | validation: 3.596358527804668]
	TIME [epoch: 0.745 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.771323705208186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.771323705208186 | validation: 3.464458115689807]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.272542808134318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.272542808134318 | validation: 2.4685891054397326]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.274131540196744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.274131540196744 | validation: 3.531159896314725]
	TIME [epoch: 0.699 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.968904225252842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.968904225252842 | validation: 6.738828243551135]
	TIME [epoch: 0.7 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.618549782855252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.618549782855252 | validation: 6.750335300633637]
	TIME [epoch: 0.693 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.022139665410255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.022139665410255 | validation: 6.7568958250024025]
	TIME [epoch: 0.692 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.066484789306312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.066484789306312 | validation: 6.607126976238627]
	TIME [epoch: 0.693 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.7837580388147165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.7837580388147165 | validation: 6.246345727427197]
	TIME [epoch: 0.694 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.1192704061229595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.1192704061229595 | validation: 5.983987288634205]
	TIME [epoch: 0.692 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.305786282114111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.305786282114111 | validation: 4.920358005569087]
	TIME [epoch: 0.692 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9835354482810645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9835354482810645 | validation: 2.7423673014249146]
	TIME [epoch: 0.693 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8559984733506987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8559984733506987 | validation: 2.3955774191426116]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2826099070905377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2826099070905377 | validation: 2.3448199955479803]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2061036993629966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2061036993629966 | validation: 1.8041921180767433]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8669331648791307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8669331648791307 | validation: 1.8023172174539548]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8382842020762953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8382842020762953 | validation: 1.867192954556741]
	TIME [epoch: 0.698 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.766694511113699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.766694511113699 | validation: 2.333192771865086]
	TIME [epoch: 0.698 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8375455422077185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8375455422077185 | validation: 1.6049411230720965]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8169219443160611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8169219443160611 | validation: 1.43499784339955]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6151560205988857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6151560205988857 | validation: 1.5395009197411735]
	TIME [epoch: 0.699 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.479548412642607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.479548412642607 | validation: 1.6466935486210283]
	TIME [epoch: 0.695 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4393438868306887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4393438868306887 | validation: 1.7296971983571368]
	TIME [epoch: 0.695 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.420199889169906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.420199889169906 | validation: 1.6034792872385504]
	TIME [epoch: 0.693 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6881994254119166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6881994254119166 | validation: 1.8727803241879362]
	TIME [epoch: 0.692 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5182362789214199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5182362789214199 | validation: 1.2661575318981595]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.527878695949921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.527878695949921 | validation: 1.3230725479041525]
	TIME [epoch: 0.695 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2734601998440387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2734601998440387 | validation: 2.0253579952833274]
	TIME [epoch: 0.693 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7320575623139298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7320575623139298 | validation: 1.6305643449835387]
	TIME [epoch: 0.692 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9714485655495742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9714485655495742 | validation: 0.9665554982279887]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4399390104570098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4399390104570098 | validation: 1.4985898750728186]
	TIME [epoch: 0.697 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2685302621150039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2685302621150039 | validation: 1.4171609338786633]
	TIME [epoch: 0.694 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.250632026577449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.250632026577449 | validation: 1.5728098785417917]
	TIME [epoch: 0.694 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2480360526626508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2480360526626508 | validation: 0.9951488428150075]
	TIME [epoch: 0.693 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3755080798643748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3755080798643748 | validation: 0.8749128294513551]
	TIME [epoch: 0.699 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2312195434709134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2312195434709134 | validation: 1.3224953597480082]
	TIME [epoch: 0.699 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1317330949367612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1317330949367612 | validation: 1.1393175020936726]
	TIME [epoch: 0.7 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2349998394713655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2349998394713655 | validation: 1.4978008510708356]
	TIME [epoch: 0.695 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3444698245857976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3444698245857976 | validation: 0.9730120338512832]
	TIME [epoch: 0.695 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4205437301190034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4205437301190034 | validation: 0.7446023756396879]
	TIME [epoch: 0.695 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1640416008551666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1640416008551666 | validation: 1.2046358367037773]
	TIME [epoch: 0.697 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0859029734742496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0859029734742496 | validation: 1.1952508966544828]
	TIME [epoch: 0.693 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1530479084231826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1530479084231826 | validation: 1.098554720614419]
	TIME [epoch: 0.694 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0964253791484881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0964253791484881 | validation: 1.0620229551110003]
	TIME [epoch: 0.693 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0371089326542338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0371089326542338 | validation: 1.0113850225996026]
	TIME [epoch: 0.692 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.990815392720553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.990815392720553 | validation: 1.102022799038613]
	TIME [epoch: 0.692 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9916273128606298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9916273128606298 | validation: 0.7591300745659727]
	TIME [epoch: 0.693 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1641278836777027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1641278836777027 | validation: 1.1800944935176823]
	TIME [epoch: 0.695 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017155764984378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017155764984378 | validation: 0.7300066276446217]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0757385828775332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0757385828775332 | validation: 1.1973198562317056]
	TIME [epoch: 0.695 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0195287370443584		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.0195287370443584 | validation: 0.6572196759260236]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0484558513711764		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.0484558513711764 | validation: 0.9279414800795787]
	TIME [epoch: 0.695 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930061799353862		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 0.930061799353862 | validation: 1.1035835630147801]
	TIME [epoch: 0.692 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9969008282626711		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 0.9969008282626711 | validation: 0.8055590496300149]
	TIME [epoch: 0.695 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3757662468493375		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 1.3757662468493375 | validation: 0.9333573897151398]
	TIME [epoch: 0.691 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9896338851470844		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9896338851470844 | validation: 1.2825013943085706]
	TIME [epoch: 0.691 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0726315345771287		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 1.0726315345771287 | validation: 0.7303906962859955]
	TIME [epoch: 0.69 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.252721198251497		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.252721198251497 | validation: 0.6084474112769369]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9898596880534686		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.9898596880534686 | validation: 1.2765417686961484]
	TIME [epoch: 0.69 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0573089514363476		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.0573089514363476 | validation: 0.6379201002245405]
	TIME [epoch: 0.689 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9628346020397379		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.9628346020397379 | validation: 0.6512776027940607]
	TIME [epoch: 0.688 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9234778823673196		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9234778823673196 | validation: 1.1014221682542253]
	TIME [epoch: 0.69 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785038636419537		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9785038636419537 | validation: 0.6978664932990901]
	TIME [epoch: 0.689 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282331712035948		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9282331712035948 | validation: 0.8186797588079309]
	TIME [epoch: 0.688 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268889702576104		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.9268889702576104 | validation: 0.9833016190970842]
	TIME [epoch: 0.689 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9597616680323549		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.9597616680323549 | validation: 0.6988501061109752]
	TIME [epoch: 0.688 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0000804738385503		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 1.0000804738385503 | validation: 0.8124673676251004]
	TIME [epoch: 0.687 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077899928556792		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9077899928556792 | validation: 0.8843999902729748]
	TIME [epoch: 0.687 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889458202243047		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.889458202243047 | validation: 0.6346780369806636]
	TIME [epoch: 0.69 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9035550554806281		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.9035550554806281 | validation: 0.9230459479189357]
	TIME [epoch: 0.69 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973775491828775		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.8973775491828775 | validation: 0.636650482664926]
	TIME [epoch: 0.688 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9067091808016161		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9067091808016161 | validation: 0.9305023152990213]
	TIME [epoch: 0.687 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981554109402987		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.8981554109402987 | validation: 0.6395217385577108]
	TIME [epoch: 0.689 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9379326224036774		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.9379326224036774 | validation: 0.998942752279869]
	TIME [epoch: 0.696 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9463821920625876		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.9463821920625876 | validation: 0.6839280913547374]
	TIME [epoch: 0.691 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9453401538561853		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.9453401538561853 | validation: 0.9465370768076143]
	TIME [epoch: 0.69 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9187256748640156		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.9187256748640156 | validation: 0.6636537065645951]
	TIME [epoch: 0.692 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9022154989267207		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.9022154989267207 | validation: 0.8723375370741269]
	TIME [epoch: 0.691 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925677081564635		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.8925677081564635 | validation: 0.6624656335973662]
	TIME [epoch: 0.691 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868170676534209		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.868170676534209 | validation: 0.84065587121884]
	TIME [epoch: 0.69 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879364203588512		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.879364203588512 | validation: 0.6374446269159704]
	TIME [epoch: 0.688 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925077042148521		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.8925077042148521 | validation: 1.1129520336284757]
	TIME [epoch: 0.689 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9787102801630871		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.9787102801630871 | validation: 0.6022839690399823]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1899218677122712		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 1.1899218677122712 | validation: 0.9509277201736897]
	TIME [epoch: 0.691 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.957139532951812		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.957139532951812 | validation: 0.763973775441588]
	TIME [epoch: 0.689 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9058538919202451		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.9058538919202451 | validation: 0.7276036756969829]
	TIME [epoch: 0.686 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8697318743944975		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.8697318743944975 | validation: 0.7397295391433255]
	TIME [epoch: 0.689 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574039642722084		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.8574039642722084 | validation: 0.7508261695052152]
	TIME [epoch: 0.692 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8451627130023442		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8451627130023442 | validation: 0.68198259038917]
	TIME [epoch: 0.691 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8449010316982196		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8449010316982196 | validation: 0.7710976034569716]
	TIME [epoch: 0.691 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472228733723839		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8472228733723839 | validation: 0.6838127211377946]
	TIME [epoch: 0.691 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486727475459602		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8486727475459602 | validation: 0.8587757382672945]
	TIME [epoch: 0.692 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8907483485401124		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.8907483485401124 | validation: 0.6597570217506187]
	TIME [epoch: 0.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0581802797010136		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 1.0581802797010136 | validation: 1.064644236027277]
	TIME [epoch: 0.691 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9848603138392282		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9848603138392282 | validation: 0.6067645761990414]
	TIME [epoch: 0.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0252404345652282		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 1.0252404345652282 | validation: 1.0430489488721109]
	TIME [epoch: 0.693 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9459644758872204		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9459644758872204 | validation: 0.6643030237961712]
	TIME [epoch: 0.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878801651671077		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8878801651671077 | validation: 0.7286029672915094]
	TIME [epoch: 0.691 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850438352260353		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.850438352260353 | validation: 0.7854479788417482]
	TIME [epoch: 0.691 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515971536244977		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8515971536244977 | validation: 0.5770637864928532]
	TIME [epoch: 0.697 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8900023407693448		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8900023407693448 | validation: 0.9316886334469591]
	TIME [epoch: 0.698 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926086644254203		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8926086644254203 | validation: 0.5762229670293214]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9123548925857924		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.9123548925857924 | validation: 0.865157847829604]
	TIME [epoch: 0.692 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588515585840978		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.8588515585840978 | validation: 0.5952258427354403]
	TIME [epoch: 0.687 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893270497403001		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8893270497403001 | validation: 0.896866289327957]
	TIME [epoch: 0.685 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774014538013376		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8774014538013376 | validation: 0.5697193415529356]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966190519738834		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8966190519738834 | validation: 0.8971632677677859]
	TIME [epoch: 0.699 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.883684626227074		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.883684626227074 | validation: 0.590067769904786]
	TIME [epoch: 0.696 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9406133568983499		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.9406133568983499 | validation: 0.8321003137087285]
	TIME [epoch: 0.694 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9094459375747076		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.9094459375747076 | validation: 0.8243287013906042]
	TIME [epoch: 0.692 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9161103639366016		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9161103639366016 | validation: 0.6442208830234044]
	TIME [epoch: 0.692 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936261672838204		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8936261672838204 | validation: 1.0862338826985212]
	TIME [epoch: 0.693 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9702098935468834		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9702098935468834 | validation: 0.5463879308093603]
	TIME [epoch: 0.694 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9524420287232798		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9524420287232798 | validation: 0.7528004118314175]
	TIME [epoch: 0.698 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8344482441235016		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8344482441235016 | validation: 0.7093091225906267]
	TIME [epoch: 0.697 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8303669192940153		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8303669192940153 | validation: 0.6636294653100033]
	TIME [epoch: 0.695 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216414423613592		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8216414423613592 | validation: 0.7194910361373671]
	TIME [epoch: 0.695 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216096746105503		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8216096746105503 | validation: 0.6414323318137695]
	TIME [epoch: 0.693 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196698656345334		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8196698656345334 | validation: 0.7969522427150325]
	TIME [epoch: 0.691 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.826489501799486		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.826489501799486 | validation: 0.5621393751641064]
	TIME [epoch: 0.695 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994707104385552		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8994707104385552 | validation: 1.2206119991196482]
	TIME [epoch: 0.694 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0632047799818494		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.0632047799818494 | validation: 0.5287031077407829]
	TIME [epoch: 0.693 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0027889091016886		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.0027889091016886 | validation: 0.7169067275549375]
	TIME [epoch: 0.695 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8861347306625822		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.8861347306625822 | validation: 0.9981162361558387]
	TIME [epoch: 0.696 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9411604011797761		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.9411604011797761 | validation: 0.604443325468019]
	TIME [epoch: 0.697 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8564643198712165		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8564643198712165 | validation: 0.7116341222542584]
	TIME [epoch: 0.694 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8088989502048871		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8088989502048871 | validation: 0.7122519619172973]
	TIME [epoch: 0.695 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8193563625441546		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8193563625441546 | validation: 0.5970653738355848]
	TIME [epoch: 0.693 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8139052003022836		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.8139052003022836 | validation: 0.740872696875408]
	TIME [epoch: 0.694 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271714018433994		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8271714018433994 | validation: 0.6185766643880459]
	TIME [epoch: 0.693 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216047779151987		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8216047779151987 | validation: 0.7924831764506789]
	TIME [epoch: 0.696 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578815859185099		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8578815859185099 | validation: 0.7351677382410804]
	TIME [epoch: 0.694 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9188970800545438		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9188970800545438 | validation: 0.9461971268801638]
	TIME [epoch: 0.694 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.994384005068015		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.994384005068015 | validation: 0.7456179713510013]
	TIME [epoch: 0.693 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840207467086765		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.840207467086765 | validation: 0.5557862101984475]
	TIME [epoch: 0.697 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9083691567459649		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.9083691567459649 | validation: 1.0449245462476335]
	TIME [epoch: 0.695 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9500036598411697		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.9500036598411697 | validation: 0.5565321228784956]
	TIME [epoch: 0.694 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8804256930469042		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8804256930469042 | validation: 0.6982362647622307]
	TIME [epoch: 0.693 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224461872833055		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8224461872833055 | validation: 0.6749710546361647]
	TIME [epoch: 0.694 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168851723861127		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8168851723861127 | validation: 0.6130457924928181]
	TIME [epoch: 0.694 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151529522863126		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8151529522863126 | validation: 0.7506335801550088]
	TIME [epoch: 0.693 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8201993080675553		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8201993080675553 | validation: 0.5782819998703632]
	TIME [epoch: 0.696 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403251506086499		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8403251506086499 | validation: 0.9628655380393459]
	TIME [epoch: 0.697 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.919511108542324		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.919511108542324 | validation: 0.5324732513296617]
	TIME [epoch: 0.694 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9566540763657464		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.9566540763657464 | validation: 0.7526347800793479]
	TIME [epoch: 0.694 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288215058485104		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8288215058485104 | validation: 0.6611965377232811]
	TIME [epoch: 0.693 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822366878421104		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.822366878421104 | validation: 0.6737510993916885]
	TIME [epoch: 0.695 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268468668358773		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8268468668358773 | validation: 0.7571803746241226]
	TIME [epoch: 0.695 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8654021887116247		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8654021887116247 | validation: 0.6776765711266953]
	TIME [epoch: 0.694 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404368706256742		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8404368706256742 | validation: 0.7109381263870758]
	TIME [epoch: 0.693 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8351821521857273		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8351821521857273 | validation: 0.691417243339259]
	TIME [epoch: 0.693 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236414068671154		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8236414068671154 | validation: 0.5666716792350125]
	TIME [epoch: 0.693 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290501753878269		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8290501753878269 | validation: 0.8311506608967447]
	TIME [epoch: 0.691 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484821413786533		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8484821413786533 | validation: 0.5396899770051958]
	TIME [epoch: 0.692 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9758879144827245		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.9758879144827245 | validation: 0.9185660784862926]
	TIME [epoch: 0.696 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8679415913204757		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8679415913204757 | validation: 0.6275726976786375]
	TIME [epoch: 0.694 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932433662010356		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7932433662010356 | validation: 0.6588946748394368]
	TIME [epoch: 0.694 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8001716179938477		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8001716179938477 | validation: 0.6706661917026591]
	TIME [epoch: 0.694 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803516642111162		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.803516642111162 | validation: 0.657191479289809]
	TIME [epoch: 0.695 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526371643484847		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8526371643484847 | validation: 0.7932280126327265]
	TIME [epoch: 0.696 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9079813998170445		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9079813998170445 | validation: 0.6975775514513495]
	TIME [epoch: 0.692 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8955069498038681		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8955069498038681 | validation: 0.6570557415897191]
	TIME [epoch: 0.692 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7929674230362415		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7929674230362415 | validation: 0.666586834681516]
	TIME [epoch: 0.696 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896212313075848		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7896212313075848 | validation: 0.582398400657776]
	TIME [epoch: 0.691 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056461227042647		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8056461227042647 | validation: 0.8004309415518471]
	TIME [epoch: 0.691 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334688647406877		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.8334688647406877 | validation: 0.539054360571592]
	TIME [epoch: 0.691 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916085806471754		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8916085806471754 | validation: 1.044213248350527]
	TIME [epoch: 0.693 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.947960613209952		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.947960613209952 | validation: 0.5606832369296881]
	TIME [epoch: 0.694 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854722277315588		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.854722277315588 | validation: 0.6976651784506185]
	TIME [epoch: 0.695 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7913891047377805		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7913891047377805 | validation: 0.6136590856940116]
	TIME [epoch: 0.696 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793074039454903		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7793074039454903 | validation: 0.606612437312702]
	TIME [epoch: 0.699 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959346298982007		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7959346298982007 | validation: 0.6771305710082126]
	TIME [epoch: 0.695 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.805679357168749		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.805679357168749 | validation: 0.7213698682888068]
	TIME [epoch: 0.694 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9200284441962747		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.9200284441962747 | validation: 0.796108950010475]
	TIME [epoch: 0.697 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9630398083357362		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.9630398083357362 | validation: 0.7288741807830963]
	TIME [epoch: 0.698 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186251259767444		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.8186251259767444 | validation: 0.6039067787559328]
	TIME [epoch: 0.695 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878326206161753		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7878326206161753 | validation: 0.7316549534958493]
	TIME [epoch: 0.694 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237129123710125		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.8237129123710125 | validation: 0.5820451785520628]
	TIME [epoch: 0.696 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969939717523759		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7969939717523759 | validation: 0.7330708918640285]
	TIME [epoch: 0.696 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046911560457654		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.8046911560457654 | validation: 0.5635326929734684]
	TIME [epoch: 0.694 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234106571875387		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.8234106571875387 | validation: 0.9221375337176179]
	TIME [epoch: 0.694 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8735163571376923		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.8735163571376923 | validation: 0.5408244790566832]
	TIME [epoch: 0.696 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575987836672354		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8575987836672354 | validation: 0.7509995015984522]
	TIME [epoch: 0.695 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140283350060734		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8140283350060734 | validation: 0.5610962831833626]
	TIME [epoch: 0.693 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7989244555637879		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7989244555637879 | validation: 0.6651438510015857]
	TIME [epoch: 0.693 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940235778594508		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7940235778594508 | validation: 0.5991220533497217]
	TIME [epoch: 0.695 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159399489988625		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.8159399489988625 | validation: 0.7669104001636127]
	TIME [epoch: 0.695 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8657011129070482		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.8657011129070482 | validation: 0.6661399472297075]
	TIME [epoch: 0.694 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777024785198595		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.8777024785198595 | validation: 0.6842364258457069]
	TIME [epoch: 0.695 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8034614698473241		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.8034614698473241 | validation: 0.7711407866673007]
	TIME [epoch: 0.699 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106390693164157		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.8106390693164157 | validation: 0.5483085425527142]
	TIME [epoch: 0.692 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327451606869127		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.8327451606869127 | validation: 0.847189026674384]
	TIME [epoch: 0.694 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856055235311245		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.856055235311245 | validation: 0.5397505955598553]
	TIME [epoch: 0.694 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091321026900444		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.8091321026900444 | validation: 0.6757232992953256]
	TIME [epoch: 0.694 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859848003799911		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7859848003799911 | validation: 0.5869096326115131]
	TIME [epoch: 0.694 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807101528236288		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7807101528236288 | validation: 0.6204084523861726]
	TIME [epoch: 0.692 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7791695321596908		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7791695321596908 | validation: 0.7330380038465818]
	TIME [epoch: 0.694 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375090783225164		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.8375090783225164 | validation: 0.8068361106785166]
	TIME [epoch: 0.694 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9430769352376867		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.9430769352376867 | validation: 0.6582346300389063]
	TIME [epoch: 0.694 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685811107162179		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.8685811107162179 | validation: 0.6627256881772472]
	TIME [epoch: 176 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859694230169805		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7859694230169805 | validation: 0.5641585191110953]
	TIME [epoch: 1.38 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936693137826325		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7936693137826325 | validation: 0.7979817084722373]
	TIME [epoch: 1.37 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814793758757508		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.814793758757508 | validation: 0.5416074132176724]
	TIME [epoch: 1.36 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080379541114704		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.8080379541114704 | validation: 0.7409832004181314]
	TIME [epoch: 1.36 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038540693700276		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.8038540693700276 | validation: 0.5462432231326952]
	TIME [epoch: 1.36 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021954495257919		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.8021954495257919 | validation: 0.7304531258393325]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991518979370466		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7991518979370466 | validation: 0.5406898313539258]
	TIME [epoch: 1.36 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8137823683306832		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.8137823683306832 | validation: 0.6959288713689058]
	TIME [epoch: 1.37 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037162533329945		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.8037162533329945 | validation: 0.5332180670763962]
	TIME [epoch: 1.36 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8104756039673857		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.8104756039673857 | validation: 0.7110819148761082]
	TIME [epoch: 1.36 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911384638183335		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7911384638183335 | validation: 0.5644577699817612]
	TIME [epoch: 1.37 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7829620511509239		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7829620511509239 | validation: 0.6459206098446792]
	TIME [epoch: 1.36 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846734103559461		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.7846734103559461 | validation: 0.770409583712473]
	TIME [epoch: 1.36 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650633016960668		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.8650633016960668 | validation: 0.8332892145598157]
	TIME [epoch: 1.36 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9881867033660504		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.9881867033660504 | validation: 0.6541541213851998]
	TIME [epoch: 1.36 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895006605513399		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.7895006605513399 | validation: 0.646492423625688]
	TIME [epoch: 1.36 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641286425807536		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.7641286425807536 | validation: 0.6089139234764751]
	TIME [epoch: 1.36 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838680820383715		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7838680820383715 | validation: 0.6781073615798621]
	TIME [epoch: 1.37 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7762940854718474		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7762940854718474 | validation: 0.568867113410395]
	TIME [epoch: 1.36 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755836274885727		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.755836274885727 | validation: 0.6176731984797306]
	TIME [epoch: 1.36 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432386965324755		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7432386965324755 | validation: 0.5005847959931208]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307694725391687		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7307694725391687 | validation: 0.7506919126120941]
	TIME [epoch: 1.37 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334445934905234		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.8334445934905234 | validation: 1.0098054928754256]
	TIME [epoch: 1.37 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1761299500557139		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 1.1761299500557139 | validation: 0.5502140274402978]
	TIME [epoch: 1.37 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7599679302495648		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7599679302495648 | validation: 0.6530034815705947]
	TIME [epoch: 1.37 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870109038334521		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7870109038334521 | validation: 0.6080593521726209]
	TIME [epoch: 1.36 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745650099013726		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7745650099013726 | validation: 0.5414774352038231]
	TIME [epoch: 1.36 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7198789583574601		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7198789583574601 | validation: 0.6098932674417982]
	TIME [epoch: 1.36 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7145748368874999		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7145748368874999 | validation: 0.5383848479919376]
	TIME [epoch: 1.36 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7156214406746984		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7156214406746984 | validation: 0.6988096787394591]
	TIME [epoch: 1.36 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7782448917513457		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.7782448917513457 | validation: 0.7330713925256989]
	TIME [epoch: 1.36 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8855504010876689		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.8855504010876689 | validation: 0.5666040740676274]
	TIME [epoch: 1.36 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570608913488533		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7570608913488533 | validation: 0.5397039475699613]
	TIME [epoch: 1.36 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7004429912409122		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7004429912409122 | validation: 0.550917295456975]
	TIME [epoch: 1.37 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065537047235475		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7065537047235475 | validation: 0.6167459742243109]
	TIME [epoch: 1.37 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297482541694614		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7297482541694614 | validation: 0.5925208714674822]
	TIME [epoch: 1.37 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746506307188844		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7746506307188844 | validation: 0.7108132266854548]
	TIME [epoch: 1.37 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771954779815183		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.771954779815183 | validation: 0.5429914682260564]
	TIME [epoch: 1.37 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263335222368825		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7263335222368825 | validation: 0.5614445973805371]
	TIME [epoch: 1.36 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080991770217488		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.7080991770217488 | validation: 0.5428620605195563]
	TIME [epoch: 1.36 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6938142435210698		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6938142435210698 | validation: 0.5327660907548067]
	TIME [epoch: 1.36 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6887025816981729		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6887025816981729 | validation: 0.5677534400841567]
	TIME [epoch: 1.37 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6939723580572519		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.6939723580572519 | validation: 0.545147390265627]
	TIME [epoch: 1.36 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091938578348578		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.7091938578348578 | validation: 0.5970601236235366]
	TIME [epoch: 1.36 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586244648986182		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.7586244648986182 | validation: 0.7789951373867173]
	TIME [epoch: 1.36 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8250216921526828		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.8250216921526828 | validation: 0.6634337023808465]
	TIME [epoch: 1.36 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293607293726192		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.8293607293726192 | validation: 0.5815304373062357]
	TIME [epoch: 1.36 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150319520764541		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7150319520764541 | validation: 0.5730346554484103]
	TIME [epoch: 1.37 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7086130039627271		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7086130039627271 | validation: 0.5647932407768969]
	TIME [epoch: 1.36 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715013116829171		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.715013116829171 | validation: 0.5983761002407305]
	TIME [epoch: 1.37 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174816164957755		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.7174816164957755 | validation: 0.514722256160487]
	TIME [epoch: 1.36 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6998070429223051		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.6998070429223051 | validation: 0.5652143626277853]
	TIME [epoch: 1.36 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.68598384337574		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.68598384337574 | validation: 0.5528350025257412]
	TIME [epoch: 1.36 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6953186593510734		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.6953186593510734 | validation: 0.5879892877480037]
	TIME [epoch: 1.36 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6958411240862121		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.6958411240862121 | validation: 0.5484197867966479]
	TIME [epoch: 1.36 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203053141043448		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.7203053141043448 | validation: 0.6474286364459849]
	TIME [epoch: 1.36 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684238096832374		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7684238096832374 | validation: 0.6008348458196342]
	TIME [epoch: 1.36 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741071537351042		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.7741071537351042 | validation: 0.5825080496637638]
	TIME [epoch: 1.36 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019677627284128		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.7019677627284128 | validation: 0.5352622472148448]
	TIME [epoch: 1.36 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760776153399207		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.6760776153399207 | validation: 0.5691042925323682]
	TIME [epoch: 1.37 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772284705429218		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.6772284705429218 | validation: 0.5891076658745367]
	TIME [epoch: 1.37 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6820810992800537		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.6820810992800537 | validation: 0.5676337852526528]
	TIME [epoch: 1.36 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175259037304429		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.7175259037304429 | validation: 0.653833154941998]
	TIME [epoch: 1.36 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257955407235247		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.7257955407235247 | validation: 0.5421785364268227]
	TIME [epoch: 1.36 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7411469192295485		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.7411469192295485 | validation: 0.6180000667074057]
	TIME [epoch: 1.36 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976872908758486		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6976872908758486 | validation: 0.5270317661461623]
	TIME [epoch: 1.37 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6765677530846008		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6765677530846008 | validation: 0.5588063274248815]
	TIME [epoch: 1.36 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629602270013579		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6629602270013579 | validation: 0.5363637552983017]
	TIME [epoch: 1.37 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775333227583671		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.6775333227583671 | validation: 0.5523077178656886]
	TIME [epoch: 1.36 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595826479554983		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.6595826479554983 | validation: 0.5480980640660271]
	TIME [epoch: 1.36 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6729691443073379		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6729691443073379 | validation: 0.6458779109971298]
	TIME [epoch: 1.37 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085434793320147		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.7085434793320147 | validation: 0.6414330592180919]
	TIME [epoch: 1.36 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090565443457853		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.8090565443457853 | validation: 0.6118437366258801]
	TIME [epoch: 1.36 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7313592041452218		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.7313592041452218 | validation: 0.5066627460519465]
	TIME [epoch: 1.37 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6627582465167768		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.6627582465167768 | validation: 0.5916746617929651]
	TIME [epoch: 1.37 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667124394580813		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.6667124394580813 | validation: 0.5759371657724498]
	TIME [epoch: 1.36 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521468398741459		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6521468398741459 | validation: 0.5059527767508125]
	TIME [epoch: 1.37 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6463602056857849		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6463602056857849 | validation: 0.5524154553101449]
	TIME [epoch: 1.37 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476245492319547		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.6476245492319547 | validation: 0.641326368398836]
	TIME [epoch: 1.36 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6723551994269092		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.6723551994269092 | validation: 0.6151833776686508]
	TIME [epoch: 1.36 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888443713774711		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.7888443713774711 | validation: 0.6191546084284544]
	TIME [epoch: 1.36 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266652766071471		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7266652766071471 | validation: 0.509900098590128]
	TIME [epoch: 1.36 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6421322400464159		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6421322400464159 | validation: 0.6002664696639171]
	TIME [epoch: 1.37 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6538820241346243		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.6538820241346243 | validation: 0.5012881123256684]
	TIME [epoch: 1.36 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6470921643061591		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6470921643061591 | validation: 0.555723274649157]
	TIME [epoch: 1.36 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6363216656409469		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6363216656409469 | validation: 0.5320564479326475]
	TIME [epoch: 1.36 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561422026312025		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6561422026312025 | validation: 0.6597574722910594]
	TIME [epoch: 1.36 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6751144744143289		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6751144744143289 | validation: 0.571085060191493]
	TIME [epoch: 1.36 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7579625664317723		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.7579625664317723 | validation: 0.5140752911045244]
	TIME [epoch: 1.37 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772605544402563		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6772605544402563 | validation: 0.5428769968187498]
	TIME [epoch: 1.36 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6382316349966578		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.6382316349966578 | validation: 0.5651760246726426]
	TIME [epoch: 1.36 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6408351128677771		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.6408351128677771 | validation: 0.6008527078143107]
	TIME [epoch: 1.36 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334460193835214		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6334460193835214 | validation: 0.5142526817864015]
	TIME [epoch: 1.36 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689435053873888		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6689435053873888 | validation: 0.5815870709955591]
	TIME [epoch: 1.36 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6464518407908423		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6464518407908423 | validation: 0.5328643009326615]
	TIME [epoch: 1.36 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6295522583297622		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6295522583297622 | validation: 0.6937187913424244]
	TIME [epoch: 1.36 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6627410523943185		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6627410523943185 | validation: 0.5404917425613913]
	TIME [epoch: 1.36 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179551002637178		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.7179551002637178 | validation: 0.5188357733057728]
	TIME [epoch: 1.36 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6655410191794358		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6655410191794358 | validation: 0.5560999672296876]
	TIME [epoch: 1.36 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.625410190378324		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.625410190378324 | validation: 0.5562150478438507]
	TIME [epoch: 1.37 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6281050077043059		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6281050077043059 | validation: 0.5480831001582587]
	TIME [epoch: 1.37 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6105805347541188		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6105805347541188 | validation: 0.5703603159471612]
	TIME [epoch: 1.36 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6159222839438347		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6159222839438347 | validation: 0.5089304575395441]
	TIME [epoch: 1.36 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443953031517724		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.6443953031517724 | validation: 0.7610239915320713]
	TIME [epoch: 1.37 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933777925081475		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.6933777925081475 | validation: 0.568923358413354]
	TIME [epoch: 1.36 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379395964869933		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.7379395964869933 | validation: 0.4802031982472368]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567285224777737		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6567285224777737 | validation: 0.6177741392707342]
	TIME [epoch: 1.36 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381344679693384		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6381344679693384 | validation: 0.5299340089832126]
	TIME [epoch: 1.37 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133958469348639		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6133958469348639 | validation: 0.6259825547692593]
	TIME [epoch: 1.36 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6095852910801484		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6095852910801484 | validation: 0.5171479278083061]
	TIME [epoch: 1.37 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6125683390604932		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6125683390604932 | validation: 0.6002329118477223]
	TIME [epoch: 1.36 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028600283177008		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6028600283177008 | validation: 0.5337505189799027]
	TIME [epoch: 1.36 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5951849454722296		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.5951849454722296 | validation: 0.6053780923135689]
	TIME [epoch: 1.36 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862769248991517		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.5862769248991517 | validation: 0.4880218775449426]
	TIME [epoch: 1.36 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6315187344837644		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6315187344837644 | validation: 0.6201095990900574]
	TIME [epoch: 1.36 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6127538312629056		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.6127538312629056 | validation: 0.5044451794122023]
	TIME [epoch: 1.36 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239873369163946		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.6239873369163946 | validation: 0.6959399541078393]
	TIME [epoch: 1.36 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317718867086479		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6317718867086479 | validation: 0.5283792638214128]
	TIME [epoch: 1.36 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830260086902251		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6830260086902251 | validation: 0.49035014235230384]
	TIME [epoch: 1.36 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218275389849973		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6218275389849973 | validation: 0.6479928482107908]
	TIME [epoch: 1.36 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112360245619047		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6112360245619047 | validation: 0.5266598887145291]
	TIME [epoch: 1.36 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6059481628966915		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6059481628966915 | validation: 0.663344362548005]
	TIME [epoch: 1.36 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092192909348101		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6092192909348101 | validation: 0.5020010828158304]
	TIME [epoch: 1.36 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6394997705779618		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6394997705779618 | validation: 0.5109448564577028]
	TIME [epoch: 1.36 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815877799238629		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.5815877799238629 | validation: 0.6583443274798846]
	TIME [epoch: 1.36 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6090085335607094		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6090085335607094 | validation: 0.5041852294688581]
	TIME [epoch: 1.36 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6281909469535422		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.6281909469535422 | validation: 0.5628299872935986]
	TIME [epoch: 1.36 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5606314401501544		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.5606314401501544 | validation: 0.6335750765006436]
	TIME [epoch: 1.36 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.582381550281024		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.582381550281024 | validation: 0.5070633559475759]
	TIME [epoch: 1.36 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6440236807210384		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6440236807210384 | validation: 0.5287208176530559]
	TIME [epoch: 1.36 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5636353149910753		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.5636353149910753 | validation: 0.6968721334030141]
	TIME [epoch: 1.37 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6083169086423703		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6083169086423703 | validation: 0.5227226822904205]
	TIME [epoch: 1.36 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785874787117058		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6785874787117058 | validation: 0.47462154868674067]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845087784855172		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.5845087784855172 | validation: 0.7320801121891798]
	TIME [epoch: 1.37 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446721307383959		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6446721307383959 | validation: 0.5072981537534591]
	TIME [epoch: 1.36 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310964120091876		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6310964120091876 | validation: 0.48580414714119907]
	TIME [epoch: 1.36 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5770112512331421		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.5770112512331421 | validation: 0.6802220550386592]
	TIME [epoch: 1.36 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6098549705510338		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6098549705510338 | validation: 0.5026369879634538]
	TIME [epoch: 1.36 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6157513416157282		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.6157513416157282 | validation: 0.49282562142622166]
	TIME [epoch: 1.36 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726812393514673		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.5726812393514673 | validation: 0.657181827356704]
	TIME [epoch: 1.36 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587000597230955		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.587000597230955 | validation: 0.5141991620170824]
	TIME [epoch: 1.36 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5792167738017437		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.5792167738017437 | validation: 0.5656885905374852]
	TIME [epoch: 1.36 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5538084322086916		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.5538084322086916 | validation: 0.5469812883792747]
	TIME [epoch: 1.36 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489047852815366		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.5489047852815366 | validation: 0.4896639558150609]
	TIME [epoch: 1.36 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5547800345186624		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.5547800345186624 | validation: 0.6616707008169671]
	TIME [epoch: 1.36 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5837746636129285		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.5837746636129285 | validation: 0.4890243696013169]
	TIME [epoch: 1.36 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845669802166185		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.5845669802166185 | validation: 0.5657113436471269]
	TIME [epoch: 1.37 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5394455450262023		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.5394455450262023 | validation: 0.5431284000629027]
	TIME [epoch: 1.36 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5408094282405704		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.5408094282405704 | validation: 0.5448066212295085]
	TIME [epoch: 1.36 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369166794303771		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.5369166794303771 | validation: 0.5683653475484567]
	TIME [epoch: 1.36 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5341094086457953		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5341094086457953 | validation: 0.49940070576946805]
	TIME [epoch: 1.36 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5426953998834627		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5426953998834627 | validation: 0.6340458285647841]
	TIME [epoch: 1.36 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5679721633205344		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.5679721633205344 | validation: 0.534243479164375]
	TIME [epoch: 1.36 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6908998801260264		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.6908998801260264 | validation: 0.4862708897943231]
	TIME [epoch: 1.36 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6419632130941979		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.6419632130941979 | validation: 0.5506864419217105]
	TIME [epoch: 1.36 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5598118843304347		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.5598118843304347 | validation: 0.7192394033804482]
	TIME [epoch: 1.36 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.629118055724843		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.629118055724843 | validation: 0.5427661851352008]
	TIME [epoch: 1.36 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7084631940952081		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.7084631940952081 | validation: 0.4477659601801889]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021865027221613		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.6021865027221613 | validation: 0.6383835271592117]
	TIME [epoch: 1.36 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5886790942541021		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.5886790942541021 | validation: 0.4834251959043119]
	TIME [epoch: 1.36 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5429427387662387		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5429427387662387 | validation: 0.5380715060738438]
	TIME [epoch: 1.36 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5238397880741775		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5238397880741775 | validation: 0.5815518303834868]
	TIME [epoch: 1.36 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369423600140526		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5369423600140526 | validation: 0.4821784298651154]
	TIME [epoch: 1.36 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5469233916996973		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5469233916996973 | validation: 0.6505212781804967]
	TIME [epoch: 1.37 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574208202737579		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.574208202737579 | validation: 0.47957293064954754]
	TIME [epoch: 1.36 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908901624257337		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.5908901624257337 | validation: 0.4856009940829344]
	TIME [epoch: 1.36 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5297484077240767		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5297484077240767 | validation: 0.6228585876874375]
	TIME [epoch: 1.36 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5707324095073655		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.5707324095073655 | validation: 0.4791414875291124]
	TIME [epoch: 1.36 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5610674900371734		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5610674900371734 | validation: 0.5190070775750301]
	TIME [epoch: 1.36 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5130034189005802		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.5130034189005802 | validation: 0.6116433926600416]
	TIME [epoch: 1.36 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5565698056879665		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5565698056879665 | validation: 0.4778430495351252]
	TIME [epoch: 1.36 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5701587446834403		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5701587446834403 | validation: 0.5003047917730744]
	TIME [epoch: 1.36 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281369681073936		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.5281369681073936 | validation: 0.6401629305498004]
	TIME [epoch: 1.36 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5668052065582597		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5668052065582597 | validation: 0.4685444461015221]
	TIME [epoch: 1.36 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5876680496580415		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5876680496580415 | validation: 0.5253438155510413]
	TIME [epoch: 1.36 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5112090574931552		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5112090574931552 | validation: 0.6171998837413071]
	TIME [epoch: 1.36 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467810403871934		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5467810403871934 | validation: 0.4624693999390071]
	TIME [epoch: 1.36 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5742738716691426		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.5742738716691426 | validation: 0.5497449010822464]
	TIME [epoch: 1.36 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5142976811109733		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.5142976811109733 | validation: 0.49790089458284736]
	TIME [epoch: 1.36 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5121191754236127		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.5121191754236127 | validation: 0.4733992877140281]
	TIME [epoch: 1.36 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5148814732635325		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.5148814732635325 | validation: 0.5503513942203794]
	TIME [epoch: 1.36 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5098969839042672		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5098969839042672 | validation: 0.46567665901775634]
	TIME [epoch: 1.36 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5412282014487825		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.5412282014487825 | validation: 0.6403493288959539]
	TIME [epoch: 1.36 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5677464476470924		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.5677464476470924 | validation: 0.48266270927024063]
	TIME [epoch: 1.36 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5889399833127344		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.5889399833127344 | validation: 0.4621100646684311]
	TIME [epoch: 1.36 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.521644042413127		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.521644042413127 | validation: 0.7237793039151632]
	TIME [epoch: 1.36 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6605257717715916		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.6605257717715916 | validation: 0.46980965210747927]
	TIME [epoch: 1.36 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5531094687578082		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5531094687578082 | validation: 0.4897732140026245]
	TIME [epoch: 1.36 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5180153490852437		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.5180153490852437 | validation: 0.6269253080167574]
	TIME [epoch: 1.36 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5359915684189219		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.5359915684189219 | validation: 0.4542195137437215]
	TIME [epoch: 1.36 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5348047710696342		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.5348047710696342 | validation: 0.5030986657035349]
	TIME [epoch: 1.36 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5081672460487769		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.5081672460487769 | validation: 0.5802528522974307]
	TIME [epoch: 1.36 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5222284581578939		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.5222284581578939 | validation: 0.4783496621113404]
	TIME [epoch: 1.36 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308864280137244		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.5308864280137244 | validation: 0.5272580467304054]
	TIME [epoch: 1.36 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.501314934510935		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.501314934510935 | validation: 0.5547498900078114]
	TIME [epoch: 1.36 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5033607659689172		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.5033607659689172 | validation: 0.4724145571482714]
	TIME [epoch: 1.36 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5291549920545966		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.5291549920545966 | validation: 0.5564188008083024]
	TIME [epoch: 1.36 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140119150400905		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.5140119150400905 | validation: 0.4519385929359059]
	TIME [epoch: 1.36 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5132925964864613		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.5132925964864613 | validation: 0.5800033037219954]
	TIME [epoch: 1.36 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522182075426677		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.522182075426677 | validation: 0.4565882324465509]
	TIME [epoch: 1.35 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6429775805808697		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.6429775805808697 | validation: 0.4437879086764178]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6281380668455286		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.6281380668455286 | validation: 0.45388742396859016]
	TIME [epoch: 1.36 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424427931060432		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.5424427931060432 | validation: 0.6479389798455006]
	TIME [epoch: 1.36 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5563062205243731		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.5563062205243731 | validation: 0.4874747430432331]
	TIME [epoch: 1.36 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5168362248889389		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.5168362248889389 | validation: 0.5190363592415537]
	TIME [epoch: 1.36 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4990864324635162		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.4990864324635162 | validation: 0.6375963032678784]
	TIME [epoch: 1.36 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5874674800439593		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.5874674800439593 | validation: 0.4538417975128972]
	TIME [epoch: 1.36 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5264842650486204		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.5264842650486204 | validation: 0.4849292541317306]
	TIME [epoch: 1.36 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49356919473352817		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.49356919473352817 | validation: 0.597086681867931]
	TIME [epoch: 1.36 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316024317847964		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.5316024317847964 | validation: 0.47712611186438036]
	TIME [epoch: 1.36 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5213290756184739		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.5213290756184739 | validation: 0.4663794526296968]
	TIME [epoch: 1.36 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49695562135036453		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.49695562135036453 | validation: 0.5364645928454229]
	TIME [epoch: 1.36 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49215797684312457		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.49215797684312457 | validation: 0.4540198322203912]
	TIME [epoch: 1.36 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5145379862510676		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.5145379862510676 | validation: 0.5294141432821975]
	TIME [epoch: 1.36 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4915366201160882		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.4915366201160882 | validation: 0.4820608082180954]
	TIME [epoch: 1.36 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4844829852122214		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.4844829852122214 | validation: 0.5028488129352471]
	TIME [epoch: 1.36 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48430865128858086		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.48430865128858086 | validation: 0.49670693214077666]
	TIME [epoch: 1.36 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4843967130091997		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.4843967130091997 | validation: 0.6682685110235664]
	TIME [epoch: 1.36 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619276467548321		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.619276467548321 | validation: 0.47861289869645707]
	TIME [epoch: 1.36 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5843429860876519		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.5843429860876519 | validation: 0.4346318406971196]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323641514458476		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.5323641514458476 | validation: 0.6323264306613964]
	TIME [epoch: 1.36 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5835511748273613		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.5835511748273613 | validation: 0.4647994600463358]
	TIME [epoch: 1.35 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4852363059105861		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.4852363059105861 | validation: 0.44967317664422907]
	TIME [epoch: 1.35 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5097489033421334		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.5097489033421334 | validation: 0.550327429146912]
	TIME [epoch: 1.35 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4954020603038606		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.4954020603038606 | validation: 0.4589620819200394]
	TIME [epoch: 1.36 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48988381113521584		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.48988381113521584 | validation: 0.49961888547814315]
	TIME [epoch: 1.36 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4710392941039194		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.4710392941039194 | validation: 0.525227635701722]
	TIME [epoch: 1.36 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47684894978721076		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.47684894978721076 | validation: 0.47412884927719656]
	TIME [epoch: 1.36 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889755577905037		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.4889755577905037 | validation: 0.4755822068999619]
	TIME [epoch: 1.36 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47673752406538394		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.47673752406538394 | validation: 0.4694074522055338]
	TIME [epoch: 1.36 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48085426031686695		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.48085426031686695 | validation: 0.5982338377254169]
	TIME [epoch: 1.36 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5243807688391094		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.5243807688391094 | validation: 0.4957139020387469]
	TIME [epoch: 1.36 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6372408264836849		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.6372408264836849 | validation: 0.4692375127139419]
	TIME [epoch: 1.36 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6181719324292508		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.6181719324292508 | validation: 0.4357299747888462]
	TIME [epoch: 1.35 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371190193487256		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.5371190193487256 | validation: 0.6778513072324235]
	TIME [epoch: 1.35 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.620412871493621		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.620412871493621 | validation: 0.5134163013518311]
	TIME [epoch: 1.35 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4813331907557962		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.4813331907557962 | validation: 0.45965502710634676]
	TIME [epoch: 1.35 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5038543485161954		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.5038543485161954 | validation: 0.5991339926338999]
	TIME [epoch: 1.35 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.519413186752958		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.519413186752958 | validation: 0.495427470186924]
	TIME [epoch: 1.35 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4730910469928641		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.4730910469928641 | validation: 0.4759246363341134]
	TIME [epoch: 1.36 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4782486441317739		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.4782486441317739 | validation: 0.549331817341602]
	TIME [epoch: 1.36 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4925259006960627		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.4925259006960627 | validation: 0.4533387675992888]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47985193592239517		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.47985193592239517 | validation: 0.49419808121984954]
	TIME [epoch: 1.35 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4709805069345652		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.4709805069345652 | validation: 0.5427328566677575]
	TIME [epoch: 1.35 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47655716665230186		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.47655716665230186 | validation: 0.4494212315428078]
	TIME [epoch: 1.35 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.489223866882264		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.489223866882264 | validation: 0.5269638038979454]
	TIME [epoch: 1.35 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47447650989240486		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.47447650989240486 | validation: 0.4423775119947608]
	TIME [epoch: 1.36 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361721400764399		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.6361721400764399 | validation: 0.4591102714709858]
	TIME [epoch: 1.36 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6461778412503965		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.6461778412503965 | validation: 0.44370052073849164]
	TIME [epoch: 1.36 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5995616614219298		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.5995616614219298 | validation: 0.45719677925916025]
	TIME [epoch: 1.36 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49374929510624627		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.49374929510624627 | validation: 0.5762442571043725]
	TIME [epoch: 1.36 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089590047440383		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.5089590047440383 | validation: 0.47142021902168296]
	TIME [epoch: 1.36 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4877993524942135		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.4877993524942135 | validation: 0.47450771339520137]
	TIME [epoch: 1.36 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894057733073453		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.4894057733073453 | validation: 0.5034261542692853]
	TIME [epoch: 1.36 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47859264329610474		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.47859264329610474 | validation: 0.49245139663278614]
	TIME [epoch: 1.36 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47045092883773193		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.47045092883773193 | validation: 0.4622354918779381]
	TIME [epoch: 1.35 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4756316293244583		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.4756316293244583 | validation: 0.4950331045238257]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4633763884574589		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.4633763884574589 | validation: 0.5092181575747688]
	TIME [epoch: 1.35 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46319248199973617		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.46319248199973617 | validation: 0.4542376612512509]
	TIME [epoch: 1.35 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47478769309885627		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.47478769309885627 | validation: 0.45855237471743254]
	TIME [epoch: 1.35 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4653189220881812		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.4653189220881812 | validation: 0.44793946168104537]
	TIME [epoch: 1.36 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46177620559990645		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.46177620559990645 | validation: 0.5064750474509521]
	TIME [epoch: 1.36 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46963865226822477		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.46963865226822477 | validation: 0.43912377155704746]
	TIME [epoch: 1.36 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4811235839085288		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.4811235839085288 | validation: 0.5285663761398024]
	TIME [epoch: 1.36 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4716119201148554		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.4716119201148554 | validation: 0.4222999973891282]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.519370041172906		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.519370041172906 | validation: 0.47251179535389964]
	TIME [epoch: 1.36 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544513394717082		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.4544513394717082 | validation: 0.4535552052557954]
	TIME [epoch: 1.36 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46860779024077076		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.46860779024077076 | validation: 0.47688185501143976]
	TIME [epoch: 1.36 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45613216977659804		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.45613216977659804 | validation: 0.4895911105753216]
	TIME [epoch: 1.36 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44818695569067246		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.44818695569067246 | validation: 0.42383300361946985]
	TIME [epoch: 1.36 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4746036546879336		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.4746036546879336 | validation: 0.5427071233430222]
	TIME [epoch: 1.36 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4853055847430211		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.4853055847430211 | validation: 0.39717526793262614]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5156213937376585		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.5156213937376585 | validation: 0.4757067866310382]
	TIME [epoch: 1.36 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4463730377687326		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.4463730377687326 | validation: 0.4741733746038418]
	TIME [epoch: 1.36 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4487423415395766		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.4487423415395766 | validation: 0.5265144470785228]
	TIME [epoch: 1.36 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4751623525739096		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.4751623525739096 | validation: 0.41936192994436605]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5427839139946327		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.5427839139946327 | validation: 0.43087306460375885]
	TIME [epoch: 1.35 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44971954836240896		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.44971954836240896 | validation: 0.5329207212041736]
	TIME [epoch: 1.35 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48046377176697264		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.48046377176697264 | validation: 0.43825950490718824]
	TIME [epoch: 1.36 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4901034370034662		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.4901034370034662 | validation: 0.4799327730074947]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44615641136296386		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.44615641136296386 | validation: 0.517223503054816]
	TIME [epoch: 1.35 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45981685867526323		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.45981685867526323 | validation: 0.4057531561656669]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48484981098740093		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.48484981098740093 | validation: 0.47104297905425274]
	TIME [epoch: 1.36 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44071447996180235		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.44071447996180235 | validation: 0.45596771992984364]
	TIME [epoch: 1.36 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4431537637076366		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.4431537637076366 | validation: 0.4276284416765666]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44106148248263466		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.44106148248263466 | validation: 0.4490632208436468]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43210658128491036		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.43210658128491036 | validation: 0.40993221017778386]
	TIME [epoch: 1.35 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44607323235283164		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.44607323235283164 | validation: 0.4927574896092082]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4468415497284173		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.4468415497284173 | validation: 0.4166378733790721]
	TIME [epoch: 1.35 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.457101031947626		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.457101031947626 | validation: 0.509274739548459]
	TIME [epoch: 1.36 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4584768309992526		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.4584768309992526 | validation: 0.4041491518379666]
	TIME [epoch: 1.36 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46458725795989836		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.46458725795989836 | validation: 0.5270062830502782]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4808513474375778		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.4808513474375778 | validation: 0.3952031931186986]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_494.pth
	Model improved!!!
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4496421915040692		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.4496421915040692 | validation: 0.41736348583622934]
	TIME [epoch: 1.36 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4466138150951347		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.4466138150951347 | validation: 0.535590298067972]
	TIME [epoch: 1.36 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4709660593036206		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.4709660593036206 | validation: 0.3920611253812278]
	TIME [epoch: 1.36 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49137228755684886		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.49137228755684886 | validation: 0.40503122902778427]
	TIME [epoch: 1.36 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4401938052925414		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.4401938052925414 | validation: 0.5174519921240499]
	TIME [epoch: 1.36 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4535628405582668		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.4535628405582668 | validation: 0.41843475999981766]
	TIME [epoch: 1.36 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4469577155960366		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.4469577155960366 | validation: 0.4271128901934342]
	TIME [epoch: 179 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4176348560987371		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.4176348560987371 | validation: 0.46462483096682883]
	TIME [epoch: 2.71 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4252415806369976		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.4252415806369976 | validation: 0.397563825314641]
	TIME [epoch: 2.68 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4394589441237352		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.4394589441237352 | validation: 0.6999825751827431]
	TIME [epoch: 2.69 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666955449229403		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.6666955449229403 | validation: 0.5342517693406463]
	TIME [epoch: 2.69 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45504582479381694		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.45504582479381694 | validation: 0.4292398318095222]
	TIME [epoch: 2.68 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5197149339626006		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.5197149339626006 | validation: 0.39918949059380826]
	TIME [epoch: 2.68 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45157586023100577		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.45157586023100577 | validation: 0.5412390615873727]
	TIME [epoch: 2.69 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4711992927867362		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.4711992927867362 | validation: 0.41882974352822666]
	TIME [epoch: 2.69 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42314001369362475		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.42314001369362475 | validation: 0.3809419442478004]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46237994562608964		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.46237994562608964 | validation: 0.4374173539033228]
	TIME [epoch: 2.68 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41676811448799966		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.41676811448799966 | validation: 0.4240641030479346]
	TIME [epoch: 2.68 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41837309950200263		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.41837309950200263 | validation: 0.4195426031293512]
	TIME [epoch: 2.69 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.416443879363076		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.416443879363076 | validation: 0.4110144492469629]
	TIME [epoch: 2.69 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4140332313969355		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.4140332313969355 | validation: 0.4229712005482199]
	TIME [epoch: 2.69 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41682976313806036		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.41682976313806036 | validation: 0.45592077408055953]
	TIME [epoch: 2.68 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41731834200292783		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.41731834200292783 | validation: 0.40293140324961724]
	TIME [epoch: 2.69 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43336830802297827		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.43336830802297827 | validation: 0.5003184389589062]
	TIME [epoch: 2.69 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4446748119127646		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.4446748119127646 | validation: 0.3836397902565933]
	TIME [epoch: 2.69 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4354379747578166		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.4354379747578166 | validation: 0.40828583637951354]
	TIME [epoch: 2.69 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41163448765395827		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.41163448765395827 | validation: 0.4583383437230672]
	TIME [epoch: 2.69 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4114244842876156		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.4114244842876156 | validation: 0.39452404023982746]
	TIME [epoch: 2.69 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4337427771436546		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.4337427771436546 | validation: 0.4576990818714699]
	TIME [epoch: 2.69 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4046582326162681		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.4046582326162681 | validation: 0.39395552962203945]
	TIME [epoch: 2.69 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40547156274333374		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.40547156274333374 | validation: 0.43299439471323914]
	TIME [epoch: 2.69 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4079190452899271		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.4079190452899271 | validation: 0.5433339912369847]
	TIME [epoch: 2.69 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4826427899534486		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.4826427899534486 | validation: 0.40211541175287463]
	TIME [epoch: 2.69 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4714779986465102		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.4714779986465102 | validation: 0.41487575441191765]
	TIME [epoch: 2.68 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40471517515137934		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.40471517515137934 | validation: 0.4532022203545749]
	TIME [epoch: 2.69 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4072634908161399		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.4072634908161399 | validation: 0.36439653443267456]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_530.pth
	Model improved!!!
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.447511447196996		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.447511447196996 | validation: 0.41149748424132127]
	TIME [epoch: 2.69 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3929884729963736		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.3929884729963736 | validation: 0.44287759981134434]
	TIME [epoch: 2.69 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40543073505606225		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.40543073505606225 | validation: 0.35854120755604807]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_533.pth
	Model improved!!!
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44110046028163497		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.44110046028163497 | validation: 0.4598291029482928]
	TIME [epoch: 2.68 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221496731471306		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.4221496731471306 | validation: 0.38853731626308186]
	TIME [epoch: 2.68 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3998877105290769		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.3998877105290769 | validation: 0.3964161597771204]
	TIME [epoch: 2.69 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3909142517023068		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.3909142517023068 | validation: 0.4140106262099704]
	TIME [epoch: 2.69 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39044119914204095		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.39044119914204095 | validation: 0.4094967757983578]
	TIME [epoch: 2.68 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.391026378703658		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.391026378703658 | validation: 0.4046797743170517]
	TIME [epoch: 2.68 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3839462361344424		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.3839462361344424 | validation: 0.41304109411350026]
	TIME [epoch: 2.69 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3831010246783818		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.3831010246783818 | validation: 0.40416259323886816]
	TIME [epoch: 2.69 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38125605976078325		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.38125605976078325 | validation: 0.36839801549530876]
	TIME [epoch: 2.69 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3958312298547532		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.3958312298547532 | validation: 0.553598085903365]
	TIME [epoch: 2.69 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5021311178695483		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.5021311178695483 | validation: 0.37610149264146553]
	TIME [epoch: 2.69 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4087383243393964		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.4087383243393964 | validation: 0.4257974816394245]
	TIME [epoch: 2.68 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3805435798584388		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.3805435798584388 | validation: 0.8844878319248899]
	TIME [epoch: 2.69 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091447347774707		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.9091447347774707 | validation: 0.6806663116483449]
	TIME [epoch: 2.68 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663986928740692		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.6663986928740692 | validation: 0.4318803738476065]
	TIME [epoch: 2.69 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43097991639834815		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.43097991639834815 | validation: 0.39770332567356026]
	TIME [epoch: 2.69 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4930492828689989		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.4930492828689989 | validation: 0.39090398613866967]
	TIME [epoch: 2.68 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37697030694541556		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.37697030694541556 | validation: 0.43286990548005505]
	TIME [epoch: 2.69 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39162407679264954		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.39162407679264954 | validation: 0.376592858254808]
	TIME [epoch: 2.69 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37990524582713975		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.37990524582713975 | validation: 0.35854730909074556]
	TIME [epoch: 2.69 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393747025207202		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.393747025207202 | validation: 0.37447146013636656]
	TIME [epoch: 2.68 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3825839765232081		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.3825839765232081 | validation: 0.4186756298001193]
	TIME [epoch: 2.69 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37548023170821937		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.37548023170821937 | validation: 0.38661099581335034]
	TIME [epoch: 2.68 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37757210059721746		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.37757210059721746 | validation: 0.3686462014592493]
	TIME [epoch: 2.68 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3736798703771227		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.3736798703771227 | validation: 0.40421186621670424]
	TIME [epoch: 2.69 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3725851887779807		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.3725851887779807 | validation: 0.3961682653616116]
	TIME [epoch: 2.69 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37179003079223727		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.37179003079223727 | validation: 0.4688695302648309]
	TIME [epoch: 2.68 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40401510378802274		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.40401510378802274 | validation: 0.3639066938551663]
	TIME [epoch: 2.69 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41728489299707194		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.41728489299707194 | validation: 0.3833445022436276]
	TIME [epoch: 2.69 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3692910117250092		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.3692910117250092 | validation: 0.3905467383323377]
	TIME [epoch: 2.69 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.368502251203479		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.368502251203479 | validation: 0.36379391467773875]
	TIME [epoch: 2.69 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3718077920944174		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.3718077920944174 | validation: 0.3666104022234211]
	TIME [epoch: 2.69 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3606086498827877		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.3606086498827877 | validation: 0.3700799999470917]
	TIME [epoch: 2.69 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36650002584463587		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.36650002584463587 | validation: 0.3715931053874938]
	TIME [epoch: 2.68 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36268112445626444		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.36268112445626444 | validation: 0.3360868354609092]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_568.pth
	Model improved!!!
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3974212296577076		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.3974212296577076 | validation: 0.4697953089825304]
	TIME [epoch: 2.69 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41624416252323165		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.41624416252323165 | validation: 0.3290655172455328]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_570.pth
	Model improved!!!
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3856529566360643		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.3856529566360643 | validation: 0.409764912886309]
	TIME [epoch: 2.69 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37606715825487114		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.37606715825487114 | validation: 0.35903483368182837]
	TIME [epoch: 2.69 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594665315800484		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.3594665315800484 | validation: 0.3459068443820335]
	TIME [epoch: 2.69 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36739149255023074		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.36739149255023074 | validation: 0.36403324688928596]
	TIME [epoch: 2.69 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3601619751878245		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.3601619751878245 | validation: 0.4435426131761193]
	TIME [epoch: 2.69 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3823964690656868		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.3823964690656868 | validation: 0.36168682654050227]
	TIME [epoch: 2.69 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40864396994667346		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.40864396994667346 | validation: 0.42563656571243014]
	TIME [epoch: 2.68 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3768536266518931		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.3768536266518931 | validation: 0.35914984347785683]
	TIME [epoch: 2.68 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3538445126341946		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.3538445126341946 | validation: 0.3563359198010957]
	TIME [epoch: 2.69 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36253703689148986		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.36253703689148986 | validation: 0.4089157575109621]
	TIME [epoch: 2.69 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3643365012291707		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.3643365012291707 | validation: 0.35644610502968765]
	TIME [epoch: 2.68 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3564352760640533		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.3564352760640533 | validation: 0.35247066579389286]
	TIME [epoch: 2.69 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3458740184079331		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.3458740184079331 | validation: 0.3736444142199401]
	TIME [epoch: 2.68 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34671260115349833		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.34671260115349833 | validation: 0.41128474646616875]
	TIME [epoch: 2.69 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6402775973041753		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.6402775973041753 | validation: 0.34856595122649603]
	TIME [epoch: 2.69 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.569204549776616		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.569204549776616 | validation: 0.3183617061923443]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_586.pth
	Model improved!!!
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4173129062673424		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.4173129062673424 | validation: 0.4967631170253178]
	TIME [epoch: 2.69 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43412886375283805		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.43412886375283805 | validation: 0.41879359767459134]
	TIME [epoch: 2.69 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3672479539795222		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.3672479539795222 | validation: 0.3522784248638005]
	TIME [epoch: 2.69 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3658248685867277		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.3658248685867277 | validation: 0.3860629723835336]
	TIME [epoch: 2.69 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3581433670014839		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.3581433670014839 | validation: 0.3546890696898968]
	TIME [epoch: 2.69 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35224414948069366		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.35224414948069366 | validation: 0.350659657697735]
	TIME [epoch: 2.69 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3537056906596388		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.3537056906596388 | validation: 0.3682360034217164]
	TIME [epoch: 2.69 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3528274368033646		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.3528274368033646 | validation: 0.350644894693511]
	TIME [epoch: 2.69 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35076087012934026		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.35076087012934026 | validation: 0.34873487776418455]
	TIME [epoch: 2.69 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3465429115101432		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.3465429115101432 | validation: 0.3498713406720045]
	TIME [epoch: 2.69 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34383379054402147		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.34383379054402147 | validation: 0.37183244383340724]
	TIME [epoch: 2.69 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3400864249602992		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.3400864249602992 | validation: 0.3553406395370183]
	TIME [epoch: 2.68 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33954986233358303		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.33954986233358303 | validation: 0.37494317393570165]
	TIME [epoch: 2.68 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3471255780954373		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.3471255780954373 | validation: 0.3376371616893186]
	TIME [epoch: 2.69 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3514133253915601		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.3514133253915601 | validation: 0.3671885733531898]
	TIME [epoch: 2.68 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.341195481446901		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.341195481446901 | validation: 0.315056010179309]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3508972652820282		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.3508972652820282 | validation: 0.38339880152792655]
	TIME [epoch: 2.69 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34304324700003835		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.34304324700003835 | validation: 0.32475142866873946]
	TIME [epoch: 2.68 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3524419594501968		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.3524419594501968 | validation: 0.35596429565363535]
	TIME [epoch: 2.69 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3310303688659947		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.3310303688659947 | validation: 0.3501943960642865]
	TIME [epoch: 2.69 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32808605078034825		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.32808605078034825 | validation: 0.33136683100219866]
	TIME [epoch: 2.69 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33289047770527375		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.33289047770527375 | validation: 0.361616133793502]
	TIME [epoch: 2.69 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3360040258920974		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.3360040258920974 | validation: 0.31961002640323866]
	TIME [epoch: 2.69 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33805879999762456		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.33805879999762456 | validation: 0.37628544487949833]
	TIME [epoch: 2.69 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3377933137474882		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.3377933137474882 | validation: 0.3856227509991206]
	TIME [epoch: 2.69 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32952123170094055		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.32952123170094055 | validation: 0.3232690119677758]
	TIME [epoch: 2.68 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.368500083999145		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.368500083999145 | validation: 0.4132862386095251]
	TIME [epoch: 2.69 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3721940283129911		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.3721940283129911 | validation: 0.30667377363355114]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3546602792423547		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.3546602792423547 | validation: 0.3579422323240011]
	TIME [epoch: 2.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32118138512527056		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.32118138512527056 | validation: 0.31875970177136975]
	TIME [epoch: 2.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32101105464286017		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.32101105464286017 | validation: 0.3132247828392643]
	TIME [epoch: 2.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31789764226102124		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.31789764226102124 | validation: 0.38349313831806625]
	TIME [epoch: 2.69 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3391227872100545		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.3391227872100545 | validation: 0.304232648727214]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_619.pth
	Model improved!!!
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3549513633327935		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.3549513633327935 | validation: 0.4185418056781968]
	TIME [epoch: 2.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36493189489755795		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.36493189489755795 | validation: 0.33710587223830724]
	TIME [epoch: 2.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31383368234252196		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.31383368234252196 | validation: 0.2982417758425866]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_622.pth
	Model improved!!!
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3291966168718669		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.3291966168718669 | validation: 0.3406282443690427]
	TIME [epoch: 2.69 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3187834952978299		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.3187834952978299 | validation: 0.299697754707505]
	TIME [epoch: 2.69 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197138971750526		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.3197138971750526 | validation: 0.3549045990949913]
	TIME [epoch: 2.69 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3182460536196667		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.3182460536196667 | validation: 0.32023571270842754]
	TIME [epoch: 2.69 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31331153563914416		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.31331153563914416 | validation: 0.32280160999840796]
	TIME [epoch: 2.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29853966274217625		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.29853966274217625 | validation: 0.33531652238513576]
	TIME [epoch: 2.69 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30456854567866865		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.30456854567866865 | validation: 0.3065689125259366]
	TIME [epoch: 2.69 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32298286648679436		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.32298286648679436 | validation: 0.3486029537727928]
	TIME [epoch: 2.69 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31341750259421364		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.31341750259421364 | validation: 0.3221409835081621]
	TIME [epoch: 2.69 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31302467833882147		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.31302467833882147 | validation: 0.3525653625448847]
	TIME [epoch: 2.68 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3041905213852067		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.3041905213852067 | validation: 0.301054726853057]
	TIME [epoch: 2.68 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.319773695780138		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.319773695780138 | validation: 0.65832120981051]
	TIME [epoch: 2.69 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434294386360471		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.6434294386360471 | validation: 0.4473645846252655]
	TIME [epoch: 2.69 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3911795244733118		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3911795244733118 | validation: 0.3274613401558257]
	TIME [epoch: 2.68 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38329153255134557		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.38329153255134557 | validation: 0.3290124034822758]
	TIME [epoch: 2.69 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35540344845336364		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.35540344845336364 | validation: 0.3538216328189978]
	TIME [epoch: 2.69 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3015780684855241		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.3015780684855241 | validation: 0.3484816219913364]
	TIME [epoch: 2.69 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077693146370259		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.3077693146370259 | validation: 0.3095425547085266]
	TIME [epoch: 2.69 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3059425116771643		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.3059425116771643 | validation: 0.30340566367240274]
	TIME [epoch: 2.69 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2937653765610003		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.2937653765610003 | validation: 0.3274402823532867]
	TIME [epoch: 2.68 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2978055998512242		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.2978055998512242 | validation: 0.313123149602613]
	TIME [epoch: 2.68 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29282889174704735		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.29282889174704735 | validation: 0.3155357097184866]
	TIME [epoch: 2.69 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014642834663925		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.3014642834663925 | validation: 0.3169026336130096]
	TIME [epoch: 2.69 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29330297400607364		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.29330297400607364 | validation: 0.30606053780896186]
	TIME [epoch: 2.69 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2894691283997128		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.2894691283997128 | validation: 0.29007213347103017]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_647.pth
	Model improved!!!
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30295345577011673		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.30295345577011673 | validation: 0.3241533055683178]
	TIME [epoch: 2.69 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2950071363637041		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.2950071363637041 | validation: 0.3053651072354339]
	TIME [epoch: 2.69 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.289452575770655		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.289452575770655 | validation: 0.31391771386183465]
	TIME [epoch: 2.69 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29308663357611225		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.29308663357611225 | validation: 0.32444571313601855]
	TIME [epoch: 2.69 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28383586021006135		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.28383586021006135 | validation: 0.2924668058735089]
	TIME [epoch: 2.69 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2897755339288601		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.2897755339288601 | validation: 0.308963102357927]
	TIME [epoch: 2.69 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2860753629037206		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.2860753629037206 | validation: 0.2857901167407484]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3019648704306171		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.3019648704306171 | validation: 0.394753260082338]
	TIME [epoch: 2.69 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34304082768134814		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.34304082768134814 | validation: 0.2955799883459781]
	TIME [epoch: 2.69 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31007260300848116		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.31007260300848116 | validation: 0.3023538938101035]
	TIME [epoch: 2.68 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.287334746539036		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.287334746539036 | validation: 0.33038991010750673]
	TIME [epoch: 2.69 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2952761445843728		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.2952761445843728 | validation: 0.440247666678024]
	TIME [epoch: 2.69 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40428079892396496		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.40428079892396496 | validation: 0.30364015902262126]
	TIME [epoch: 2.69 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30731254275366043		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.30731254275366043 | validation: 0.28807751614061655]
	TIME [epoch: 2.69 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29603822161154886		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.29603822161154886 | validation: 0.36821072808746375]
	TIME [epoch: 2.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184564273809806		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.3184564273809806 | validation: 0.2961979587058278]
	TIME [epoch: 2.69 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28112689378055306		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.28112689378055306 | validation: 0.2756651244895109]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_664.pth
	Model improved!!!
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3013333713294819		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.3013333713294819 | validation: 0.3121639840239797]
	TIME [epoch: 2.68 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2739725896997373		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.2739725896997373 | validation: 0.32714255683692794]
	TIME [epoch: 2.69 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28846435973422446		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.28846435973422446 | validation: 0.29617858992725393]
	TIME [epoch: 2.69 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28448874762300885		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.28448874762300885 | validation: 0.3328805649564957]
	TIME [epoch: 2.68 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2782447878414214		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.2782447878414214 | validation: 0.6074035466522285]
	TIME [epoch: 2.69 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5934513916672435		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.5934513916672435 | validation: 0.43614136547379445]
	TIME [epoch: 2.69 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3970655522943003		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.3970655522943003 | validation: 0.32175549137904874]
	TIME [epoch: 2.69 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3291329817476594		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.3291329817476594 | validation: 0.3041271753028588]
	TIME [epoch: 2.69 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318070317049279		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.318070317049279 | validation: 0.3376283317119825]
	TIME [epoch: 2.69 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905202221673883		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.2905202221673883 | validation: 0.33703019500464854]
	TIME [epoch: 2.68 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27942514763504883		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.27942514763504883 | validation: 0.28940494850952153]
	TIME [epoch: 2.68 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27667786501241304		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.27667786501241304 | validation: 0.2861479962040236]
	TIME [epoch: 2.68 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791710218224604		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.2791710218224604 | validation: 0.3109014418808618]
	TIME [epoch: 2.68 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27314219627932923		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.27314219627932923 | validation: 0.2792492753307947]
	TIME [epoch: 2.68 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2717837567826878		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.2717837567826878 | validation: 0.29741013138029526]
	TIME [epoch: 2.68 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706908374684235		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.2706908374684235 | validation: 0.29894963036786026]
	TIME [epoch: 2.68 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26339145759063853		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.26339145759063853 | validation: 0.3115508129753827]
	TIME [epoch: 2.69 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26341218925611876		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.26341218925611876 | validation: 0.29835344554550997]
	TIME [epoch: 2.69 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2625887932633422		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.2625887932633422 | validation: 0.2799835309882829]
	TIME [epoch: 2.69 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2620437498252339		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.2620437498252339 | validation: 0.3011047821446997]
	TIME [epoch: 2.69 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26167058585965863		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.26167058585965863 | validation: 0.28885774877679443]
	TIME [epoch: 2.69 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2713114078448124		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.2713114078448124 | validation: 0.29441338867286143]
	TIME [epoch: 2.68 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2664734937618367		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.2664734937618367 | validation: 0.2708358228943985]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39357418150085		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.39357418150085 | validation: 0.27185300752155767]
	TIME [epoch: 2.69 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27197151687494964		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.27197151687494964 | validation: 0.35023803757945954]
	TIME [epoch: 2.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28932172290166		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.28932172290166 | validation: 0.2899027097105122]
	TIME [epoch: 2.69 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2617807396877126		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.2617807396877126 | validation: 0.2816281445165622]
	TIME [epoch: 2.69 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546039277088748		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.2546039277088748 | validation: 0.30022202329516473]
	TIME [epoch: 2.69 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502651468349801		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.2502651468349801 | validation: 0.2898654019597185]
	TIME [epoch: 2.69 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25342489670673807		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.25342489670673807 | validation: 0.2791305772869585]
	TIME [epoch: 2.69 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26623501523074317		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.26623501523074317 | validation: 0.3078017560503207]
	TIME [epoch: 2.69 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2610038946314605		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.2610038946314605 | validation: 0.29290669063265357]
	TIME [epoch: 2.69 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2521049291224892		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.2521049291224892 | validation: 0.27753457884376137]
	TIME [epoch: 2.69 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25101410654777995		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.25101410654777995 | validation: 0.30117540079576754]
	TIME [epoch: 2.69 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2571669587461594		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.2571669587461594 | validation: 0.2702263509572767]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26145122765385576		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.26145122765385576 | validation: 0.29404448031562447]
	TIME [epoch: 2.68 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24785277201843472		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.24785277201843472 | validation: 0.29218439060192897]
	TIME [epoch: 2.69 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24690894246592648		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.24690894246592648 | validation: 0.25940643763204924]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_702.pth
	Model improved!!!
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27778984439250487		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.27778984439250487 | validation: 0.3105470453566115]
	TIME [epoch: 2.68 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2577590943974814		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.2577590943974814 | validation: 0.27212204936593365]
	TIME [epoch: 2.68 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24679118851486237		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.24679118851486237 | validation: 0.2822246650877561]
	TIME [epoch: 2.68 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23492333449059571		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.23492333449059571 | validation: 0.2750474956078581]
	TIME [epoch: 2.68 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2350865895133782		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.2350865895133782 | validation: 0.24787677642387101]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_707.pth
	Model improved!!!
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28081285403179146		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.28081285403179146 | validation: 0.3045042430335989]
	TIME [epoch: 2.67 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25632492438980436		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.25632492438980436 | validation: 0.2634107643003462]
	TIME [epoch: 2.67 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23465419949853528		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.23465419949853528 | validation: 0.25664723931661565]
	TIME [epoch: 2.68 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2396323880798728		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.2396323880798728 | validation: 0.29886935160814304]
	TIME [epoch: 2.67 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.240901437132557		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.240901437132557 | validation: 0.262844081553466]
	TIME [epoch: 2.67 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23997810821375873		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.23997810821375873 | validation: 0.2750829318575609]
	TIME [epoch: 2.67 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2261159407905187		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.2261159407905187 | validation: 0.253983366011377]
	TIME [epoch: 2.67 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25037570308183105		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.25037570308183105 | validation: 0.29526256115249994]
	TIME [epoch: 2.68 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25176456883033077		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.25176456883033077 | validation: 0.24841705603406414]
	TIME [epoch: 2.68 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23518278827134562		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.23518278827134562 | validation: 0.26238628630446387]
	TIME [epoch: 2.67 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22821652351416874		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.22821652351416874 | validation: 0.2790080596903039]
	TIME [epoch: 2.68 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23198739190962875		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.23198739190962875 | validation: 0.24497287694709038]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_719.pth
	Model improved!!!
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24826251672634508		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.24826251672634508 | validation: 0.3682231483509332]
	TIME [epoch: 2.71 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31675363294827535		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.31675363294827535 | validation: 0.2599360039231851]
	TIME [epoch: 2.71 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282162361431348		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.2282162361431348 | validation: 0.24320145934794293]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_722.pth
	Model improved!!!
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651189725081666		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.2651189725081666 | validation: 0.26242074679864485]
	TIME [epoch: 2.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22288450456751435		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.22288450456751435 | validation: 0.2716319199565896]
	TIME [epoch: 2.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22607041227274785		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.22607041227274785 | validation: 0.2529273934247231]
	TIME [epoch: 2.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22025826537937954		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.22025826537937954 | validation: 0.24915846111036066]
	TIME [epoch: 2.71 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21459776256530066		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.21459776256530066 | validation: 0.24788593786158902]
	TIME [epoch: 2.68 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21563442988403292		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.21563442988403292 | validation: 0.2618291811014471]
	TIME [epoch: 2.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20629381358565746		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.20629381358565746 | validation: 0.2292635634565533]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_729.pth
	Model improved!!!
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21415800678800345		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.21415800678800345 | validation: 0.3205021226939602]
	TIME [epoch: 2.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24944466532135806		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.24944466532135806 | validation: 0.24032637684317193]
	TIME [epoch: 2.7 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21607448487347614		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.21607448487347614 | validation: 0.23992522848833833]
	TIME [epoch: 2.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20680164565656733		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.20680164565656733 | validation: 0.2865775368491292]
	TIME [epoch: 2.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2219440488453246		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.2219440488453246 | validation: 0.2236985732666865]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_734.pth
	Model improved!!!
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2866246068426081		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.2866246068426081 | validation: 0.244195062314562]
	TIME [epoch: 2.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2078909610191632		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.2078909610191632 | validation: 0.3013388160130304]
	TIME [epoch: 2.68 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23347565469733617		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.23347565469733617 | validation: 0.23850567237002954]
	TIME [epoch: 2.68 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2017568940438638		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.2017568940438638 | validation: 0.24358214865803848]
	TIME [epoch: 2.68 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1967255316063954		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.1967255316063954 | validation: 0.25060454091131945]
	TIME [epoch: 2.67 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19892317232867673		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.19892317232867673 | validation: 0.25806885947221886]
	TIME [epoch: 2.67 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20000598189105706		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.20000598189105706 | validation: 0.2378073061749685]
	TIME [epoch: 2.67 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19950047594749745		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.19950047594749745 | validation: 0.22909250485545526]
	TIME [epoch: 2.68 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18948404152359094		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.18948404152359094 | validation: 0.2179155272964374]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1995530427447678		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.1995530427447678 | validation: 0.24655640155396827]
	TIME [epoch: 2.68 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19634225612577907		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.19634225612577907 | validation: 0.24411805857216595]
	TIME [epoch: 2.68 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1870955778793291		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.1870955778793291 | validation: 0.21908778665115755]
	TIME [epoch: 2.68 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1939445293042233		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.1939445293042233 | validation: 0.24030846646231893]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1920145047374178		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.1920145047374178 | validation: 0.2155124742720722]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035680983254775		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.2035680983254775 | validation: 0.2698497271858949]
	TIME [epoch: 2.68 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20856936392330802		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.20856936392330802 | validation: 0.2170983156926862]
	TIME [epoch: 2.67 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21391399137565817		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.21391399137565817 | validation: 0.23260924741656563]
	TIME [epoch: 2.67 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17963587512454626		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.17963587512454626 | validation: 0.24800137996351654]
	TIME [epoch: 2.67 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18552426627636134		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.18552426627636134 | validation: 0.19421702742323044]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_753.pth
	Model improved!!!
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18854415083616916		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.18854415083616916 | validation: 0.2612529406802335]
	TIME [epoch: 2.67 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19628308088564594		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.19628308088564594 | validation: 0.2069759797290327]
	TIME [epoch: 2.68 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18588142803615257		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.18588142803615257 | validation: 0.21506383037346663]
	TIME [epoch: 2.67 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1844366153494881		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.1844366153494881 | validation: 0.18156109475784052]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17896247717449498		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.17896247717449498 | validation: 0.23123527286974613]
	TIME [epoch: 2.68 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17519288784705006		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.17519288784705006 | validation: 0.2033412040947538]
	TIME [epoch: 2.68 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17964376490675435		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.17964376490675435 | validation: 0.22360662727929614]
	TIME [epoch: 2.68 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17201357335829215		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.17201357335829215 | validation: 0.21021890501825588]
	TIME [epoch: 2.67 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687757267834455		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.1687757267834455 | validation: 0.1962764944312455]
	TIME [epoch: 2.68 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16891122584455734		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.16891122584455734 | validation: 0.19761610058899942]
	TIME [epoch: 2.68 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16441652167454637		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.16441652167454637 | validation: 0.21240897430746478]
	TIME [epoch: 2.67 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1690495552837499		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.1690495552837499 | validation: 0.17808589529198754]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17146265808978256		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.17146265808978256 | validation: 0.26240465479114833]
	TIME [epoch: 2.67 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17673122980117906		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.17673122980117906 | validation: 0.19307619326874442]
	TIME [epoch: 2.68 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19317651802856917		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.19317651802856917 | validation: 0.23130862722359768]
	TIME [epoch: 2.68 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16755668095961457		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.16755668095961457 | validation: 0.19906716848749356]
	TIME [epoch: 2.68 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573696891951975		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.1573696891951975 | validation: 0.17955480098287832]
	TIME [epoch: 2.68 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551923055781767		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.1551923055781767 | validation: 0.23007808657752388]
	TIME [epoch: 2.68 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17064600053336218		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.17064600053336218 | validation: 0.19663685050554292]
	TIME [epoch: 2.68 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25767319108764797		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.25767319108764797 | validation: 0.18638768148379697]
	TIME [epoch: 2.67 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16051283061758514		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.16051283061758514 | validation: 0.2511329547320249]
	TIME [epoch: 2.67 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17552290608877832		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.17552290608877832 | validation: 0.20310306751921195]
	TIME [epoch: 2.68 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15357983373893508		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.15357983373893508 | validation: 0.20537761046076497]
	TIME [epoch: 2.68 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15129424705450206		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.15129424705450206 | validation: 0.17364725443965967]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_777.pth
	Model improved!!!
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15682971140705115		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.15682971140705115 | validation: 0.19435980502563785]
	TIME [epoch: 2.68 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14645562113016886		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.14645562113016886 | validation: 0.1988524512529375]
	TIME [epoch: 2.68 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14622473219012844		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.14622473219012844 | validation: 0.17075095931147974]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14552598333877417		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.14552598333877417 | validation: 0.18414727257372707]
	TIME [epoch: 2.68 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14467103944473367		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.14467103944473367 | validation: 0.17281977461751472]
	TIME [epoch: 2.68 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14701998749363476		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.14701998749363476 | validation: 0.19835059576029465]
	TIME [epoch: 2.68 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14747049585581937		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.14747049585581937 | validation: 0.16752121041268464]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1763658902606189		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.1763658902606189 | validation: 0.24537935244168657]
	TIME [epoch: 2.68 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18143605477788824		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.18143605477788824 | validation: 0.17655276649679436]
	TIME [epoch: 2.68 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1335205268464538		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.1335205268464538 | validation: 0.17762514847711294]
	TIME [epoch: 2.67 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14977075997917927		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.14977075997917927 | validation: 0.19349646430222944]
	TIME [epoch: 2.67 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13971228353261006		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.13971228353261006 | validation: 0.17460057668939827]
	TIME [epoch: 2.68 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13499611376871276		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.13499611376871276 | validation: 0.15365259978776094]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_790.pth
	Model improved!!!
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13692196191881909		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.13692196191881909 | validation: 0.18168830704522818]
	TIME [epoch: 2.68 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13818545798599333		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.13818545798599333 | validation: 0.14933562996224817]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13875639482434604		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.13875639482434604 | validation: 0.18848324739150202]
	TIME [epoch: 2.67 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13637224029872033		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.13637224029872033 | validation: 0.17279402795899174]
	TIME [epoch: 2.68 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12550010144133833		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.12550010144133833 | validation: 0.15521544060175407]
	TIME [epoch: 2.68 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13109932238628996		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.13109932238628996 | validation: 0.18713038775101853]
	TIME [epoch: 2.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13066452286324842		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.13066452286324842 | validation: 0.17901857942877134]
	TIME [epoch: 2.68 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12730482294559858		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.12730482294559858 | validation: 0.14985503483440415]
	TIME [epoch: 2.67 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130239633502327		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.130239633502327 | validation: 0.18305268682685055]
	TIME [epoch: 2.68 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351429604726506		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.1351429604726506 | validation: 0.1463326882375963]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439054851050066		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.1439054851050066 | validation: 0.2766783576168571]
	TIME [epoch: 2.68 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20669149064514847		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.20669149064514847 | validation: 0.1652261674949363]
	TIME [epoch: 2.68 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12370794374115957		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.12370794374115957 | validation: 0.15605932400633638]
	TIME [epoch: 2.68 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14693274724235728		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.14693274724235728 | validation: 0.16733693632629434]
	TIME [epoch: 2.67 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997594003483808		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.11997594003483808 | validation: 0.14041693353523033]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13226384619367348		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.13226384619367348 | validation: 0.1441239345965027]
	TIME [epoch: 2.67 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12117673521534177		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.12117673521534177 | validation: 0.1800791329200219]
	TIME [epoch: 2.67 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13407960878642217		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.13407960878642217 | validation: 0.1435777278910137]
	TIME [epoch: 2.66 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11609575413366825		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.11609575413366825 | validation: 0.199308169283254]
	TIME [epoch: 2.67 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14694255236228912		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.14694255236228912 | validation: 0.14324899852844822]
	TIME [epoch: 2.67 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12075912287995262		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.12075912287995262 | validation: 0.14491987584052077]
	TIME [epoch: 2.67 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11980666198099092		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.11980666198099092 | validation: 0.15590605742907335]
	TIME [epoch: 2.67 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1088975497053426		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.1088975497053426 | validation: 0.14104283043019183]
	TIME [epoch: 2.67 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11446300826079807		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.11446300826079807 | validation: 0.3049773920549168]
	TIME [epoch: 2.67 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27566232179546646		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.27566232179546646 | validation: 0.18579774772953783]
	TIME [epoch: 2.67 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.136753004809186		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.136753004809186 | validation: 0.17606779427519292]
	TIME [epoch: 2.67 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19571073374914646		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.19571073374914646 | validation: 0.14121984513336033]
	TIME [epoch: 2.67 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279219291009061		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.1279219291009061 | validation: 0.17504861104191405]
	TIME [epoch: 2.67 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12629562587382612		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.12629562587382612 | validation: 0.15463824650685304]
	TIME [epoch: 2.68 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340932181445414		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.11340932181445414 | validation: 0.14869401051697836]
	TIME [epoch: 2.67 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11243461450279565		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.11243461450279565 | validation: 0.14971318366272432]
	TIME [epoch: 2.68 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1077169389978783		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.1077169389978783 | validation: 0.14742274870890393]
	TIME [epoch: 2.68 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10886128981522152		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.10886128981522152 | validation: 0.1428371894414807]
	TIME [epoch: 2.68 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1062598799545651		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.1062598799545651 | validation: 0.15730200090653718]
	TIME [epoch: 2.67 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10405375163124142		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.10405375163124142 | validation: 0.20625418891901975]
	TIME [epoch: 2.67 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15316031114619183		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.15316031114619183 | validation: 0.13940325369327522]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_826.pth
	Model improved!!!
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10801809280462665		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.10801809280462665 | validation: 0.13596518595788143]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_827.pth
	Model improved!!!
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11901567966821461		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.11901567966821461 | validation: 0.152534941582952]
	TIME [epoch: 2.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10419123701701834		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.10419123701701834 | validation: 0.16146297936499848]
	TIME [epoch: 2.69 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10481955656109289		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.10481955656109289 | validation: 0.13801973947552823]
	TIME [epoch: 2.69 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10478358206561528		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.10478358206561528 | validation: 0.134756306146918]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10060102646598942		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.10060102646598942 | validation: 0.26012240637531714]
	TIME [epoch: 2.69 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21131062605961443		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.21131062605961443 | validation: 0.15195265086831922]
	TIME [epoch: 2.69 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11731349686320026		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.11731349686320026 | validation: 0.15721778444819942]
	TIME [epoch: 2.69 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18520882497949673		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.18520882497949673 | validation: 0.1310513219530942]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_835.pth
	Model improved!!!
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11403354029176038		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.11403354029176038 | validation: 0.19210162721548296]
	TIME [epoch: 2.67 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14181662765408726		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.14181662765408726 | validation: 0.14082550181197867]
	TIME [epoch: 2.67 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0993871466780471		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.0993871466780471 | validation: 0.12830662390634195]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_838.pth
	Model improved!!!
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12558649769971247		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.12558649769971247 | validation: 0.14700034313804805]
	TIME [epoch: 2.69 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10352322867553664		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.10352322867553664 | validation: 0.15136506275972944]
	TIME [epoch: 2.69 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10516824630539226		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.10516824630539226 | validation: 0.14286494409568423]
	TIME [epoch: 2.68 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09671991033020677		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.09671991033020677 | validation: 0.13580854949182034]
	TIME [epoch: 2.69 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10404088854461703		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.10404088854461703 | validation: 0.14332776197542732]
	TIME [epoch: 2.69 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09867194098439314		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.09867194098439314 | validation: 0.12740892904547563]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_844.pth
	Model improved!!!
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09823726680322623		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.09823726680322623 | validation: 0.12254252351193431]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09846662489812869		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.09846662489812869 | validation: 0.13114379546795682]
	TIME [epoch: 2.67 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09324300771568568		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.09324300771568568 | validation: 0.12460805229711586]
	TIME [epoch: 2.67 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959856999467218		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.0959856999467218 | validation: 0.1305054357583952]
	TIME [epoch: 2.67 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09105074821874588		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.09105074821874588 | validation: 0.12348064874856056]
	TIME [epoch: 2.67 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09697497142036994		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.09697497142036994 | validation: 0.15496260631360032]
	TIME [epoch: 2.67 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626224802627483		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.10626224802627483 | validation: 0.11200108241649782]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09290104126450897		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.09290104126450897 | validation: 0.12618317245234456]
	TIME [epoch: 2.68 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0971088868220204		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.0971088868220204 | validation: 0.1406512600431491]
	TIME [epoch: 2.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09418635205297406		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.09418635205297406 | validation: 0.12349523671067941]
	TIME [epoch: 2.69 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936571316171041		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.08936571316171041 | validation: 0.14023539398058343]
	TIME [epoch: 2.69 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1032955412918124		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.1032955412918124 | validation: 0.11156519829525527]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09708824593347407		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.09708824593347407 | validation: 0.13553811738923366]
	TIME [epoch: 2.68 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08963321880303145		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.08963321880303145 | validation: 0.11398772595561063]
	TIME [epoch: 2.69 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08754779913189058		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.08754779913189058 | validation: 0.10946564744505641]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468642048279587		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.08468642048279587 | validation: 0.11928261841267736]
	TIME [epoch: 2.69 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08514976358713977		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.08514976358713977 | validation: 0.1329206914471183]
	TIME [epoch: 2.69 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219240992392193		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.09219240992392193 | validation: 0.12052013472176265]
	TIME [epoch: 2.68 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367570573873261		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.1367570573873261 | validation: 0.15584798985293272]
	TIME [epoch: 2.69 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10824569828238226		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.10824569828238226 | validation: 0.11707730089433187]
	TIME [epoch: 2.69 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08168949558602032		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.08168949558602032 | validation: 0.1100309989874676]
	TIME [epoch: 2.69 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08162764993999129		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.08162764993999129 | validation: 0.12619451326148817]
	TIME [epoch: 2.68 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08653497688941798		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.08653497688941798 | validation: 0.10878861350676848]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08152602666425926		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.08152602666425926 | validation: 0.13250579998245168]
	TIME [epoch: 2.68 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08235190390781139		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.08235190390781139 | validation: 0.1060907866613095]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_869.pth
	Model improved!!!
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784588646513629		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.0784588646513629 | validation: 0.11015930523527949]
	TIME [epoch: 2.69 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940594766543614		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.07940594766543614 | validation: 0.12180389965763845]
	TIME [epoch: 2.69 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08135968544085104		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.08135968544085104 | validation: 0.1077944291212605]
	TIME [epoch: 2.68 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08187752458426893		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.08187752458426893 | validation: 0.1207850478690335]
	TIME [epoch: 2.69 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08610719328696022		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.08610719328696022 | validation: 0.12031236985999322]
	TIME [epoch: 2.69 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08654432481023022		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.08654432481023022 | validation: 0.1540432621816731]
	TIME [epoch: 2.69 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10524162214824408		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.10524162214824408 | validation: 0.10824774171718499]
	TIME [epoch: 2.69 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08874952095047427		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.08874952095047427 | validation: 0.1184501648211876]
	TIME [epoch: 2.69 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08262170994317501		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.08262170994317501 | validation: 0.11091916792054307]
	TIME [epoch: 2.69 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07712912758843922		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.07712912758843922 | validation: 0.11725096908188935]
	TIME [epoch: 2.68 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08507219301460428		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.08507219301460428 | validation: 0.1813333890181671]
	TIME [epoch: 2.68 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1415763490615491		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.1415763490615491 | validation: 0.10925619576895229]
	TIME [epoch: 2.68 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893836942224704		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.0893836942224704 | validation: 0.12083643153023436]
	TIME [epoch: 2.68 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515340417042247		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.07515340417042247 | validation: 0.1169244990188842]
	TIME [epoch: 2.68 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0763067327811106		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.0763067327811106 | validation: 0.11426320679066491]
	TIME [epoch: 2.69 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07102647426837363		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.07102647426837363 | validation: 0.11490492170503615]
	TIME [epoch: 2.68 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07562167978128212		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.07562167978128212 | validation: 0.11239932864697336]
	TIME [epoch: 2.68 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411688107506867		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.07411688107506867 | validation: 0.10330586366182866]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_887.pth
	Model improved!!!
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06991194973543301		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.06991194973543301 | validation: 0.12444372139264637]
	TIME [epoch: 2.68 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13410491262463523		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.13410491262463523 | validation: 0.15275737450239402]
	TIME [epoch: 2.69 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11312402813193691		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.11312402813193691 | validation: 0.12731531000563176]
	TIME [epoch: 2.69 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08676470406794387		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.08676470406794387 | validation: 0.11844960315859875]
	TIME [epoch: 2.68 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08676394984753476		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.08676394984753476 | validation: 0.11361375624855807]
	TIME [epoch: 2.69 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906259957256698		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.06906259957256698 | validation: 0.12254748540047122]
	TIME [epoch: 2.68 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07450679650785014		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.07450679650785014 | validation: 0.10442303193309882]
	TIME [epoch: 2.69 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06730363155410767		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.06730363155410767 | validation: 0.1059898969943183]
	TIME [epoch: 2.69 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09054414460537767		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.09054414460537767 | validation: 0.11257238954695224]
	TIME [epoch: 2.69 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905449473884162		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.07905449473884162 | validation: 0.11874747368672922]
	TIME [epoch: 2.69 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07553370796427197		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.07553370796427197 | validation: 0.11980438737124005]
	TIME [epoch: 2.69 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07911301408821417		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.07911301408821417 | validation: 0.10633580178589719]
	TIME [epoch: 2.69 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846465224319662		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.06846465224319662 | validation: 0.1110555376513514]
	TIME [epoch: 2.69 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0690265347032987		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.0690265347032987 | validation: 0.09731092832988332]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823508202266437		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.06823508202266437 | validation: 0.1098950872967564]
	TIME [epoch: 2.68 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687978208839159		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.0687978208839159 | validation: 0.10018625739722974]
	TIME [epoch: 2.69 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07476682922939604		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.07476682922939604 | validation: 0.10011697157101807]
	TIME [epoch: 2.68 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07658943724659599		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.07658943724659599 | validation: 0.10709411697676555]
	TIME [epoch: 2.68 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06648066248185029		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.06648066248185029 | validation: 0.0934932036025828]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442589511741552		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.06442589511741552 | validation: 0.09485683628727455]
	TIME [epoch: 2.7 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679939872877362		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.06679939872877362 | validation: 0.0937801621810842]
	TIME [epoch: 2.69 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06541486304810981		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.06541486304810981 | validation: 0.08436461739694667]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_909.pth
	Model improved!!!
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455726535903274		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.06455726535903274 | validation: 0.10023052080920108]
	TIME [epoch: 2.68 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072255817968408		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.07072255817968408 | validation: 0.1678112985357271]
	TIME [epoch: 2.68 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12094634950364984		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.12094634950364984 | validation: 0.09870413098250103]
	TIME [epoch: 2.67 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08347245598601113		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.08347245598601113 | validation: 0.09174620815366111]
	TIME [epoch: 2.69 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06987647690917195		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.06987647690917195 | validation: 0.11871280242466167]
	TIME [epoch: 2.68 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08048480779909248		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.08048480779909248 | validation: 0.09231756151254292]
	TIME [epoch: 2.68 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137165247792566		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.06137165247792566 | validation: 0.09629946701242542]
	TIME [epoch: 2.69 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06586576528136405		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.06586576528136405 | validation: 0.1036727521810445]
	TIME [epoch: 2.69 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06215166731611126		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.06215166731611126 | validation: 0.1026284000223523]
	TIME [epoch: 2.69 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761920838636756		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.06761920838636756 | validation: 0.08544758797688716]
	TIME [epoch: 2.69 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612538700124619		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.06612538700124619 | validation: 0.09785025809517005]
	TIME [epoch: 2.69 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071567070859741		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.06071567070859741 | validation: 0.0880759724761302]
	TIME [epoch: 2.68 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06391325888509489		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.06391325888509489 | validation: 0.09539499035441541]
	TIME [epoch: 2.69 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06021839807047627		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.06021839807047627 | validation: 0.09127975291044708]
	TIME [epoch: 2.69 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06303529472138789		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.06303529472138789 | validation: 0.08661083470757684]
	TIME [epoch: 2.69 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062040874794088825		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.062040874794088825 | validation: 0.09445119665628951]
	TIME [epoch: 2.69 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06119970890776975		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.06119970890776975 | validation: 0.07891989939864247]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_926.pth
	Model improved!!!
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06055813221800374		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.06055813221800374 | validation: 0.09719958181529031]
	TIME [epoch: 2.69 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06496116352050244		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.06496116352050244 | validation: 0.09413302362722203]
	TIME [epoch: 2.69 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08947831414174448		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.08947831414174448 | validation: 0.11424057076504264]
	TIME [epoch: 2.69 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129283794442449		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.08129283794442449 | validation: 0.07989564163782709]
	TIME [epoch: 2.7 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06120443075589707		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.06120443075589707 | validation: 0.08884186522246644]
	TIME [epoch: 2.69 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06097184239731725		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.06097184239731725 | validation: 0.10553382729875548]
	TIME [epoch: 2.69 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07163894495359908		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.07163894495359908 | validation: 0.09049987374087477]
	TIME [epoch: 2.69 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058712827816846946		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.058712827816846946 | validation: 0.07923442736923011]
	TIME [epoch: 2.69 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057724384985957736		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.057724384985957736 | validation: 0.08260669974152701]
	TIME [epoch: 2.69 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056881596551012804		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.056881596551012804 | validation: 0.09318947803559408]
	TIME [epoch: 2.69 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05942698487003841		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.05942698487003841 | validation: 0.07122342927990942]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_937.pth
	Model improved!!!
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058929379759694846		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.058929379759694846 | validation: 0.09317100635149025]
	TIME [epoch: 2.69 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06255782989500462		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.06255782989500462 | validation: 0.08993485626899748]
	TIME [epoch: 2.69 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07473975136357931		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.07473975136357931 | validation: 0.13573233595333198]
	TIME [epoch: 2.69 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09057624848088576		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.09057624848088576 | validation: 0.08811698878723845]
	TIME [epoch: 2.69 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05706307607132945		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.05706307607132945 | validation: 0.08626020059640072]
	TIME [epoch: 2.68 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065618116139452		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.07065618116139452 | validation: 0.20422025012617456]
	TIME [epoch: 2.69 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15569142367099112		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.15569142367099112 | validation: 0.15827158325411825]
	TIME [epoch: 2.68 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11961961037533396		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.11961961037533396 | validation: 0.0866476150275381]
	TIME [epoch: 2.69 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058154166515967186		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.058154166515967186 | validation: 0.09336472198549862]
	TIME [epoch: 2.69 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07160585613836754		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.07160585613836754 | validation: 0.07701073194021825]
	TIME [epoch: 2.68 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058169502840598755		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.058169502840598755 | validation: 0.09335429410452592]
	TIME [epoch: 2.68 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061541158957001046		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.061541158957001046 | validation: 0.08201432143452422]
	TIME [epoch: 2.69 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059224703578587876		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.059224703578587876 | validation: 0.08054034256247257]
	TIME [epoch: 2.69 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06004518250603694		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.06004518250603694 | validation: 0.08410211078201184]
	TIME [epoch: 2.69 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0554176443492683		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0554176443492683 | validation: 0.08276828432855862]
	TIME [epoch: 2.69 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05187076147259532		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.05187076147259532 | validation: 0.07858270128010196]
	TIME [epoch: 2.69 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05662920918397269		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.05662920918397269 | validation: 0.08091981061554691]
	TIME [epoch: 2.68 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053045894966147894		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.053045894966147894 | validation: 0.087059315604511]
	TIME [epoch: 2.69 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052593363717110105		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.052593363717110105 | validation: 0.07843163127251312]
	TIME [epoch: 2.69 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05594715846084217		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.05594715846084217 | validation: 0.08547536331903405]
	TIME [epoch: 2.69 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242625125492358		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.05242625125492358 | validation: 0.08267206275926647]
	TIME [epoch: 2.68 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282759991187673		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.05282759991187673 | validation: 0.08525296390534891]
	TIME [epoch: 2.69 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05697065198885809		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.05697065198885809 | validation: 0.07065018012504025]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764076041648883		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.05764076041648883 | validation: 0.08557114774440525]
	TIME [epoch: 2.69 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05753905951074093		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.05753905951074093 | validation: 0.08179598403874787]
	TIME [epoch: 2.68 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05209155618583404		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.05209155618583404 | validation: 0.083461625643728]
	TIME [epoch: 2.69 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05366426723831772		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.05366426723831772 | validation: 0.08604201088475845]
	TIME [epoch: 2.68 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05278809154256869		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.05278809154256869 | validation: 0.08297562642682135]
	TIME [epoch: 2.69 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05167345997529935		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.05167345997529935 | validation: 0.0765822001599771]
	TIME [epoch: 2.69 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05271035023252618		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.05271035023252618 | validation: 0.084674293412464]
	TIME [epoch: 2.69 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972751455350631		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.04972751455350631 | validation: 0.06779419440233679]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_968.pth
	Model improved!!!
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05116648668733082		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.05116648668733082 | validation: 0.0748741777891437]
	TIME [epoch: 2.68 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05096493954924146		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.05096493954924146 | validation: 0.1266473473237729]
	TIME [epoch: 2.68 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2419985479031505		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.2419985479031505 | validation: 0.07945977087216004]
	TIME [epoch: 2.69 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0823446241008237		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.0823446241008237 | validation: 0.126811511988622]
	TIME [epoch: 2.69 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09301784592294031		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.09301784592294031 | validation: 0.10752537405820471]
	TIME [epoch: 2.69 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07293670994408029		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.07293670994408029 | validation: 0.06655389861667621]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05959593211337343		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.05959593211337343 | validation: 0.08365729805453906]
	TIME [epoch: 2.69 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05710912840501761		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.05710912840501761 | validation: 0.08664273671762435]
	TIME [epoch: 2.68 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06006083610178826		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.06006083610178826 | validation: 0.07867263002417016]
	TIME [epoch: 2.68 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05095271271631705		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05095271271631705 | validation: 0.07162566381790268]
	TIME [epoch: 2.68 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05481571920161139		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.05481571920161139 | validation: 0.07880927114575698]
	TIME [epoch: 2.68 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050973827457077514		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.050973827457077514 | validation: 0.07540277932945823]
	TIME [epoch: 2.68 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062365456784751104		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.062365456784751104 | validation: 0.06900196170396042]
	TIME [epoch: 2.68 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050699554496724204		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.050699554496724204 | validation: 0.06809105494866059]
	TIME [epoch: 2.69 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0619166451729847		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.0619166451729847 | validation: 0.07492898686925879]
	TIME [epoch: 2.7 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05251608664959097		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.05251608664959097 | validation: 0.07063179685503221]
	TIME [epoch: 2.7 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049227614660161116		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.049227614660161116 | validation: 0.06138737634875755]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_985.pth
	Model improved!!!
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05072191154653997		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.05072191154653997 | validation: 0.0697025778426941]
	TIME [epoch: 2.67 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05143602535216672		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.05143602535216672 | validation: 0.07583693005431784]
	TIME [epoch: 2.67 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04900284768796404		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.04900284768796404 | validation: 0.08056261396057518]
	TIME [epoch: 2.68 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051406203872138116		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.051406203872138116 | validation: 0.06969306403252748]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782638878281604		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.05782638878281604 | validation: 0.08087568573738091]
	TIME [epoch: 2.67 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050934094657426175		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.050934094657426175 | validation: 0.07299128853852846]
	TIME [epoch: 2.67 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05091343563549042		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.05091343563549042 | validation: 0.07130935286085587]
	TIME [epoch: 2.67 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049374961163704184		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.049374961163704184 | validation: 0.06979690203695771]
	TIME [epoch: 2.68 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050299732898764965		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.050299732898764965 | validation: 0.0750400293201668]
	TIME [epoch: 2.67 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04633403745861769		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.04633403745861769 | validation: 0.07205037768207864]
	TIME [epoch: 2.67 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04874999672938632		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.04874999672938632 | validation: 0.09663891074188008]
	TIME [epoch: 2.67 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12233850553579716		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.12233850553579716 | validation: 0.0900966402551841]
	TIME [epoch: 2.67 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914968125948214		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.05914968125948214 | validation: 0.09980666606085142]
	TIME [epoch: 2.67 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059065344190434355		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.059065344190434355 | validation: 0.07055374198626244]
	TIME [epoch: 2.67 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017541995098729		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05017541995098729 | validation: 0.07539627730097266]
	TIME [epoch: 2.67 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04679423117589956		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.04679423117589956 | validation: 0.07186298671114245]
	TIME [epoch: 184 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046182242951444734		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.046182242951444734 | validation: 0.07064620870181985]
	TIME [epoch: 5.76 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049354401179593596		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.049354401179593596 | validation: 0.06911121831376603]
	TIME [epoch: 5.77 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760964399703866		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.04760964399703866 | validation: 0.07187268221774583]
	TIME [epoch: 5.76 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050826107903138615		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.050826107903138615 | validation: 0.07801082391465203]
	TIME [epoch: 5.76 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048873342085246865		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.048873342085246865 | validation: 0.07007643237520168]
	TIME [epoch: 5.76 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047884301222166886		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.047884301222166886 | validation: 0.0740701497217248]
	TIME [epoch: 5.76 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049761038147696314		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.049761038147696314 | validation: 0.07100405757944671]
	TIME [epoch: 5.76 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626246985159936		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.04626246985159936 | validation: 0.08603328104511891]
	TIME [epoch: 5.77 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052029421152538266		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.052029421152538266 | validation: 0.0660447710970317]
	TIME [epoch: 5.75 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0676797280949888		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.0676797280949888 | validation: 0.07333955709568166]
	TIME [epoch: 5.76 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04718253597258601		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.04718253597258601 | validation: 0.07953939769340818]
	TIME [epoch: 5.75 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550730810852471		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.0550730810852471 | validation: 0.06917713925401893]
	TIME [epoch: 5.77 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639684286139672		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.0639684286139672 | validation: 0.11534716855797007]
	TIME [epoch: 5.77 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07924080649556332		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.07924080649556332 | validation: 0.08211043253327524]
	TIME [epoch: 5.76 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058534186413501776		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.058534186413501776 | validation: 0.06686508497322202]
	TIME [epoch: 5.76 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05340923353005174		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.05340923353005174 | validation: 0.07942446207364992]
	TIME [epoch: 5.76 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049368937492942355		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.049368937492942355 | validation: 0.0763467505620797]
	TIME [epoch: 5.77 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05268431508983895		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.05268431508983895 | validation: 0.10367076607112594]
	TIME [epoch: 5.77 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663106182658364		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.07663106182658364 | validation: 0.07486968790297675]
	TIME [epoch: 5.76 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046280042802403464		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.046280042802403464 | validation: 0.0745953937800435]
	TIME [epoch: 5.76 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06564589381430487		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.06564589381430487 | validation: 0.08325591775304236]
	TIME [epoch: 5.75 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048320798965386595		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.048320798965386595 | validation: 0.08354492243442933]
	TIME [epoch: 5.76 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05483481140888247		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.05483481140888247 | validation: 0.06737388258740667]
	TIME [epoch: 5.77 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04706529597931788		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.04706529597931788 | validation: 0.07132423283443307]
	TIME [epoch: 5.78 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282884430905254		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.05282884430905254 | validation: 0.06489624476414345]
	TIME [epoch: 5.76 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04528664766024715		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.04528664766024715 | validation: 0.06949121818959517]
	TIME [epoch: 5.76 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045030078254920954		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.045030078254920954 | validation: 0.06423770603842377]
	TIME [epoch: 5.76 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04440581315165939		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.04440581315165939 | validation: 0.06714637228257735]
	TIME [epoch: 5.76 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04566444573570026		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.04566444573570026 | validation: 0.06482468125673278]
	TIME [epoch: 5.77 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045338825825434255		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.045338825825434255 | validation: 0.08042067561599253]
	TIME [epoch: 5.76 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05879451098203743		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.05879451098203743 | validation: 0.051287062499150396]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1032.pth
	Model improved!!!
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04899912309236273		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.04899912309236273 | validation: 0.06810158028507975]
	TIME [epoch: 5.76 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04505559468648738		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.04505559468648738 | validation: 0.06622667652721571]
	TIME [epoch: 5.77 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04528633195287895		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.04528633195287895 | validation: 0.06732273485368827]
	TIME [epoch: 5.77 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04629940959171273		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.04629940959171273 | validation: 0.07960076306088631]
	TIME [epoch: 5.76 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592201472065851		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.04592201472065851 | validation: 0.06965693684994709]
	TIME [epoch: 5.76 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513687496781797		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.04513687496781797 | validation: 0.0540329239180804]
	TIME [epoch: 5.76 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044785763839889546		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.044785763839889546 | validation: 0.06555238063913141]
	TIME [epoch: 5.77 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813104536922547		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.06813104536922547 | validation: 0.07678898352989577]
	TIME [epoch: 5.77 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05098586310049244		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.05098586310049244 | validation: 0.06598004184395193]
	TIME [epoch: 5.76 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045649298739869726		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.045649298739869726 | validation: 0.06295790125497906]
	TIME [epoch: 5.78 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05542856851226024		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.05542856851226024 | validation: 0.06776625938824259]
	TIME [epoch: 5.77 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551690265434285		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.04551690265434285 | validation: 0.06432228929286131]
	TIME [epoch: 5.77 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044726033891231455		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.044726033891231455 | validation: 0.06316384157482478]
	TIME [epoch: 5.77 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848218221978666		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.05848218221978666 | validation: 0.07863874029184938]
	TIME [epoch: 5.76 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051685606375437766		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.051685606375437766 | validation: 0.06692731021236215]
	TIME [epoch: 5.76 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04655135582345642		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.04655135582345642 | validation: 0.05597712262964119]
	TIME [epoch: 5.77 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05182573105612952		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.05182573105612952 | validation: 0.05880932246000914]
	TIME [epoch: 5.77 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04306586860052474		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.04306586860052474 | validation: 0.08843809042511869]
	TIME [epoch: 5.76 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0499641333556525		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.0499641333556525 | validation: 0.05830277485318533]
	TIME [epoch: 5.76 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04643897602970786		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.04643897602970786 | validation: 0.055239148815358324]
	TIME [epoch: 5.76 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042923888645809705		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.042923888645809705 | validation: 0.061121206732672406]
	TIME [epoch: 5.77 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041151335982974116		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.041151335982974116 | validation: 0.06281239294452601]
	TIME [epoch: 5.76 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04256496906047314		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.04256496906047314 | validation: 0.08211202846106248]
	TIME [epoch: 5.77 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04964165794158138		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.04964165794158138 | validation: 0.06360785365413352]
	TIME [epoch: 5.76 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08669283380925051		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.08669283380925051 | validation: 0.056045643126877924]
	TIME [epoch: 5.76 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041176209791865946		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.041176209791865946 | validation: 0.08252423908442175]
	TIME [epoch: 5.76 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043456197357139		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.05043456197357139 | validation: 0.05308106693283483]
	TIME [epoch: 5.76 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0440809014184441		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.0440809014184441 | validation: 0.06160888644798386]
	TIME [epoch: 5.76 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044360437725168456		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.044360437725168456 | validation: 0.06463315722041571]
	TIME [epoch: 5.77 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04484267614367021		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.04484267614367021 | validation: 0.06433114183748241]
	TIME [epoch: 5.76 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04496164796618569		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.04496164796618569 | validation: 0.0603510531250698]
	TIME [epoch: 5.77 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04200954386200851		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.04200954386200851 | validation: 0.06988306315360303]
	TIME [epoch: 5.77 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04540941318323522		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.04540941318323522 | validation: 0.06563717665273283]
	TIME [epoch: 5.77 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0435510600399148		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.0435510600399148 | validation: 0.0696234491703541]
	TIME [epoch: 5.76 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04467778078352021		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.04467778078352021 | validation: 0.11268009293064947]
	TIME [epoch: 5.76 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06883438938960719		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.06883438938960719 | validation: 0.06751816081868504]
	TIME [epoch: 5.75 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041130374519992186		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.041130374519992186 | validation: 0.06255849483230402]
	TIME [epoch: 5.75 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053350193237863296		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.053350193237863296 | validation: 0.07657163612138197]
	TIME [epoch: 5.76 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044842551957722174		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.044842551957722174 | validation: 0.0766892421418685]
	TIME [epoch: 5.77 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04565490824736674		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.04565490824736674 | validation: 0.06532517499104996]
	TIME [epoch: 5.76 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04345879376992662		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.04345879376992662 | validation: 0.06394120229091001]
	TIME [epoch: 5.77 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03871323788389557		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.03871323788389557 | validation: 0.07669281914203091]
	TIME [epoch: 5.76 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03894777598533732		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.03894777598533732 | validation: 0.05417258470176328]
	TIME [epoch: 5.77 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039274350910489385		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.039274350910489385 | validation: 0.05889853808047742]
	TIME [epoch: 5.76 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041350164425940826		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.041350164425940826 | validation: 0.07248249095716884]
	TIME [epoch: 5.75 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040889891359078175		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.040889891359078175 | validation: 0.061460790525074374]
	TIME [epoch: 5.76 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0464595094830708		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.0464595094830708 | validation: 0.06793363454609555]
	TIME [epoch: 5.76 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04198386102009045		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.04198386102009045 | validation: 0.06406133977081382]
	TIME [epoch: 5.77 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03915159529066437		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.03915159529066437 | validation: 0.056202921191215484]
	TIME [epoch: 5.77 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04531752319643505		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.04531752319643505 | validation: 0.06335714871759944]
	TIME [epoch: 5.76 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04249974137093421		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.04249974137093421 | validation: 0.06454918174859708]
	TIME [epoch: 5.76 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126544711590565		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.04126544711590565 | validation: 0.06339429145899247]
	TIME [epoch: 5.77 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040205706278007		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.040205706278007 | validation: 0.07557751200708401]
	TIME [epoch: 5.77 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046599770917534686		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.046599770917534686 | validation: 0.056588040744496554]
	TIME [epoch: 5.76 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04832758387088415		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.04832758387088415 | validation: 0.09044685666096368]
	TIME [epoch: 5.76 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06324560954398352		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.06324560954398352 | validation: 0.06011484773643151]
	TIME [epoch: 5.75 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04096325771560006		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.04096325771560006 | validation: 0.06045040727529073]
	TIME [epoch: 5.77 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626254273207324		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.0626254273207324 | validation: 0.05858544720922938]
	TIME [epoch: 5.75 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04043515596199732		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.04043515596199732 | validation: 0.06894627672057169]
	TIME [epoch: 5.77 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04735810607239826		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.04735810607239826 | validation: 0.055075649745772164]
	TIME [epoch: 5.76 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039101469972602644		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.039101469972602644 | validation: 0.05600701474134663]
	TIME [epoch: 5.76 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039381657023068097		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.039381657023068097 | validation: 0.06616445335931034]
	TIME [epoch: 5.76 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040088303804317035		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.040088303804317035 | validation: 0.06771156810846027]
	TIME [epoch: 5.77 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039005048829026		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.039005048829026 | validation: 0.05791574047294799]
	TIME [epoch: 5.77 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03874058527917771		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.03874058527917771 | validation: 0.048549921230598106]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1097.pth
	Model improved!!!
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04138891990741323		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.04138891990741323 | validation: 0.06012276951455364]
	TIME [epoch: 5.75 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040588940506187006		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.040588940506187006 | validation: 0.06727520867374741]
	TIME [epoch: 5.76 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040149721590848966		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.040149721590848966 | validation: 0.05337682174481034]
	TIME [epoch: 5.75 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039552192842185564		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.039552192842185564 | validation: 0.05043030283486939]
	TIME [epoch: 5.77 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0418463082419255		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.0418463082419255 | validation: 0.06000981527076678]
	TIME [epoch: 5.77 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041599408019198876		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.041599408019198876 | validation: 0.06516736436505707]
	TIME [epoch: 5.75 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054026647903119		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.05054026647903119 | validation: 0.06473533135175875]
	TIME [epoch: 5.75 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04094679179810232		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.04094679179810232 | validation: 0.05939228580333938]
	TIME [epoch: 5.75 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04003168627100871		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.04003168627100871 | validation: 0.05807729021072692]
	TIME [epoch: 5.75 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0428448597644687		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.0428448597644687 | validation: 0.06669971138989363]
	TIME [epoch: 5.77 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04201888114825876		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.04201888114825876 | validation: 0.05955371596453342]
	TIME [epoch: 5.75 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04131585613751664		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.04131585613751664 | validation: 0.05205338346620141]
	TIME [epoch: 5.75 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04094485321241139		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.04094485321241139 | validation: 0.061601976524225804]
	TIME [epoch: 5.75 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03921978779119675		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.03921978779119675 | validation: 0.06389185457384147]
	TIME [epoch: 5.76 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03792533736315376		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.03792533736315376 | validation: 0.058974944149161226]
	TIME [epoch: 5.77 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04187262899825971		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.04187262899825971 | validation: 0.07900934739168017]
	TIME [epoch: 5.76 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043324602875128466		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.043324602875128466 | validation: 0.06010822800722074]
	TIME [epoch: 5.75 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03973994923529999		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.03973994923529999 | validation: 0.056132143298118053]
	TIME [epoch: 5.76 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057959284509849		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.04057959284509849 | validation: 0.06315556506817009]
	TIME [epoch: 5.77 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04109742137599203		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.04109742137599203 | validation: 0.04591979906439794]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1117.pth
	Model improved!!!
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039498994049907935		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.039498994049907935 | validation: 0.05485901201092713]
	TIME [epoch: 5.76 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03598413768192144		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.03598413768192144 | validation: 0.06327273582387971]
	TIME [epoch: 5.75 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04356891661687147		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.04356891661687147 | validation: 0.05229504767164716]
	TIME [epoch: 5.76 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043391672124005666		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.043391672124005666 | validation: 0.07969463383245329]
	TIME [epoch: 5.75 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055009995922263716		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.055009995922263716 | validation: 0.0529823040092995]
	TIME [epoch: 5.76 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038847374417734275		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.038847374417734275 | validation: 0.05419529479756875]
	TIME [epoch: 5.76 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04236010718116049		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.04236010718116049 | validation: 0.05916965457515928]
	TIME [epoch: 5.75 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03837749190179579		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.03837749190179579 | validation: 0.06454396489084933]
	TIME [epoch: 5.76 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103171422721969		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.04103171422721969 | validation: 0.0479153429325782]
	TIME [epoch: 5.75 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04132805831005114		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.04132805831005114 | validation: 0.06094672784878829]
	TIME [epoch: 5.77 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03783463692376746		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.03783463692376746 | validation: 0.06013814156100455]
	TIME [epoch: 5.77 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03657273303800006		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.03657273303800006 | validation: 0.06315687401961194]
	TIME [epoch: 5.76 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038336418562443506		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.038336418562443506 | validation: 0.05642075081255935]
	TIME [epoch: 5.76 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036107879555611004		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.036107879555611004 | validation: 0.058301578317805025]
	TIME [epoch: 5.75 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03847067851269064		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.03847067851269064 | validation: 0.06319230172615035]
	TIME [epoch: 5.76 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04343291706165175		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.04343291706165175 | validation: 0.053784140294437746]
	TIME [epoch: 5.77 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04088953654407337		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.04088953654407337 | validation: 0.06489349214504482]
	TIME [epoch: 5.76 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968308040137775		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.03968308040137775 | validation: 0.06071899752403507]
	TIME [epoch: 5.76 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036361275919339214		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.036361275919339214 | validation: 0.053849099678214045]
	TIME [epoch: 5.76 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03595185508545785		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.03595185508545785 | validation: 0.06281078471673003]
	TIME [epoch: 5.76 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037472461492368894		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.037472461492368894 | validation: 0.052056752543931155]
	TIME [epoch: 5.77 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03777405297374327		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.03777405297374327 | validation: 0.06123411881126304]
	TIME [epoch: 5.75 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03763200376345315		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.03763200376345315 | validation: 0.05252806285901083]
	TIME [epoch: 5.75 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806973951487929		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.03806973951487929 | validation: 0.054511069976627816]
	TIME [epoch: 5.75 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034696277692693116		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.034696277692693116 | validation: 0.0551391255198945]
	TIME [epoch: 5.76 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601519660560338		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.03601519660560338 | validation: 0.06608808706520407]
	TIME [epoch: 5.77 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04241413102167575		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.04241413102167575 | validation: 0.05186306987097015]
	TIME [epoch: 5.76 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041497880664402055		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.041497880664402055 | validation: 0.08627824495969752]
	TIME [epoch: 5.76 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05389666333465025		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.05389666333465025 | validation: 0.061945875899886975]
	TIME [epoch: 5.76 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037049944728271056		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.037049944728271056 | validation: 0.05326618303448208]
	TIME [epoch: 5.76 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037655937009724326		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.037655937009724326 | validation: 0.04743944679384265]
	TIME [epoch: 5.77 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03607572111677484		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.03607572111677484 | validation: 0.060867230685663354]
	TIME [epoch: 5.76 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037369350399283414		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.037369350399283414 | validation: 0.05544248636594701]
	TIME [epoch: 5.75 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03575261626536309		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.03575261626536309 | validation: 0.0588291402099137]
	TIME [epoch: 5.75 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03558234019684724		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.03558234019684724 | validation: 0.05395771982991156]
	TIME [epoch: 5.76 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03563303126369787		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.03563303126369787 | validation: 0.05151520613955074]
	TIME [epoch: 5.77 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565070730955729		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.03565070730955729 | validation: 0.052152777067663664]
	TIME [epoch: 5.76 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616972144791174		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.03616972144791174 | validation: 0.05529853456892441]
	TIME [epoch: 5.75 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03454370190837891		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.03454370190837891 | validation: 0.056526421705804075]
	TIME [epoch: 5.76 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03558189522330891		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.03558189522330891 | validation: 0.04910074098140691]
	TIME [epoch: 5.77 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780954202328347		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.03780954202328347 | validation: 0.06668594869452728]
	TIME [epoch: 5.77 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588699524888119		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.03588699524888119 | validation: 0.056773417438299494]
	TIME [epoch: 5.76 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03657718008632875		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.03657718008632875 | validation: 0.0458175617468484]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1160.pth
	Model improved!!!
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039861936883581306		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.039861936883581306 | validation: 0.0814033534852529]
	TIME [epoch: 5.76 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05191102072023804		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.05191102072023804 | validation: 0.06209177729544262]
	TIME [epoch: 5.78 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034709709784537655		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.034709709784537655 | validation: 0.04041412222424268]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0401993161899242		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.0401993161899242 | validation: 0.06108734610456734]
	TIME [epoch: 5.77 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03482327233959374		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.03482327233959374 | validation: 0.06822481744885449]
	TIME [epoch: 5.75 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857877444167284		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.03857877444167284 | validation: 0.054640200441243174]
	TIME [epoch: 5.76 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501088644272829		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.03501088644272829 | validation: 0.052045544472981445]
	TIME [epoch: 5.77 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03649151447331178		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.03649151447331178 | validation: 0.08436416178572119]
	TIME [epoch: 5.77 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049279226727987		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.049279226727987 | validation: 0.05573145161711207]
	TIME [epoch: 5.76 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03622809619839806		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.03622809619839806 | validation: 0.0516738553626837]
	TIME [epoch: 5.75 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04436988595157879		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.04436988595157879 | validation: 0.04864060570643487]
	TIME [epoch: 5.75 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03560108779611068		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.03560108779611068 | validation: 0.04953224127831577]
	TIME [epoch: 5.77 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036915054281022146		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.036915054281022146 | validation: 0.049117063691688015]
	TIME [epoch: 5.77 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03580830384785808		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.03580830384785808 | validation: 0.0459471476345594]
	TIME [epoch: 5.78 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035452596852089374		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.035452596852089374 | validation: 0.05676748309126361]
	TIME [epoch: 5.76 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032104412515941635		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.032104412515941635 | validation: 0.052929895748527446]
	TIME [epoch: 5.76 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410931088452001		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.03410931088452001 | validation: 0.05246630956641031]
	TIME [epoch: 5.76 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03528549778342382		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.03528549778342382 | validation: 0.0503098652805534]
	TIME [epoch: 5.76 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034432964517858336		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.034432964517858336 | validation: 0.05081501484052824]
	TIME [epoch: 5.77 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03375020774781278		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.03375020774781278 | validation: 0.05740857772596421]
	TIME [epoch: 5.75 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033528899028283335		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.033528899028283335 | validation: 0.05498290602979994]
	TIME [epoch: 5.75 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03321378673864662		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.03321378673864662 | validation: 0.04847178946134523]
	TIME [epoch: 5.76 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03299345831052196		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.03299345831052196 | validation: 0.05532500992832898]
	TIME [epoch: 5.76 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0348991339323416		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.0348991339323416 | validation: 0.049425037971345766]
	TIME [epoch: 5.77 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03519234747670702		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.03519234747670702 | validation: 0.05543561431467681]
	TIME [epoch: 5.77 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03330370316806056		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.03330370316806056 | validation: 0.06625846120061313]
	TIME [epoch: 5.76 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04247074132792819		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.04247074132792819 | validation: 0.0482077813545862]
	TIME [epoch: 5.77 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03569375943396238		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.03569375943396238 | validation: 0.047339788215562]
	TIME [epoch: 5.76 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03401002768868483		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.03401002768868483 | validation: 0.07085209162993528]
	TIME [epoch: 5.77 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038203294629960895		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.038203294629960895 | validation: 0.05031559959289936]
	TIME [epoch: 5.76 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032233845694062326		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.032233845694062326 | validation: 0.048023310580785794]
	TIME [epoch: 5.75 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034464081663586804		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.034464081663586804 | validation: 0.07200304877501953]
	TIME [epoch: 5.76 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04237914580592303		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.04237914580592303 | validation: 0.05368291167706464]
	TIME [epoch: 5.77 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033530500242648344		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.033530500242648344 | validation: 0.042072558475315915]
	TIME [epoch: 5.77 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137439735236577		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.04137439735236577 | validation: 0.05631391358410982]
	TIME [epoch: 5.77 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03461749677840254		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.03461749677840254 | validation: 0.0548408230541232]
	TIME [epoch: 5.75 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0343083507001852		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.0343083507001852 | validation: 0.048684373849392074]
	TIME [epoch: 5.77 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0329148647106973		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.0329148647106973 | validation: 0.06236244999927121]
	TIME [epoch: 5.77 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804624499594685		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.03804624499594685 | validation: 0.050304547284419215]
	TIME [epoch: 5.78 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032452909938346305		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.032452909938346305 | validation: 0.04702234257177543]
	TIME [epoch: 5.77 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0328138596394923		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.0328138596394923 | validation: 0.05015819178421174]
	TIME [epoch: 5.77 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03264212511166649		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.03264212511166649 | validation: 0.056051057537813045]
	TIME [epoch: 5.77 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032690135696361595		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.032690135696361595 | validation: 0.04765526880174854]
	TIME [epoch: 5.78 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032522883459372834		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.032522883459372834 | validation: 0.05137512269992204]
	TIME [epoch: 5.77 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033216575434122084		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.033216575434122084 | validation: 0.05649500323855297]
	TIME [epoch: 5.77 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03432431318223884		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.03432431318223884 | validation: 0.0622749582907751]
	TIME [epoch: 5.75 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03646557227220629		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.03646557227220629 | validation: 0.03963356862341458]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1207.pth
	Model improved!!!
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03844991272457619		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.03844991272457619 | validation: 0.04902491613990861]
	TIME [epoch: 5.77 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03523357103035682		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.03523357103035682 | validation: 0.05149198463168693]
	TIME [epoch: 5.77 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03368806059694285		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.03368806059694285 | validation: 0.05017220095485169]
	TIME [epoch: 5.77 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032593122840605684		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.032593122840605684 | validation: 0.04673856308374089]
	TIME [epoch: 5.75 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02994496545322264		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.02994496545322264 | validation: 0.06781360372326284]
	TIME [epoch: 5.75 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04268290418442557		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.04268290418442557 | validation: 0.044915733391753726]
	TIME [epoch: 5.77 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03451730441599965		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.03451730441599965 | validation: 0.04803981534480607]
	TIME [epoch: 5.77 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03222922360002709		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.03222922360002709 | validation: 0.04635920655343791]
	TIME [epoch: 5.77 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03372557433386358		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.03372557433386358 | validation: 0.04173214980299789]
	TIME [epoch: 5.76 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03252328545590671		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.03252328545590671 | validation: 0.049368632336619284]
	TIME [epoch: 5.75 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030600682771023475		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.030600682771023475 | validation: 0.04780088289844329]
	TIME [epoch: 5.77 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034635349205742995		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.034635349205742995 | validation: 0.04988210428379495]
	TIME [epoch: 5.76 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03357403662505481		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.03357403662505481 | validation: 0.054800173519343576]
	TIME [epoch: 5.77 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400992354858762		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.03400992354858762 | validation: 0.05075008283211855]
	TIME [epoch: 5.76 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03231191687601632		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.03231191687601632 | validation: 0.05564102649077941]
	TIME [epoch: 5.75 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034132979283398764		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.034132979283398764 | validation: 0.04800483661057701]
	TIME [epoch: 5.75 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03365066759486717		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.03365066759486717 | validation: 0.0441167989043576]
	TIME [epoch: 5.76 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031091195687796835		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.031091195687796835 | validation: 0.05794301520322895]
	TIME [epoch: 5.77 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608553273053759		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.03608553273053759 | validation: 0.04421911283628702]
	TIME [epoch: 5.77 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03208731643151673		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.03208731643151673 | validation: 0.04172923830953277]
	TIME [epoch: 5.77 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03132000083171022		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.03132000083171022 | validation: 0.044724319019082026]
	TIME [epoch: 5.77 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031467648553499754		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.031467648553499754 | validation: 0.04167615136751023]
	TIME [epoch: 5.78 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055564967653056886		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.055564967653056886 | validation: 0.049338923231453315]
	TIME [epoch: 5.77 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032637350517468355		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.032637350517468355 | validation: 0.060413549185415194]
	TIME [epoch: 5.77 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042806765223106676		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.042806765223106676 | validation: 0.05338441639769297]
	TIME [epoch: 5.75 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03383923013296017		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.03383923013296017 | validation: 0.043923986494244976]
	TIME [epoch: 5.75 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223856912957021		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.03223856912957021 | validation: 0.04570201790618076]
	TIME [epoch: 5.77 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031666507785112534		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.031666507785112534 | validation: 0.04991184180861155]
	TIME [epoch: 5.76 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03184334024740642		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.03184334024740642 | validation: 0.05029520030744897]
	TIME [epoch: 5.78 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032704024290932804		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.032704024290932804 | validation: 0.04490065760124826]
	TIME [epoch: 5.76 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032558380322857		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.032558380322857 | validation: 0.04913342495615666]
	TIME [epoch: 5.75 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03174457306353767		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.03174457306353767 | validation: 0.04302161624946124]
	TIME [epoch: 5.77 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03134843394542385		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.03134843394542385 | validation: 0.04552914705226351]
	TIME [epoch: 5.78 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030868400624316596		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.030868400624316596 | validation: 0.0505316073915931]
	TIME [epoch: 5.77 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03338358316027387		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.03338358316027387 | validation: 0.04655525641977574]
	TIME [epoch: 5.76 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032167894774159676		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.032167894774159676 | validation: 0.04590118555322092]
	TIME [epoch: 5.75 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031157408217491005		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.031157408217491005 | validation: 0.0516702652025878]
	TIME [epoch: 5.77 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03152734173224708		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.03152734173224708 | validation: 0.048445458598694716]
	TIME [epoch: 5.76 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030902988984077753		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.030902988984077753 | validation: 0.04539262555244503]
	TIME [epoch: 5.76 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03212786359803621		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.03212786359803621 | validation: 0.05492595608699229]
	TIME [epoch: 5.77 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029884431500398607		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.029884431500398607 | validation: 0.053772652654479086]
	TIME [epoch: 5.75 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03140475183612084		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.03140475183612084 | validation: 0.06264675899693331]
	TIME [epoch: 5.76 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564349506433524		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.03564349506433524 | validation: 0.05005503260429381]
	TIME [epoch: 5.77 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03099527913983751		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.03099527913983751 | validation: 0.04565170731005953]
	TIME [epoch: 5.77 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031724585594505667		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.031724585594505667 | validation: 0.04092054303855241]
	TIME [epoch: 5.75 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032198122519175774		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.032198122519175774 | validation: 0.05663291596073077]
	TIME [epoch: 5.75 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032542249779106656		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.032542249779106656 | validation: 0.0467051545326634]
	TIME [epoch: 5.76 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030018225059488098		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.030018225059488098 | validation: 0.04136403178773778]
	TIME [epoch: 5.78 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03147391016541405		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.03147391016541405 | validation: 0.05507679485118397]
	TIME [epoch: 5.77 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032434517207296766		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.032434517207296766 | validation: 0.07075800536456532]
	TIME [epoch: 5.78 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03680031858003564		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.03680031858003564 | validation: 0.04162581572857069]
	TIME [epoch: 5.76 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03680351819858339		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.03680351819858339 | validation: 0.052895467051724515]
	TIME [epoch: 5.77 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02943941088593455		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.02943941088593455 | validation: 0.049970227074523914]
	TIME [epoch: 5.77 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03138237544243662		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.03138237544243662 | validation: 0.043996682256824196]
	TIME [epoch: 5.77 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029255101962280382		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.029255101962280382 | validation: 0.07525130088279187]
	TIME [epoch: 5.76 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042723958615581065		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.042723958615581065 | validation: 0.050536588462865344]
	TIME [epoch: 5.76 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029729856822788107		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.029729856822788107 | validation: 0.04051616263997915]
	TIME [epoch: 5.76 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036358888635269315		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.036358888635269315 | validation: 0.04543427370906329]
	TIME [epoch: 5.78 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029573958650453553		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.029573958650453553 | validation: 0.057871567520842854]
	TIME [epoch: 5.77 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03467632714193102		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.03467632714193102 | validation: 0.04041993297004868]
	TIME [epoch: 5.77 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030538458995065937		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.030538458995065937 | validation: 0.04443145692087213]
	TIME [epoch: 5.76 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03281355850018246		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.03281355850018246 | validation: 0.048036952187359164]
	TIME [epoch: 5.75 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029253598469488856		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.029253598469488856 | validation: 0.04704192735770707]
	TIME [epoch: 5.77 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242729149709822		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.03242729149709822 | validation: 0.04859756918640582]
	TIME [epoch: 5.78 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029873970349039408		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.029873970349039408 | validation: 0.04876271290512882]
	TIME [epoch: 5.77 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03304541295254325		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.03304541295254325 | validation: 0.0428980147896148]
	TIME [epoch: 5.76 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030332065170673373		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.030332065170673373 | validation: 0.05326017035762884]
	TIME [epoch: 5.76 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033376640991013645		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.033376640991013645 | validation: 0.03942197558049344]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1275.pth
	Model improved!!!
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029847503635680585		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.029847503635680585 | validation: 0.04010787816566315]
	TIME [epoch: 5.78 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029366592537003983		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.029366592537003983 | validation: 0.04494868495982555]
	TIME [epoch: 5.78 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02963306642902072		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.02963306642902072 | validation: 0.04392392458381355]
	TIME [epoch: 5.78 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029252653064899128		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.029252653064899128 | validation: 0.04277481760318506]
	TIME [epoch: 5.76 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0294313665585114		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.0294313665585114 | validation: 0.04313663297665388]
	TIME [epoch: 5.78 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03162750599784071		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.03162750599784071 | validation: 0.04292655599285588]
	TIME [epoch: 5.78 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029885947827036175		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.029885947827036175 | validation: 0.03612296552599964]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1282.pth
	Model improved!!!
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03080313078184		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.03080313078184 | validation: 0.05500775126641046]
	TIME [epoch: 5.72 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327870331228052		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.0327870331228052 | validation: 0.03823175532876658]
	TIME [epoch: 5.72 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03147686326646401		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.03147686326646401 | validation: 0.04829144804064905]
	TIME [epoch: 5.73 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02806166161443917		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.02806166161443917 | validation: 0.04852671450566413]
	TIME [epoch: 5.78 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029590510909981625		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.029590510909981625 | validation: 0.04215051224089877]
	TIME [epoch: 5.78 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029596718827100625		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.029596718827100625 | validation: 0.05296123842330844]
	TIME [epoch: 5.79 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029812898624142548		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.029812898624142548 | validation: 0.03932657156721728]
	TIME [epoch: 5.77 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02877686122539932		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.02877686122539932 | validation: 0.04089162998776265]
	TIME [epoch: 5.78 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03046413788297369		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.03046413788297369 | validation: 0.05275289573639627]
	TIME [epoch: 5.79 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029720549165915662		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.029720549165915662 | validation: 0.04068486345261085]
	TIME [epoch: 5.78 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028404882699314068		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.028404882699314068 | validation: 0.03924074657196332]
	TIME [epoch: 5.79 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030414059360930557		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.030414059360930557 | validation: 0.04827110299566678]
	TIME [epoch: 5.77 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028747272475493534		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.028747272475493534 | validation: 0.04753299590319582]
	TIME [epoch: 5.78 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029116963694518782		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.029116963694518782 | validation: 0.03913834879608964]
	TIME [epoch: 5.78 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029179174182618832		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.029179174182618832 | validation: 0.04166702197774094]
	TIME [epoch: 5.78 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031157468285393036		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.031157468285393036 | validation: 0.05267008726989382]
	TIME [epoch: 5.77 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03565128655081834		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.03565128655081834 | validation: 0.04082503413864286]
	TIME [epoch: 5.78 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029960862977046813		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.029960862977046813 | validation: 0.03857366812552271]
	TIME [epoch: 5.78 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028305062168420582		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.028305062168420582 | validation: 0.04528842248282079]
	TIME [epoch: 5.72 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02919152886335402		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.02919152886335402 | validation: 0.03655252512922519]
	TIME [epoch: 5.72 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029088709974294524		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.029088709974294524 | validation: 0.06344684585492778]
	TIME [epoch: 5.73 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038108412896327735		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.038108412896327735 | validation: 0.04502029179034167]
	TIME [epoch: 5.72 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030342079393602655		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.030342079393602655 | validation: 0.044635178372739163]
	TIME [epoch: 5.71 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031840643435048534		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.031840643435048534 | validation: 0.05164036806038426]
	TIME [epoch: 5.73 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030178254071650944		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.030178254071650944 | validation: 0.044332198551605]
	TIME [epoch: 5.72 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029259806042654785		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.029259806042654785 | validation: 0.04096111300227487]
	TIME [epoch: 5.77 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029332944699118627		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.029332944699118627 | validation: 0.042666364772826385]
	TIME [epoch: 5.77 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02863741152287298		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.02863741152287298 | validation: 0.046455620694980194]
	TIME [epoch: 5.76 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02824401896975987		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.02824401896975987 | validation: 0.04621722362767561]
	TIME [epoch: 5.78 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029256881879128015		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.029256881879128015 | validation: 0.04122406517426869]
	TIME [epoch: 5.78 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0319733145431532		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.0319733145431532 | validation: 0.04651362076837121]
	TIME [epoch: 5.78 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029099894637750544		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.029099894637750544 | validation: 0.07635785217616392]
	TIME [epoch: 5.78 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04694760393018831		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.04694760393018831 | validation: 0.047343099254180336]
	TIME [epoch: 5.77 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02916841481659274		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.02916841481659274 | validation: 0.03800576743687628]
	TIME [epoch: 5.78 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03221438904809577		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.03221438904809577 | validation: 0.04486286755047972]
	TIME [epoch: 5.79 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029895633618887887		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.029895633618887887 | validation: 0.045245964538705365]
	TIME [epoch: 5.79 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02875931472469709		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.02875931472469709 | validation: 0.04396729132926881]
	TIME [epoch: 5.78 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02870935827070691		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.02870935827070691 | validation: 0.04337731992578245]
	TIME [epoch: 5.71 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028420319415091388		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.028420319415091388 | validation: 0.0420902163683913]
	TIME [epoch: 5.72 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028950285377694996		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.028950285377694996 | validation: 0.03906864152010442]
	TIME [epoch: 5.72 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029011191681865642		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.029011191681865642 | validation: 0.04504136425763603]
	TIME [epoch: 5.72 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02778812294805529		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.02778812294805529 | validation: 0.046022637331628535]
	TIME [epoch: 5.72 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02694962784380466		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.02694962784380466 | validation: 0.04217455776748048]
	TIME [epoch: 5.71 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02782146139683058		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.02782146139683058 | validation: 0.04760696900510894]
	TIME [epoch: 5.72 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027354220153012615		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.027354220153012615 | validation: 0.039977608307227255]
	TIME [epoch: 5.73 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029687562439535745		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.029687562439535745 | validation: 0.03593814076117158]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1328.pth
	Model improved!!!
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029236361943514578		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.029236361943514578 | validation: 0.05466063979353593]
	TIME [epoch: 5.79 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028918360603667446		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.028918360603667446 | validation: 0.04289070307623808]
	TIME [epoch: 5.78 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03015284612546057		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.03015284612546057 | validation: 0.04197547567432978]
	TIME [epoch: 5.77 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028775654255386553		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.028775654255386553 | validation: 0.04535669982561329]
	TIME [epoch: 5.78 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027734515885158496		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.027734515885158496 | validation: 0.04810197144080245]
	TIME [epoch: 5.78 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030345235847472046		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.030345235847472046 | validation: 0.05156663232741654]
	TIME [epoch: 5.78 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028029103527511083		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.028029103527511083 | validation: 0.042229563824572214]
	TIME [epoch: 5.77 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027894518267142426		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.027894518267142426 | validation: 0.05115008765643904]
	TIME [epoch: 5.77 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02953390944215295		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.02953390944215295 | validation: 0.045740615894356865]
	TIME [epoch: 5.73 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027315743796368665		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.027315743796368665 | validation: 0.038806191310561636]
	TIME [epoch: 5.73 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028605362983047406		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.028605362983047406 | validation: 0.04845776808689328]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03088477505698074		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.03088477505698074 | validation: 0.035958517483654744]
	TIME [epoch: 5.73 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027289760153999254		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.027289760153999254 | validation: 0.036713005169272415]
	TIME [epoch: 5.71 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02729378915936495		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.02729378915936495 | validation: 0.04976347854721369]
	TIME [epoch: 5.78 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02826577843197535		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.02826577843197535 | validation: 0.050352405011213686]
	TIME [epoch: 5.77 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03111047808889084		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.03111047808889084 | validation: 0.040275446576750355]
	TIME [epoch: 5.78 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029042334984184636		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.029042334984184636 | validation: 0.04158162865879769]
	TIME [epoch: 5.78 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02765040779092961		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.02765040779092961 | validation: 0.040213670278022695]
	TIME [epoch: 5.77 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029296286075016404		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.029296286075016404 | validation: 0.04271132805134346]
	TIME [epoch: 5.78 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027519395392987322		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.027519395392987322 | validation: 0.03925390363441636]
	TIME [epoch: 5.78 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028184490774879858		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.028184490774879858 | validation: 0.0424093767143144]
	TIME [epoch: 5.77 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030966799102295335		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.030966799102295335 | validation: 0.04102947539681513]
	TIME [epoch: 5.76 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02990484171454543		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.02990484171454543 | validation: 0.04378511880873092]
	TIME [epoch: 5.76 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02761544082641918		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.02761544082641918 | validation: 0.041256939413597774]
	TIME [epoch: 5.76 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026513043752318062		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.026513043752318062 | validation: 0.03934948187125286]
	TIME [epoch: 5.77 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027465991238897737		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.027465991238897737 | validation: 0.034430309292398355]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1354.pth
	Model improved!!!
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02862785326755347		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.02862785326755347 | validation: 0.043750228844488925]
	TIME [epoch: 5.74 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027877831503720935		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.027877831503720935 | validation: 0.03844246244440946]
	TIME [epoch: 5.72 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02750849495068773		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.02750849495068773 | validation: 0.04797103488001651]
	TIME [epoch: 5.73 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02853653797697562		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.02853653797697562 | validation: 0.04646005591451285]
	TIME [epoch: 5.74 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030416120991224246		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.030416120991224246 | validation: 0.04812963220512743]
	TIME [epoch: 5.74 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029846204324294235		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.029846204324294235 | validation: 0.03980532861368674]
	TIME [epoch: 5.73 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0273996233791521		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.0273996233791521 | validation: 0.04411960371258188]
	TIME [epoch: 5.73 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02763408741558207		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.02763408741558207 | validation: 0.03948656323000025]
	TIME [epoch: 5.73 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030989249149700877		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.030989249149700877 | validation: 0.042929599302832334]
	TIME [epoch: 5.74 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026837856813223492		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.026837856813223492 | validation: 0.04491992888256657]
	TIME [epoch: 5.77 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025470482436336747		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.025470482436336747 | validation: 0.04553123430170841]
	TIME [epoch: 5.78 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02638464405521593		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.02638464405521593 | validation: 0.04361256221622502]
	TIME [epoch: 5.76 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02611169465503517		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.02611169465503517 | validation: 0.03953459508680336]
	TIME [epoch: 5.76 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02759727370852695		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.02759727370852695 | validation: 0.037738325594234394]
	TIME [epoch: 5.77 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02768687846114312		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.02768687846114312 | validation: 0.045548138335724436]
	TIME [epoch: 5.77 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028611007624556296		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.028611007624556296 | validation: 0.05587611049606739]
	TIME [epoch: 5.77 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029808202839545044		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.029808202839545044 | validation: 0.04567175210895613]
	TIME [epoch: 5.77 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029966447260564168		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.029966447260564168 | validation: 0.04370282153790975]
	TIME [epoch: 5.76 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027485725280999476		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.027485725280999476 | validation: 0.0465616892072645]
	TIME [epoch: 5.75 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030539184343499608		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.030539184343499608 | validation: 0.04596619183946482]
	TIME [epoch: 5.75 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026838844187510896		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.026838844187510896 | validation: 0.03776297604448953]
	TIME [epoch: 5.75 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02689034242793978		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.02689034242793978 | validation: 0.03945780281535564]
	TIME [epoch: 5.75 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02619325857245019		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.02619325857245019 | validation: 0.041165215064567655]
	TIME [epoch: 5.73 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029148702081212542		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.029148702081212542 | validation: 0.03529965549882232]
	TIME [epoch: 5.75 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02880244505767885		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.02880244505767885 | validation: 0.043581132372676294]
	TIME [epoch: 5.75 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026299122857181115		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.026299122857181115 | validation: 0.043712807472655116]
	TIME [epoch: 5.75 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02729061559000607		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.02729061559000607 | validation: 0.0410975798666738]
	TIME [epoch: 5.74 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026968230629687243		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.026968230629687243 | validation: 0.057579420587114474]
	TIME [epoch: 5.73 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03052821455764422		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.03052821455764422 | validation: 0.03617088633490252]
	TIME [epoch: 5.75 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028013248784658877		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.028013248784658877 | validation: 0.04467004948467244]
	TIME [epoch: 5.75 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025135419499239937		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.025135419499239937 | validation: 0.04306847196050354]
	TIME [epoch: 5.75 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024589723959055183		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.024589723959055183 | validation: 0.040065489929069324]
	TIME [epoch: 5.75 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028311294787331808		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.028311294787331808 | validation: 0.04033816878378942]
	TIME [epoch: 5.74 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026514668882051246		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.026514668882051246 | validation: 0.05063867769062563]
	TIME [epoch: 5.74 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029805472358376148		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.029805472358376148 | validation: 0.03536045719091152]
	TIME [epoch: 5.75 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027761998441374447		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.027761998441374447 | validation: 0.03251481561401546]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1390.pth
	Model improved!!!
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027755742490147523		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.027755742490147523 | validation: 0.03848240749700121]
	TIME [epoch: 5.83 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026886901125301614		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.026886901125301614 | validation: 0.04793938738226383]
	TIME [epoch: 5.75 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02733001912853161		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.02733001912853161 | validation: 0.037243719358626304]
	TIME [epoch: 5.75 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02694676320431888		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.02694676320431888 | validation: 0.03615733348899254]
	TIME [epoch: 5.76 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027264721392541515		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.027264721392541515 | validation: 0.05276267411436737]
	TIME [epoch: 5.76 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033522251797400635		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.033522251797400635 | validation: 0.04176949853557968]
	TIME [epoch: 5.76 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026044835288074372		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.026044835288074372 | validation: 0.03341037363869378]
	TIME [epoch: 5.75 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029048506359347553		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.029048506359347553 | validation: 0.038370751123643965]
	TIME [epoch: 5.75 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026680544242870818		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.026680544242870818 | validation: 0.04877002685425154]
	TIME [epoch: 5.73 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032673598188546794		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.032673598188546794 | validation: 0.03677273010053735]
	TIME [epoch: 5.73 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02682453727427319		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.02682453727427319 | validation: 0.03830713661685094]
	TIME [epoch: 5.73 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02577294346162735		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.02577294346162735 | validation: 0.04084993293428213]
	TIME [epoch: 5.74 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027399215342913764		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.027399215342913764 | validation: 0.04440524346583159]
	TIME [epoch: 5.75 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027080592207945148		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.027080592207945148 | validation: 0.04011669062914931]
	TIME [epoch: 5.75 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02528317602353602		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.02528317602353602 | validation: 0.0403391437825231]
	TIME [epoch: 5.75 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025857578635736577		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.025857578635736577 | validation: 0.036901963118832495]
	TIME [epoch: 5.75 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026984927694933738		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.026984927694933738 | validation: 0.03809633785655955]
	TIME [epoch: 5.75 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02689260922472803		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.02689260922472803 | validation: 0.038892166071326145]
	TIME [epoch: 5.74 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026161424273170183		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.026161424273170183 | validation: 0.04874769640168424]
	TIME [epoch: 5.75 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028233927005997084		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.028233927005997084 | validation: 0.03773867032790795]
	TIME [epoch: 5.75 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02647965300389509		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.02647965300389509 | validation: 0.038384492088044835]
	TIME [epoch: 5.76 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026711365846663674		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.026711365846663674 | validation: 0.03949093636093831]
	TIME [epoch: 5.76 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025706071965847625		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.025706071965847625 | validation: 0.03892105415574411]
	TIME [epoch: 5.75 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0269406112588156		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.0269406112588156 | validation: 0.060764625653036745]
	TIME [epoch: 5.76 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033596444465339034		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.033596444465339034 | validation: 0.04221648028748544]
	TIME [epoch: 5.76 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028900772922887268		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.028900772922887268 | validation: 0.04881080934015634]
	TIME [epoch: 5.76 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026471400327811867		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.026471400327811867 | validation: 0.03939789883020024]
	TIME [epoch: 5.76 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026852451505437722		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.026852451505437722 | validation: 0.03808613259334424]
	TIME [epoch: 5.73 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02628678086043558		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.02628678086043558 | validation: 0.041695989824612326]
	TIME [epoch: 5.75 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02614108663733707		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.02614108663733707 | validation: 0.03417727192771899]
	TIME [epoch: 5.76 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02713992780644122		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.02713992780644122 | validation: 0.030673011612191915]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1421.pth
	Model improved!!!
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02644306982633363		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.02644306982633363 | validation: 0.04308408386398156]
	TIME [epoch: 5.75 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026445563463744413		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.026445563463744413 | validation: 0.040428631349015226]
	TIME [epoch: 5.75 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02592584824904023		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.02592584824904023 | validation: 0.04101606448975118]
	TIME [epoch: 5.76 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026355911551140743		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.026355911551140743 | validation: 0.045671307927824326]
	TIME [epoch: 5.76 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02852784916065392		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.02852784916065392 | validation: 0.044089363020748]
	TIME [epoch: 5.75 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026737836117551757		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.026737836117551757 | validation: 0.03582829096133183]
	TIME [epoch: 5.76 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025460158535356398		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.025460158535356398 | validation: 0.032813508214283804]
	TIME [epoch: 5.75 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025552221267235522		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.025552221267235522 | validation: 0.03829209425569435]
	TIME [epoch: 5.75 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02562818567499864		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.02562818567499864 | validation: 0.03692110063722814]
	TIME [epoch: 5.76 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026471251778377784		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.026471251778377784 | validation: 0.03789434562664832]
	TIME [epoch: 5.76 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027005637136541975		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.027005637136541975 | validation: 0.03800085898327646]
	TIME [epoch: 5.76 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024598577918362875		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.024598577918362875 | validation: 0.04344947151270845]
	TIME [epoch: 5.75 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026951022719133605		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.026951022719133605 | validation: 0.03369448850260892]
	TIME [epoch: 5.75 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026602293826889424		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.026602293826889424 | validation: 0.03373457194123867]
	TIME [epoch: 5.77 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025393501467683655		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.025393501467683655 | validation: 0.03306404830290426]
	TIME [epoch: 5.76 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025243854840454265		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.025243854840454265 | validation: 0.04103867868866893]
	TIME [epoch: 5.76 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025549651180117483		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.025549651180117483 | validation: 0.03293399860561915]
	TIME [epoch: 5.76 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024499906003786657		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.024499906003786657 | validation: 0.04253118803589319]
	TIME [epoch: 5.75 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025458751572645016		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.025458751572645016 | validation: 0.03918663720037993]
	TIME [epoch: 5.76 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024601448874502278		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.024601448874502278 | validation: 0.03392645434836938]
	TIME [epoch: 5.75 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02697375187780773		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.02697375187780773 | validation: 0.04286360676316508]
	TIME [epoch: 5.76 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026356279536598998		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.026356279536598998 | validation: 0.0433035896301875]
	TIME [epoch: 5.75 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026486369109625645		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.026486369109625645 | validation: 0.0329270704727505]
	TIME [epoch: 5.74 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027223307805848834		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.027223307805848834 | validation: 0.03588896418520201]
	TIME [epoch: 5.75 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026545940308494033		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.026545940308494033 | validation: 0.034319604421242156]
	TIME [epoch: 5.75 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025853521615436206		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.025853521615436206 | validation: 0.04609375725318019]
	TIME [epoch: 5.76 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025795487242443876		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.025795487242443876 | validation: 0.03925022829452887]
	TIME [epoch: 5.76 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026526587022541286		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.026526587022541286 | validation: 0.043175164306359176]
	TIME [epoch: 5.75 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027674715848830495		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.027674715848830495 | validation: 0.04149302162200491]
	TIME [epoch: 5.76 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024296377311525747		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.024296377311525747 | validation: 0.04312312306628942]
	TIME [epoch: 5.76 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026186057900987457		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.026186057900987457 | validation: 0.037815409323411334]
	TIME [epoch: 5.76 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025503533300835112		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.025503533300835112 | validation: 0.0410700096589]
	TIME [epoch: 5.75 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02920411349496789		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.02920411349496789 | validation: 0.048007106986826364]
	TIME [epoch: 5.74 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027143052902341855		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.027143052902341855 | validation: 0.04307139209863131]
	TIME [epoch: 5.74 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026095947157920924		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.026095947157920924 | validation: 0.035990533281718654]
	TIME [epoch: 5.75 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026417143749290634		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.026417143749290634 | validation: 0.0398148471510813]
	TIME [epoch: 5.75 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025739384383510356		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.025739384383510356 | validation: 0.03204562392910796]
	TIME [epoch: 5.75 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03658300400013248		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.03658300400013248 | validation: 0.033562745449969324]
	TIME [epoch: 5.74 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03154287412548132		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.03154287412548132 | validation: 0.03546394707460506]
	TIME [epoch: 5.74 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02685203818182115		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.02685203818182115 | validation: 0.0391125413264958]
	TIME [epoch: 5.74 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026707415928716522		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.026707415928716522 | validation: 0.0411720202440658]
	TIME [epoch: 5.75 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026269477551178186		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.026269477551178186 | validation: 0.03554598225883291]
	TIME [epoch: 5.74 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026271234387670494		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.026271234387670494 | validation: 0.03609977700515733]
	TIME [epoch: 5.74 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026391224958314887		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.026391224958314887 | validation: 0.036832847797180504]
	TIME [epoch: 5.74 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027067291729808325		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.027067291729808325 | validation: 0.029571033227128365]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1466.pth
	Model improved!!!
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027036817564349773		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.027036817564349773 | validation: 0.042939692601546176]
	TIME [epoch: 5.76 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024583990026607216		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.024583990026607216 | validation: 0.03871632979887871]
	TIME [epoch: 5.77 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02521837812262228		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.02521837812262228 | validation: 0.035625363613719443]
	TIME [epoch: 5.75 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02470398647920783		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.02470398647920783 | validation: 0.03309675342076762]
	TIME [epoch: 5.74 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027034993346263293		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.027034993346263293 | validation: 0.039113979333298056]
	TIME [epoch: 5.75 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025301122555903232		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.025301122555903232 | validation: 0.039402057691475215]
	TIME [epoch: 5.76 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024715250187204767		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.024715250187204767 | validation: 0.034158330788943425]
	TIME [epoch: 5.76 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026765563939289087		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.026765563939289087 | validation: 0.03151362239536964]
	TIME [epoch: 5.75 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026006529665524646		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.026006529665524646 | validation: 0.04063044286622663]
	TIME [epoch: 5.74 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02381020908887432		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.02381020908887432 | validation: 0.03537971567961752]
	TIME [epoch: 5.75 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02572787341301489		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.02572787341301489 | validation: 0.03927143997730436]
	TIME [epoch: 5.76 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02680392709920546		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.02680392709920546 | validation: 0.03689479064824854]
	TIME [epoch: 5.76 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024977145820445124		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.024977145820445124 | validation: 0.02886152067954161]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1479.pth
	Model improved!!!
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027951070228825575		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.027951070228825575 | validation: 0.02573573951721333]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1480.pth
	Model improved!!!
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03187548970121907		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.03187548970121907 | validation: 0.03151163132993743]
	TIME [epoch: 5.76 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027251577635208052		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.027251577635208052 | validation: 0.03580471638110392]
	TIME [epoch: 5.76 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025377392878846515		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.025377392878846515 | validation: 0.03421018811609662]
	TIME [epoch: 5.75 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025582059212813292		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.025582059212813292 | validation: 0.03438094551038434]
	TIME [epoch: 5.75 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025888592698372383		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.025888592698372383 | validation: 0.03636531495354722]
	TIME [epoch: 5.73 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025565095084635808		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.025565095084635808 | validation: 0.036259439459270036]
	TIME [epoch: 5.74 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023405344785209142		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.023405344785209142 | validation: 0.044310883970943626]
	TIME [epoch: 5.75 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02684971589796773		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.02684971589796773 | validation: 0.034283201647708574]
	TIME [epoch: 5.75 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02511636811474976		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.02511636811474976 | validation: 0.03493216523864173]
	TIME [epoch: 5.74 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024077734836606492		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.024077734836606492 | validation: 0.03551715280620791]
	TIME [epoch: 5.74 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02461979076313739		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.02461979076313739 | validation: 0.04248934442912618]
	TIME [epoch: 5.74 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02467066488163229		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.02467066488163229 | validation: 0.03800648211085692]
	TIME [epoch: 5.76 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02459166624140463		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.02459166624140463 | validation: 0.036975419097098276]
	TIME [epoch: 5.76 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026438929072987474		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.026438929072987474 | validation: 0.03567134087774385]
	TIME [epoch: 5.76 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025919077264151102		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.025919077264151102 | validation: 0.029957116622481864]
	TIME [epoch: 5.75 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029507645195555817		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.029507645195555817 | validation: 0.03555933578817403]
	TIME [epoch: 5.75 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027420002902698803		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.027420002902698803 | validation: 0.051291970796452396]
	TIME [epoch: 5.75 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027516197256069665		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.027516197256069665 | validation: 0.051643464783963734]
	TIME [epoch: 5.75 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029837619136839955		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.029837619136839955 | validation: 0.04228751040459771]
	TIME [epoch: 5.75 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025885609705899367		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.025885609705899367 | validation: 0.03207279327172996]
	TIME [epoch: 5.74 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02693654910860281		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.02693654910860281 | validation: 0.03262002383649128]
	TIME [epoch: 5.73 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02661133654176836		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.02661133654176836 | validation: 0.0355842407046328]
	TIME [epoch: 5.76 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02490651888438428		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.02490651888438428 | validation: 0.035835691720112965]
	TIME [epoch: 5.75 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02473932696520703		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.02473932696520703 | validation: 0.037601972729241095]
	TIME [epoch: 5.76 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02431904017418007		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.02431904017418007 | validation: 0.032540155492700173]
	TIME [epoch: 5.75 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026017047279925935		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.026017047279925935 | validation: 0.030182193926438818]
	TIME [epoch: 5.75 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026220727587390462		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.026220727587390462 | validation: 0.04592248834016578]
	TIME [epoch: 5.76 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02399433607774852		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.02399433607774852 | validation: 0.0433972443867648]
	TIME [epoch: 5.76 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025435418814675384		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.025435418814675384 | validation: 0.04122355478049225]
	TIME [epoch: 5.76 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027065331796613374		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.027065331796613374 | validation: 0.03453058021868356]
	TIME [epoch: 5.75 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024615622567913924		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.024615622567913924 | validation: 0.03581834039872979]
	TIME [epoch: 5.75 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0254618693924201		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.0254618693924201 | validation: 0.03799866802165485]
	TIME [epoch: 5.75 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025525218573604836		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.025525218573604836 | validation: 0.04134187582693682]
	TIME [epoch: 5.75 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02501383767788993		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.02501383767788993 | validation: 0.03616593021286735]
	TIME [epoch: 5.76 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02415778444300978		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.02415778444300978 | validation: 0.03287867908485931]
	TIME [epoch: 5.75 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025504375033339084		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.025504375033339084 | validation: 0.032240729290631646]
	TIME [epoch: 5.74 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023142130637349167		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.023142130637349167 | validation: 0.045905812832358533]
	TIME [epoch: 5.74 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024869095295858984		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.024869095295858984 | validation: 0.03773092088721092]
	TIME [epoch: 5.75 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025024865791981724		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.025024865791981724 | validation: 0.043839110544942085]
	TIME [epoch: 5.75 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02449537189664876		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.02449537189664876 | validation: 0.03132806390896594]
	TIME [epoch: 5.76 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02415885871677541		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.02415885871677541 | validation: 0.03878868710856625]
	TIME [epoch: 5.74 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025092584168139375		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.025092584168139375 | validation: 0.03500853991462832]
	TIME [epoch: 5.74 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02469010841536715		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.02469010841536715 | validation: 0.0345708125073366]
	TIME [epoch: 5.76 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02555466182201112		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.02555466182201112 | validation: 0.035041537321043335]
	TIME [epoch: 5.76 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02672571558259655		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.02672571558259655 | validation: 0.034473721114873755]
	TIME [epoch: 5.75 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023937545958195996		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.023937545958195996 | validation: 0.03692762146985824]
	TIME [epoch: 5.74 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025309806355327903		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.025309806355327903 | validation: 0.030678081508247336]
	TIME [epoch: 5.74 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02535600040801124		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.02535600040801124 | validation: 0.037141275832842525]
	TIME [epoch: 5.75 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023794459016062905		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.023794459016062905 | validation: 0.03665645778169554]
	TIME [epoch: 5.75 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02487738077298838		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.02487738077298838 | validation: 0.03613907514456636]
	TIME [epoch: 5.75 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02436198824409949		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.02436198824409949 | validation: 0.04777590565498109]
	TIME [epoch: 5.75 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028654760705671127		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.028654760705671127 | validation: 0.04254203854389707]
	TIME [epoch: 5.74 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028580514132405057		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.028580514132405057 | validation: 0.04133666539161788]
	TIME [epoch: 5.75 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025445813583867383		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.025445813583867383 | validation: 0.032983147288174135]
	TIME [epoch: 5.75 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02514678644330617		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.02514678644330617 | validation: 0.04417197946974517]
	TIME [epoch: 5.75 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023496139641616814		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.023496139641616814 | validation: 0.039929975538889675]
	TIME [epoch: 5.75 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02511743708101106		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.02511743708101106 | validation: 0.03712679983517943]
	TIME [epoch: 5.73 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024337867640398265		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.024337867640398265 | validation: 0.03344450183846871]
	TIME [epoch: 5.74 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024103079103871154		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.024103079103871154 | validation: 0.03830322159248376]
	TIME [epoch: 5.75 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02450238471536828		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.02450238471536828 | validation: 0.03812320871560441]
	TIME [epoch: 5.75 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024243525227019346		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.024243525227019346 | validation: 0.04202168547448681]
	TIME [epoch: 5.75 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023582797952928107		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.023582797952928107 | validation: 0.040873613917216015]
	TIME [epoch: 5.74 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02408012477316435		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.02408012477316435 | validation: 0.03573792894699671]
	TIME [epoch: 5.74 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0242249915293244		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.0242249915293244 | validation: 0.03874313733265797]
	TIME [epoch: 5.75 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024881386182992796		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.024881386182992796 | validation: 0.035515409040719724]
	TIME [epoch: 5.75 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024542150174189415		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.024542150174189415 | validation: 0.03367410484320208]
	TIME [epoch: 5.74 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0244219675158175		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.0244219675158175 | validation: 0.03520158460300399]
	TIME [epoch: 5.74 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024091403218689775		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.024091403218689775 | validation: 0.04048217536754816]
	TIME [epoch: 5.75 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024960938009928314		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.024960938009928314 | validation: 0.036448376754484695]
	TIME [epoch: 5.74 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02466350871814387		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.02466350871814387 | validation: 0.042266415332941655]
	TIME [epoch: 5.75 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023710086896628707		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.023710086896628707 | validation: 0.038873773088268074]
	TIME [epoch: 5.74 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024091674839122508		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.024091674839122508 | validation: 0.03391858878818188]
	TIME [epoch: 5.74 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023472623466747007		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.023472623466747007 | validation: 0.040508091123354274]
	TIME [epoch: 5.75 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025009822158689837		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.025009822158689837 | validation: 0.03579517560050086]
	TIME [epoch: 5.74 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026575822758584382		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.026575822758584382 | validation: 0.03991622430766493]
	TIME [epoch: 5.75 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024276271895961283		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.024276271895961283 | validation: 0.03600220770646969]
	TIME [epoch: 5.75 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025163457471283453		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.025163457471283453 | validation: 0.04191136128451655]
	TIME [epoch: 5.75 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024457509807479738		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.024457509807479738 | validation: 0.038419520726424544]
	TIME [epoch: 5.75 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023801067221128333		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.023801067221128333 | validation: 0.03458152901155921]
	TIME [epoch: 5.76 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024188482909724005		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.024188482909724005 | validation: 0.03581909927995907]
	TIME [epoch: 5.75 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024925786287076025		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.024925786287076025 | validation: 0.0402055946606945]
	TIME [epoch: 5.74 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02525301100453027		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.02525301100453027 | validation: 0.03057601059288949]
	TIME [epoch: 5.75 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02277368282449334		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.02277368282449334 | validation: 0.0385525093311207]
	TIME [epoch: 5.74 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023634055457986435		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.023634055457986435 | validation: 0.03803490561920025]
	TIME [epoch: 5.75 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024071837154837735		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.024071837154837735 | validation: 0.03271686923157079]
	TIME [epoch: 5.74 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02646884723176109		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.02646884723176109 | validation: 0.03535186902570321]
	TIME [epoch: 5.75 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02480014791327572		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.02480014791327572 | validation: 0.04412513129355094]
	TIME [epoch: 5.75 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025105223486196905		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.025105223486196905 | validation: 0.05034141411158602]
	TIME [epoch: 5.75 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029345203588055348		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.029345203588055348 | validation: 0.04060228363732288]
	TIME [epoch: 5.76 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024492037185156516		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.024492037185156516 | validation: 0.03352868859725947]
	TIME [epoch: 5.74 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024576526141056425		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.024576526141056425 | validation: 0.034042386091369725]
	TIME [epoch: 5.76 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025385658761548618		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.025385658761548618 | validation: 0.033273001533761774]
	TIME [epoch: 5.75 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02362838187583863		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.02362838187583863 | validation: 0.043641340274682416]
	TIME [epoch: 5.74 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024113403608116375		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.024113403608116375 | validation: 0.036405837166135845]
	TIME [epoch: 5.75 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023034993147571736		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.023034993147571736 | validation: 0.03479641831793339]
	TIME [epoch: 5.75 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02249921405728483		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.02249921405728483 | validation: 0.03156056964106868]
	TIME [epoch: 5.75 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025692457761659463		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.025692457761659463 | validation: 0.04107012583358042]
	TIME [epoch: 5.74 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024005971375318166		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.024005971375318166 | validation: 0.04088352610603516]
	TIME [epoch: 5.75 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023872719410874252		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.023872719410874252 | validation: 0.04037901942452134]
	TIME [epoch: 5.76 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02348619021087209		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.02348619021087209 | validation: 0.03056105796831853]
	TIME [epoch: 5.75 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025441260117026694		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.025441260117026694 | validation: 0.033818999974501676]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241125_131737/states/model_phi1_4a_v_mmd1_1581.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6043.633 seconds.
