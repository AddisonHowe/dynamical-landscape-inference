Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1285646001

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.472518393389978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.472518393389978 | validation: 5.764992486800122]
	TIME [epoch: 160 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.191254695917085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.191254695917085 | validation: 5.986203118488908]
	TIME [epoch: 0.711 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.241020657777597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.241020657777597 | validation: 5.792869204891058]
	TIME [epoch: 0.685 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.020577722693903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.020577722693903 | validation: 5.594749153103416]
	TIME [epoch: 0.917 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9000988221591992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9000988221591992 | validation: 5.550958913406691]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.793070192522684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.793070192522684 | validation: 5.5181666375926035]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.648897503702049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.648897503702049 | validation: 5.515897275063031]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6305559114169874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6305559114169874 | validation: 4.9873130998954425]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4000404669835746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4000404669835746 | validation: 4.156144192165138]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9595198015618225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9595198015618225 | validation: 3.5967495580865982]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.446233594599462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.446233594599462 | validation: 4.20663042858199]
	TIME [epoch: 0.687 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.73561191827615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.73561191827615 | validation: 3.0654629855013376]
	TIME [epoch: 0.683 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5609239620478084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5609239620478084 | validation: 1.7722293115427707]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0888794540649473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0888794540649473 | validation: 3.117359123265306]
	TIME [epoch: 0.689 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.497990493105459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.497990493105459 | validation: 2.464550982041189]
	TIME [epoch: 0.689 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1286661279332937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1286661279332937 | validation: 1.7685805749478947]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8542350546028783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8542350546028783 | validation: 1.8113289522727485]
	TIME [epoch: 0.689 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8729075327003966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8729075327003966 | validation: 1.568236281910715]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7032726126654394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7032726126654394 | validation: 1.5925269959679702]
	TIME [epoch: 0.69 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6463796387420586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6463796387420586 | validation: 1.391110112670619]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.555030435251437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.555030435251437 | validation: 1.4170926447858658]
	TIME [epoch: 0.684 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5243395200634937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5243395200634937 | validation: 1.3714790509684225]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5012691345691098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5012691345691098 | validation: 1.3563126467003759]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4805089569690484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4805089569690484 | validation: 1.3407359995793295]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4632597754513188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4632597754513188 | validation: 1.3397078045567667]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4469939835912211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4469939835912211 | validation: 1.3278844051800411]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4667788900019314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4667788900019314 | validation: 1.541416696617071]
	TIME [epoch: 0.689 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5578199119529275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5578199119529275 | validation: 1.558438719884007]
	TIME [epoch: 0.689 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6448926790579033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6448926790579033 | validation: 1.4322876330773981]
	TIME [epoch: 0.687 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.479164959666832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.479164959666832 | validation: 1.239978797980755]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3827852472250606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3827852472250606 | validation: 1.18066311163801]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3428528946391993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3428528946391993 | validation: 1.166953316535551]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3248029864711424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3248029864711424 | validation: 1.166420451766573]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.305676950416884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.305676950416884 | validation: 1.1386931631486776]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2860628324436025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2860628324436025 | validation: 1.1174706026712216]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2720675429479116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2720675429479116 | validation: 1.1069212233753267]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.256476123762125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.256476123762125 | validation: 1.094924515597214]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2376685973831176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2376685973831176 | validation: 1.132210456358651]
	TIME [epoch: 0.689 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.222452812261584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.222452812261584 | validation: 1.1096624861464246]
	TIME [epoch: 0.686 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067812625561036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2067812625561036 | validation: 1.1341982581280967]
	TIME [epoch: 0.687 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2467700421352195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2467700421352195 | validation: 1.6781746329931104]
	TIME [epoch: 0.685 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5209685524639553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5209685524639553 | validation: 1.3557046864323976]
	TIME [epoch: 0.687 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4180277933240124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4180277933240124 | validation: 1.1602056351239076]
	TIME [epoch: 0.689 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1892879722656062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1892879722656062 | validation: 1.0009098283416378]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1279211540522387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1279211540522387 | validation: 1.0654664078792573]
	TIME [epoch: 0.69 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.102168751357365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.102168751357365 | validation: 0.9944459639472266]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0991814371763997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0991814371763997 | validation: 1.0467977026186932]
	TIME [epoch: 0.69 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0901812777328086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0901812777328086 | validation: 1.0826861548988407]
	TIME [epoch: 0.689 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1026249635160892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1026249635160892 | validation: 0.9696547904895545]
	TIME [epoch: 0.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1451682928311586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1451682928311586 | validation: 1.4597347804990772]
	TIME [epoch: 0.687 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2499267390356483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2499267390356483 | validation: 0.7125214090072656]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075752576206228		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.1075752576206228 | validation: 0.8978865910026279]
	TIME [epoch: 0.691 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0794928410039142		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 1.0794928410039142 | validation: 1.2253433936473543]
	TIME [epoch: 0.688 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1111736421781913		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.1111736421781913 | validation: 0.8400341519529075]
	TIME [epoch: 0.688 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0012295521319348		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0012295521319348 | validation: 0.8467873406476493]
	TIME [epoch: 0.689 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9961174102679298		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.9961174102679298 | validation: 0.9555488814150004]
	TIME [epoch: 0.687 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9970283155633799		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9970283155633799 | validation: 0.9644715546428122]
	TIME [epoch: 0.688 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.989707107182422		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.989707107182422 | validation: 0.7784020765265273]
	TIME [epoch: 0.685 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.011767990316721		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 1.011767990316721 | validation: 1.099179171787813]
	TIME [epoch: 0.688 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0338894306645177		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 1.0338894306645177 | validation: 0.7521008732082084]
	TIME [epoch: 0.688 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0661788537470154		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 1.0661788537470154 | validation: 0.8527420119189918]
	TIME [epoch: 0.687 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9571753714210997		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.9571753714210997 | validation: 1.0708027976554209]
	TIME [epoch: 0.687 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9782505005406329		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9782505005406329 | validation: 0.6835213702715172]
	TIME [epoch: 0.687 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9798057895282394		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9798057895282394 | validation: 0.8476069119487945]
	TIME [epoch: 0.688 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9211230435546116		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 0.9211230435546116 | validation: 0.9438390000205275]
	TIME [epoch: 0.689 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9324161407769043		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.9324161407769043 | validation: 0.7578329863499951]
	TIME [epoch: 0.688 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9329597366591221		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.9329597366591221 | validation: 0.9168841869422701]
	TIME [epoch: 0.689 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9242834022346997		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.9242834022346997 | validation: 0.809680541175214]
	TIME [epoch: 0.687 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9251671266817184		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 0.9251671266817184 | validation: 1.000333525233313]
	TIME [epoch: 0.69 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9780983302207884		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.9780983302207884 | validation: 0.9271113271107055]
	TIME [epoch: 0.688 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9759722294562173		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 0.9759722294562173 | validation: 0.8170607973833932]
	TIME [epoch: 0.687 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0134785224640286		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 1.0134785224640286 | validation: 0.8882160183512685]
	TIME [epoch: 0.687 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9048647482735233		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9048647482735233 | validation: 0.7558523743209806]
	TIME [epoch: 0.688 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8881992676106992		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.8881992676106992 | validation: 0.8341660126447895]
	TIME [epoch: 0.688 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8809364003045904		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8809364003045904 | validation: 0.7102521023868111]
	TIME [epoch: 0.688 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.880542583743588		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.880542583743588 | validation: 0.8881300168201549]
	TIME [epoch: 0.686 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870689086225003		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8870689086225003 | validation: 0.665258525252201]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998246303420356		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8998246303420356 | validation: 1.017922154865033]
	TIME [epoch: 0.688 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.932684553854914		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.932684553854914 | validation: 0.6054708889014755]
	TIME [epoch: 0.691 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761763815202587		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.9761763815202587 | validation: 0.9507895972342155]
	TIME [epoch: 0.689 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9255669110529859		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.9255669110529859 | validation: 0.739305235441656]
	TIME [epoch: 0.69 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.941989563751174		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.941989563751174 | validation: 0.7945406383056044]
	TIME [epoch: 0.691 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269552034437715		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.9269552034437715 | validation: 0.9640825362330683]
	TIME [epoch: 0.689 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9438975771231524		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.9438975771231524 | validation: 0.7451775511614445]
	TIME [epoch: 0.691 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8563710868180047		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8563710868180047 | validation: 0.6256265413816773]
	TIME [epoch: 0.688 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8771172639509414		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.8771172639509414 | validation: 0.8592559455525118]
	TIME [epoch: 0.691 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672270411699816		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.8672270411699816 | validation: 0.6665750863620589]
	TIME [epoch: 0.689 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8632043285346614		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.8632043285346614 | validation: 0.7791010494670741]
	TIME [epoch: 0.688 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8446172801934931		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.8446172801934931 | validation: 0.7204016170608837]
	TIME [epoch: 0.688 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8450547537047076		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.8450547537047076 | validation: 0.8015109450610298]
	TIME [epoch: 0.69 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778555907336248		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8778555907336248 | validation: 0.8367685664529101]
	TIME [epoch: 0.687 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0585645169993685		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 1.0585645169993685 | validation: 0.8636560830876712]
	TIME [epoch: 0.692 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284762094867517		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.9284762094867517 | validation: 0.8277998421797599]
	TIME [epoch: 0.69 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841512593977755		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.8841512593977755 | validation: 0.6647927915836616]
	TIME [epoch: 0.69 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392317786742898		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.8392317786742898 | validation: 0.7451555273161087]
	TIME [epoch: 0.69 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417735521071569		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.8417735521071569 | validation: 0.671534721204017]
	TIME [epoch: 0.69 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491705274231007		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8491705274231007 | validation: 0.7825795511470126]
	TIME [epoch: 0.69 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585317528727998		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8585317528727998 | validation: 0.7122629974122645]
	TIME [epoch: 0.69 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791376283308157		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8791376283308157 | validation: 0.7876517184727307]
	TIME [epoch: 0.689 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8755311237712815		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8755311237712815 | validation: 0.744085719931081]
	TIME [epoch: 0.69 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897371928133133		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.897371928133133 | validation: 0.7491397436867109]
	TIME [epoch: 0.69 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624534056749719		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8624534056749719 | validation: 0.8939844245970052]
	TIME [epoch: 0.69 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886211416741017		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8886211416741017 | validation: 0.6505846519782686]
	TIME [epoch: 0.69 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8521158553605679		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.8521158553605679 | validation: 0.8738805226812387]
	TIME [epoch: 0.689 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86510625287901		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.86510625287901 | validation: 0.597534384182828]
	TIME [epoch: 0.689 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843435600429419		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.843435600429419 | validation: 0.7903165868293979]
	TIME [epoch: 0.689 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458660474408225		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8458660474408225 | validation: 0.6412336909019297]
	TIME [epoch: 0.69 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001877916250952		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.9001877916250952 | validation: 0.757789876668808]
	TIME [epoch: 0.689 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8643278608406576		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8643278608406576 | validation: 0.7989142525222315]
	TIME [epoch: 0.688 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656859315592388		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8656859315592388 | validation: 0.6719960675699359]
	TIME [epoch: 0.689 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377506667468038		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8377506667468038 | validation: 0.8151895596724809]
	TIME [epoch: 0.688 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375160633422388		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.8375160633422388 | validation: 0.6605993487857978]
	TIME [epoch: 0.689 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8171268246601214		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8171268246601214 | validation: 0.692244802862711]
	TIME [epoch: 0.688 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7974783932535979		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.7974783932535979 | validation: 0.6736713093577738]
	TIME [epoch: 0.689 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006392658268914		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8006392658268914 | validation: 0.6964740953347134]
	TIME [epoch: 0.69 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216283772811417		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8216283772811417 | validation: 0.7979951275893469]
	TIME [epoch: 0.686 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268171713972467		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.9268171713972467 | validation: 0.8264881304120281]
	TIME [epoch: 0.689 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0561969837762972		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.0561969837762972 | validation: 0.6446711778635925]
	TIME [epoch: 0.689 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939430543586781		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.7939430543586781 | validation: 0.8464294312370542]
	TIME [epoch: 0.689 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728719704591175		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8728719704591175 | validation: 0.5679705095696596]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066779958582809		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.9066779958582809 | validation: 0.730330959442548]
	TIME [epoch: 0.685 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886039293431597		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7886039293431597 | validation: 0.7366561681002072]
	TIME [epoch: 0.689 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8376837443342655		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8376837443342655 | validation: 0.6806872324401404]
	TIME [epoch: 0.689 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320421587769745		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.8320421587769745 | validation: 0.6488771581909125]
	TIME [epoch: 0.689 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906932610449457		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.7906932610449457 | validation: 0.8044433714309212]
	TIME [epoch: 0.691 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130224071832438		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.8130224071832438 | validation: 0.6232645013066809]
	TIME [epoch: 0.69 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8468053760019734		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.8468053760019734 | validation: 0.8846189570307255]
	TIME [epoch: 0.691 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471623046456679		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.8471623046456679 | validation: 0.6050512066990346]
	TIME [epoch: 0.689 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.78155321777728		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.78155321777728 | validation: 0.5975391935671647]
	TIME [epoch: 0.69 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794382440135149		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7794382440135149 | validation: 0.6952991155297411]
	TIME [epoch: 0.688 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906711541916731		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.7906711541916731 | validation: 0.5751464297539848]
	TIME [epoch: 0.69 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8059149549235809		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8059149549235809 | validation: 0.7106674367624324]
	TIME [epoch: 0.689 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8180901805091847		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8180901805091847 | validation: 0.7882846088177574]
	TIME [epoch: 0.689 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077222329740279		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9077222329740279 | validation: 0.7188073379504363]
	TIME [epoch: 0.691 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8775241566398275		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8775241566398275 | validation: 0.8384579255054249]
	TIME [epoch: 0.69 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385569683827879		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8385569683827879 | validation: 0.619834416364579]
	TIME [epoch: 0.688 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598945229082718		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7598945229082718 | validation: 0.6271626949389573]
	TIME [epoch: 0.689 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889663861209841		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7889663861209841 | validation: 0.700952776842112]
	TIME [epoch: 0.69 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978073771023851		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.7978073771023851 | validation: 0.6440869397868413]
	TIME [epoch: 0.692 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7755464180757329		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.7755464180757329 | validation: 0.5948175183882519]
	TIME [epoch: 0.692 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7766596114267174		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7766596114267174 | validation: 0.6987421028998324]
	TIME [epoch: 0.69 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875164345706014		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7875164345706014 | validation: 0.5574894609704971]
	TIME [epoch: 0.692 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084132585902296		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8084132585902296 | validation: 0.7239309783646736]
	TIME [epoch: 0.688 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7968547852445491		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.7968547852445491 | validation: 0.5756557147924385]
	TIME [epoch: 0.692 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7880059693007305		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7880059693007305 | validation: 0.6445989643887782]
	TIME [epoch: 0.69 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7615006616192259		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7615006616192259 | validation: 0.7527415724788478]
	TIME [epoch: 0.689 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791709640671381		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.791709640671381 | validation: 0.7609141202937932]
	TIME [epoch: 0.69 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964869778190189		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8964869778190189 | validation: 0.9222841001136468]
	TIME [epoch: 0.689 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666399333112367		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8666399333112367 | validation: 0.6562731896688935]
	TIME [epoch: 0.69 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7506476591167938		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.7506476591167938 | validation: 0.5680329239248568]
	TIME [epoch: 0.689 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855836314023523		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.7855836314023523 | validation: 0.7300592545357376]
	TIME [epoch: 0.688 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640997876262254		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7640997876262254 | validation: 0.6084594103466912]
	TIME [epoch: 0.691 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362599976481571		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7362599976481571 | validation: 0.5502371687719974]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350881028336858		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.7350881028336858 | validation: 0.6544580823745194]
	TIME [epoch: 0.691 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7346239534507035		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7346239534507035 | validation: 0.5610035489126689]
	TIME [epoch: 0.692 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380350808092127		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7380350808092127 | validation: 0.7167684070190208]
	TIME [epoch: 0.69 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534054113763082		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7534054113763082 | validation: 0.566008960723872]
	TIME [epoch: 0.693 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505728620149854		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.7505728620149854 | validation: 0.7325730079579069]
	TIME [epoch: 0.69 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648345558234215		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7648345558234215 | validation: 0.5217357798985564]
	TIME [epoch: 0.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7936197301923811		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.7936197301923811 | validation: 0.6930683300080098]
	TIME [epoch: 0.689 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822165070640472		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.822165070640472 | validation: 0.8175662340103278]
	TIME [epoch: 0.689 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9168733236208894		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.9168733236208894 | validation: 0.6181106472284519]
	TIME [epoch: 0.692 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7918718171491814		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7918718171491814 | validation: 0.6303603848103638]
	TIME [epoch: 0.689 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7337669249870683		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7337669249870683 | validation: 0.5837180210030601]
	TIME [epoch: 0.69 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214834710674886		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7214834710674886 | validation: 0.5829558541707636]
	TIME [epoch: 0.69 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221460260747928		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7221460260747928 | validation: 0.6473609233359374]
	TIME [epoch: 0.689 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7407869451380795		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7407869451380795 | validation: 0.6020038941675896]
	TIME [epoch: 0.689 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401538209111722		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7401538209111722 | validation: 0.6719973776725444]
	TIME [epoch: 0.691 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654101768510537		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.7654101768510537 | validation: 0.6287954167313151]
	TIME [epoch: 0.688 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7761787320929097		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7761787320929097 | validation: 0.6432112572951649]
	TIME [epoch: 0.691 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636927979866802		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7636927979866802 | validation: 0.5860166888741178]
	TIME [epoch: 0.689 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7271268776479266		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7271268776479266 | validation: 0.5907106653476516]
	TIME [epoch: 0.69 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7078103715499922		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7078103715499922 | validation: 0.5726883171555871]
	TIME [epoch: 0.689 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7036794073881972		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7036794073881972 | validation: 0.6135525813652256]
	TIME [epoch: 0.689 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107128847369433		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7107128847369433 | validation: 0.590119262265503]
	TIME [epoch: 0.691 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7161912959875707		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7161912959875707 | validation: 0.7618167167218677]
	TIME [epoch: 0.69 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684795016751952		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7684795016751952 | validation: 0.6342620257824687]
	TIME [epoch: 0.687 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151366352538918		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.8151366352538918 | validation: 0.6669730849246636]
	TIME [epoch: 0.69 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773481398606986		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.773481398606986 | validation: 0.6359807202627055]
	TIME [epoch: 0.691 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7255775834237254		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7255775834237254 | validation: 0.5160131770066212]
	TIME [epoch: 0.688 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257961439287087		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7257961439287087 | validation: 0.6230502646768247]
	TIME [epoch: 0.69 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699078673998324		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.699078673998324 | validation: 0.5535281552481852]
	TIME [epoch: 0.703 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884953037884491		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6884953037884491 | validation: 0.5703478831029015]
	TIME [epoch: 0.691 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813857195809865		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6813857195809865 | validation: 0.5459403826730139]
	TIME [epoch: 0.689 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6858648727290501		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.6858648727290501 | validation: 0.5763840937277587]
	TIME [epoch: 0.689 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949336489863199		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6949336489863199 | validation: 0.5845330018420533]
	TIME [epoch: 0.688 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459544020332994		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7459544020332994 | validation: 0.7653387223553756]
	TIME [epoch: 0.691 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248306033638377		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.8248306033638377 | validation: 0.5666284963525986]
	TIME [epoch: 0.688 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7385791875332361		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7385791875332361 | validation: 0.548725541725152]
	TIME [epoch: 0.702 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679072942568144		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.679072942568144 | validation: 0.5647002816895383]
	TIME [epoch: 0.692 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759655502143997		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.6759655502143997 | validation: 0.5488240809840889]
	TIME [epoch: 0.686 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6894233177023164		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6894233177023164 | validation: 0.6262419186188778]
	TIME [epoch: 0.686 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001700437118801		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7001700437118801 | validation: 0.5500043059632859]
	TIME [epoch: 0.687 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7088199970992832		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.7088199970992832 | validation: 0.6554702652741857]
	TIME [epoch: 0.688 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7061614621621286		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.7061614621621286 | validation: 0.5014469066071506]
	TIME [epoch: 0.686 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859454204176606		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6859454204176606 | validation: 0.5800921404014691]
	TIME [epoch: 0.688 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666977533978239		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.666977533978239 | validation: 0.5164357186551181]
	TIME [epoch: 0.69 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679559656586542		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.6679559656586542 | validation: 0.6013823911043716]
	TIME [epoch: 0.689 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674313697626257		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.674313697626257 | validation: 0.5181080756412515]
	TIME [epoch: 0.689 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6847057395863172		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6847057395863172 | validation: 0.6221630369293942]
	TIME [epoch: 0.687 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6963153209415778		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.6963153209415778 | validation: 0.5442192531335571]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7250531086648075		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.7250531086648075 | validation: 0.6192964850685189]
	TIME [epoch: 1.36 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200082435383061		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.7200082435383061 | validation: 0.5635116893990112]
	TIME [epoch: 1.34 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803878000955996		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6803878000955996 | validation: 0.49021734047852183]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6742315959347133		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.6742315959347133 | validation: 0.5971203597329696]
	TIME [epoch: 1.34 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721150558547785		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.6721150558547785 | validation: 0.48991331189448445]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612694504221551		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6612694504221551 | validation: 0.5641379638630132]
	TIME [epoch: 1.34 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6607214013940959		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.6607214013940959 | validation: 0.5075088469959255]
	TIME [epoch: 1.34 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6712256374791079		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6712256374791079 | validation: 0.534155144743292]
	TIME [epoch: 1.34 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725960412473981		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6725960412473981 | validation: 0.5078502593495948]
	TIME [epoch: 1.34 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739420792258392		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.6739420792258392 | validation: 0.574615096605141]
	TIME [epoch: 1.34 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6728480152941151		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.6728480152941151 | validation: 0.5053503619852037]
	TIME [epoch: 1.34 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6780058980177855		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.6780058980177855 | validation: 0.5880109793052802]
	TIME [epoch: 1.34 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665509712212537		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.665509712212537 | validation: 0.4788108696921251]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467154602417441		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.6467154602417441 | validation: 0.5760128440790613]
	TIME [epoch: 1.34 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6489545260526128		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.6489545260526128 | validation: 0.4770897344420882]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521190693750876		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6521190693750876 | validation: 0.5480748583299181]
	TIME [epoch: 1.34 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6500774731688606		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6500774731688606 | validation: 0.46716251981623574]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6399527934659031		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.6399527934659031 | validation: 0.5543632683813682]
	TIME [epoch: 1.35 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6386217123438248		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.6386217123438248 | validation: 0.4959129742971977]
	TIME [epoch: 1.34 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6574016867863657		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.6574016867863657 | validation: 0.6068234910922672]
	TIME [epoch: 1.34 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6873289685391396		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.6873289685391396 | validation: 0.5300794369382921]
	TIME [epoch: 1.34 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7150500180811074		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7150500180811074 | validation: 0.5692345113811975]
	TIME [epoch: 1.35 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6653360520162059		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.6653360520162059 | validation: 0.5170373784590055]
	TIME [epoch: 1.35 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6307425340042851		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.6307425340042851 | validation: 0.4782726094722207]
	TIME [epoch: 1.35 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318276421800139		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.6318276421800139 | validation: 0.5975518403350268]
	TIME [epoch: 1.35 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493708459195068		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.6493708459195068 | validation: 0.4643104694551335]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629196464287824		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.6629196464287824 | validation: 0.5795309284267784]
	TIME [epoch: 1.34 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431061222823822		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.6431061222823822 | validation: 0.4753934393736172]
	TIME [epoch: 1.34 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237600917045711		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.6237600917045711 | validation: 0.4768505096779181]
	TIME [epoch: 1.33 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621962906841459		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.621962906841459 | validation: 0.5421693573048418]
	TIME [epoch: 1.34 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271510657460952		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6271510657460952 | validation: 0.45834840620685013]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6223033909124785		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.6223033909124785 | validation: 0.523088318721636]
	TIME [epoch: 1.33 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6178952222063591		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6178952222063591 | validation: 0.4940594720212273]
	TIME [epoch: 1.34 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616904031910367		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.616904031910367 | validation: 0.47478035034867666]
	TIME [epoch: 1.33 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6219962986277797		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.6219962986277797 | validation: 0.6191045710964846]
	TIME [epoch: 1.33 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6693799933789836		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.6693799933789836 | validation: 0.5124733838473934]
	TIME [epoch: 1.33 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7017109028263773		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7017109028263773 | validation: 0.491566219255269]
	TIME [epoch: 1.33 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163349985494816		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.6163349985494816 | validation: 0.5321038977463972]
	TIME [epoch: 1.33 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6151460898135285		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.6151460898135285 | validation: 0.4518033525625684]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6238133159080588		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.6238133159080588 | validation: 0.5640449753480377]
	TIME [epoch: 1.34 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6141523446462306		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.6141523446462306 | validation: 0.4590107866624007]
	TIME [epoch: 1.34 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043867745380497		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.6043867745380497 | validation: 0.47713781483823425]
	TIME [epoch: 1.34 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5913513429724103		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.5913513429724103 | validation: 0.46713223383571995]
	TIME [epoch: 1.34 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.595081377206778		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.595081377206778 | validation: 0.48602271381952245]
	TIME [epoch: 1.34 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855511553511191		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.5855511553511191 | validation: 0.49876368221105116]
	TIME [epoch: 1.34 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855085355697324		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.5855085355697324 | validation: 0.4641274822732365]
	TIME [epoch: 1.34 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.584325269170913		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.584325269170913 | validation: 0.5027049395586837]
	TIME [epoch: 1.35 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5972324764928288		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.5972324764928288 | validation: 0.6042429399109006]
	TIME [epoch: 1.34 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6328084249190341		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.6328084249190341 | validation: 0.5378165172327033]
	TIME [epoch: 1.34 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935039672739995		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.6935039672739995 | validation: 0.4873480856009993]
	TIME [epoch: 1.34 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5810264954370843		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.5810264954370843 | validation: 0.521824804641208]
	TIME [epoch: 1.34 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5971250448523463		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.5971250448523463 | validation: 0.46572402593306894]
	TIME [epoch: 1.34 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6154535962525047		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.6154535962525047 | validation: 0.5872191308324233]
	TIME [epoch: 1.34 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5845791391117307		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.5845791391117307 | validation: 0.46963426760931615]
	TIME [epoch: 1.34 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5527958855672016		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.5527958855672016 | validation: 0.4561899197103591]
	TIME [epoch: 1.33 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444740605770244		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.5444740605770244 | validation: 0.5267015528374219]
	TIME [epoch: 1.34 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5492351366514074		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.5492351366514074 | validation: 0.449567766797375]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5534109596275373		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.5534109596275373 | validation: 0.5783945646857339]
	TIME [epoch: 1.34 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5712889282687393		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.5712889282687393 | validation: 0.4408936858237458]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605243087726709		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.605243087726709 | validation: 0.5186206581775029]
	TIME [epoch: 1.33 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417873905963662		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.5417873905963662 | validation: 0.4930137389620455]
	TIME [epoch: 1.34 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5367148563308818		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.5367148563308818 | validation: 0.4449586783867907]
	TIME [epoch: 1.33 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5500497403774794		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5500497403774794 | validation: 0.6375375516127628]
	TIME [epoch: 1.34 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5667193395957518		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.5667193395957518 | validation: 0.4329269811205106]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5987884906571027		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.5987884906571027 | validation: 0.4347777769026415]
	TIME [epoch: 1.34 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5128537863788561		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.5128537863788561 | validation: 0.6667017312324375]
	TIME [epoch: 1.34 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5743788099796006		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5743788099796006 | validation: 0.47338647482044743]
	TIME [epoch: 1.34 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6090715496560298		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.6090715496560298 | validation: 0.41373606424597514]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5252344532800243		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.5252344532800243 | validation: 0.6162518426337122]
	TIME [epoch: 1.35 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5614215254373943		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.5614215254373943 | validation: 0.403404368034862]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5232043949707077		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.5232043949707077 | validation: 0.4630900580121889]
	TIME [epoch: 1.34 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49220375109511644		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.49220375109511644 | validation: 0.5768375197652473]
	TIME [epoch: 1.34 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162575689612942		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.5162575689612942 | validation: 0.4035889018168436]
	TIME [epoch: 1.34 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191404124523951		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.5191404124523951 | validation: 0.45826686619898505]
	TIME [epoch: 1.34 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4769853331559301		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.4769853331559301 | validation: 0.4968236504338111]
	TIME [epoch: 1.34 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47101001484787236		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.47101001484787236 | validation: 0.40116942535889105]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48897074760084097		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.48897074760084097 | validation: 0.580651973510926]
	TIME [epoch: 1.34 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4988349508558775		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.4988349508558775 | validation: 0.4256187241004744]
	TIME [epoch: 1.34 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571614892905948		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.5571614892905948 | validation: 0.39431947960964675]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4491901133145542		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.4491901133145542 | validation: 0.6559968863788683]
	TIME [epoch: 1.34 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5260781829252972		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.5260781829252972 | validation: 0.4454350429216112]
	TIME [epoch: 1.35 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5900169145779038		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.5900169145779038 | validation: 0.3513145202335138]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_283.pth
	Model improved!!!
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4813430005200938		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.4813430005200938 | validation: 0.6449268369181941]
	TIME [epoch: 1.35 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5643317017707904		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.5643317017707904 | validation: 0.3767947542636677]
	TIME [epoch: 1.37 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619421818823816		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.4619421818823816 | validation: 0.3880262533827719]
	TIME [epoch: 1.35 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393863391947915		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.4393863391947915 | validation: 0.4952993960747207]
	TIME [epoch: 1.35 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44373712904468887		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.44373712904468887 | validation: 0.3695935442693823]
	TIME [epoch: 1.35 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43316027837436993		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.43316027837436993 | validation: 0.4879645804039477]
	TIME [epoch: 1.35 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41743553971910563		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.41743553971910563 | validation: 0.35559496432372195]
	TIME [epoch: 1.34 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4319058017651312		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.4319058017651312 | validation: 0.5363505577195101]
	TIME [epoch: 1.34 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4299197731433169		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.4299197731433169 | validation: 0.3557304761777165]
	TIME [epoch: 1.34 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5234108060072168		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.5234108060072168 | validation: 0.3821505258670208]
	TIME [epoch: 1.34 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38156515095697996		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.38156515095697996 | validation: 0.5624901301878334]
	TIME [epoch: 1.34 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45284149015375136		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.45284149015375136 | validation: 0.4295315564914959]
	TIME [epoch: 1.34 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.59401907115379		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.59401907115379 | validation: 0.2773186635221339]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44382621636801506		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.44382621636801506 | validation: 0.6557848719848578]
	TIME [epoch: 1.34 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518155058253715		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.518155058253715 | validation: 0.35808306437284443]
	TIME [epoch: 1.34 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40584275764459976		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.40584275764459976 | validation: 0.3264366514974877]
	TIME [epoch: 1.34 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3791113885524205		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.3791113885524205 | validation: 0.5044335797747136]
	TIME [epoch: 1.34 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40604708650880467		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.40604708650880467 | validation: 0.36057438868116165]
	TIME [epoch: 1.34 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42567346772413983		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.42567346772413983 | validation: 0.38100396385968766]
	TIME [epoch: 1.34 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594652108581498		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.3594652108581498 | validation: 0.3821708624270393]
	TIME [epoch: 1.34 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33416523334994885		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.33416523334994885 | validation: 0.33048845338777594]
	TIME [epoch: 1.34 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3384632589580163		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.3384632589580163 | validation: 0.48581277663975286]
	TIME [epoch: 1.35 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4016228601010541		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.4016228601010541 | validation: 0.34786872424043963]
	TIME [epoch: 1.34 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5159702857235396		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.5159702857235396 | validation: 0.33683115312854306]
	TIME [epoch: 1.34 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34796715330545325		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.34796715330545325 | validation: 0.6683031926366976]
	TIME [epoch: 1.35 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6077307698485395		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6077307698485395 | validation: 0.2699760534816284]
	TIME [epoch: 1.35 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3428817213442379		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.3428817213442379 | validation: 0.35576402522252704]
	TIME [epoch: 1.35 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30382629044985915		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.30382629044985915 | validation: 0.37170066806365576]
	TIME [epoch: 1.34 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3009476145433982		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.3009476145433982 | validation: 0.2748770994712479]
	TIME [epoch: 1.35 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32986328714704827		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.32986328714704827 | validation: 0.4562624557405984]
	TIME [epoch: 1.34 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34094730744247415		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.34094730744247415 | validation: 0.26614575492842446]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38607208423168926		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.38607208423168926 | validation: 0.36497737627022464]
	TIME [epoch: 1.33 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28209844478445373		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.28209844478445373 | validation: 0.2863976250979614]
	TIME [epoch: 1.33 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28973361798423974		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.28973361798423974 | validation: 0.4272150366890373]
	TIME [epoch: 1.33 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33684622677894815		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.33684622677894815 | validation: 0.2704280412301595]
	TIME [epoch: 1.33 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41817247499773097		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.41817247499773097 | validation: 0.3270290193710045]
	TIME [epoch: 1.33 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2543646081354358		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.2543646081354358 | validation: 0.3516400073296966]
	TIME [epoch: 1.32 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.254688384651046		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.254688384651046 | validation: 0.2502052425391263]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3289740271793122		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.3289740271793122 | validation: 0.44626216249270867]
	TIME [epoch: 1.34 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34930675902967595		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.34930675902967595 | validation: 0.27340579337512616]
	TIME [epoch: 1.34 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3367400772499663		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.3367400772499663 | validation: 0.2805658657851967]
	TIME [epoch: 1.34 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546977679698458		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.2546977679698458 | validation: 0.30012822451588284]
	TIME [epoch: 1.34 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2357977941027246		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.2357977941027246 | validation: 0.29761278357871657]
	TIME [epoch: 1.34 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.233380187340279		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.233380187340279 | validation: 0.23140902262406204]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2964408231360377		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.2964408231360377 | validation: 0.46140108606608143]
	TIME [epoch: 1.33 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3305383466812515		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.3305383466812515 | validation: 0.23382764344433557]
	TIME [epoch: 1.34 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30598232513978146		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.30598232513978146 | validation: 0.3234283656795244]
	TIME [epoch: 1.33 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25821891189833646		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.25821891189833646 | validation: 0.23032172539342088]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_331.pth
	Model improved!!!
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24438116790194042		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.24438116790194042 | validation: 0.3199823189184581]
	TIME [epoch: 1.34 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24031285979923858		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.24031285979923858 | validation: 0.23597311347852312]
	TIME [epoch: 1.34 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2531830211785077		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.2531830211785077 | validation: 0.33424304621081635]
	TIME [epoch: 1.34 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.239200260602057		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.239200260602057 | validation: 0.20582376029811297]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23168091635404836		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.23168091635404836 | validation: 0.3862243974089776]
	TIME [epoch: 1.34 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2546932357919722		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.2546932357919722 | validation: 0.22121594437032585]
	TIME [epoch: 1.34 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26682010033995657		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.26682010033995657 | validation: 0.30574293660102697]
	TIME [epoch: 1.34 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2164689116922502		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.2164689116922502 | validation: 0.22246559712192263]
	TIME [epoch: 1.34 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20074119591776485		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.20074119591776485 | validation: 0.3106562771652861]
	TIME [epoch: 1.34 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21925802104851855		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.21925802104851855 | validation: 0.2759529780498605]
	TIME [epoch: 1.34 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3104577225997363		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.3104577225997363 | validation: 0.27060443460729167]
	TIME [epoch: 1.34 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282809450124296		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.2282809450124296 | validation: 0.2223159323482976]
	TIME [epoch: 1.34 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17204715941859655		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.17204715941859655 | validation: 0.2762090693477613]
	TIME [epoch: 1.34 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16556766548551458		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.16556766548551458 | validation: 0.174032956703972]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_345.pth
	Model improved!!!
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17415701822898708		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.17415701822898708 | validation: 0.3233736514310237]
	TIME [epoch: 1.34 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2029019140654008		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.2029019140654008 | validation: 0.2154517527436708]
	TIME [epoch: 1.34 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30371664591033515		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.30371664591033515 | validation: 0.28307085654920733]
	TIME [epoch: 1.34 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20502479576652427		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.20502479576652427 | validation: 0.22353262035748137]
	TIME [epoch: 1.34 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20501374278170634		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20501374278170634 | validation: 0.2704442616368297]
	TIME [epoch: 1.34 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20551027093615695		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.20551027093615695 | validation: 0.205421465537189]
	TIME [epoch: 1.34 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20154415276217216		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.20154415276217216 | validation: 0.2981588920865213]
	TIME [epoch: 1.34 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062699803996386		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.2062699803996386 | validation: 0.19335113717326344]
	TIME [epoch: 1.34 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20516394449459951		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.20516394449459951 | validation: 0.2645299788936405]
	TIME [epoch: 1.34 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18449938822515688		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.18449938822515688 | validation: 0.21028995750772364]
	TIME [epoch: 1.34 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16590123551223576		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.16590123551223576 | validation: 0.3254851257496539]
	TIME [epoch: 1.34 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2165804467537287		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.2165804467537287 | validation: 0.1972841903144909]
	TIME [epoch: 1.34 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18205949837820093		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.18205949837820093 | validation: 0.28741356618744285]
	TIME [epoch: 1.34 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1917133480091583		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.1917133480091583 | validation: 0.21917993091076263]
	TIME [epoch: 1.34 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16866927168708457		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.16866927168708457 | validation: 0.2224685061505035]
	TIME [epoch: 1.34 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738284875167041		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.1738284875167041 | validation: 0.2396876487116428]
	TIME [epoch: 1.34 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16223223637738862		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.16223223637738862 | validation: 0.2142571902158984]
	TIME [epoch: 1.34 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15765572107661396		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.15765572107661396 | validation: 0.25464291915413695]
	TIME [epoch: 1.33 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21574368572935393		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.21574368572935393 | validation: 0.29152582892706075]
	TIME [epoch: 1.33 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20519417615313176		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.20519417615313176 | validation: 0.1617027850121957]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15849094567636043		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.15849094567636043 | validation: 0.2802128987831436]
	TIME [epoch: 1.33 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14360615606446653		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.14360615606446653 | validation: 0.14709674665918135]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13224929255802603		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.13224929255802603 | validation: 0.23388092551110123]
	TIME [epoch: 1.32 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1346231842416924		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.1346231842416924 | validation: 0.190238554722656]
	TIME [epoch: 1.33 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17626332059273123		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.17626332059273123 | validation: 0.3513069785641706]
	TIME [epoch: 1.33 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564353400344912		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.2564353400344912 | validation: 0.17804327811474693]
	TIME [epoch: 1.33 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14142629263162787		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.14142629263162787 | validation: 0.2256075930620384]
	TIME [epoch: 1.33 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12744652852716495		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.12744652852716495 | validation: 0.1629455415744784]
	TIME [epoch: 1.33 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12143650043901211		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.12143650043901211 | validation: 0.20860067880532807]
	TIME [epoch: 1.33 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11480266774989492		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.11480266774989492 | validation: 0.1738107504822881]
	TIME [epoch: 1.33 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12809978626365992		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.12809978626365992 | validation: 0.21519187840293086]
	TIME [epoch: 1.33 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15611095951616058		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.15611095951616058 | validation: 0.22699468355060134]
	TIME [epoch: 1.33 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19230258187581656		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.19230258187581656 | validation: 0.268124295970044]
	TIME [epoch: 1.33 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19733256817785974		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.19733256817785974 | validation: 0.15724534125591305]
	TIME [epoch: 1.33 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11618390157028202		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.11618390157028202 | validation: 0.21594415233784786]
	TIME [epoch: 1.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11170885219220704		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.11170885219220704 | validation: 0.14473544245583364]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_381.pth
	Model improved!!!
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11961942116070372		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.11961942116070372 | validation: 0.24037824186298007]
	TIME [epoch: 1.33 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12527726535106995		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.12527726535106995 | validation: 0.14646612610653525]
	TIME [epoch: 1.33 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1080122143549046		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1080122143549046 | validation: 0.22516176619560802]
	TIME [epoch: 1.33 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13833220782249622		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.13833220782249622 | validation: 0.21587165635206884]
	TIME [epoch: 1.33 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30873700378824603		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.30873700378824603 | validation: 0.2116949947196597]
	TIME [epoch: 1.33 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1124610173711735		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.1124610173711735 | validation: 0.1641637041185883]
	TIME [epoch: 1.33 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510862690209154		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.09510862690209154 | validation: 0.15229860382350277]
	TIME [epoch: 1.33 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256714660461799		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.10256714660461799 | validation: 0.25871996210091114]
	TIME [epoch: 1.33 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551708939996369		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.1551708939996369 | validation: 0.16211320660439862]
	TIME [epoch: 1.33 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2263025730888152		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.2263025730888152 | validation: 0.2847279882699793]
	TIME [epoch: 1.33 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1482003911454498		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.1482003911454498 | validation: 0.15136713398480592]
	TIME [epoch: 1.34 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09402096091490089		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.09402096091490089 | validation: 0.13960037536809614]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203683963588073		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.09203683963588073 | validation: 0.19936885228069068]
	TIME [epoch: 1.34 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337672560653026		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.09337672560653026 | validation: 0.14864233218765185]
	TIME [epoch: 1.33 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09643451680103292		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.09643451680103292 | validation: 0.20777029822820214]
	TIME [epoch: 1.33 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11081797667816425		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.11081797667816425 | validation: 0.17456610603236375]
	TIME [epoch: 1.33 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1412759743348409		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.1412759743348409 | validation: 0.29631874332092895]
	TIME [epoch: 1.33 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26582648569160866		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.26582648569160866 | validation: 0.15722690904332387]
	TIME [epoch: 1.33 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12697192793084117		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.12697192793084117 | validation: 0.19356139938762684]
	TIME [epoch: 1.34 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09518603300271179		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.09518603300271179 | validation: 0.1585111392325673]
	TIME [epoch: 1.33 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0947031887924598		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.0947031887924598 | validation: 0.15962896275880412]
	TIME [epoch: 1.34 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11725742582410323		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.11725742582410323 | validation: 0.24936632783386226]
	TIME [epoch: 1.33 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13823045124146582		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.13823045124146582 | validation: 0.14840805154041736]
	TIME [epoch: 1.33 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09498277241484886		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.09498277241484886 | validation: 0.14304535903469948]
	TIME [epoch: 1.33 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08590405918794122		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.08590405918794122 | validation: 0.16428467226152677]
	TIME [epoch: 1.33 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0888278698740266		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.0888278698740266 | validation: 0.15213174183771785]
	TIME [epoch: 1.33 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09327210973387218		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.09327210973387218 | validation: 0.1476974223145006]
	TIME [epoch: 1.33 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162641266488062		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.10162641266488062 | validation: 0.3013979598806601]
	TIME [epoch: 1.33 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28364728353859475		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.28364728353859475 | validation: 0.11968041219334653]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10127988070935988		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.10127988070935988 | validation: 0.2454930999289153]
	TIME [epoch: 1.32 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12451319200703118		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.12451319200703118 | validation: 0.15856086676058279]
	TIME [epoch: 1.33 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11237035799706896		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.11237035799706896 | validation: 0.13032334810016005]
	TIME [epoch: 1.33 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13970907332594373		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.13970907332594373 | validation: 0.23679227064291997]
	TIME [epoch: 1.33 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12348234831373008		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.12348234831373008 | validation: 0.14516013621744794]
	TIME [epoch: 1.34 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08448398322748736		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.08448398322748736 | validation: 0.15154430692434054]
	TIME [epoch: 1.33 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08661860180676109		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.08661860180676109 | validation: 0.13300345776235223]
	TIME [epoch: 1.33 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08564216293317081		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.08564216293317081 | validation: 0.1649269540061114]
	TIME [epoch: 1.33 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08777182850944495		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.08777182850944495 | validation: 0.13804499159947925]
	TIME [epoch: 1.34 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11045494984246927		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.11045494984246927 | validation: 0.258697218482484]
	TIME [epoch: 1.33 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19060411271029742		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.19060411271029742 | validation: 0.14872626224083726]
	TIME [epoch: 1.33 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08052619272515284		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.08052619272515284 | validation: 0.13430853360549994]
	TIME [epoch: 1.33 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07188412123987653		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.07188412123987653 | validation: 0.15959609472623185]
	TIME [epoch: 1.33 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08557575823118796		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.08557575823118796 | validation: 0.1376966562008746]
	TIME [epoch: 1.33 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13061602779619236		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.13061602779619236 | validation: 0.23478039427556233]
	TIME [epoch: 1.33 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15485553046616932		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.15485553046616932 | validation: 0.12074847259987083]
	TIME [epoch: 1.32 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10138657446202846		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.10138657446202846 | validation: 0.1969655345951472]
	TIME [epoch: 1.33 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0885308171940432		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.0885308171940432 | validation: 0.12565230830220317]
	TIME [epoch: 1.33 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729868382917858		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.0729868382917858 | validation: 0.13714067490137818]
	TIME [epoch: 1.34 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678224526853952		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.0678224526853952 | validation: 0.15059636094213433]
	TIME [epoch: 1.34 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08919258437887366		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.08919258437887366 | validation: 0.1327632526383627]
	TIME [epoch: 1.34 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10757971952240997		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.10757971952240997 | validation: 0.21157816422375753]
	TIME [epoch: 1.34 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11554264533264279		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.11554264533264279 | validation: 0.12455834931136384]
	TIME [epoch: 1.34 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10993642445986858		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.10993642445986858 | validation: 0.20613777657615864]
	TIME [epoch: 1.33 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11000039037545005		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.11000039037545005 | validation: 0.13718234966298362]
	TIME [epoch: 1.34 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08021145702534006		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.08021145702534006 | validation: 0.12052065224714684]
	TIME [epoch: 1.34 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814402670501259		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.06814402670501259 | validation: 0.16323603270587836]
	TIME [epoch: 1.34 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07054655520794761		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.07054655520794761 | validation: 0.1298533430198466]
	TIME [epoch: 1.34 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07075316799005625		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.07075316799005625 | validation: 0.17166480748346813]
	TIME [epoch: 1.34 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385189865209122		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.1385189865209122 | validation: 0.1445766358551072]
	TIME [epoch: 1.34 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11011418377689978		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.11011418377689978 | validation: 0.181710997106363]
	TIME [epoch: 1.34 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09900309756327824		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.09900309756327824 | validation: 0.12759544672120113]
	TIME [epoch: 1.34 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0903967679909993		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.0903967679909993 | validation: 0.245962672584972]
	TIME [epoch: 1.35 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13106137887256705		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.13106137887256705 | validation: 0.11845456371939377]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06501540876157152		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.06501540876157152 | validation: 0.12203993927890587]
	TIME [epoch: 1.34 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06412343710625466		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.06412343710625466 | validation: 0.1499360653216281]
	TIME [epoch: 1.33 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07800145603992382		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.07800145603992382 | validation: 0.12021685728684568]
	TIME [epoch: 1.34 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354991899922736		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.07354991899922736 | validation: 0.11004761545454006]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_448.pth
	Model improved!!!
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07984769954882785		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.07984769954882785 | validation: 0.14719104965476468]
	TIME [epoch: 1.34 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08154258081858362		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.08154258081858362 | validation: 0.12716986585932266]
	TIME [epoch: 1.34 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08625159234631584		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.08625159234631584 | validation: 0.13373662798633074]
	TIME [epoch: 1.35 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09571777353706046		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.09571777353706046 | validation: 0.16983045788412784]
	TIME [epoch: 1.34 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1099511874118885		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.1099511874118885 | validation: 0.11707866542254396]
	TIME [epoch: 1.34 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0758113801237074		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.0758113801237074 | validation: 0.15425769277099488]
	TIME [epoch: 1.34 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06547711172442741		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.06547711172442741 | validation: 0.10404017494840548]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05921697157170048		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.05921697157170048 | validation: 0.1264966533660011]
	TIME [epoch: 1.34 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793970929545475		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.05793970929545475 | validation: 0.0984676093182531]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061441982847862474		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.061441982847862474 | validation: 0.1575967847940101]
	TIME [epoch: 1.35 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10120361342768082		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.10120361342768082 | validation: 0.17341864605937465]
	TIME [epoch: 1.34 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13157421347886017		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.13157421347886017 | validation: 0.11285323129202114]
	TIME [epoch: 1.34 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11030499837431243		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.11030499837431243 | validation: 0.26748653885058815]
	TIME [epoch: 1.34 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13835271777635832		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.13835271777635832 | validation: 0.15358258858755103]
	TIME [epoch: 1.35 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23077293547485347		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.23077293547485347 | validation: 0.1041052099168207]
	TIME [epoch: 1.34 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10094500191463389		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.10094500191463389 | validation: 0.260814190191861]
	TIME [epoch: 1.34 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15253618784034514		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.15253618784034514 | validation: 0.16065772502399134]
	TIME [epoch: 1.35 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0664166857635636		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.0664166857635636 | validation: 0.12855862452926245]
	TIME [epoch: 1.34 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08671343856316248		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.08671343856316248 | validation: 0.11729105488912112]
	TIME [epoch: 1.34 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223498841863865		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.07223498841863865 | validation: 0.11670779738743153]
	TIME [epoch: 1.34 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056854259519237775		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.056854259519237775 | validation: 0.12013026381290158]
	TIME [epoch: 1.34 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05808427518009619		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.05808427518009619 | validation: 0.10330373690343364]
	TIME [epoch: 1.34 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700723403497838		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.05700723403497838 | validation: 0.10300167818406508]
	TIME [epoch: 1.34 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05210133160783596		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.05210133160783596 | validation: 0.10024161706137343]
	TIME [epoch: 1.34 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394131177697842		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.05394131177697842 | validation: 0.10888432892106481]
	TIME [epoch: 1.35 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05621973552909418		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.05621973552909418 | validation: 0.12109912001762578]
	TIME [epoch: 1.34 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06161806909527128		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.06161806909527128 | validation: 0.12148302157309239]
	TIME [epoch: 1.34 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07159529110054141		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.07159529110054141 | validation: 0.11962974994161837]
	TIME [epoch: 1.38 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07504593022071752		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.07504593022071752 | validation: 0.1442497040165536]
	TIME [epoch: 1.35 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09920624437971302		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.09920624437971302 | validation: 0.12358566038973057]
	TIME [epoch: 1.34 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08630904653962981		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.08630904653962981 | validation: 0.10510033448814067]
	TIME [epoch: 1.34 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06646470448000316		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.06646470448000316 | validation: 0.10624576445458918]
	TIME [epoch: 1.35 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06641643917965377		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.06641643917965377 | validation: 0.14347379897181503]
	TIME [epoch: 1.35 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07646511485848184		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.07646511485848184 | validation: 0.092521558931672]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06042775713102953		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.06042775713102953 | validation: 0.13128949504311044]
	TIME [epoch: 1.35 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06431373204673976		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.06431373204673976 | validation: 0.10682895866038333]
	TIME [epoch: 1.35 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192952542181932		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.08192952542181932 | validation: 0.18936958302411855]
	TIME [epoch: 1.34 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08956504084756788		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.08956504084756788 | validation: 0.1002048954504649]
	TIME [epoch: 1.35 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06360896529959105		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.06360896529959105 | validation: 0.11122352601960582]
	TIME [epoch: 1.35 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05849893850723783		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.05849893850723783 | validation: 0.12149126706563203]
	TIME [epoch: 1.34 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060002570018233614		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.060002570018233614 | validation: 0.12036069019147076]
	TIME [epoch: 1.35 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413475424312638		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.06413475424312638 | validation: 0.11111892293191472]
	TIME [epoch: 1.34 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08194301233664318		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.08194301233664318 | validation: 0.16056453022524764]
	TIME [epoch: 1.35 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0985155833245273		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.0985155833245273 | validation: 0.10529921326971535]
	TIME [epoch: 1.34 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0622881279391907		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.0622881279391907 | validation: 0.16297698296442437]
	TIME [epoch: 1.35 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17008811324442738		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.17008811324442738 | validation: 0.11572025531928128]
	TIME [epoch: 1.35 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05882606861485083		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.05882606861485083 | validation: 0.15494733632667265]
	TIME [epoch: 1.35 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14246595542705393		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.14246595542705393 | validation: 0.15083749002736418]
	TIME [epoch: 1.35 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727076659013258		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.06727076659013258 | validation: 0.11838983304994143]
	TIME [epoch: 1.34 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056315356328602004		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.056315356328602004 | validation: 0.10621659697311032]
	TIME [epoch: 1.35 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307399447849987		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.06307399447849987 | validation: 0.1254293237881033]
	TIME [epoch: 1.35 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05685747506962634		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.05685747506962634 | validation: 0.08672571387683672]
	TIME [epoch: 1.34 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05295634097652114		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.05295634097652114 | validation: 0.11126839889383285]
	TIME [epoch: 171 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05221199473343053		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.05221199473343053 | validation: 0.09391521531844885]
	TIME [epoch: 2.68 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04841353006161267		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.04841353006161267 | validation: 0.10689755308927969]
	TIME [epoch: 2.66 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048551531933698835		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.048551531933698835 | validation: 0.0974078871335272]
	TIME [epoch: 2.67 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0493048595335066		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.0493048595335066 | validation: 0.10216732397273204]
	TIME [epoch: 2.67 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049819654302372084		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.049819654302372084 | validation: 0.08277895827971862]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05269044648809741		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.05269044648809741 | validation: 0.14383401607731588]
	TIME [epoch: 2.66 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06372232857056874		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.06372232857056874 | validation: 0.09943209945489301]
	TIME [epoch: 2.67 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674630715825966		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.0674630715825966 | validation: 0.10344629017976846]
	TIME [epoch: 2.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12057427805060383		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.12057427805060383 | validation: 0.11362603150518902]
	TIME [epoch: 2.73 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05549910343871827		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.05549910343871827 | validation: 0.09572969405439749]
	TIME [epoch: 2.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05191778658747799		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.05191778658747799 | validation: 0.11844786348864296]
	TIME [epoch: 2.66 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659344468864759		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.0659344468864759 | validation: 0.10259605814392586]
	TIME [epoch: 2.67 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09217888962851603		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.09217888962851603 | validation: 0.22231127513552515]
	TIME [epoch: 2.66 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10596014672258448		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.10596014672258448 | validation: 0.11821139809885561]
	TIME [epoch: 2.66 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053864391821629526		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.053864391821629526 | validation: 0.08447287570410705]
	TIME [epoch: 2.66 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07041429744210112		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.07041429744210112 | validation: 0.1510499932394858]
	TIME [epoch: 2.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697024286452176		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.0697024286452176 | validation: 0.10584623688893796]
	TIME [epoch: 2.66 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05360788880954477		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.05360788880954477 | validation: 0.07136674525835615]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_519.pth
	Model improved!!!
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05250786013724595		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.05250786013724595 | validation: 0.09827198247890494]
	TIME [epoch: 2.66 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04227001287167406		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.04227001287167406 | validation: 0.10482985839575172]
	TIME [epoch: 2.66 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044823416712326924		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.044823416712326924 | validation: 0.07423943571562062]
	TIME [epoch: 2.66 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04621165074812273		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.04621165074812273 | validation: 0.315133781975794]
	TIME [epoch: 2.65 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1330290948933103		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1330290948933103 | validation: 0.15344663811494025]
	TIME [epoch: 2.67 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976526185852469		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.07976526185852469 | validation: 0.09869540242380258]
	TIME [epoch: 2.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07977543980704865		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.07977543980704865 | validation: 0.09090253674165848]
	TIME [epoch: 2.67 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07393476749202396		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.07393476749202396 | validation: 0.14666311453356382]
	TIME [epoch: 2.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06635972374339012		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.06635972374339012 | validation: 0.11496384863116227]
	TIME [epoch: 2.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04814369460326489		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.04814369460326489 | validation: 0.08432148798971181]
	TIME [epoch: 2.66 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04633189032190709		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.04633189032190709 | validation: 0.08263448451204197]
	TIME [epoch: 2.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04596477655901393		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.04596477655901393 | validation: 0.1004832170206282]
	TIME [epoch: 2.67 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047835906837555286		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.047835906837555286 | validation: 0.10662303420493376]
	TIME [epoch: 2.66 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0532722590096343		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.0532722590096343 | validation: 0.10034583852196324]
	TIME [epoch: 2.65 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530835356342065		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.07530835356342065 | validation: 0.19122024030598728]
	TIME [epoch: 2.66 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11323766374481292		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.11323766374481292 | validation: 0.09598328488073533]
	TIME [epoch: 2.66 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615264127465798		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.04615264127465798 | validation: 0.07987222017841734]
	TIME [epoch: 2.66 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279248651335671		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.04279248651335671 | validation: 0.09695004504761906]
	TIME [epoch: 2.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05164108630147764		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.05164108630147764 | validation: 0.0967442707268934]
	TIME [epoch: 2.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635807311972479		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.0635807311972479 | validation: 0.10804956801058184]
	TIME [epoch: 2.66 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06061603534917207		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.06061603534917207 | validation: 0.09764372166076363]
	TIME [epoch: 2.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05345454184182704		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.05345454184182704 | validation: 0.08130800278697524]
	TIME [epoch: 2.67 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04853080450078636		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.04853080450078636 | validation: 0.08252228180952136]
	TIME [epoch: 2.66 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05683639024491131		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.05683639024491131 | validation: 0.1397516179326946]
	TIME [epoch: 2.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06884507925885483		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.06884507925885483 | validation: 0.09191511131958674]
	TIME [epoch: 2.66 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047923048610560674		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.047923048610560674 | validation: 0.07401619209738218]
	TIME [epoch: 2.66 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04320079532474535		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.04320079532474535 | validation: 0.1172403752667298]
	TIME [epoch: 2.66 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04814591890714121		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.04814591890714121 | validation: 0.07836468551774245]
	TIME [epoch: 2.66 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0592085735636218		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.0592085735636218 | validation: 0.14894657606081105]
	TIME [epoch: 2.66 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951754947601668		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.07951754947601668 | validation: 0.12389379653986482]
	TIME [epoch: 2.66 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10940223931811399		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.10940223931811399 | validation: 0.09480413995508699]
	TIME [epoch: 2.66 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04992736882557043		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.04992736882557043 | validation: 0.08502549414256833]
	TIME [epoch: 2.65 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051033359850766596		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.051033359850766596 | validation: 0.11067832855616673]
	TIME [epoch: 2.65 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05386355467702215		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.05386355467702215 | validation: 0.08010173646288299]
	TIME [epoch: 2.66 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04994660696649202		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.04994660696649202 | validation: 0.11269818044941998]
	TIME [epoch: 2.65 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050958715798878354		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.050958715798878354 | validation: 0.0836164662536909]
	TIME [epoch: 2.66 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04848823863595485		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.04848823863595485 | validation: 0.10160928456334267]
	TIME [epoch: 2.66 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04588108103085764		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.04588108103085764 | validation: 0.0842198985696316]
	TIME [epoch: 2.66 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0446454610576159		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.0446454610576159 | validation: 0.07227274746059398]
	TIME [epoch: 2.66 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04457190400408173		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.04457190400408173 | validation: 0.0908414099607237]
	TIME [epoch: 2.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04086378027119765		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.04086378027119765 | validation: 0.07914573952641164]
	TIME [epoch: 2.66 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03979325335690573		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.03979325335690573 | validation: 0.06773348472706521]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04238732266178845		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.04238732266178845 | validation: 0.11423560954360855]
	TIME [epoch: 2.66 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05154803837347663		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.05154803837347663 | validation: 0.09125239604158729]
	TIME [epoch: 2.66 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07136446142334757		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.07136446142334757 | validation: 0.11189279998584781]
	TIME [epoch: 2.66 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10110051443658315		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.10110051443658315 | validation: 0.08540002110108709]
	TIME [epoch: 2.66 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04475876927883801		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.04475876927883801 | validation: 0.07163538675676387]
	TIME [epoch: 2.66 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927707494857146		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.03927707494857146 | validation: 0.08888992173959878]
	TIME [epoch: 2.67 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443221581300517		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.04443221581300517 | validation: 0.08135039681200273]
	TIME [epoch: 2.66 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022578970204081		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.05022578970204081 | validation: 0.13363316475506296]
	TIME [epoch: 2.65 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06551930736674084		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.06551930736674084 | validation: 0.06911942030922816]
	TIME [epoch: 2.65 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262777921876424		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.04262777921876424 | validation: 0.0978510557472338]
	TIME [epoch: 2.65 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03875628268100743		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.03875628268100743 | validation: 0.06563538517283125]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_572.pth
	Model improved!!!
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04140284604068845		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.04140284604068845 | validation: 0.08091867140801995]
	TIME [epoch: 2.66 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04097342404307242		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.04097342404307242 | validation: 0.11384488950653511]
	TIME [epoch: 2.66 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057318582772058056		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.057318582772058056 | validation: 0.11293567598327581]
	TIME [epoch: 2.65 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10275919829107318		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.10275919829107318 | validation: 0.1762639874532663]
	TIME [epoch: 2.66 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08077693201826959		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.08077693201826959 | validation: 0.09881056113758735]
	TIME [epoch: 2.66 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04193957205572885		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.04193957205572885 | validation: 0.08416665561508455]
	TIME [epoch: 2.67 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059235689684523536		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.059235689684523536 | validation: 0.07289972293752754]
	TIME [epoch: 2.66 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14463778510969616		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.14463778510969616 | validation: 0.06680343246098633]
	TIME [epoch: 2.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06969347604237944		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.06969347604237944 | validation: 0.1288886067047165]
	TIME [epoch: 2.66 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07372079610264823		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.07372079610264823 | validation: 0.11564366744850957]
	TIME [epoch: 2.66 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046166124761139654		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.046166124761139654 | validation: 0.0996106628326302]
	TIME [epoch: 2.66 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844825670910617		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.04844825670910617 | validation: 0.07263318861046474]
	TIME [epoch: 2.66 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052854367647011354		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.052854367647011354 | validation: 0.09285421961624343]
	TIME [epoch: 2.66 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530658062774611		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.04530658062774611 | validation: 0.09914168000640233]
	TIME [epoch: 2.65 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041636710142626536		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.041636710142626536 | validation: 0.0751769060829189]
	TIME [epoch: 2.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03867491124887902		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.03867491124887902 | validation: 0.088852568733853]
	TIME [epoch: 2.65 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03739529299253237		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.03739529299253237 | validation: 0.07224486759751082]
	TIME [epoch: 2.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0374401831006118		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.0374401831006118 | validation: 0.07566872264431307]
	TIME [epoch: 2.65 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860660961914836		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.03860660961914836 | validation: 0.08812879019347039]
	TIME [epoch: 2.66 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03790022245763378		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.03790022245763378 | validation: 0.07605515251446221]
	TIME [epoch: 2.65 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662600224338733		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.03662600224338733 | validation: 0.06323299276428264]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034280594561547924		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.034280594561547924 | validation: 0.07057988846488174]
	TIME [epoch: 2.65 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0349661306718779		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.0349661306718779 | validation: 0.05681220178901833]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_595.pth
	Model improved!!!
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034622534806472136		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.034622534806472136 | validation: 0.05702130825053259]
	TIME [epoch: 2.65 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03885830051156058		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.03885830051156058 | validation: 0.12397883871300075]
	TIME [epoch: 2.64 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05847314575462928		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.05847314575462928 | validation: 0.0817711695471735]
	TIME [epoch: 2.65 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07070746993270004		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.07070746993270004 | validation: 0.13741957787546094]
	TIME [epoch: 2.65 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886523606729049		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0886523606729049 | validation: 0.09062297068806131]
	TIME [epoch: 2.65 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559670238921319		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.04559670238921319 | validation: 0.07961763917489738]
	TIME [epoch: 2.65 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043272942706549834		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.043272942706549834 | validation: 0.08141539477821283]
	TIME [epoch: 2.66 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041423210082840826		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.041423210082840826 | validation: 0.07398058011195177]
	TIME [epoch: 2.66 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03713819594195785		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.03713819594195785 | validation: 0.07407338500185155]
	TIME [epoch: 2.66 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037562258543231336		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.037562258543231336 | validation: 0.06779335610895158]
	TIME [epoch: 2.66 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03456163622963554		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.03456163622963554 | validation: 0.055256693180669336]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03494888604959219		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.03494888604959219 | validation: 0.06889893252713546]
	TIME [epoch: 2.64 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035215457460641225		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.035215457460641225 | validation: 0.06449937819621156]
	TIME [epoch: 2.65 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033229370283639356		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.033229370283639356 | validation: 0.0655447140683908]
	TIME [epoch: 2.65 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033668832102432676		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.033668832102432676 | validation: 0.10937932427605504]
	TIME [epoch: 2.65 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05875631075160105		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.05875631075160105 | validation: 0.0853000805147368]
	TIME [epoch: 2.66 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06882565974882543		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.06882565974882543 | validation: 0.12844345538702204]
	TIME [epoch: 2.66 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06061473347106658		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.06061473347106658 | validation: 0.06327663162348802]
	TIME [epoch: 2.66 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034208923452493906		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.034208923452493906 | validation: 0.06341885277311395]
	TIME [epoch: 2.66 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037025729470377196		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.037025729470377196 | validation: 0.10680685502834211]
	TIME [epoch: 2.66 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048952441584034556		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.048952441584034556 | validation: 0.06839198843268207]
	TIME [epoch: 2.67 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043601841927383894		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.043601841927383894 | validation: 0.07951592053701184]
	TIME [epoch: 2.65 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03736291135349475		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.03736291135349475 | validation: 0.07038150532613775]
	TIME [epoch: 2.66 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778802377773453		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.03778802377773453 | validation: 0.05754238818704981]
	TIME [epoch: 2.66 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04756369753516717		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.04756369753516717 | validation: 0.08562809543312627]
	TIME [epoch: 2.65 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0581250795081418		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.0581250795081418 | validation: 0.14684160049667624]
	TIME [epoch: 2.65 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0731326511379857		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.0731326511379857 | validation: 0.07041992681923576]
	TIME [epoch: 2.66 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0345559901137638		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.0345559901137638 | validation: 0.07085368099796938]
	TIME [epoch: 2.65 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049761455880342094		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.049761455880342094 | validation: 0.1053171441631171]
	TIME [epoch: 2.66 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050006446602051544		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.050006446602051544 | validation: 0.06553884955832455]
	TIME [epoch: 2.66 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034156581603483865		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.034156581603483865 | validation: 0.07078759674649067]
	TIME [epoch: 2.66 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968251396294248		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.03968251396294248 | validation: 0.0837437205376694]
	TIME [epoch: 2.67 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04398463212409632		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.04398463212409632 | validation: 0.06530222256478868]
	TIME [epoch: 2.66 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705090345548065		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.03705090345548065 | validation: 0.059424655276655414]
	TIME [epoch: 2.67 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804563680380351		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.03804563680380351 | validation: 0.076301366022087]
	TIME [epoch: 2.65 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038933251346653305		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.038933251346653305 | validation: 0.16610785683449047]
	TIME [epoch: 2.67 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07450367832241146		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07450367832241146 | validation: 0.09104516915969926]
	TIME [epoch: 2.66 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04154046075088207		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.04154046075088207 | validation: 0.08097241836414326]
	TIME [epoch: 2.66 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05202584144100109		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.05202584144100109 | validation: 0.09422276042796937]
	TIME [epoch: 2.67 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04358169248466516		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.04358169248466516 | validation: 0.08445988233585408]
	TIME [epoch: 2.66 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03956554001865008		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.03956554001865008 | validation: 0.05747829839196533]
	TIME [epoch: 2.66 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03338145001600449		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.03338145001600449 | validation: 0.06498897466061439]
	TIME [epoch: 2.66 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033345412366496005		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.033345412366496005 | validation: 0.05801485247990497]
	TIME [epoch: 2.66 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0323949563928635		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.0323949563928635 | validation: 0.06054776356909461]
	TIME [epoch: 2.65 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035877802681859		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.035877802681859 | validation: 0.07365755786794449]
	TIME [epoch: 2.66 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04194312903725825		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.04194312903725825 | validation: 0.06919139802493068]
	TIME [epoch: 2.65 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04942614744548813		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.04942614744548813 | validation: 0.0671945105853972]
	TIME [epoch: 2.66 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04201446016966504		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.04201446016966504 | validation: 0.07469064148527475]
	TIME [epoch: 2.66 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042354899777400694		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.042354899777400694 | validation: 0.06157264126502228]
	TIME [epoch: 2.66 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036890918936738		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.036890918936738 | validation: 0.060686154372999616]
	TIME [epoch: 2.67 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03247942047259788		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.03247942047259788 | validation: 0.05785379221927808]
	TIME [epoch: 2.66 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032888986024567884		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.032888986024567884 | validation: 0.06882392039026222]
	TIME [epoch: 2.66 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034327735554451434		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.034327735554451434 | validation: 0.05900968046672611]
	TIME [epoch: 2.66 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03733263002384998		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.03733263002384998 | validation: 0.06628456715594304]
	TIME [epoch: 2.66 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04337942448936216		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.04337942448936216 | validation: 0.10062316087727899]
	TIME [epoch: 2.66 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06738082403536746		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.06738082403536746 | validation: 0.0689236589862757]
	TIME [epoch: 2.67 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04105013735642387		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.04105013735642387 | validation: 0.06168905981128177]
	TIME [epoch: 2.66 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03241789865711683		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.03241789865711683 | validation: 0.05740860807608761]
	TIME [epoch: 2.66 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030510638112581454		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.030510638112581454 | validation: 0.06765631586147068]
	TIME [epoch: 2.66 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03423753211030042		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.03423753211030042 | validation: 0.053398759302552085]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034297032422664325		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.034297032422664325 | validation: 0.066516997756893]
	TIME [epoch: 2.66 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0354517052389989		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.0354517052389989 | validation: 0.05260017514086231]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03398395027058457		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.03398395027058457 | validation: 0.05810652749880654]
	TIME [epoch: 2.66 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034162955112390175		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.034162955112390175 | validation: 0.08239354900106349]
	TIME [epoch: 2.66 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04406015851350839		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.04406015851350839 | validation: 0.05657756387195504]
	TIME [epoch: 2.66 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03430249036308322		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.03430249036308322 | validation: 0.052233700008190255]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03266966079912269		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.03266966079912269 | validation: 0.07042685001891831]
	TIME [epoch: 2.66 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375471092876804		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.0375471092876804 | validation: 0.06618216083805673]
	TIME [epoch: 2.66 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04255353088645017		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.04255353088645017 | validation: 0.07219325510718601]
	TIME [epoch: 2.66 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058931894403386205		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.058931894403386205 | validation: 0.12170575192378048]
	TIME [epoch: 2.67 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061059185218980336		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.061059185218980336 | validation: 0.07773860306312902]
	TIME [epoch: 2.66 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036047797240304366		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.036047797240304366 | validation: 0.08390744157026175]
	TIME [epoch: 2.66 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06172258680941228		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.06172258680941228 | validation: 0.07437035653688301]
	TIME [epoch: 2.66 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03770328168160452		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.03770328168160452 | validation: 0.06744084072779476]
	TIME [epoch: 2.68 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03231013395743267		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.03231013395743267 | validation: 0.05765240386295224]
	TIME [epoch: 2.66 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03283006593937757		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.03283006593937757 | validation: 0.06627343188226872]
	TIME [epoch: 2.67 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029874241304871034		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.029874241304871034 | validation: 0.05920304123011745]
	TIME [epoch: 2.66 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03067812480395821		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.03067812480395821 | validation: 0.07858550994336841]
	TIME [epoch: 2.66 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03363960219201009		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.03363960219201009 | validation: 0.05287503297532351]
	TIME [epoch: 2.65 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285478271069596		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.03285478271069596 | validation: 0.06208788244866081]
	TIME [epoch: 2.67 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03365848608184368		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.03365848608184368 | validation: 0.06345824371523594]
	TIME [epoch: 2.66 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033563249080636584		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.033563249080636584 | validation: 0.051545652420290425]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_677.pth
	Model improved!!!
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03000756718410101		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.03000756718410101 | validation: 0.07104325261180476]
	TIME [epoch: 2.65 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03210449252549709		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.03210449252549709 | validation: 0.05179715956520281]
	TIME [epoch: 2.65 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034525304011906253		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.034525304011906253 | validation: 0.08157886891668042]
	TIME [epoch: 2.67 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04281885294300351		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.04281885294300351 | validation: 0.06060519141427298]
	TIME [epoch: 2.66 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03685757900296561		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.03685757900296561 | validation: 0.05245903352101311]
	TIME [epoch: 2.67 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980040314276655		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.03980040314276655 | validation: 0.06900963339171379]
	TIME [epoch: 2.66 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03325918324376375		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.03325918324376375 | validation: 0.05564854316901992]
	TIME [epoch: 2.67 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03163271438784736		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.03163271438784736 | validation: 0.05305802979157656]
	TIME [epoch: 2.67 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02903347359962516		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.02903347359962516 | validation: 0.07249015641798863]
	TIME [epoch: 2.66 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03288386327945061		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.03288386327945061 | validation: 0.050945750585742994]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_687.pth
	Model improved!!!
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03310905866381438		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.03310905866381438 | validation: 0.05800264397234167]
	TIME [epoch: 2.66 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03527221775627556		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.03527221775627556 | validation: 0.08594882079693891]
	TIME [epoch: 2.65 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05402524419939941		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.05402524419939941 | validation: 0.06255544134811834]
	TIME [epoch: 2.65 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03519070174819284		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.03519070174819284 | validation: 0.04829242320120478]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_691.pth
	Model improved!!!
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030187880223694168		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.030187880223694168 | validation: 0.05056108755548221]
	TIME [epoch: 2.64 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027474688703709362		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.027474688703709362 | validation: 0.06743750301166167]
	TIME [epoch: 2.65 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02981799046320716		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.02981799046320716 | validation: 0.05842706751785483]
	TIME [epoch: 2.65 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029782393871403273		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.029782393871403273 | validation: 0.053775389855567915]
	TIME [epoch: 2.65 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028093339663082195		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.028093339663082195 | validation: 0.05792406531965976]
	TIME [epoch: 2.66 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028055794931926874		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.028055794931926874 | validation: 0.05013924884014742]
	TIME [epoch: 2.65 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028419994094778477		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.028419994094778477 | validation: 0.04739116884604974]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027679166215859072		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.027679166215859072 | validation: 0.04034077890874543]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_699.pth
	Model improved!!!
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02832019956581446		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.02832019956581446 | validation: 0.061481026986522094]
	TIME [epoch: 2.66 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030858863049156776		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.030858863049156776 | validation: 0.0833431470046685]
	TIME [epoch: 2.65 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046926022021074605		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.046926022021074605 | validation: 0.0801009074416873]
	TIME [epoch: 2.65 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810122741606507		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.0810122741606507 | validation: 0.08067205561766091]
	TIME [epoch: 2.65 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04111394335564061		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.04111394335564061 | validation: 0.06892776474867822]
	TIME [epoch: 2.65 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030460467486716384		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.030460467486716384 | validation: 0.04949246492111248]
	TIME [epoch: 2.66 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036153832895064006		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.036153832895064006 | validation: 0.05625751071032825]
	TIME [epoch: 2.66 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342033578704885		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.0342033578704885 | validation: 0.05676479071296993]
	TIME [epoch: 2.65 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029403761158110556		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.029403761158110556 | validation: 0.04912917121280788]
	TIME [epoch: 2.66 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811723211664587		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.02811723211664587 | validation: 0.047158516732995565]
	TIME [epoch: 2.65 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027133011500785488		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.027133011500785488 | validation: 0.058053673055194537]
	TIME [epoch: 2.66 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03134669437383674		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.03134669437383674 | validation: 0.04485996062368574]
	TIME [epoch: 2.65 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03197255032535599		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.03197255032535599 | validation: 0.05675873714335305]
	TIME [epoch: 2.66 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03158202456813493		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.03158202456813493 | validation: 0.04707441290512631]
	TIME [epoch: 2.65 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028861966146982256		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.028861966146982256 | validation: 0.057165868282531646]
	TIME [epoch: 2.66 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026644336306178955		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.026644336306178955 | validation: 0.04646839048052029]
	TIME [epoch: 2.65 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02777216461205299		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.02777216461205299 | validation: 0.060028449353191515]
	TIME [epoch: 2.65 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031736357634934056		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.031736357634934056 | validation: 0.05537494081888873]
	TIME [epoch: 2.65 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194836028141986		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.03194836028141986 | validation: 0.060352264650520876]
	TIME [epoch: 2.66 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041058706915038136		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.041058706915038136 | validation: 0.0642580558871489]
	TIME [epoch: 2.67 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03521496299129988		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.03521496299129988 | validation: 0.04154871014806088]
	TIME [epoch: 2.65 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02898247597296277		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.02898247597296277 | validation: 0.0465781773792848]
	TIME [epoch: 2.66 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026718379419247054		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.026718379419247054 | validation: 0.057182525248697526]
	TIME [epoch: 2.66 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028405821696424614		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.028405821696424614 | validation: 0.05153492385631875]
	TIME [epoch: 2.65 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034123113859170905		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.034123113859170905 | validation: 0.05181230785423842]
	TIME [epoch: 2.65 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03990435596582752		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.03990435596582752 | validation: 0.07932201672827255]
	TIME [epoch: 2.66 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04060108035892748		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.04060108035892748 | validation: 0.04742996511762985]
	TIME [epoch: 2.65 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02733856729267574		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.02733856729267574 | validation: 0.0460670631646561]
	TIME [epoch: 2.66 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02883942869692256		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.02883942869692256 | validation: 0.06143206081783838]
	TIME [epoch: 2.65 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02935018374065103		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.02935018374065103 | validation: 0.05109763677384306]
	TIME [epoch: 2.66 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028472068431088734		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.028472068431088734 | validation: 0.043919393694763956]
	TIME [epoch: 2.65 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02913810548288982		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.02913810548288982 | validation: 0.05498485868562391]
	TIME [epoch: 2.66 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03175698333553505		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.03175698333553505 | validation: 0.04973549194739366]
	TIME [epoch: 2.65 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03119442909406119		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.03119442909406119 | validation: 0.04746155681746472]
	TIME [epoch: 2.65 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03342034361919114		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.03342034361919114 | validation: 0.055152262110168104]
	TIME [epoch: 2.66 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02989434012145239		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.02989434012145239 | validation: 0.04525397765983274]
	TIME [epoch: 2.65 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026919591881961785		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.026919591881961785 | validation: 0.03995848802126392]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026198746961441907		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.026198746961441907 | validation: 0.06263310409306903]
	TIME [epoch: 2.65 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03562089233862598		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.03562089233862598 | validation: 0.05568605164629353]
	TIME [epoch: 2.66 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0385722933472496		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.0385722933472496 | validation: 0.07319570084874535]
	TIME [epoch: 2.65 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03802172925427381		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.03802172925427381 | validation: 0.0481909483978057]
	TIME [epoch: 2.66 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025856016275880415		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.025856016275880415 | validation: 0.04177314306449599]
	TIME [epoch: 2.66 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026943672898892858		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.026943672898892858 | validation: 0.050583552392848154]
	TIME [epoch: 2.66 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811759782415352		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.02811759782415352 | validation: 0.06078983543370914]
	TIME [epoch: 2.66 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028756546534000966		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.028756546534000966 | validation: 0.05061310697691712]
	TIME [epoch: 2.65 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02868829431385563		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.02868829431385563 | validation: 0.05782555043357818]
	TIME [epoch: 2.65 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028799768572027692		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.028799768572027692 | validation: 0.040676268842371834]
	TIME [epoch: 2.65 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028474616289960702		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.028474616289960702 | validation: 0.05957687038301085]
	TIME [epoch: 2.67 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02983151291774308		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.02983151291774308 | validation: 0.05084665616972437]
	TIME [epoch: 2.65 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031749723189852815		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.031749723189852815 | validation: 0.05299195084300517]
	TIME [epoch: 2.66 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03259570915934348		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.03259570915934348 | validation: 0.057338815915658536]
	TIME [epoch: 2.64 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02802448155230682		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.02802448155230682 | validation: 0.04424185018564111]
	TIME [epoch: 2.66 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026428703195269132		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.026428703195269132 | validation: 0.03991736984089714]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02451327514094488		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.02451327514094488 | validation: 0.04173286707516939]
	TIME [epoch: 2.65 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025547532099247976		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.025547532099247976 | validation: 0.047388661149870295]
	TIME [epoch: 2.66 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02707501393148905		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.02707501393148905 | validation: 0.037953468717692375]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02987900578461747		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.02987900578461747 | validation: 0.052194657230771095]
	TIME [epoch: 2.65 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02784741374745753		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.02784741374745753 | validation: 0.0438249508722852]
	TIME [epoch: 2.65 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025464512013320777		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.025464512013320777 | validation: 0.046264464001293554]
	TIME [epoch: 2.65 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028154865700852336		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.028154865700852336 | validation: 0.06582336579125884]
	TIME [epoch: 2.64 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03670329687859014		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.03670329687859014 | validation: 0.05059636801119382]
	TIME [epoch: 2.65 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03816867439461539		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.03816867439461539 | validation: 0.05179029927235512]
	TIME [epoch: 2.65 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042995701694388926		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.042995701694388926 | validation: 0.07181567283119299]
	TIME [epoch: 2.65 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626961816281705		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.03626961816281705 | validation: 0.05631397271302605]
	TIME [epoch: 2.65 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025824914241100876		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.025824914241100876 | validation: 0.05701440699737153]
	TIME [epoch: 2.65 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03307695447245545		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.03307695447245545 | validation: 0.04744114545997984]
	TIME [epoch: 2.65 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03336265397291967		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.03336265397291967 | validation: 0.044263290069124085]
	TIME [epoch: 2.64 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025248903022352956		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.025248903022352956 | validation: 0.04121479757548255]
	TIME [epoch: 2.65 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0247681996942706		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.0247681996942706 | validation: 0.04855576727889508]
	TIME [epoch: 2.65 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025152068237752637		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.025152068237752637 | validation: 0.04777519316853061]
	TIME [epoch: 2.65 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02452294704570411		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.02452294704570411 | validation: 0.04669435117793062]
	TIME [epoch: 2.64 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02647729966897989		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.02647729966897989 | validation: 0.04537988447407412]
	TIME [epoch: 2.65 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026409797129238344		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.026409797129238344 | validation: 0.05795403051974736]
	TIME [epoch: 2.65 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029427369255860498		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.029427369255860498 | validation: 0.04379741759672793]
	TIME [epoch: 2.66 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02465648402933307		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.02465648402933307 | validation: 0.03630693944114505]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02466886300274951		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.02466886300274951 | validation: 0.04752969162883498]
	TIME [epoch: 2.65 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027478205068856162		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.027478205068856162 | validation: 0.04990131012034352]
	TIME [epoch: 2.66 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02872560546135835		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.02872560546135835 | validation: 0.05274089657680289]
	TIME [epoch: 2.65 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030392693956605075		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.030392693956605075 | validation: 0.04697880703307847]
	TIME [epoch: 2.65 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024523695183846443		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.024523695183846443 | validation: 0.04281895599907413]
	TIME [epoch: 2.65 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023268950171638388		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.023268950171638388 | validation: 0.03934479117621076]
	TIME [epoch: 2.66 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023226295735199135		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.023226295735199135 | validation: 0.04383315803420367]
	TIME [epoch: 2.65 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024715188829601314		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.024715188829601314 | validation: 0.051570465425072466]
	TIME [epoch: 2.66 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028026995363182063		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.028026995363182063 | validation: 0.07309953407674251]
	TIME [epoch: 2.64 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049039167658543166		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.049039167658543166 | validation: 0.051867985212503624]
	TIME [epoch: 2.65 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029806383574304745		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.029806383574304745 | validation: 0.04216128822662377]
	TIME [epoch: 2.66 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025223696434772246		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.025223696434772246 | validation: 0.04461164783839658]
	TIME [epoch: 2.65 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0253486389558112		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.0253486389558112 | validation: 0.0448158587360331]
	TIME [epoch: 2.65 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025849966872394665		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.025849966872394665 | validation: 0.03492244456964936]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_788.pth
	Model improved!!!
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02551008733721158		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.02551008733721158 | validation: 0.037927104748789964]
	TIME [epoch: 2.65 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022285901530436397		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.022285901530436397 | validation: 0.04526213131361698]
	TIME [epoch: 2.65 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02321297070057442		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.02321297070057442 | validation: 0.04498573345441001]
	TIME [epoch: 2.65 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024372739743208784		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.024372739743208784 | validation: 0.04742808445513441]
	TIME [epoch: 2.65 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023911786147416507		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.023911786147416507 | validation: 0.040893787551417864]
	TIME [epoch: 2.66 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024633436797972964		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.024633436797972964 | validation: 0.047364731789327]
	TIME [epoch: 2.65 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027207581571265342		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.027207581571265342 | validation: 0.0722636678168081]
	TIME [epoch: 2.66 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03969052154013363		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.03969052154013363 | validation: 0.05578916419061948]
	TIME [epoch: 2.65 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027324405057180298		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.027324405057180298 | validation: 0.041225080662296326]
	TIME [epoch: 2.65 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02417950051303107		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.02417950051303107 | validation: 0.04592771534432415]
	TIME [epoch: 2.65 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02352503219664953		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.02352503219664953 | validation: 0.037782189241896184]
	TIME [epoch: 2.65 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02418879842777689		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.02418879842777689 | validation: 0.04611392909586266]
	TIME [epoch: 2.65 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023555940379524864		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.023555940379524864 | validation: 0.04248904868643821]
	TIME [epoch: 2.64 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030985423630032476		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.030985423630032476 | validation: 0.062384921899247686]
	TIME [epoch: 2.66 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0414593296055542		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.0414593296055542 | validation: 0.050701906380260765]
	TIME [epoch: 2.65 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030241728285771922		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.030241728285771922 | validation: 0.036103634171191994]
	TIME [epoch: 2.65 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025031161178182795		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.025031161178182795 | validation: 0.03931772462402984]
	TIME [epoch: 2.65 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024053071189625158		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.024053071189625158 | validation: 0.046093983535087604]
	TIME [epoch: 2.65 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024179204016808013		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.024179204016808013 | validation: 0.03777495983209113]
	TIME [epoch: 2.65 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023606415152094634		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.023606415152094634 | validation: 0.03730592545242165]
	TIME [epoch: 2.65 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022040990317369332		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.022040990317369332 | validation: 0.04009536120922663]
	TIME [epoch: 2.65 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022221193271534982		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.022221193271534982 | validation: 0.03922265272785901]
	TIME [epoch: 2.65 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021573506154216222		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.021573506154216222 | validation: 0.03389632727784005]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022391605406818563		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.022391605406818563 | validation: 0.03897215523477372]
	TIME [epoch: 2.65 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025082026015996243		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.025082026015996243 | validation: 0.05176388839255907]
	TIME [epoch: 2.66 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031658858729325196		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.031658858729325196 | validation: 0.05501050757669588]
	TIME [epoch: 2.65 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038708365631369306		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.038708365631369306 | validation: 0.04790945163198073]
	TIME [epoch: 2.65 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030551058733979668		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.030551058733979668 | validation: 0.04044980559912947]
	TIME [epoch: 2.66 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02307710790776685		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.02307710790776685 | validation: 0.04302434114908021]
	TIME [epoch: 2.65 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02567748555796128		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.02567748555796128 | validation: 0.05574010571482049]
	TIME [epoch: 2.66 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02903407860904058		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.02903407860904058 | validation: 0.03599140968884118]
	TIME [epoch: 2.64 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0211683683118103		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.0211683683118103 | validation: 0.04405584811795944]
	TIME [epoch: 2.65 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024815093959505		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.024815093959505 | validation: 0.04637243965561299]
	TIME [epoch: 2.65 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02354507234507945		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.02354507234507945 | validation: 0.051223523591502895]
	TIME [epoch: 2.65 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027551712073573875		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.027551712073573875 | validation: 0.035728778407457244]
	TIME [epoch: 2.64 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02409502352492203		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.02409502352492203 | validation: 0.03445790478847046]
	TIME [epoch: 2.65 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02246409305223735		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.02246409305223735 | validation: 0.041219262293531136]
	TIME [epoch: 2.65 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022749952716639172		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.022749952716639172 | validation: 0.03871574014682099]
	TIME [epoch: 2.65 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026520305735594246		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.026520305735594246 | validation: 0.06347310336080252]
	TIME [epoch: 2.65 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03229111742157119		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.03229111742157119 | validation: 0.049942806794649734]
	TIME [epoch: 2.65 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02394821179082643		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.02394821179082643 | validation: 0.03488394863115845]
	TIME [epoch: 2.65 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022259609835700217		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.022259609835700217 | validation: 0.035314146220131105]
	TIME [epoch: 2.65 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023227904531832225		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.023227904531832225 | validation: 0.046484591107220044]
	TIME [epoch: 2.65 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025369086346784277		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.025369086346784277 | validation: 0.03726778906631114]
	TIME [epoch: 2.64 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022454364617545224		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.022454364617545224 | validation: 0.0332266910248792]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_833.pth
	Model improved!!!
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02396632604506351		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.02396632604506351 | validation: 0.03988746965960447]
	TIME [epoch: 2.66 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02108824603742927		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.02108824603742927 | validation: 0.040328824248305195]
	TIME [epoch: 2.66 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021665622580482297		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.021665622580482297 | validation: 0.03795947838276628]
	TIME [epoch: 2.65 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022799193836012338		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.022799193836012338 | validation: 0.030787584314472483]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021305479747912037		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.021305479747912037 | validation: 0.03788123991046242]
	TIME [epoch: 2.64 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022432584472286844		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.022432584472286844 | validation: 0.04350668271595644]
	TIME [epoch: 2.65 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026531450515018658		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.026531450515018658 | validation: 0.0399707011350961]
	TIME [epoch: 2.65 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02911743391221611		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.02911743391221611 | validation: 0.042360223903990794]
	TIME [epoch: 2.65 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028743166022153998		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.028743166022153998 | validation: 0.032582247408152146]
	TIME [epoch: 2.65 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030404197300476957		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.030404197300476957 | validation: 0.03981612524306208]
	TIME [epoch: 2.65 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022715304380899776		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.022715304380899776 | validation: 0.050705808539394194]
	TIME [epoch: 2.66 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020993159606673188		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.020993159606673188 | validation: 0.029251146018318786]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020764589392107603		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.020764589392107603 | validation: 0.04034296833509084]
	TIME [epoch: 2.65 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021394400891987474		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.021394400891987474 | validation: 0.039846093157729925]
	TIME [epoch: 2.66 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025021908898928344		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.025021908898928344 | validation: 0.04084414552013587]
	TIME [epoch: 2.65 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024896593973725407		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.024896593973725407 | validation: 0.03447344314363032]
	TIME [epoch: 2.65 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024024608202360857		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.024024608202360857 | validation: 0.04010838091713767]
	TIME [epoch: 2.65 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02159189675773874		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.02159189675773874 | validation: 0.03193979676824349]
	TIME [epoch: 2.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020962504449100305		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.020962504449100305 | validation: 0.04052650547849043]
	TIME [epoch: 2.65 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02507356470867184		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.02507356470867184 | validation: 0.03811500897000794]
	TIME [epoch: 2.66 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031395764464203585		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.031395764464203585 | validation: 0.04106057169149015]
	TIME [epoch: 2.66 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030381181487891987		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.030381181487891987 | validation: 0.051823231495718575]
	TIME [epoch: 2.66 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027975751238149073		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.027975751238149073 | validation: 0.040704039288136186]
	TIME [epoch: 2.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021614492873045094		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.021614492873045094 | validation: 0.039256730305985525]
	TIME [epoch: 2.66 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024123171983838815		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.024123171983838815 | validation: 0.03349490223509888]
	TIME [epoch: 2.66 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02319862134753981		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.02319862134753981 | validation: 0.03664374011318823]
	TIME [epoch: 2.66 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0206330735526542		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.0206330735526542 | validation: 0.03992997959362024]
	TIME [epoch: 2.66 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020281259559231053		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.020281259559231053 | validation: 0.03188651831775187]
	TIME [epoch: 2.65 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023363174008956506		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.023363174008956506 | validation: 0.03831019742905232]
	TIME [epoch: 2.66 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027157419025039868		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.027157419025039868 | validation: 0.03170799824350667]
	TIME [epoch: 2.66 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029933588823968613		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.029933588823968613 | validation: 0.046850662452160255]
	TIME [epoch: 2.66 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02386056550846095		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.02386056550846095 | validation: 0.03334022565302317]
	TIME [epoch: 2.66 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02079244782249577		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.02079244782249577 | validation: 0.03515918830444936]
	TIME [epoch: 2.66 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022547594839807943		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.022547594839807943 | validation: 0.039712353053849236]
	TIME [epoch: 2.67 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020881787442549378		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.020881787442549378 | validation: 0.0354913236864013]
	TIME [epoch: 2.66 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021614982064806002		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.021614982064806002 | validation: 0.03860425165801456]
	TIME [epoch: 2.66 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02107019512318395		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.02107019512318395 | validation: 0.03403957511349912]
	TIME [epoch: 2.66 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020421682391026243		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.020421682391026243 | validation: 0.03020481951836671]
	TIME [epoch: 2.66 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021548904178189675		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.021548904178189675 | validation: 0.038995431973428356]
	TIME [epoch: 2.66 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021310321402622005		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.021310321402622005 | validation: 0.026558196173908633]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_873.pth
	Model improved!!!
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02333409518943733		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.02333409518943733 | validation: 0.035835726271839355]
	TIME [epoch: 2.65 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020855278789904272		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.020855278789904272 | validation: 0.0335814056249792]
	TIME [epoch: 2.65 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019920997084592097		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.019920997084592097 | validation: 0.036369679589258276]
	TIME [epoch: 2.66 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026540279114666887		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.026540279114666887 | validation: 0.04309751831869473]
	TIME [epoch: 2.66 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028600623160863314		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.028600623160863314 | validation: 0.03850127003794527]
	TIME [epoch: 2.65 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024333174522734157		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.024333174522734157 | validation: 0.03200968116818294]
	TIME [epoch: 2.67 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020822811493396928		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.020822811493396928 | validation: 0.038633902060984264]
	TIME [epoch: 2.66 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01941544100761302		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.01941544100761302 | validation: 0.03563847895277714]
	TIME [epoch: 2.7 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020205928314535063		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.020205928314535063 | validation: 0.03392999825854642]
	TIME [epoch: 2.66 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020610467416103468		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.020610467416103468 | validation: 0.03513279319922356]
	TIME [epoch: 2.66 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0199310363970898		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.0199310363970898 | validation: 0.037186209420090335]
	TIME [epoch: 2.66 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019637596555932068		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.019637596555932068 | validation: 0.03825607522863225]
	TIME [epoch: 2.66 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02104577116553322		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.02104577116553322 | validation: 0.032935646235331975]
	TIME [epoch: 2.66 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021556428050681437		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.021556428050681437 | validation: 0.03892965981828587]
	TIME [epoch: 2.65 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023013339416139207		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.023013339416139207 | validation: 0.04200668146718947]
	TIME [epoch: 2.66 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02562177138657189		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.02562177138657189 | validation: 0.04277805287516193]
	TIME [epoch: 2.65 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023351982046319664		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.023351982046319664 | validation: 0.02775078708329054]
	TIME [epoch: 2.65 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02411249604122724		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.02411249604122724 | validation: 0.03100432008203793]
	TIME [epoch: 2.65 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021541166830206144		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.021541166830206144 | validation: 0.03890689311404232]
	TIME [epoch: 2.66 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02179736131448536		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.02179736131448536 | validation: 0.03209698959224453]
	TIME [epoch: 2.65 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024584585679531493		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.024584585679531493 | validation: 0.040228579180992444]
	TIME [epoch: 2.65 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02652450600146751		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.02652450600146751 | validation: 0.03970048626062581]
	TIME [epoch: 2.65 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027685705131884553		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.027685705131884553 | validation: 0.04092063809330476]
	TIME [epoch: 2.65 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02080754965247377		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.02080754965247377 | validation: 0.03259357441742864]
	TIME [epoch: 2.66 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020751991084280937		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.020751991084280937 | validation: 0.028410063814990272]
	TIME [epoch: 2.65 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021208365109901026		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.021208365109901026 | validation: 0.03156012383245249]
	TIME [epoch: 2.66 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01946832564041856		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.01946832564041856 | validation: 0.037701939224262196]
	TIME [epoch: 2.65 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020674566705892527		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.020674566705892527 | validation: 0.03184061101073843]
	TIME [epoch: 2.65 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019742426700285844		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.019742426700285844 | validation: 0.02874208561854609]
	TIME [epoch: 2.66 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019876018906927476		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.019876018906927476 | validation: 0.041191545183972444]
	TIME [epoch: 2.67 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021752949539526653		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.021752949539526653 | validation: 0.031344898869846496]
	TIME [epoch: 2.66 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021443260821542937		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.021443260821542937 | validation: 0.038022629148364545]
	TIME [epoch: 2.65 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023011820909885624		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.023011820909885624 | validation: 0.04163077010076286]
	TIME [epoch: 2.66 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028409386242943296		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.028409386242943296 | validation: 0.042942825152828656]
	TIME [epoch: 2.66 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024814459162866457		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.024814459162866457 | validation: 0.030203676453159824]
	TIME [epoch: 2.65 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019974368630651784		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.019974368630651784 | validation: 0.031840828518923406]
	TIME [epoch: 2.65 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01899240277898885		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.01899240277898885 | validation: 0.03309209870575815]
	TIME [epoch: 2.66 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019374250575084094		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.019374250575084094 | validation: 0.03385114205097789]
	TIME [epoch: 2.65 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02046379033171666		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.02046379033171666 | validation: 0.032056432865585203]
	TIME [epoch: 2.65 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02012842756370551		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.02012842756370551 | validation: 0.029446113189877755]
	TIME [epoch: 2.65 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01912291152502193		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.01912291152502193 | validation: 0.029404378902130657]
	TIME [epoch: 2.66 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01939723839039923		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.01939723839039923 | validation: 0.03676031868887786]
	TIME [epoch: 2.65 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019747981206326766		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.019747981206326766 | validation: 0.029948312714932424]
	TIME [epoch: 2.66 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02225757233533372		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.02225757233533372 | validation: 0.03542518895588596]
	TIME [epoch: 2.66 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02467699912706449		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.02467699912706449 | validation: 0.041869030109733946]
	TIME [epoch: 2.65 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024510562412906767		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.024510562412906767 | validation: 0.026942155394443168]
	TIME [epoch: 2.65 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02433564864572322		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.02433564864572322 | validation: 0.03095933449285785]
	TIME [epoch: 2.65 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019292912284851207		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.019292912284851207 | validation: 0.030386388533825404]
	TIME [epoch: 2.65 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01921561587179062		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.01921561587179062 | validation: 0.04336444917954252]
	TIME [epoch: 2.65 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022868521744108628		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.022868521744108628 | validation: 0.037385190811602624]
	TIME [epoch: 2.65 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01971738065672777		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.01971738065672777 | validation: 0.026936659164723088]
	TIME [epoch: 2.65 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019920667772306857		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.019920667772306857 | validation: 0.03695809021607751]
	TIME [epoch: 2.66 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019664650962948012		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.019664650962948012 | validation: 0.03384807974990598]
	TIME [epoch: 2.65 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019425601383272994		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.019425601383272994 | validation: 0.03426655564880745]
	TIME [epoch: 2.65 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02150135848344973		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.02150135848344973 | validation: 0.03555536130914507]
	TIME [epoch: 2.65 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0212812846869151		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.0212812846869151 | validation: 0.03879030609499589]
	TIME [epoch: 2.65 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02207571032811936		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.02207571032811936 | validation: 0.029753960692485718]
	TIME [epoch: 2.66 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02000570972124281		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.02000570972124281 | validation: 0.026726938902728627]
	TIME [epoch: 2.65 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02037655681136009		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.02037655681136009 | validation: 0.03214791186570122]
	TIME [epoch: 2.66 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02035451262252985		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.02035451262252985 | validation: 0.0326269310692753]
	TIME [epoch: 2.65 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021693690511313642		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.021693690511313642 | validation: 0.030062002739135486]
	TIME [epoch: 2.66 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020658940600924557		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.020658940600924557 | validation: 0.040912312110220034]
	TIME [epoch: 2.66 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0213731662435191		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.0213731662435191 | validation: 0.03201869486419723]
	TIME [epoch: 2.66 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019181253240638126		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.019181253240638126 | validation: 0.030391143913383045]
	TIME [epoch: 2.66 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018693482854755295		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.018693482854755295 | validation: 0.030216086869852877]
	TIME [epoch: 2.65 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019774767585772103		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.019774767585772103 | validation: 0.03197314588823248]
	TIME [epoch: 2.65 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01796087011117409		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.01796087011117409 | validation: 0.02807562285918859]
	TIME [epoch: 2.65 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020513072496363537		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.020513072496363537 | validation: 0.037473654944859505]
	TIME [epoch: 2.65 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022017663169972494		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.022017663169972494 | validation: 0.034874074583749706]
	TIME [epoch: 2.66 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025008370527298374		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.025008370527298374 | validation: 0.03161343228518362]
	TIME [epoch: 2.65 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021137286542041434		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.021137286542041434 | validation: 0.02657093887488037]
	TIME [epoch: 2.65 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01876015442382227		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.01876015442382227 | validation: 0.033457623645500104]
	TIME [epoch: 2.65 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020040531933834763		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.020040531933834763 | validation: 0.03262141490954271]
	TIME [epoch: 2.65 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01840877985188518		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.01840877985188518 | validation: 0.029870967836484144]
	TIME [epoch: 2.65 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897083870441908		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.01897083870441908 | validation: 0.038610082341135755]
	TIME [epoch: 2.65 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01910194485957177		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.01910194485957177 | validation: 0.024813294381852792]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_949.pth
	Model improved!!!
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01936258529580527		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.01936258529580527 | validation: 0.032533709346799014]
	TIME [epoch: 2.66 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021412496031912778		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.021412496031912778 | validation: 0.042830999475556256]
	TIME [epoch: 2.66 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027471599247737438		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.027471599247737438 | validation: 0.029700802675577745]
	TIME [epoch: 2.65 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018310556949966972		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.018310556949966972 | validation: 0.028884948900238995]
	TIME [epoch: 2.66 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020209724461338677		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.020209724461338677 | validation: 0.029100501096240128]
	TIME [epoch: 2.67 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019426624414066482		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.019426624414066482 | validation: 0.03568727020640052]
	TIME [epoch: 2.66 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01842346424810249		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.01842346424810249 | validation: 0.026066211726090163]
	TIME [epoch: 2.66 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019678771099398234		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.019678771099398234 | validation: 0.02677415758702606]
	TIME [epoch: 2.65 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018690723114579103		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.018690723114579103 | validation: 0.036594180623202126]
	TIME [epoch: 2.65 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019546327529492116		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.019546327529492116 | validation: 0.03153997246381986]
	TIME [epoch: 2.65 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019297059654916685		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.019297059654916685 | validation: 0.028313094420744447]
	TIME [epoch: 2.66 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023174244896807823		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.023174244896807823 | validation: 0.028156883051097925]
	TIME [epoch: 2.65 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02135119923794261		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.02135119923794261 | validation: 0.03359912266325898]
	TIME [epoch: 2.66 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019783768701668477		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.019783768701668477 | validation: 0.028789771764379704]
	TIME [epoch: 2.65 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01845462161284823		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.01845462161284823 | validation: 0.02978095617097525]
	TIME [epoch: 2.65 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017784738313958447		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.017784738313958447 | validation: 0.03427820094482277]
	TIME [epoch: 2.65 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019211326062475484		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.019211326062475484 | validation: 0.027608843344601522]
	TIME [epoch: 2.66 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021166563539699902		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.021166563539699902 | validation: 0.033513659052882984]
	TIME [epoch: 2.66 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019618008308005502		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.019618008308005502 | validation: 0.024035124413867237]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_968.pth
	Model improved!!!
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01901845138979845		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.01901845138979845 | validation: 0.025153553023298327]
	TIME [epoch: 2.65 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01847803320694847		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.01847803320694847 | validation: 0.028992394026334547]
	TIME [epoch: 2.65 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018907829074050777		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.018907829074050777 | validation: 0.034258466367299935]
	TIME [epoch: 2.66 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019990332814209463		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.019990332814209463 | validation: 0.031177916923804307]
	TIME [epoch: 2.66 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019684918617218185		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.019684918617218185 | validation: 0.032871270937730804]
	TIME [epoch: 2.66 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018284739143092407		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.018284739143092407 | validation: 0.02950449006887216]
	TIME [epoch: 2.65 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018737798914544256		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.018737798914544256 | validation: 0.02897660231329784]
	TIME [epoch: 2.66 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019632366802856595		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.019632366802856595 | validation: 0.030631598924535264]
	TIME [epoch: 2.67 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019633923246552856		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.019633923246552856 | validation: 0.02994685658592764]
	TIME [epoch: 2.66 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019999092386776548		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.019999092386776548 | validation: 0.028178724624377472]
	TIME [epoch: 2.66 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017712119087024965		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.017712119087024965 | validation: 0.030438555006482126]
	TIME [epoch: 2.66 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01825904378841474		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.01825904378841474 | validation: 0.03346629877339324]
	TIME [epoch: 2.66 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019141551538018768		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.019141551538018768 | validation: 0.031245987318081838]
	TIME [epoch: 2.65 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018961940521148506		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.018961940521148506 | validation: 0.03425791726782864]
	TIME [epoch: 2.66 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01726799812655794		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.01726799812655794 | validation: 0.027139487820105692]
	TIME [epoch: 2.65 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018627216109186575		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.018627216109186575 | validation: 0.042274252245535174]
	TIME [epoch: 2.66 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023029403791183373		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.023029403791183373 | validation: 0.03571461224543842]
	TIME [epoch: 2.65 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02230644931892532		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.02230644931892532 | validation: 0.02842440069778839]
	TIME [epoch: 2.66 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017941612649431758		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.017941612649431758 | validation: 0.026850788335233224]
	TIME [epoch: 2.66 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018655462727002324		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.018655462727002324 | validation: 0.02488113994796224]
	TIME [epoch: 2.66 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019180451299074926		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.019180451299074926 | validation: 0.027722655291894372]
	TIME [epoch: 2.67 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019344369771871216		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.019344369771871216 | validation: 0.024933933502246833]
	TIME [epoch: 2.66 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01773898649323586		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.01773898649323586 | validation: 0.02628102309172552]
	TIME [epoch: 2.66 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017004992965841483		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.017004992965841483 | validation: 0.0290507638309246]
	TIME [epoch: 2.66 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019324364390973278		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.019324364390973278 | validation: 0.028263337920614764]
	TIME [epoch: 2.66 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018797815653939214		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.018797815653939214 | validation: 0.028771559279989447]
	TIME [epoch: 2.66 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019383095492062005		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.019383095492062005 | validation: 0.02313516328549664]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_995.pth
	Model improved!!!
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01865872992820827		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.01865872992820827 | validation: 0.030556341372603546]
	TIME [epoch: 2.65 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019499234583491315		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.019499234583491315 | validation: 0.029662155960589878]
	TIME [epoch: 2.65 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017692379858023775		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.017692379858023775 | validation: 0.025645695670616055]
	TIME [epoch: 2.64 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01971491389549194		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.01971491389549194 | validation: 0.03494311210841829]
	TIME [epoch: 2.64 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024269887017538096		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.024269887017538096 | validation: 0.030609184322157902]
	TIME [epoch: 2.65 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022316701731455262		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.022316701731455262 | validation: 0.02882142363301019]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016834460923709438		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.016834460923709438 | validation: 0.03576870173575363]
	TIME [epoch: 5.75 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01932678491370432		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.01932678491370432 | validation: 0.03189514493296324]
	TIME [epoch: 5.71 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018242546849533907		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.018242546849533907 | validation: 0.02533712684259506]
	TIME [epoch: 5.73 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017238771689719056		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.017238771689719056 | validation: 0.028472989001999318]
	TIME [epoch: 5.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017476021671103157		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.017476021671103157 | validation: 0.026098272229997922]
	TIME [epoch: 5.69 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01872569517783955		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.01872569517783955 | validation: 0.02662133726892062]
	TIME [epoch: 5.69 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01752375540753234		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.01752375540753234 | validation: 0.03352802100691415]
	TIME [epoch: 5.71 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017363531113296155		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.017363531113296155 | validation: 0.021369826195492627]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1009.pth
	Model improved!!!
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016661666094703848		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.016661666094703848 | validation: 0.02692839276452468]
	TIME [epoch: 5.71 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017647667167769762		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.017647667167769762 | validation: 0.030475257873491993]
	TIME [epoch: 5.69 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017282411978615245		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.017282411978615245 | validation: 0.025663953019322085]
	TIME [epoch: 5.67 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020343886773532764		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.020343886773532764 | validation: 0.03471952874101508]
	TIME [epoch: 5.72 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017523913722733157		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.017523913722733157 | validation: 0.026659763442659568]
	TIME [epoch: 5.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017562570194458598		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.017562570194458598 | validation: 0.029297257273743507]
	TIME [epoch: 5.69 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01816539201954105		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.01816539201954105 | validation: 0.026879466360026374]
	TIME [epoch: 5.67 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01763159122966066		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.01763159122966066 | validation: 0.026022170393924317]
	TIME [epoch: 5.68 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016825136556267436		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.016825136556267436 | validation: 0.03534675156533997]
	TIME [epoch: 5.68 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01798341856172108		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.01798341856172108 | validation: 0.02487436273863666]
	TIME [epoch: 5.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01781882342352549		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.01781882342352549 | validation: 0.025146194373569953]
	TIME [epoch: 5.67 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018741988557760946		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.018741988557760946 | validation: 0.026711711218963097]
	TIME [epoch: 5.71 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01843114132747059		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.01843114132747059 | validation: 0.026899791660758666]
	TIME [epoch: 5.73 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018305283604719298		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.018305283604719298 | validation: 0.030236338153607514]
	TIME [epoch: 5.71 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018859083440191778		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.018859083440191778 | validation: 0.02130813989831326]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1024.pth
	Model improved!!!
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039161965591654		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.02039161965591654 | validation: 0.02635471317067264]
	TIME [epoch: 5.67 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018452420561209153		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.018452420561209153 | validation: 0.02104182382396025]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1026.pth
	Model improved!!!
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018753413236547785		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.018753413236547785 | validation: 0.02943946357755961]
	TIME [epoch: 5.69 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01648346066604513		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.01648346066604513 | validation: 0.030242441343243755]
	TIME [epoch: 5.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017021721826541856		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.017021721826541856 | validation: 0.025051435273761125]
	TIME [epoch: 5.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020920506835877813		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.020920506835877813 | validation: 0.03127565492033579]
	TIME [epoch: 5.66 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018983179505199156		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.018983179505199156 | validation: 0.025519872635117735]
	TIME [epoch: 5.66 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019432630974828217		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.019432630974828217 | validation: 0.026508532285138022]
	TIME [epoch: 5.68 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017444366048563042		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.017444366048563042 | validation: 0.02500472938493993]
	TIME [epoch: 5.63 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016777924035059302		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.016777924035059302 | validation: 0.02316074400433088]
	TIME [epoch: 5.66 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016732098467761972		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.016732098467761972 | validation: 0.029183058343639646]
	TIME [epoch: 5.63 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01827690589532606		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.01827690589532606 | validation: 0.02375097540743483]
	TIME [epoch: 5.65 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017635986090512264		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.017635986090512264 | validation: 0.026836143018884807]
	TIME [epoch: 5.71 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017621298208219033		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.017621298208219033 | validation: 0.021724321578832275]
	TIME [epoch: 5.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01716089099637701		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.01716089099637701 | validation: 0.02615066310374721]
	TIME [epoch: 5.71 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016562234602801405		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.016562234602801405 | validation: 0.023594014777594943]
	TIME [epoch: 5.69 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01744227491527872		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.01744227491527872 | validation: 0.027777557483310823]
	TIME [epoch: 5.68 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017846670910267974		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.017846670910267974 | validation: 0.027692008643155366]
	TIME [epoch: 5.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020355766641268588		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.020355766641268588 | validation: 0.035176873453070356]
	TIME [epoch: 5.69 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01902163463526737		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.01902163463526737 | validation: 0.024228913557233757]
	TIME [epoch: 5.68 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01831832399479453		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.01831832399479453 | validation: 0.023704313769256403]
	TIME [epoch: 5.67 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015371347114004959		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.015371347114004959 | validation: 0.026146533426311604]
	TIME [epoch: 5.69 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015772044931761847		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.015772044931761847 | validation: 0.03318441049294081]
	TIME [epoch: 5.68 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017194630102212803		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.017194630102212803 | validation: 0.030297302104535663]
	TIME [epoch: 5.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01612597614215784		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.01612597614215784 | validation: 0.02452460000432576]
	TIME [epoch: 5.71 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016821718524069766		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.016821718524069766 | validation: 0.0267879487931734]
	TIME [epoch: 5.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01625148374457479		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.01625148374457479 | validation: 0.0304989177089975]
	TIME [epoch: 5.67 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01758017088551607		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.01758017088551607 | validation: 0.0222782541642093]
	TIME [epoch: 5.68 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016730137827073707		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.016730137827073707 | validation: 0.02221341010038058]
	TIME [epoch: 5.72 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017454982007666972		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.017454982007666972 | validation: 0.029637409841093178]
	TIME [epoch: 5.71 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017527120745586156		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.017527120745586156 | validation: 0.022776328219773004]
	TIME [epoch: 5.73 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01769331326464983		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.01769331326464983 | validation: 0.030837513774369475]
	TIME [epoch: 5.72 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01651044527893646		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.01651044527893646 | validation: 0.02818586759133195]
	TIME [epoch: 5.72 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016976552380577683		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.016976552380577683 | validation: 0.019862886399000038]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1058.pth
	Model improved!!!
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016659665196610593		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.016659665196610593 | validation: 0.029644399484275243]
	TIME [epoch: 5.63 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019764569506634723		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.019764569506634723 | validation: 0.030005759489013618]
	TIME [epoch: 5.69 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020641588294873135		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.020641588294873135 | validation: 0.023308945551951534]
	TIME [epoch: 5.65 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01734666049265264		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.01734666049265264 | validation: 0.020208260160107183]
	TIME [epoch: 5.66 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017191456078491044		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.017191456078491044 | validation: 0.029115246588138445]
	TIME [epoch: 5.62 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017920009970855057		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.017920009970855057 | validation: 0.021068197007669057]
	TIME [epoch: 5.66 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017945428098272816		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.017945428098272816 | validation: 0.028565165687669716]
	TIME [epoch: 5.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01683698213843238		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.01683698213843238 | validation: 0.03060700562455304]
	TIME [epoch: 5.69 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01653729930036987		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.01653729930036987 | validation: 0.028397603238434002]
	TIME [epoch: 5.71 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015995358359083625		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.015995358359083625 | validation: 0.027019517790310088]
	TIME [epoch: 5.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016608520457509754		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.016608520457509754 | validation: 0.030298927133422828]
	TIME [epoch: 5.69 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019286196849493346		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.019286196849493346 | validation: 0.02588705585591157]
	TIME [epoch: 5.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01717210013974026		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.01717210013974026 | validation: 0.027788165495247586]
	TIME [epoch: 5.63 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015605888297277288		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.015605888297277288 | validation: 0.033936274482691896]
	TIME [epoch: 5.64 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017182938167359014		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.017182938167359014 | validation: 0.027949803209850868]
	TIME [epoch: 5.59 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018386379131649092		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.018386379131649092 | validation: 0.022841684904132867]
	TIME [epoch: 5.71 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016956093092168275		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.016956093092168275 | validation: 0.02840540233257165]
	TIME [epoch: 5.69 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01718380461114384		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.01718380461114384 | validation: 0.019155063646926342]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1076.pth
	Model improved!!!
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016981573911079358		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.016981573911079358 | validation: 0.027673341553635957]
	TIME [epoch: 5.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01459906680273404		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.01459906680273404 | validation: 0.025734761151084886]
	TIME [epoch: 5.72 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015719396395156713		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.015719396395156713 | validation: 0.02616556105489423]
	TIME [epoch: 5.67 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017527716734189167		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.017527716734189167 | validation: 0.022139802071185544]
	TIME [epoch: 5.63 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016606835752483777		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.016606835752483777 | validation: 0.02785474681366843]
	TIME [epoch: 5.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017003599929042042		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.017003599929042042 | validation: 0.027857211981612354]
	TIME [epoch: 5.69 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017692256102439102		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.017692256102439102 | validation: 0.025212765389507875]
	TIME [epoch: 5.71 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016027062422899672		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.016027062422899672 | validation: 0.022348283207913756]
	TIME [epoch: 5.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01621174586370732		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.01621174586370732 | validation: 0.02522050417325049]
	TIME [epoch: 5.71 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015844306256394186		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.015844306256394186 | validation: 0.023177703571287414]
	TIME [epoch: 5.72 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016905515828209063		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.016905515828209063 | validation: 0.02322841446298123]
	TIME [epoch: 5.72 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016368861558174946		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.016368861558174946 | validation: 0.028496541869911365]
	TIME [epoch: 5.71 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016875812284144726		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.016875812284144726 | validation: 0.025114444628336608]
	TIME [epoch: 5.71 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016789102284552194		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.016789102284552194 | validation: 0.025177169160724835]
	TIME [epoch: 5.72 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01652053546045229		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.01652053546045229 | validation: 0.026533647520474293]
	TIME [epoch: 5.71 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015146974149148238		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.015146974149148238 | validation: 0.0231142489485229]
	TIME [epoch: 5.71 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01575650946441568		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.01575650946441568 | validation: 0.019199367634898424]
	TIME [epoch: 5.69 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014547925440239395		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.014547925440239395 | validation: 0.023942563903086612]
	TIME [epoch: 5.72 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01584164194148043		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.01584164194148043 | validation: 0.030118270393964586]
	TIME [epoch: 5.71 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01682215540626422		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.01682215540626422 | validation: 0.024799215644742047]
	TIME [epoch: 5.72 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01978906033247926		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.01978906033247926 | validation: 0.025147010300239414]
	TIME [epoch: 5.71 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017320642918032515		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.017320642918032515 | validation: 0.02804034795652316]
	TIME [epoch: 5.72 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01723976364047016		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.01723976364047016 | validation: 0.028424090992722074]
	TIME [epoch: 5.71 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016103392416470318		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.016103392416470318 | validation: 0.023728180719236658]
	TIME [epoch: 5.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01521003971910925		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.01521003971910925 | validation: 0.02279340274599764]
	TIME [epoch: 5.66 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01717820884654626		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.01717820884654626 | validation: 0.024316037170403516]
	TIME [epoch: 5.67 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01629571465847967		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.01629571465847967 | validation: 0.026147257769715816]
	TIME [epoch: 5.67 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015978276175861235		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.015978276175861235 | validation: 0.02958778767670485]
	TIME [epoch: 5.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01695138290561816		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.01695138290561816 | validation: 0.02509487989710442]
	TIME [epoch: 5.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015463431105025299		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.015463431105025299 | validation: 0.025956497842566453]
	TIME [epoch: 5.69 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016162975693580214		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.016162975693580214 | validation: 0.024630048888596857]
	TIME [epoch: 5.71 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016276755364290515		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.016276755364290515 | validation: 0.023456672015503967]
	TIME [epoch: 5.71 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015653083683724853		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.015653083683724853 | validation: 0.026540980425586082]
	TIME [epoch: 5.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016187345201627183		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.016187345201627183 | validation: 0.029157246674903617]
	TIME [epoch: 5.71 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016013614763311503		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.016013614763311503 | validation: 0.025276828848407675]
	TIME [epoch: 5.71 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014623742528323236		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.014623742528323236 | validation: 0.02289169376144199]
	TIME [epoch: 5.71 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016178368548784357		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.016178368548784357 | validation: 0.024271908790530285]
	TIME [epoch: 5.71 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016353893016051276		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.016353893016051276 | validation: 0.02426311944172004]
	TIME [epoch: 5.72 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0170130988220907		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.0170130988220907 | validation: 0.022861380918774522]
	TIME [epoch: 5.71 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015240404103562132		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.015240404103562132 | validation: 0.031327019537094915]
	TIME [epoch: 5.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01654963625304243		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.01654963625304243 | validation: 0.023545088878361178]
	TIME [epoch: 5.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015663403962642628		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.015663403962642628 | validation: 0.027410501974794867]
	TIME [epoch: 5.72 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015530947781077562		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.015530947781077562 | validation: 0.03347203166227071]
	TIME [epoch: 5.71 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01634507310674587		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.01634507310674587 | validation: 0.020539791040122504]
	TIME [epoch: 5.71 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016013991226599886		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.016013991226599886 | validation: 0.0225133695922793]
	TIME [epoch: 5.72 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015364602474132504		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.015364602474132504 | validation: 0.026818679672666636]
	TIME [epoch: 5.71 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016864605921907184		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.016864605921907184 | validation: 0.035260118478542324]
	TIME [epoch: 5.71 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018981926913899518		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.018981926913899518 | validation: 0.025501780495763262]
	TIME [epoch: 5.72 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016760385307082		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.016760385307082 | validation: 0.024067893142802988]
	TIME [epoch: 5.71 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015677016992622556		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.015677016992622556 | validation: 0.029990482519871433]
	TIME [epoch: 5.71 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015587197858507238		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.015587197858507238 | validation: 0.019141508738565828]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015264480789540463		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.015264480789540463 | validation: 0.022024337600027068]
	TIME [epoch: 5.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01619823634494517		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.01619823634494517 | validation: 0.02228792505299362]
	TIME [epoch: 5.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01554444343637834		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.01554444343637834 | validation: 0.025030531219540766]
	TIME [epoch: 5.71 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014801110778709547		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.014801110778709547 | validation: 0.025049686831005093]
	TIME [epoch: 5.69 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016385151858444077		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.016385151858444077 | validation: 0.026055792710427017]
	TIME [epoch: 5.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015315442261315955		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.015315442261315955 | validation: 0.01972210991272463]
	TIME [epoch: 5.71 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01663004005939734		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.01663004005939734 | validation: 0.023385468017823]
	TIME [epoch: 5.71 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014856309077637287		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.014856309077637287 | validation: 0.022008901178680942]
	TIME [epoch: 5.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015262609666534704		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.015262609666534704 | validation: 0.02832271321126817]
	TIME [epoch: 5.71 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014898068481314012		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.014898068481314012 | validation: 0.02387399501063069]
	TIME [epoch: 5.71 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015201940051857972		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.015201940051857972 | validation: 0.02297114758721195]
	TIME [epoch: 5.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01497159941089992		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.01497159941089992 | validation: 0.02107289932771437]
	TIME [epoch: 5.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01547137514468874		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.01547137514468874 | validation: 0.02547434684915663]
	TIME [epoch: 5.71 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01529100853499335		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.01529100853499335 | validation: 0.026928064798177012]
	TIME [epoch: 5.72 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015251392679159915		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.015251392679159915 | validation: 0.02407727049970906]
	TIME [epoch: 5.74 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016556534921483852		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.016556534921483852 | validation: 0.026825159286823344]
	TIME [epoch: 5.71 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019361407296130246		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.019361407296130246 | validation: 0.02174382896459295]
	TIME [epoch: 5.71 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0157463578493643		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.0157463578493643 | validation: 0.028265611963805948]
	TIME [epoch: 5.71 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016398417229139425		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.016398417229139425 | validation: 0.022208000632534454]
	TIME [epoch: 5.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01825090438279871		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.01825090438279871 | validation: 0.02286598103286396]
	TIME [epoch: 5.68 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015096431141650797		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.015096431141650797 | validation: 0.02448377996478216]
	TIME [epoch: 5.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015105141442813958		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.015105141442813958 | validation: 0.0265621783487945]
	TIME [epoch: 5.71 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015446150259878824		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.015446150259878824 | validation: 0.024945536673910973]
	TIME [epoch: 5.72 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015022609113722695		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.015022609113722695 | validation: 0.023922145853803645]
	TIME [epoch: 5.71 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016193493516221583		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.016193493516221583 | validation: 0.023279401176542914]
	TIME [epoch: 5.69 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015145746461356371		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.015145746461356371 | validation: 0.02108863656308282]
	TIME [epoch: 5.72 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015767596328367595		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.015767596328367595 | validation: 0.023703262861020615]
	TIME [epoch: 5.72 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01603444016346629		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.01603444016346629 | validation: 0.02364027569534709]
	TIME [epoch: 5.73 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015016268669896462		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.015016268669896462 | validation: 0.03161746676032897]
	TIME [epoch: 5.72 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015030690248205188		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.015030690248205188 | validation: 0.028556945530345035]
	TIME [epoch: 5.71 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015279686439982898		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.015279686439982898 | validation: 0.021715184143650493]
	TIME [epoch: 5.72 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01630190345749346		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.01630190345749346 | validation: 0.028741430845337358]
	TIME [epoch: 5.75 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01567149188576395		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.01567149188576395 | validation: 0.01931631038222822]
	TIME [epoch: 5.72 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01532161375375312		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.01532161375375312 | validation: 0.01947471262583851]
	TIME [epoch: 5.71 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014796733362441033		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.014796733362441033 | validation: 0.02381462483521435]
	TIME [epoch: 5.7 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01552335754779618		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.01552335754779618 | validation: 0.018712356383134378]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1163.pth
	Model improved!!!
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01514266997241749		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.01514266997241749 | validation: 0.027160131135088673]
	TIME [epoch: 5.72 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015710729244154627		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.015710729244154627 | validation: 0.025688623956330304]
	TIME [epoch: 5.71 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015187952415447487		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.015187952415447487 | validation: 0.024134005690097694]
	TIME [epoch: 5.71 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014971734292843063		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.014971734292843063 | validation: 0.021205447506584776]
	TIME [epoch: 5.71 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01672947473468721		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.01672947473468721 | validation: 0.022992685461541065]
	TIME [epoch: 5.72 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01700571024612367		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.01700571024612367 | validation: 0.023867438889327398]
	TIME [epoch: 5.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014235232308690324		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.014235232308690324 | validation: 0.020320518373460796]
	TIME [epoch: 5.71 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014686391972475508		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.014686391972475508 | validation: 0.023239177888024567]
	TIME [epoch: 5.71 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01560608053984124		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.01560608053984124 | validation: 0.026927018540712857]
	TIME [epoch: 5.71 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015521982918802997		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.015521982918802997 | validation: 0.025842759190633904]
	TIME [epoch: 5.71 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015366179420034282		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.015366179420034282 | validation: 0.02000013300329614]
	TIME [epoch: 5.71 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01564682186786723		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.01564682186786723 | validation: 0.025565375118311907]
	TIME [epoch: 5.73 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015018695930601834		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.015018695930601834 | validation: 0.029556190105456983]
	TIME [epoch: 5.69 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017618363314893186		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.017618363314893186 | validation: 0.021640645304831244]
	TIME [epoch: 5.71 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01486909679751597		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.01486909679751597 | validation: 0.018042471839090513]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1178.pth
	Model improved!!!
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017227075448545126		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.017227075448545126 | validation: 0.02387217575879872]
	TIME [epoch: 5.71 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015222870260110757		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.015222870260110757 | validation: 0.027073633872169545]
	TIME [epoch: 5.71 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015326105095253438		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.015326105095253438 | validation: 0.0232983423967751]
	TIME [epoch: 5.71 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014914718793069206		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.014914718793069206 | validation: 0.023391873659126164]
	TIME [epoch: 5.71 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016997035434733906		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.016997035434733906 | validation: 0.0183241290041434]
	TIME [epoch: 5.71 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015236151626781807		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.015236151626781807 | validation: 0.023422193081592727]
	TIME [epoch: 5.71 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017617527136261687		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.017617527136261687 | validation: 0.028467902545000635]
	TIME [epoch: 5.72 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014312703071138055		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.014312703071138055 | validation: 0.021868797307646837]
	TIME [epoch: 5.71 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014566329599621427		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.014566329599621427 | validation: 0.024547988341598017]
	TIME [epoch: 5.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013992419322891927		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.013992419322891927 | validation: 0.025441533240768144]
	TIME [epoch: 5.72 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014870277398282462		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.014870277398282462 | validation: 0.02482277482507358]
	TIME [epoch: 5.71 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01394483984662466		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.01394483984662466 | validation: 0.024421683077661006]
	TIME [epoch: 5.72 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015797995071276306		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.015797995071276306 | validation: 0.02079990686816411]
	TIME [epoch: 5.69 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014002750254214333		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.014002750254214333 | validation: 0.025109936980029515]
	TIME [epoch: 5.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01442375663767392		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.01442375663767392 | validation: 0.021467747507069014]
	TIME [epoch: 5.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014880559678263775		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.014880559678263775 | validation: 0.0224255616287655]
	TIME [epoch: 5.72 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01545338877802269		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.01545338877802269 | validation: 0.02448236894101772]
	TIME [epoch: 5.72 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014950838850867534		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.014950838850867534 | validation: 0.0204594538809336]
	TIME [epoch: 5.71 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015300269036157386		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.015300269036157386 | validation: 0.020535808714011807]
	TIME [epoch: 5.72 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014531377157725922		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.014531377157725922 | validation: 0.022947775021613184]
	TIME [epoch: 5.72 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014136218492388561		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.014136218492388561 | validation: 0.028931580632509737]
	TIME [epoch: 5.71 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015009530117141817		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.015009530117141817 | validation: 0.022721236924871713]
	TIME [epoch: 5.71 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014460788732873282		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.014460788732873282 | validation: 0.02294560439499952]
	TIME [epoch: 5.71 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014632717602562031		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.014632717602562031 | validation: 0.022142538280721637]
	TIME [epoch: 5.72 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015416032838511649		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.015416032838511649 | validation: 0.021403984440484317]
	TIME [epoch: 5.72 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013554746588440443		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.013554746588440443 | validation: 0.020942625627458102]
	TIME [epoch: 5.72 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014616168605971043		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.014616168605971043 | validation: 0.02218604006978099]
	TIME [epoch: 5.71 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01452927802616176		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.01452927802616176 | validation: 0.02103524421469203]
	TIME [epoch: 5.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015898068260169616		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.015898068260169616 | validation: 0.020176767720969713]
	TIME [epoch: 5.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016034285880953753		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.016034285880953753 | validation: 0.022669104203803605]
	TIME [epoch: 5.71 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014999728483249944		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.014999728483249944 | validation: 0.018691355748024142]
	TIME [epoch: 5.72 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014147775133211016		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.014147775133211016 | validation: 0.027088079891104868]
	TIME [epoch: 5.71 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01551534610121109		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.01551534610121109 | validation: 0.021830406088031974]
	TIME [epoch: 5.71 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01450279515605193		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.01450279515605193 | validation: 0.022966357891084244]
	TIME [epoch: 5.71 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016185698677908814		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.016185698677908814 | validation: 0.020771540135340584]
	TIME [epoch: 5.72 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015071485637569403		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.015071485637569403 | validation: 0.021895414560921724]
	TIME [epoch: 5.72 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013297114115704098		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.013297114115704098 | validation: 0.023540668542037915]
	TIME [epoch: 5.71 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014069689389413291		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.014069689389413291 | validation: 0.021998774211223007]
	TIME [epoch: 5.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0150264337407193		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.0150264337407193 | validation: 0.021980292741669596]
	TIME [epoch: 5.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01444999371231944		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.01444999371231944 | validation: 0.02259382919275679]
	TIME [epoch: 5.72 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014209770507315835		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.014209770507315835 | validation: 0.02048534219947952]
	TIME [epoch: 5.71 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014020632631746084		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.014020632631746084 | validation: 0.016853717425818437]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1220.pth
	Model improved!!!
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015280494955592984		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.015280494955592984 | validation: 0.027201157332050887]
	TIME [epoch: 5.73 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01802558286466757		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.01802558286466757 | validation: 0.020342135968780973]
	TIME [epoch: 5.74 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014448605845430059		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.014448605845430059 | validation: 0.022150820052076605]
	TIME [epoch: 5.73 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0165997342502472		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.0165997342502472 | validation: 0.020821471909579073]
	TIME [epoch: 5.74 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015064786210809077		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.015064786210809077 | validation: 0.0221835003450395]
	TIME [epoch: 5.73 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013508041037880739		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.013508041037880739 | validation: 0.025513447106903033]
	TIME [epoch: 5.74 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015128001147658774		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.015128001147658774 | validation: 0.019939729170464437]
	TIME [epoch: 5.73 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014130317769279255		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.014130317769279255 | validation: 0.01734037877581326]
	TIME [epoch: 5.73 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015025363831632582		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.015025363831632582 | validation: 0.01880931434837927]
	TIME [epoch: 5.73 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014894864549789468		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.014894864549789468 | validation: 0.02315446031185189]
	TIME [epoch: 5.74 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015787422691539738		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.015787422691539738 | validation: 0.023638336139209605]
	TIME [epoch: 5.73 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01432613197580566		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.01432613197580566 | validation: 0.019593430960934067]
	TIME [epoch: 5.75 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015329498135713842		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.015329498135713842 | validation: 0.01779571236607037]
	TIME [epoch: 5.72 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014744521176183458		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.014744521176183458 | validation: 0.02116679624964124]
	TIME [epoch: 5.73 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013588523474212178		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.013588523474212178 | validation: 0.021443816063337076]
	TIME [epoch: 5.74 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015661148111108657		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.015661148111108657 | validation: 0.02367290262650611]
	TIME [epoch: 5.74 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014332719915440571		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.014332719915440571 | validation: 0.019165120689239958]
	TIME [epoch: 5.74 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013843706746972143		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.013843706746972143 | validation: 0.017444953621073678]
	TIME [epoch: 5.74 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013615105057302897		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.013615105057302897 | validation: 0.021413144314893873]
	TIME [epoch: 5.74 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015304154451255006		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.015304154451255006 | validation: 0.02156051379457219]
	TIME [epoch: 5.73 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014687006609437808		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.014687006609437808 | validation: 0.019472987809254695]
	TIME [epoch: 5.74 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014722662113687862		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.014722662113687862 | validation: 0.021383107905571342]
	TIME [epoch: 5.74 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016021963411431778		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.016021963411431778 | validation: 0.023609937569963504]
	TIME [epoch: 5.74 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01417764890887875		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.01417764890887875 | validation: 0.027767511582650797]
	TIME [epoch: 5.74 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015062473868770227		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.015062473868770227 | validation: 0.015587420698514633]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1245.pth
	Model improved!!!
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013788329899191194		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.013788329899191194 | validation: 0.01773768123062469]
	TIME [epoch: 5.7 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013713024730555134		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.013713024730555134 | validation: 0.02866154524144501]
	TIME [epoch: 5.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014043086344514646		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.014043086344514646 | validation: 0.020500525105974967]
	TIME [epoch: 5.68 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013817664706145111		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.013817664706145111 | validation: 0.022818082427376374]
	TIME [epoch: 5.71 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014067461201869259		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.014067461201869259 | validation: 0.021415553866713455]
	TIME [epoch: 5.7 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015500911928149425		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.015500911928149425 | validation: 0.02141773681396939]
	TIME [epoch: 5.71 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014243760325716568		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.014243760325716568 | validation: 0.021594192189435025]
	TIME [epoch: 5.71 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016626561321268		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.016626561321268 | validation: 0.019261330881126682]
	TIME [epoch: 5.71 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014627013375585479		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.014627013375585479 | validation: 0.026376452959941413]
	TIME [epoch: 5.71 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014184686240918612		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.014184686240918612 | validation: 0.02520258482118707]
	TIME [epoch: 5.71 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013989576436820413		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.013989576436820413 | validation: 0.020125426168885897]
	TIME [epoch: 5.7 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014654073145031005		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.014654073145031005 | validation: 0.015546289196064635]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1257.pth
	Model improved!!!
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014614523469134846		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.014614523469134846 | validation: 0.021598230936173492]
	TIME [epoch: 5.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013338186164934674		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.013338186164934674 | validation: 0.020798536840501827]
	TIME [epoch: 5.72 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014512265483841644		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.014512265483841644 | validation: 0.020037911455764924]
	TIME [epoch: 5.74 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014920584688475956		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.014920584688475956 | validation: 0.020472699781073835]
	TIME [epoch: 5.72 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01461996513213009		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.01461996513213009 | validation: 0.020528583270345437]
	TIME [epoch: 5.74 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013798577728649054		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.013798577728649054 | validation: 0.021815169689540472]
	TIME [epoch: 5.74 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014292320595531444		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.014292320595531444 | validation: 0.021866224411590852]
	TIME [epoch: 5.73 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014178202027699385		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.014178202027699385 | validation: 0.02471907026761322]
	TIME [epoch: 5.73 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014974344126448847		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.014974344126448847 | validation: 0.016361620714706072]
	TIME [epoch: 5.74 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015475752214643573		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.015475752214643573 | validation: 0.023000635419043493]
	TIME [epoch: 5.72 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014166932729672892		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.014166932729672892 | validation: 0.019472922605003964]
	TIME [epoch: 5.72 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013992871722622542		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.013992871722622542 | validation: 0.01883216075739105]
	TIME [epoch: 5.73 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013186643496862018		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.013186643496862018 | validation: 0.020236803498509426]
	TIME [epoch: 5.73 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014342849054070352		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.014342849054070352 | validation: 0.02000463432300479]
	TIME [epoch: 5.73 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0143869976090968		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.0143869976090968 | validation: 0.01725033475025246]
	TIME [epoch: 5.73 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013802230044466915		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.013802230044466915 | validation: 0.020955331843842608]
	TIME [epoch: 5.72 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014245615309023622		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.014245615309023622 | validation: 0.02047299927447456]
	TIME [epoch: 5.73 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014447702338791984		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.014447702338791984 | validation: 0.022535421192008465]
	TIME [epoch: 5.73 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014435861611241607		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.014435861611241607 | validation: 0.02327419898080171]
	TIME [epoch: 5.74 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0144333247841934		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.0144333247841934 | validation: 0.023219694233307754]
	TIME [epoch: 5.73 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014509214738290989		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.014509214738290989 | validation: 0.021643519810319523]
	TIME [epoch: 5.74 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015422311957383247		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.015422311957383247 | validation: 0.021584649426646365]
	TIME [epoch: 5.74 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015431292958864425		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.015431292958864425 | validation: 0.022833026625272812]
	TIME [epoch: 5.74 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014243566304868421		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.014243566304868421 | validation: 0.018548249892663304]
	TIME [epoch: 5.72 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012963134083865917		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.012963134083865917 | validation: 0.019710717913175226]
	TIME [epoch: 5.73 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014028103063971403		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.014028103063971403 | validation: 0.016949351316441808]
	TIME [epoch: 5.73 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013893618051606467		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.013893618051606467 | validation: 0.021126777870503745]
	TIME [epoch: 5.74 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014269108738501133		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.014269108738501133 | validation: 0.019848396820451975]
	TIME [epoch: 5.74 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014515973092532684		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.014515973092532684 | validation: 0.0179934615458364]
	TIME [epoch: 5.74 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01444022676014727		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.01444022676014727 | validation: 0.01998150295798791]
	TIME [epoch: 5.74 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013734915753604748		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.013734915753604748 | validation: 0.019482018821259862]
	TIME [epoch: 5.75 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013236291061614471		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.013236291061614471 | validation: 0.019594810321996992]
	TIME [epoch: 5.73 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014283400225565259		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.014283400225565259 | validation: 0.02209382661831182]
	TIME [epoch: 5.74 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015618388545535314		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.015618388545535314 | validation: 0.025542923398676632]
	TIME [epoch: 5.73 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014206294170056338		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.014206294170056338 | validation: 0.021634249598806644]
	TIME [epoch: 5.76 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014502408193862552		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.014502408193862552 | validation: 0.018409772879403076]
	TIME [epoch: 5.72 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014179609381443926		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.014179609381443926 | validation: 0.022096668416112054]
	TIME [epoch: 5.75 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013838238414021563		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.013838238414021563 | validation: 0.01779282268183725]
	TIME [epoch: 5.74 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012972700769195369		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.012972700769195369 | validation: 0.02027725481276176]
	TIME [epoch: 5.74 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014046393988944418		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.014046393988944418 | validation: 0.019939600856032236]
	TIME [epoch: 5.74 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014182138825675136		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.014182138825675136 | validation: 0.015482673965866235]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1298.pth
	Model improved!!!
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01414943987695676		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.01414943987695676 | validation: 0.021873310453948747]
	TIME [epoch: 5.72 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013952390161000043		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.013952390161000043 | validation: 0.021968163744364857]
	TIME [epoch: 5.73 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015120918558476668		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.015120918558476668 | validation: 0.016335401482623246]
	TIME [epoch: 5.73 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015032028543662666		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.015032028543662666 | validation: 0.019436708572665218]
	TIME [epoch: 5.73 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014144026800787919		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.014144026800787919 | validation: 0.02217230259036386]
	TIME [epoch: 5.71 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01477592222591047		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.01477592222591047 | validation: 0.02085438271375899]
	TIME [epoch: 5.71 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013450922428089875		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.013450922428089875 | validation: 0.02269705248199132]
	TIME [epoch: 5.7 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01338610516414004		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.01338610516414004 | validation: 0.019190596399781858]
	TIME [epoch: 5.71 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013800287430181484		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.013800287430181484 | validation: 0.02179101313422659]
	TIME [epoch: 5.7 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013140345617427723		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.013140345617427723 | validation: 0.018713126187508644]
	TIME [epoch: 5.7 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013999959007826183		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.013999959007826183 | validation: 0.019533187765701378]
	TIME [epoch: 5.7 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013467494426030853		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.013467494426030853 | validation: 0.021968256117682928]
	TIME [epoch: 5.71 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015134181737535104		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.015134181737535104 | validation: 0.01916008950981342]
	TIME [epoch: 5.71 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013514996563962224		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.013514996563962224 | validation: 0.020782451403495373]
	TIME [epoch: 5.7 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015284360935803487		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.015284360935803487 | validation: 0.01915086635026937]
	TIME [epoch: 5.7 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013655765511691986		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.013655765511691986 | validation: 0.015611834612052324]
	TIME [epoch: 5.7 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014785615364742018		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.014785615364742018 | validation: 0.021098446166624953]
	TIME [epoch: 5.69 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014823689949441122		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.014823689949441122 | validation: 0.020027506683184496]
	TIME [epoch: 5.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015577686196545489		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.015577686196545489 | validation: 0.0212318821921206]
	TIME [epoch: 5.69 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014363045084932566		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.014363045084932566 | validation: 0.019980635576573494]
	TIME [epoch: 5.69 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013688478435029605		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.013688478435029605 | validation: 0.021976920496180875]
	TIME [epoch: 5.69 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01453401009946727		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.01453401009946727 | validation: 0.01824313988320623]
	TIME [epoch: 5.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013703851361957021		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.013703851361957021 | validation: 0.02039986617927474]
	TIME [epoch: 5.69 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0133052919573944		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.0133052919573944 | validation: 0.022558793037220238]
	TIME [epoch: 5.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013163125307806452		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.013163125307806452 | validation: 0.016800907794198485]
	TIME [epoch: 5.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013375986724932756		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.013375986724932756 | validation: 0.01884242578364247]
	TIME [epoch: 5.71 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013799071387182185		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.013799071387182185 | validation: 0.025958015107125712]
	TIME [epoch: 5.7 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01380138965274062		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.01380138965274062 | validation: 0.01882109541754532]
	TIME [epoch: 5.69 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014495951588985931		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.014495951588985931 | validation: 0.022160744005941892]
	TIME [epoch: 5.7 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013845806974933692		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.013845806974933692 | validation: 0.019278703170257205]
	TIME [epoch: 5.69 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013984814888151976		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.013984814888151976 | validation: 0.017535907203632795]
	TIME [epoch: 5.7 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013544701201903535		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.013544701201903535 | validation: 0.018055624708801933]
	TIME [epoch: 5.7 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012642724638350193		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.012642724638350193 | validation: 0.026397077107936175]
	TIME [epoch: 5.7 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013251676446646888		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.013251676446646888 | validation: 0.023836477728074425]
	TIME [epoch: 5.71 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013829784792460578		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.013829784792460578 | validation: 0.016668632020568265]
	TIME [epoch: 5.71 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013321435745399787		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.013321435745399787 | validation: 0.01928455808056766]
	TIME [epoch: 5.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013783899897553561		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.013783899897553561 | validation: 0.017710730206868985]
	TIME [epoch: 5.7 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013588948299978346		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.013588948299978346 | validation: 0.018670766614673873]
	TIME [epoch: 5.72 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014162492117578956		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.014162492117578956 | validation: 0.016842833260455003]
	TIME [epoch: 5.72 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014149918945125396		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.014149918945125396 | validation: 0.018731018146204315]
	TIME [epoch: 5.73 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013671148569571258		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.013671148569571258 | validation: 0.01745975707536086]
	TIME [epoch: 5.72 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012242560766100237		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.012242560766100237 | validation: 0.01607366772261337]
	TIME [epoch: 5.73 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014125636193845268		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.014125636193845268 | validation: 0.023176434348441723]
	TIME [epoch: 5.73 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014046822899322058		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.014046822899322058 | validation: 0.019108326951404544]
	TIME [epoch: 5.72 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014115868740991827		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.014115868740991827 | validation: 0.021795427753686248]
	TIME [epoch: 5.72 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014170552020136623		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.014170552020136623 | validation: 0.016363957974586063]
	TIME [epoch: 5.74 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014802045061341454		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.014802045061341454 | validation: 0.01685322601848468]
	TIME [epoch: 5.73 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013447290477457048		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.013447290477457048 | validation: 0.021291872796609615]
	TIME [epoch: 5.74 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013443633564594026		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.013443633564594026 | validation: 0.022851814880817136]
	TIME [epoch: 5.73 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014834332610983015		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.014834332610983015 | validation: 0.018005529893431405]
	TIME [epoch: 5.74 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012982881281987036		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.012982881281987036 | validation: 0.017830921000657998]
	TIME [epoch: 5.73 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014807020913774368		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.014807020913774368 | validation: 0.01939976107052929]
	TIME [epoch: 5.73 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013686809652766642		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.013686809652766642 | validation: 0.02575876206096539]
	TIME [epoch: 5.72 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014207090213077684		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.014207090213077684 | validation: 0.020356205365871162]
	TIME [epoch: 5.73 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014086255547879829		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.014086255547879829 | validation: 0.01791193980224799]
	TIME [epoch: 5.73 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013966527062889242		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.013966527062889242 | validation: 0.014466041441810552]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1354.pth
	Model improved!!!
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013521348158305458		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.013521348158305458 | validation: 0.01857969908444984]
	TIME [epoch: 5.74 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014397134302489508		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.014397134302489508 | validation: 0.019170249874058288]
	TIME [epoch: 5.72 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014008912627092678		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.014008912627092678 | validation: 0.017738000838905244]
	TIME [epoch: 5.71 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013776641759717488		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.013776641759717488 | validation: 0.02061769095040147]
	TIME [epoch: 5.71 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013278994749843998		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.013278994749843998 | validation: 0.02068553213250294]
	TIME [epoch: 5.68 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014821546298921615		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.014821546298921615 | validation: 0.018523120828877283]
	TIME [epoch: 5.71 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012855746053366056		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.012855746053366056 | validation: 0.02085417831786103]
	TIME [epoch: 5.7 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013244766436761953		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.013244766436761953 | validation: 0.017266027503240855]
	TIME [epoch: 5.7 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01395813383774547		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.01395813383774547 | validation: 0.02159572045061361]
	TIME [epoch: 5.72 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013202773606383698		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.013202773606383698 | validation: 0.019181996418194937]
	TIME [epoch: 5.73 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013898607784909816		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.013898607784909816 | validation: 0.017319652721169556]
	TIME [epoch: 5.73 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013513737116806541		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.013513737116806541 | validation: 0.019836502338931774]
	TIME [epoch: 5.73 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013992482426320679		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.013992482426320679 | validation: 0.01826737364986101]
	TIME [epoch: 5.72 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014364753032841376		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.014364753032841376 | validation: 0.02099571654655501]
	TIME [epoch: 5.72 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012867188816334914		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.012867188816334914 | validation: 0.015999116331829245]
	TIME [epoch: 5.73 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01374946173886376		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.01374946173886376 | validation: 0.019435821102260765]
	TIME [epoch: 5.72 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01236482048478934		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.01236482048478934 | validation: 0.022949839845461308]
	TIME [epoch: 5.73 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012933175253295124		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.012933175253295124 | validation: 0.022209523065306092]
	TIME [epoch: 5.73 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013815866552466551		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.013815866552466551 | validation: 0.0235075898969475]
	TIME [epoch: 5.73 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014866451585236947		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.014866451585236947 | validation: 0.019170739538708983]
	TIME [epoch: 5.73 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014100922741659852		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.014100922741659852 | validation: 0.01792079423274383]
	TIME [epoch: 5.73 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012251881182804899		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.012251881182804899 | validation: 0.016786645352887188]
	TIME [epoch: 5.73 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01277691096818567		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.01277691096818567 | validation: 0.020213173637874074]
	TIME [epoch: 5.73 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012879501197090975		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.012879501197090975 | validation: 0.01796435415033001]
	TIME [epoch: 5.72 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013225882683925874		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.013225882683925874 | validation: 0.020314234333330274]
	TIME [epoch: 5.73 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012857628076920676		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.012857628076920676 | validation: 0.018366044538338965]
	TIME [epoch: 5.72 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013303820915636538		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.013303820915636538 | validation: 0.018862092822350886]
	TIME [epoch: 5.72 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014312860146219611		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.014312860146219611 | validation: 0.015105632500606337]
	TIME [epoch: 5.72 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012861134510584758		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.012861134510584758 | validation: 0.017652496033944486]
	TIME [epoch: 5.69 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014238561455957602		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.014238561455957602 | validation: 0.015538854893597066]
	TIME [epoch: 5.73 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012981025470613835		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.012981025470613835 | validation: 0.022911050983294302]
	TIME [epoch: 5.73 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013005143750444035		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.013005143750444035 | validation: 0.01867099685058693]
	TIME [epoch: 5.72 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012732533279212128		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.012732533279212128 | validation: 0.016893555079167788]
	TIME [epoch: 5.72 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012956471795841883		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.012956471795841883 | validation: 0.020552283822992547]
	TIME [epoch: 5.71 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013210700711619125		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.013210700711619125 | validation: 0.02294866218262175]
	TIME [epoch: 5.72 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013260851443891438		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.013260851443891438 | validation: 0.01961206332376171]
	TIME [epoch: 5.73 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01387917351342844		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.01387917351342844 | validation: 0.018704069004177516]
	TIME [epoch: 5.73 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013270496509292545		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.013270496509292545 | validation: 0.01876870951794677]
	TIME [epoch: 5.73 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013452493764997549		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.013452493764997549 | validation: 0.018277707428508906]
	TIME [epoch: 5.73 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013303813563478267		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.013303813563478267 | validation: 0.018274657091108138]
	TIME [epoch: 5.72 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013081434134710537		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.013081434134710537 | validation: 0.01872184023978787]
	TIME [epoch: 5.73 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014222142432658269		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.014222142432658269 | validation: 0.020364451724026858]
	TIME [epoch: 5.72 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012829795257674262		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.012829795257674262 | validation: 0.015971725223837264]
	TIME [epoch: 5.72 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012519426814064984		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.012519426814064984 | validation: 0.017399715128761928]
	TIME [epoch: 5.73 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01389209384087648		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.01389209384087648 | validation: 0.01774323909228782]
	TIME [epoch: 5.73 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013429003417668079		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.013429003417668079 | validation: 0.017566761857060686]
	TIME [epoch: 5.73 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013229809408426557		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.013229809408426557 | validation: 0.019163163313336742]
	TIME [epoch: 5.75 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012805801623052417		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.012805801623052417 | validation: 0.01803905547693472]
	TIME [epoch: 5.72 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013117135674079477		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.013117135674079477 | validation: 0.016445530085528737]
	TIME [epoch: 5.71 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013384433700797836		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.013384433700797836 | validation: 0.01652727404362664]
	TIME [epoch: 5.7 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013400050805858826		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.013400050805858826 | validation: 0.020016841255642695]
	TIME [epoch: 5.73 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013059496707978803		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.013059496707978803 | validation: 0.020459742567739816]
	TIME [epoch: 5.72 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01355367904284013		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.01355367904284013 | validation: 0.017943692998041827]
	TIME [epoch: 5.73 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012999889039495043		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.012999889039495043 | validation: 0.01650495977348466]
	TIME [epoch: 5.72 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01242107807207594		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.01242107807207594 | validation: 0.021060920901299962]
	TIME [epoch: 5.73 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013158162666540385		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.013158162666540385 | validation: 0.019384520989522425]
	TIME [epoch: 5.73 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013917454898273673		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.013917454898273673 | validation: 0.019768095664591624]
	TIME [epoch: 5.72 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014388842189380843		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.014388842189380843 | validation: 0.018590720871256416]
	TIME [epoch: 5.72 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013256782828921225		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.013256782828921225 | validation: 0.018258735298870156]
	TIME [epoch: 5.72 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013722369654612297		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.013722369654612297 | validation: 0.016902388498280695]
	TIME [epoch: 5.73 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012772901472673313		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.012772901472673313 | validation: 0.017341949268571365]
	TIME [epoch: 5.73 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014075186847845225		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.014075186847845225 | validation: 0.01966249752214664]
	TIME [epoch: 5.72 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01266173527409894		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.01266173527409894 | validation: 0.02216755114136412]
	TIME [epoch: 5.73 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013655535122144742		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.013655535122144742 | validation: 0.02001593955309147]
	TIME [epoch: 5.73 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013273088634912958		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.013273088634912958 | validation: 0.013874869035405203]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1419.pth
	Model improved!!!
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014000842220514259		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.014000842220514259 | validation: 0.02505964165640745]
	TIME [epoch: 5.72 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01329202590625933		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.01329202590625933 | validation: 0.018247438558408047]
	TIME [epoch: 5.73 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012845711895475388		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.012845711895475388 | validation: 0.01699402500235301]
	TIME [epoch: 5.73 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012530149629974115		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.012530149629974115 | validation: 0.015380387464454815]
	TIME [epoch: 5.71 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013111704631978239		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.013111704631978239 | validation: 0.018079666235249894]
	TIME [epoch: 5.73 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013359628667380132		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.013359628667380132 | validation: 0.019360848764608807]
	TIME [epoch: 5.71 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013424202016885132		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.013424202016885132 | validation: 0.021814655284967656]
	TIME [epoch: 5.75 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014092760610218634		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.014092760610218634 | validation: 0.01868162231637758]
	TIME [epoch: 5.73 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013223886550654264		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.013223886550654264 | validation: 0.01871003260433255]
	TIME [epoch: 5.72 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013097875514556066		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.013097875514556066 | validation: 0.015981889719287636]
	TIME [epoch: 5.73 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012466718926326724		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.012466718926326724 | validation: 0.019173709962218878]
	TIME [epoch: 5.73 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01247221007540301		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.01247221007540301 | validation: 0.016359032428744435]
	TIME [epoch: 5.72 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013577478017790101		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.013577478017790101 | validation: 0.01989023286184918]
	TIME [epoch: 5.73 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012932988465865552		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.012932988465865552 | validation: 0.022306273996850548]
	TIME [epoch: 5.73 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012356687108667457		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.012356687108667457 | validation: 0.015303098592580633]
	TIME [epoch: 5.73 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013924674951537241		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.013924674951537241 | validation: 0.015285110628467026]
	TIME [epoch: 5.71 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012932697224784		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.012932697224784 | validation: 0.015292229071329278]
	TIME [epoch: 5.73 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01266075013235693		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.01266075013235693 | validation: 0.015460510333314048]
	TIME [epoch: 5.72 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012859315706281507		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.012859315706281507 | validation: 0.015829928410030194]
	TIME [epoch: 5.73 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013097142020160253		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.013097142020160253 | validation: 0.01799003822568176]
	TIME [epoch: 5.72 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012958668060779385		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.012958668060779385 | validation: 0.016905944111504544]
	TIME [epoch: 5.72 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013108341499508586		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.013108341499508586 | validation: 0.013812285115417622]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1441.pth
	Model improved!!!
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01294789896805205		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.01294789896805205 | validation: 0.01597194941931217]
	TIME [epoch: 5.72 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013491134848968142		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.013491134848968142 | validation: 0.024564937310354352]
	TIME [epoch: 5.73 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013159868240137573		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.013159868240137573 | validation: 0.023799431908346116]
	TIME [epoch: 5.73 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01359194371733245		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.01359194371733245 | validation: 0.01985018739806891]
	TIME [epoch: 5.71 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012877568349692455		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.012877568349692455 | validation: 0.018632442031464646]
	TIME [epoch: 5.73 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012149476844622713		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.012149476844622713 | validation: 0.01686789137157837]
	TIME [epoch: 5.69 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012847247017840494		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.012847247017840494 | validation: 0.017577879974463795]
	TIME [epoch: 5.74 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012736506399860223		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.012736506399860223 | validation: 0.019592328577773224]
	TIME [epoch: 5.73 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012916157927521783		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.012916157927521783 | validation: 0.016837204484052393]
	TIME [epoch: 5.73 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012961532241058232		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.012961532241058232 | validation: 0.020432558331236618]
	TIME [epoch: 5.72 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01337207634069534		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.01337207634069534 | validation: 0.020812055631984128]
	TIME [epoch: 5.73 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014315037369069517		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.014315037369069517 | validation: 0.015929160089655704]
	TIME [epoch: 5.72 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012321696717330011		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.012321696717330011 | validation: 0.01592805521990508]
	TIME [epoch: 5.72 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013882015699300984		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.013882015699300984 | validation: 0.01896330161701759]
	TIME [epoch: 5.71 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012021145459096199		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.012021145459096199 | validation: 0.018088359921554366]
	TIME [epoch: 5.73 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0125424843455616		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.0125424843455616 | validation: 0.01846200226081477]
	TIME [epoch: 5.73 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013033179325540248		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.013033179325540248 | validation: 0.015685462974609377]
	TIME [epoch: 5.74 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013386785083537205		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.013386785083537205 | validation: 0.0163520127542929]
	TIME [epoch: 5.72 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013458539834602608		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.013458539834602608 | validation: 0.02185296689746682]
	TIME [epoch: 5.74 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01331470200358899		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.01331470200358899 | validation: 0.01618566818101973]
	TIME [epoch: 5.73 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01291117911161543		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.01291117911161543 | validation: 0.020373588018314605]
	TIME [epoch: 5.74 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013360609346254469		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.013360609346254469 | validation: 0.018700027268587816]
	TIME [epoch: 5.71 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011291387059272356		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.011291387059272356 | validation: 0.01730477195500856]
	TIME [epoch: 5.74 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012465149352385194		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.012465149352385194 | validation: 0.017794509917337744]
	TIME [epoch: 5.73 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011638358559175783		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.011638358559175783 | validation: 0.017380207372211808]
	TIME [epoch: 5.71 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013453849504007425		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.013453849504007425 | validation: 0.01943860030773106]
	TIME [epoch: 5.73 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012401427930033187		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.012401427930033187 | validation: 0.015529095168167384]
	TIME [epoch: 5.72 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012418394119482073		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.012418394119482073 | validation: 0.015622730228258065]
	TIME [epoch: 5.71 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012135468372305733		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.012135468372305733 | validation: 0.01939147515874632]
	TIME [epoch: 5.73 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012238879102621746		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.012238879102621746 | validation: 0.015706048948707207]
	TIME [epoch: 5.72 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014011479250698358		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.014011479250698358 | validation: 0.01754647967658182]
	TIME [epoch: 5.74 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012396274697437808		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.012396274697437808 | validation: 0.020235352200188995]
	TIME [epoch: 5.72 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012727941381281247		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.012727941381281247 | validation: 0.012861085515747595]
	TIME [epoch: 5.73 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1474.pth
	Model improved!!!
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013557280513845735		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.013557280513845735 | validation: 0.020195318268545537]
	TIME [epoch: 5.71 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012545373237160071		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.012545373237160071 | validation: 0.01801521548003875]
	TIME [epoch: 5.72 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013799305015313292		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.013799305015313292 | validation: 0.020372345534267113]
	TIME [epoch: 5.7 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013095437762432369		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.013095437762432369 | validation: 0.017884626360469613]
	TIME [epoch: 5.71 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012672708483543037		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.012672708483543037 | validation: 0.02171349373531639]
	TIME [epoch: 5.72 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0126176668589192		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.0126176668589192 | validation: 0.018920251319968995]
	TIME [epoch: 5.73 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012807150797333806		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.012807150797333806 | validation: 0.0171943402743288]
	TIME [epoch: 5.72 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012644561073057652		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.012644561073057652 | validation: 0.01804100819977064]
	TIME [epoch: 5.74 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013877586896994436		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.013877586896994436 | validation: 0.018747043714136935]
	TIME [epoch: 5.73 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013604183604127653		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.013604183604127653 | validation: 0.017476424450863004]
	TIME [epoch: 5.73 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011956998778688666		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.011956998778688666 | validation: 0.01824774981992494]
	TIME [epoch: 5.73 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013180768067699742		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.013180768067699742 | validation: 0.01689560249611226]
	TIME [epoch: 5.73 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01341778473110538		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.01341778473110538 | validation: 0.017484632061347517]
	TIME [epoch: 5.71 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012582227994048114		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.012582227994048114 | validation: 0.018619991180160812]
	TIME [epoch: 5.72 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012775506362702131		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.012775506362702131 | validation: 0.01674891280173754]
	TIME [epoch: 5.72 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012336897119416466		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.012336897119416466 | validation: 0.011472732428039402]
	TIME [epoch: 5.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1490.pth
	Model improved!!!
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012288854591389675		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.012288854591389675 | validation: 0.018262029799737702]
	TIME [epoch: 5.73 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0125536674756927		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.0125536674756927 | validation: 0.018138540229918645]
	TIME [epoch: 5.7 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012445502653459903		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.012445502653459903 | validation: 0.01713669809591285]
	TIME [epoch: 5.73 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013045318009069582		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.013045318009069582 | validation: 0.019694962353474555]
	TIME [epoch: 5.7 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012602778460773754		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.012602778460773754 | validation: 0.013717656501564458]
	TIME [epoch: 5.73 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011154017994149444		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.011154017994149444 | validation: 0.019311719547569085]
	TIME [epoch: 5.73 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012747177011353085		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.012747177011353085 | validation: 0.016666337105363638]
	TIME [epoch: 5.72 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012638969958951649		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.012638969958951649 | validation: 0.01843150435528489]
	TIME [epoch: 5.72 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01288306982157005		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.01288306982157005 | validation: 0.01564725866169787]
	TIME [epoch: 5.74 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01313345938600578		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.01313345938600578 | validation: 0.020240297747543157]
	TIME [epoch: 5.73 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012545518921756283		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.012545518921756283 | validation: 0.017895171388718278]
	TIME [epoch: 5.73 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011377257231254007		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.011377257231254007 | validation: 0.022771377490338498]
	TIME [epoch: 5.72 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012115830575097433		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.012115830575097433 | validation: 0.015725035463410444]
	TIME [epoch: 5.72 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012721444828794937		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.012721444828794937 | validation: 0.016639200349121752]
	TIME [epoch: 5.73 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012909238687627497		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.012909238687627497 | validation: 0.0171479450373804]
	TIME [epoch: 5.72 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01218710852277072		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.01218710852277072 | validation: 0.02000130410477642]
	TIME [epoch: 6.31 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012992078822015649		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.012992078822015649 | validation: 0.016676479651285037]
	TIME [epoch: 5.72 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012781834466099547		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.012781834466099547 | validation: 0.013939968158818962]
	TIME [epoch: 5.73 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012605672372563208		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.012605672372563208 | validation: 0.017922663548364304]
	TIME [epoch: 5.71 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011721759638961325		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.011721759638961325 | validation: 0.016369885491787427]
	TIME [epoch: 5.72 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013457247954693892		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.013457247954693892 | validation: 0.01831677882961861]
	TIME [epoch: 5.68 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012947939932351913		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.012947939932351913 | validation: 0.017726394870926088]
	TIME [epoch: 5.71 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012194150727361559		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.012194150727361559 | validation: 0.017832151266904694]
	TIME [epoch: 5.72 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013219836586353435		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.013219836586353435 | validation: 0.016962706418952735]
	TIME [epoch: 5.71 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013262226691851967		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.013262226691851967 | validation: 0.016121877089588257]
	TIME [epoch: 5.71 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01414473921244552		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.01414473921244552 | validation: 0.017706014303823014]
	TIME [epoch: 5.72 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013810325355818812		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.013810325355818812 | validation: 0.019156327519577243]
	TIME [epoch: 5.73 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012791358920019005		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.012791358920019005 | validation: 0.01580419281164086]
	TIME [epoch: 5.7 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012081649060768172		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.012081649060768172 | validation: 0.018485977525833974]
	TIME [epoch: 5.72 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011766483454004573		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.011766483454004573 | validation: 0.01541476437305448]
	TIME [epoch: 5.72 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012673353981028765		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.012673353981028765 | validation: 0.017743139662654873]
	TIME [epoch: 5.73 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01392589814387285		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.01392589814387285 | validation: 0.017086248383106895]
	TIME [epoch: 5.73 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012429984025350288		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.012429984025350288 | validation: 0.019542670916021833]
	TIME [epoch: 5.71 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01290202618849102		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.01290202618849102 | validation: 0.017152766617416725]
	TIME [epoch: 5.72 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012152949732855222		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.012152949732855222 | validation: 0.01807179600331257]
	TIME [epoch: 5.72 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011944582542986755		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.011944582542986755 | validation: 0.0198357349558448]
	TIME [epoch: 5.73 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012212138695997677		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.012212138695997677 | validation: 0.01709060936635066]
	TIME [epoch: 5.7 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011986513192356708		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.011986513192356708 | validation: 0.01765889681948676]
	TIME [epoch: 5.7 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013237044157452811		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.013237044157452811 | validation: 0.014941287710121987]
	TIME [epoch: 5.71 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012654491287941974		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.012654491287941974 | validation: 0.01540595017105061]
	TIME [epoch: 5.72 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012886046462546168		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.012886046462546168 | validation: 0.022513458715648162]
	TIME [epoch: 5.72 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012689516398634398		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.012689516398634398 | validation: 0.01755697721908054]
	TIME [epoch: 5.71 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012290067355260557		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.012290067355260557 | validation: 0.01903135843239382]
	TIME [epoch: 5.71 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012897182120367043		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.012897182120367043 | validation: 0.01991856571654901]
	TIME [epoch: 5.71 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012196301991923172		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.012196301991923172 | validation: 0.01244596702070634]
	TIME [epoch: 5.72 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012731973630282438		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.012731973630282438 | validation: 0.014870156646803856]
	TIME [epoch: 5.72 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012585413099171596		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.012585413099171596 | validation: 0.014854442929622924]
	TIME [epoch: 5.72 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011142751855883514		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.011142751855883514 | validation: 0.0152161815936954]
	TIME [epoch: 5.72 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012194516161548555		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.012194516161548555 | validation: 0.015657828355574357]
	TIME [epoch: 5.72 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012004483444687445		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.012004483444687445 | validation: 0.019064375801927748]
	TIME [epoch: 5.72 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013244031508417724		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.013244031508417724 | validation: 0.01753769050145212]
	TIME [epoch: 5.72 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012495392546093678		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.012495392546093678 | validation: 0.01785232043178836]
	TIME [epoch: 5.72 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012864143853724246		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.012864143853724246 | validation: 0.02354858138847915]
	TIME [epoch: 5.73 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012627255745864879		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.012627255745864879 | validation: 0.02109261447286367]
	TIME [epoch: 5.72 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012730068086438022		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.012730068086438022 | validation: 0.017298391679789037]
	TIME [epoch: 5.73 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012279403125787808		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.012279403125787808 | validation: 0.016130676276437994]
	TIME [epoch: 5.71 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012718877302245529		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.012718877302245529 | validation: 0.014074858639098842]
	TIME [epoch: 5.73 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012382316299418904		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.012382316299418904 | validation: 0.020455840612432986]
	TIME [epoch: 5.72 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011795022266919381		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.011795022266919381 | validation: 0.021579612143077954]
	TIME [epoch: 5.72 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01321074982264678		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.01321074982264678 | validation: 0.015222979413187189]
	TIME [epoch: 5.72 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01155992682712856		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.01155992682712856 | validation: 0.01686989065038792]
	TIME [epoch: 5.72 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012340318360946019		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.012340318360946019 | validation: 0.019633788033828106]
	TIME [epoch: 5.72 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01278842187991109		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.01278842187991109 | validation: 0.01723009747956682]
	TIME [epoch: 5.72 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012731342924728		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.012731342924728 | validation: 0.017735476689543717]
	TIME [epoch: 5.72 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013049355364351298		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.013049355364351298 | validation: 0.016946801403064815]
	TIME [epoch: 5.72 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01259137646239664		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.01259137646239664 | validation: 0.01465463121012539]
	TIME [epoch: 5.72 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012272477678678163		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.012272477678678163 | validation: 0.02136692299338835]
	TIME [epoch: 5.72 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012694542515741647		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.012694542515741647 | validation: 0.01733952681660954]
	TIME [epoch: 5.7 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012492238202121362		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.012492238202121362 | validation: 0.018008137197041776]
	TIME [epoch: 5.72 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01343604054458051		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.01343604054458051 | validation: 0.019371950654176396]
	TIME [epoch: 5.71 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013261657395563736		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.013261657395563736 | validation: 0.015081794169827523]
	TIME [epoch: 5.72 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012458139864502988		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.012458139864502988 | validation: 0.02076484598135693]
	TIME [epoch: 5.68 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012365243962996613		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.012365243962996613 | validation: 0.01677580695356774]
	TIME [epoch: 5.71 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012412890380016979		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.012412890380016979 | validation: 0.01928144866974603]
	TIME [epoch: 5.7 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012115962895398166		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.012115962895398166 | validation: 0.017489012123132586]
	TIME [epoch: 5.73 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011752975779824832		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.011752975779824832 | validation: 0.01777381833129258]
	TIME [epoch: 5.73 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01160388206928853		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.01160388206928853 | validation: 0.020832424389151373]
	TIME [epoch: 5.72 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011951447263171358		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.011951447263171358 | validation: 0.017673902012689802]
	TIME [epoch: 5.73 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012035663884974743		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.012035663884974743 | validation: 0.019759280797737613]
	TIME [epoch: 5.72 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011696218411723063		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.011696218411723063 | validation: 0.01892006937521692]
	TIME [epoch: 5.72 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013154127839148112		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.013154127839148112 | validation: 0.01654283266609117]
	TIME [epoch: 5.72 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01317019033314005		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.01317019033314005 | validation: 0.01879076964057863]
	TIME [epoch: 5.69 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01205515121309391		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.01205515121309391 | validation: 0.013809908728602206]
	TIME [epoch: 5.71 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013077082769494427		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.013077082769494427 | validation: 0.013777851792331175]
	TIME [epoch: 5.7 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013244785675436906		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.013244785675436906 | validation: 0.014486483491035308]
	TIME [epoch: 5.71 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012127129701310269		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.012127129701310269 | validation: 0.02122398742089493]
	TIME [epoch: 5.71 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01271868277280336		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.01271868277280336 | validation: 0.016624243939120743]
	TIME [epoch: 5.73 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012604624975092758		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.012604624975092758 | validation: 0.019676452011957193]
	TIME [epoch: 5.71 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01222189543402894		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.01222189543402894 | validation: 0.015262499641531048]
	TIME [epoch: 5.71 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012034319123452831		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.012034319123452831 | validation: 0.016558228063979996]
	TIME [epoch: 5.69 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012404750076491484		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.012404750076491484 | validation: 0.0187380551931595]
	TIME [epoch: 5.72 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011614894810679269		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.011614894810679269 | validation: 0.019141213908720867]
	TIME [epoch: 5.69 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012795770419366007		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.012795770419366007 | validation: 0.017390397990321438]
	TIME [epoch: 5.71 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013486952184162937		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.013486952184162937 | validation: 0.017633024128847662]
	TIME [epoch: 5.68 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013070205511360209		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.013070205511360209 | validation: 0.01498107508200839]
	TIME [epoch: 5.72 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011755727903547187		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.011755727903547187 | validation: 0.01677676698094596]
	TIME [epoch: 5.71 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012954841533787922		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.012954841533787922 | validation: 0.021263329402209798]
	TIME [epoch: 5.71 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012789479942414306		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.012789479942414306 | validation: 0.01612790988912687]
	TIME [epoch: 5.72 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01262552237043267		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.01262552237043267 | validation: 0.01863474919328817]
	TIME [epoch: 5.69 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013092511014525634		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.013092511014525634 | validation: 0.01603817834102236]
	TIME [epoch: 5.72 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013036568334192596		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.013036568334192596 | validation: 0.015277582894540287]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_152654/states/model_phi1_4a_v_mmd1_1591.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6061.388 seconds.
