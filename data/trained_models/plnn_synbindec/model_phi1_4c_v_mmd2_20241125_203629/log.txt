Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3950357928

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.606629246274287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.606629246274287 | validation: 6.294960832086989]
	TIME [epoch: 159 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.22002206895879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.22002206895879 | validation: 5.8507139538877]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.5904661418174735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.5904661418174735 | validation: 5.245563671149456]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.217704832795971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.217704832795971 | validation: 4.901165081758866]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.865422666578282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.865422666578282 | validation: 4.599404539410169]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.503356663942457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.503356663942457 | validation: 4.113914489216038]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.04129100957479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.04129100957479 | validation: 4.241012780825566]
	TIME [epoch: 2.66 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.197256748424531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.197256748424531 | validation: 4.248632061402308]
	TIME [epoch: 2.67 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.171346976089274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.171346976089274 | validation: 4.01473513916115]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9481801831270493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9481801831270493 | validation: 4.036642712445363]
	TIME [epoch: 2.69 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9748119297818008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9748119297818008 | validation: 4.026998189708059]
	TIME [epoch: 2.67 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9514098232028974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9514098232028974 | validation: 3.968779229280419]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.903843840248207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.903843840248207 | validation: 3.9459328390157333]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8749234417197767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8749234417197767 | validation: 3.9192477439515017]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.851522974050054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.851522974050054 | validation: 3.9098229208022865]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.831821565756694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.831821565756694 | validation: 3.8880730459150303]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.821391174469836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.821391174469836 | validation: 3.9276769541196446]
	TIME [epoch: 2.67 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.836967028499065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.836967028499065 | validation: 3.9582081417625234]
	TIME [epoch: 2.69 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.895290450624417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.895290450624417 | validation: 4.045893145254084]
	TIME [epoch: 2.66 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9604871011667355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9604871011667355 | validation: 3.825003184383522]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.742489333832751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.742489333832751 | validation: 3.8323315398971918]
	TIME [epoch: 2.66 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7620506891319203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7620506891319203 | validation: 3.874830754406339]
	TIME [epoch: 2.66 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7939697894253994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7939697894253994 | validation: 3.7745995568325514]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.694231293355152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.694231293355152 | validation: 3.728085861740113]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6480847797357128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6480847797357128 | validation: 3.678600253524216]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6091410555383003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6091410555383003 | validation: 3.6868103364364453]
	TIME [epoch: 2.66 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.576724687374153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.576724687374153 | validation: 3.6538729091016267]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5637187337320895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5637187337320895 | validation: 3.81815745801412]
	TIME [epoch: 2.68 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.674365535687627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.674365535687627 | validation: 3.9271439688006695]
	TIME [epoch: 2.67 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9113970111759615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9113970111759615 | validation: 3.814306007681988]
	TIME [epoch: 2.67 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6805520975150547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6805520975150547 | validation: 3.601500628626493]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4843240253268926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4843240253268926 | validation: 3.623152212078793]
	TIME [epoch: 2.67 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5506320131356723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5506320131356723 | validation: 3.7443181130486964]
	TIME [epoch: 2.68 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5759683324076152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5759683324076152 | validation: 3.527480673180122]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4530143748807167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4530143748807167 | validation: 3.522647187741107]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4182206727614877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4182206727614877 | validation: 3.5425163848139074]
	TIME [epoch: 2.67 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4178469689058044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4178469689058044 | validation: 3.479138501780043]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4234581170761813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4234581170761813 | validation: 3.573415811008914]
	TIME [epoch: 2.69 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.425343700331243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.425343700331243 | validation: 3.456304353256608]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.381564174443417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.381564174443417 | validation: 3.5070246627453616]
	TIME [epoch: 2.7 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3810489950938973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3810489950938973 | validation: 3.431365798298781]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.359688105504294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.359688105504294 | validation: 3.5285787745932886]
	TIME [epoch: 2.69 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.378308243204604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.378308243204604 | validation: 3.403907685227094]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.326721464922006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.326721464922006 | validation: 3.4505335626555667]
	TIME [epoch: 2.67 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.313938242788008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.313938242788008 | validation: 3.3532626751855545]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2860844054104525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2860844054104525 | validation: 3.475770510462254]
	TIME [epoch: 2.66 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.29800497980538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.29800497980538 | validation: 3.3438965904099733]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3395352955404634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3395352955404634 | validation: 3.343339751122862]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.223371610174528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.223371610174528 | validation: 3.2990175161446644]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2068717488618588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2068717488618588 | validation: 3.3138475517686765]
	TIME [epoch: 2.67 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.211946611320053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.211946611320053 | validation: 3.3703003095168884]
	TIME [epoch: 2.65 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.240511347166897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.240511347166897 | validation: 3.2906517624880154]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.217311940663669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.217311940663669 | validation: 3.5677710655881323]
	TIME [epoch: 2.66 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3578621135014624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3578621135014624 | validation: 3.3946654512689594]
	TIME [epoch: 2.67 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4128330055568434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4128330055568434 | validation: 3.22036113329216]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1445536132990175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1445536132990175 | validation: 3.4687120455909985]
	TIME [epoch: 2.66 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.264382374530519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.264382374530519 | validation: 3.1753625675511845]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0940099904328577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0940099904328577 | validation: 3.1686627271998495]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1212668012128164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1212668012128164 | validation: 3.257625325536819]
	TIME [epoch: 2.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1092413248624102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1092413248624102 | validation: 3.1372853057634162]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0574657743425546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0574657743425546 | validation: 3.116053689927257]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0244496947022204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0244496947022204 | validation: 3.1380680671138834]
	TIME [epoch: 2.66 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.014660670216799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.014660670216799 | validation: 3.0797649967306917]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.027392804277543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.027392804277543 | validation: 3.493090279494247]
	TIME [epoch: 2.67 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.250893443742665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.250893443742665 | validation: 3.283292902914636]
	TIME [epoch: 2.67 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.28373815033109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.28373815033109 | validation: 3.07373966947419]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.01926739936097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.01926739936097 | validation: 3.274179765540727]
	TIME [epoch: 2.67 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0869197278013396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0869197278013396 | validation: 3.0455456584708855]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.972659371839447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.972659371839447 | validation: 3.031655462305702]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.977187794874017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.977187794874017 | validation: 3.1088680673869606]
	TIME [epoch: 2.68 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9861347018983775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9861347018983775 | validation: 3.0264776980060333]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9710622581918846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9710622581918846 | validation: 3.1066898662750777]
	TIME [epoch: 2.68 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.975802251956008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.975802251956008 | validation: 3.019701091733175]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9660580931873426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9660580931873426 | validation: 3.188859568953578]
	TIME [epoch: 2.68 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0161839097656826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0161839097656826 | validation: 3.0391915046881373]
	TIME [epoch: 2.67 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0323777296265764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0323777296265764 | validation: 3.053132612508092]
	TIME [epoch: 2.68 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.915913497864997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.915913497864997 | validation: 2.958765129542833]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8923678058449678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8923678058449678 | validation: 3.0164263351564053]
	TIME [epoch: 2.68 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.890667507863503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.890667507863503 | validation: 2.9601804542070953]
	TIME [epoch: 2.68 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.932854091617826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.932854091617826 | validation: 3.1541283204826054]
	TIME [epoch: 2.67 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9860303120250182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9860303120250182 | validation: 3.0280745038073373]
	TIME [epoch: 2.69 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0140470836290407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0140470836290407 | validation: 2.97943811833286]
	TIME [epoch: 2.67 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.879042640205306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.879042640205306 | validation: 2.9598723469182087]
	TIME [epoch: 2.68 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8599060241099976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8599060241099976 | validation: 2.920613566489784]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.871326958704604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.871326958704604 | validation: 3.0464522745277893]
	TIME [epoch: 2.69 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.885444982869812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.885444982869812 | validation: 2.933871422923149]
	TIME [epoch: 2.67 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9128180021196077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9128180021196077 | validation: 3.0668816678965185]
	TIME [epoch: 2.68 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.927639103563354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.927639103563354 | validation: 2.8889330550610084]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8701348357361427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8701348357361427 | validation: 2.9250708421772456]
	TIME [epoch: 2.66 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8213158183328075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8213158183328075 | validation: 2.8755078512532153]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8018184033667657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8018184033667657 | validation: 2.891390295183724]
	TIME [epoch: 2.66 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.787863076101875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.787863076101875 | validation: 2.8434983421716655]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.800487027750878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.800487027750878 | validation: 3.284991439850478]
	TIME [epoch: 2.67 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.081119483869328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.081119483869328 | validation: 3.096385486893057]
	TIME [epoch: 2.67 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.105974550702861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.105974550702861 | validation: 2.8876408799890503]
	TIME [epoch: 2.67 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8631196848483196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8631196848483196 | validation: 3.002608483881467]
	TIME [epoch: 2.67 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.863747222366841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.863747222366841 | validation: 2.890971074642298]
	TIME [epoch: 2.67 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.816555509557156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.816555509557156 | validation: 2.864141199545121]
	TIME [epoch: 2.66 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7931214838565417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7931214838565417 | validation: 2.8834620683964944]
	TIME [epoch: 2.67 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.79659407841664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.79659407841664 | validation: 2.8474083443606975]
	TIME [epoch: 2.67 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7679597187656464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7679597187656464 | validation: 2.861462760900925]
	TIME [epoch: 2.66 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7582213705103173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7582213705103173 | validation: 2.8289692006299614]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.743125910092293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.743125910092293 | validation: 2.823337544662965]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7309757940760755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7309757940760755 | validation: 2.7683837217278575]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7147633356559386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7147633356559386 | validation: 3.0384650814281073]
	TIME [epoch: 2.68 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7948924451891255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7948924451891255 | validation: 3.0032314902646764]
	TIME [epoch: 2.68 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.994216548499084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.994216548499084 | validation: 2.790120737627968]
	TIME [epoch: 2.68 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7054265779028333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7054265779028333 | validation: 2.7459050126464706]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.658913521425771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.658913521425771 | validation: 2.622913134260356]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4664202751082027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4664202751082027 | validation: 2.151423644931279]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.00974710807462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.00974710807462 | validation: 1.765061986082079]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.966663398401479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.966663398401479 | validation: 2.520024527289244]
	TIME [epoch: 2.68 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5601419626874735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5601419626874735 | validation: 1.2804553197537107]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_113.pth
	Model improved!!!
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3560374974300515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3560374974300515 | validation: 1.3369120022267742]
	TIME [epoch: 2.68 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.446128409317524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.446128409317524 | validation: 1.1312785547838904]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2657205399638165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2657205399638165 | validation: 1.1150177107176689]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.19426048285972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.19426048285972 | validation: 0.9752046672544801]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1209173355897626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1209173355897626 | validation: 0.9722063977597464]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048379768199781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.048379768199781 | validation: 0.925689001180286]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0257336417375582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0257336417375582 | validation: 0.9254947447318941]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0094334590337377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0094334590337377 | validation: 0.9173957910917179]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0108416739351482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0108416739351482 | validation: 0.8815753786525171]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9996224068478539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9996224068478539 | validation: 1.1235590586076705]
	TIME [epoch: 2.64 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1749339726631352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1749339726631352 | validation: 1.0185914215045615]
	TIME [epoch: 2.63 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1926950122266968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1926950122266968 | validation: 0.8713535023501313]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9832711259737118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9832711259737118 | validation: 0.9610006656263543]
	TIME [epoch: 2.66 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0327687587382584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0327687587382584 | validation: 0.8984550945305245]
	TIME [epoch: 2.65 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0615337768103974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0615337768103974 | validation: 0.9962494559452747]
	TIME [epoch: 2.65 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0636070788235035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0636070788235035 | validation: 0.8674537326782041]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_129.pth
	Model improved!!!
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9585562083252732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9585562083252732 | validation: 0.8373197052948966]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9639934131561472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9639934131561472 | validation: 0.8216880129805539]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367976216440198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9367976216440198 | validation: 0.8570589245319007]
	TIME [epoch: 2.65 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542105415312722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9542105415312722 | validation: 0.8472255186015097]
	TIME [epoch: 2.63 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9988936725538716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9988936725538716 | validation: 0.8332599041485491]
	TIME [epoch: 2.65 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.936671560854709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.936671560854709 | validation: 0.8081558904630317]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9106869635526382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9106869635526382 | validation: 0.8065517827665701]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9162709427766845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9162709427766845 | validation: 0.8085916038518883]
	TIME [epoch: 2.65 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077783662912188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9077783662912188 | validation: 0.8865524256810349]
	TIME [epoch: 2.65 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9823050431516444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9823050431516444 | validation: 0.9048144769787904]
	TIME [epoch: 2.66 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0658893278726145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0658893278726145 | validation: 0.8341969959551788]
	TIME [epoch: 2.67 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266041080494424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9266041080494424 | validation: 0.7852763817677907]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9047815225426752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9047815225426752 | validation: 0.7955792287292809]
	TIME [epoch: 2.65 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8965674157839613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8965674157839613 | validation: 0.8014838975135534]
	TIME [epoch: 2.64 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025470230355194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9025470230355194 | validation: 0.8232790779873647]
	TIME [epoch: 2.66 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9082697809124923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9082697809124923 | validation: 0.8313361312438495]
	TIME [epoch: 2.65 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9371674612298285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9371674612298285 | validation: 0.8481926931009494]
	TIME [epoch: 2.66 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291908181957456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9291908181957456 | validation: 0.782069826287631]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994784598652612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8994784598652612 | validation: 0.8398050579622478]
	TIME [epoch: 2.67 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9270328921481286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9270328921481286 | validation: 0.8074478117282793]
	TIME [epoch: 2.67 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555137792446072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9555137792446072 | validation: 0.9760787151215156]
	TIME [epoch: 2.66 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0625987673210617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0625987673210617 | validation: 0.7958424008527495]
	TIME [epoch: 2.66 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293118577314181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9293118577314181 | validation: 0.8440178538644112]
	TIME [epoch: 2.66 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9416186261406762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9416186261406762 | validation: 0.7828413635023121]
	TIME [epoch: 2.67 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9010318691725631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9010318691725631 | validation: 0.7737908655350627]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837311496033107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837311496033107 | validation: 0.8140431329101724]
	TIME [epoch: 2.67 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944805573712833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8944805573712833 | validation: 0.9064654582803483]
	TIME [epoch: 2.65 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9953695287258527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9953695287258527 | validation: 0.886759951401324]
	TIME [epoch: 2.67 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0497252336314604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0497252336314604 | validation: 0.7923640240355335]
	TIME [epoch: 2.66 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9018441807411514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9018441807411514 | validation: 0.7971167468377617]
	TIME [epoch: 2.68 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9184894589496775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9184894589496775 | validation: 0.8239921963319157]
	TIME [epoch: 2.66 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486393779344278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9486393779344278 | validation: 0.788500990112698]
	TIME [epoch: 2.67 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891914766390916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891914766390916 | validation: 0.8144996750138068]
	TIME [epoch: 2.66 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9744400164765067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9744400164765067 | validation: 0.8014749833320401]
	TIME [epoch: 2.65 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212563736966146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9212563736966146 | validation: 0.8045741836150552]
	TIME [epoch: 2.65 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9358311454210229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9358311454210229 | validation: 0.812524792176438]
	TIME [epoch: 2.65 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9345144772346601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9345144772346601 | validation: 0.8033901857351247]
	TIME [epoch: 2.66 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9382921078068955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9382921078068955 | validation: 0.772089874613045]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9105939434788485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9105939434788485 | validation: 0.7666924690689108]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889072237569224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889072237569224 | validation: 0.8170780021616185]
	TIME [epoch: 2.65 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9551842003095253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9551842003095253 | validation: 0.7804425730878686]
	TIME [epoch: 2.67 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025935334503784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9025935334503784 | validation: 0.8137458827250615]
	TIME [epoch: 2.65 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.967071338137373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.967071338137373 | validation: 0.7665276449078532]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_172.pth
	Model improved!!!
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797945581902912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8797945581902912 | validation: 0.7626842919404178]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9145024603207008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9145024603207008 | validation: 0.8057938832610152]
	TIME [epoch: 2.66 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9765279602828971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9765279602828971 | validation: 0.7764502635360142]
	TIME [epoch: 2.65 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936429878966713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8936429878966713 | validation: 0.7531702539714211]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9108892532012466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9108892532012466 | validation: 0.7790839978680932]
	TIME [epoch: 2.65 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8866762547486521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8866762547486521 | validation: 0.7707063382121776]
	TIME [epoch: 2.66 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906295730821717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.906295730821717 | validation: 0.8297497399031286]
	TIME [epoch: 2.64 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278851106472411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278851106472411 | validation: 0.761131075266547]
	TIME [epoch: 2.65 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9144764156028293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9144764156028293 | validation: 0.8248656914674846]
	TIME [epoch: 2.65 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262965456343464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9262965456343464 | validation: 0.7988056972777158]
	TIME [epoch: 2.64 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457939485388377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9457939485388377 | validation: 0.7565320447979439]
	TIME [epoch: 2.63 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8999325286595297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8999325286595297 | validation: 0.8475927831060123]
	TIME [epoch: 2.65 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014394245781196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.014394245781196 | validation: 0.8684160576479532]
	TIME [epoch: 2.64 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0304664400044226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0304664400044226 | validation: 0.7617894698630809]
	TIME [epoch: 2.65 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831105142259423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831105142259423 | validation: 0.8368911216047077]
	TIME [epoch: 2.62 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9838231497351939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9838231497351939 | validation: 0.7902762354806173]
	TIME [epoch: 2.66 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9244546449430721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9244546449430721 | validation: 0.752311181339829]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8944730276045664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8944730276045664 | validation: 0.8079346461522458]
	TIME [epoch: 2.65 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360702494869949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9360702494869949 | validation: 0.7604441527704263]
	TIME [epoch: 2.65 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814344544982626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814344544982626 | validation: 0.797737738253389]
	TIME [epoch: 2.65 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107610310314325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9107610310314325 | validation: 0.8007246004747677]
	TIME [epoch: 2.66 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115115453437772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9115115453437772 | validation: 0.7838512294845237]
	TIME [epoch: 2.66 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873093904178816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873093904178816 | validation: 0.7700413271466293]
	TIME [epoch: 2.65 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8881033888191965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8881033888191965 | validation: 0.7932843051317267]
	TIME [epoch: 2.64 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9212446693984732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9212446693984732 | validation: 0.7560589360588384]
	TIME [epoch: 2.66 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754782323525597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8754782323525597 | validation: 0.7723275779952087]
	TIME [epoch: 2.66 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102728686934942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9102728686934942 | validation: 0.7719063397165155]
	TIME [epoch: 2.63 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9183534705401374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9183534705401374 | validation: 0.7659324626335255]
	TIME [epoch: 2.66 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9040044495466054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9040044495466054 | validation: 0.7833318472256755]
	TIME [epoch: 169 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9283401132042292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9283401132042292 | validation: 0.8200690844355712]
	TIME [epoch: 5.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9265924344176999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9265924344176999 | validation: 0.7979369046506286]
	TIME [epoch: 5.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9485281769961432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9485281769961432 | validation: 0.7422019438572021]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858043627375364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8858043627375364 | validation: 0.7471131426051296]
	TIME [epoch: 5.75 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757025208660576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8757025208660576 | validation: 0.7581699565962357]
	TIME [epoch: 5.74 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938617938140394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8938617938140394 | validation: 0.766184653842885]
	TIME [epoch: 5.71 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9043663935562026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043663935562026 | validation: 0.7469332936818571]
	TIME [epoch: 5.73 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8757335163888778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8757335163888778 | validation: 0.7738056426773561]
	TIME [epoch: 5.67 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842675271698403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842675271698403 | validation: 0.7500851670536627]
	TIME [epoch: 5.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9041712190519311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9041712190519311 | validation: 0.7740001516023111]
	TIME [epoch: 5.69 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811161393340818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811161393340818 | validation: 0.7713998942255266]
	TIME [epoch: 5.72 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9034656685946937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9034656685946937 | validation: 0.8348129011260138]
	TIME [epoch: 5.69 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939413244752956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9939413244752956 | validation: 0.8084577929969282]
	TIME [epoch: 5.72 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9524217194681137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9524217194681137 | validation: 0.8616062812546229]
	TIME [epoch: 5.7 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424501396566575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9424501396566575 | validation: 0.8192077570458564]
	TIME [epoch: 5.72 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9352346334627002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9352346334627002 | validation: 0.7544696857415085]
	TIME [epoch: 5.67 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868767692991763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868767692991763 | validation: 0.7906157381366766]
	TIME [epoch: 5.76 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940635115802562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8940635115802562 | validation: 0.7784583675723372]
	TIME [epoch: 5.74 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038746213976747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038746213976747 | validation: 0.7790179722327427]
	TIME [epoch: 5.76 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871151166120361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871151166120361 | validation: 0.7801943683646537]
	TIME [epoch: 5.7 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9192205355022484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9192205355022484 | validation: 0.7602024197655264]
	TIME [epoch: 5.73 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821308404610444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8821308404610444 | validation: 0.7528058912308738]
	TIME [epoch: 5.73 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798992438221553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8798992438221553 | validation: 0.7876914627756882]
	TIME [epoch: 5.75 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998722508380895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998722508380895 | validation: 0.7597653050716101]
	TIME [epoch: 5.7 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925178805244747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8925178805244747 | validation: 0.7743304165834569]
	TIME [epoch: 5.77 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9072856708100039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9072856708100039 | validation: 0.799757045870141]
	TIME [epoch: 5.69 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767071633476506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767071633476506 | validation: 0.8315999440420602]
	TIME [epoch: 5.75 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945125096237088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945125096237088 | validation: 0.7746960889924026]
	TIME [epoch: 5.68 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8887930714985831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8887930714985831 | validation: 0.7437822462320214]
	TIME [epoch: 5.75 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886406723368556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8886406723368556 | validation: 0.777722391258449]
	TIME [epoch: 5.67 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814977692629118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814977692629118 | validation: 0.7769029357953313]
	TIME [epoch: 5.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122910792630028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9122910792630028 | validation: 0.7887738330298029]
	TIME [epoch: 5.7 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983950235831667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983950235831667 | validation: 0.8006845320297699]
	TIME [epoch: 5.74 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9448377328700537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9448377328700537 | validation: 0.7537769109184862]
	TIME [epoch: 5.69 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888786267388707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888786267388707 | validation: 0.7841020623921171]
	TIME [epoch: 5.75 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8918106397504884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8918106397504884 | validation: 0.7948819990783016]
	TIME [epoch: 5.69 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011293150472605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9011293150472605 | validation: 0.7919600615502869]
	TIME [epoch: 5.77 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887874958008829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.887874958008829 | validation: 0.7632788999913345]
	TIME [epoch: 5.67 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099109292547966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9099109292547966 | validation: 0.7929756093378458]
	TIME [epoch: 5.76 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815915207094389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815915207094389 | validation: 0.7448725882181617]
	TIME [epoch: 5.71 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8701593461374028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8701593461374028 | validation: 0.8037097692851524]
	TIME [epoch: 5.75 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9146398376090621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9146398376090621 | validation: 0.7919919635648384]
	TIME [epoch: 5.73 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133648734323054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133648734323054 | validation: 0.7744559279069724]
	TIME [epoch: 5.76 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8870397780687796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8870397780687796 | validation: 0.758926320818377]
	TIME [epoch: 5.69 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634617468936796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634617468936796 | validation: 0.8024881252158181]
	TIME [epoch: 5.77 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9472391641383289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9472391641383289 | validation: 0.7592551410365419]
	TIME [epoch: 5.7 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637878574659316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8637878574659316 | validation: 0.7570942174814506]
	TIME [epoch: 5.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642913202753366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642913202753366 | validation: 0.7513576431234553]
	TIME [epoch: 5.65 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661651122113816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661651122113816 | validation: 0.7643761693071243]
	TIME [epoch: 5.73 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815206111817315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815206111817315 | validation: 0.9342657829348575]
	TIME [epoch: 5.67 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0056570886003748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0056570886003748 | validation: 1.0012365163553776]
	TIME [epoch: 5.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.120787910584127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.120787910584127 | validation: 0.7487504285482715]
	TIME [epoch: 5.68 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672179671708841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672179671708841 | validation: 0.7830508200000827]
	TIME [epoch: 5.74 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896627903288956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.896627903288956 | validation: 0.7567808680243088]
	TIME [epoch: 5.68 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092308256714648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9092308256714648 | validation: 0.7433746602712924]
	TIME [epoch: 5.73 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568407341083562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568407341083562 | validation: 0.7581612114944223]
	TIME [epoch: 5.68 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614552218268238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8614552218268238 | validation: 0.7376836652876823]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526259195380566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526259195380566 | validation: 0.7338372248046655]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8377326643511984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8377326643511984 | validation: 0.7186484620424356]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408645684315187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408645684315187 | validation: 0.7161771636791049]
	TIME [epoch: 5.74 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_261.pth
	Model improved!!!
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8210599115521657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8210599115521657 | validation: 0.7120029689697669]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334100756966883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8334100756966883 | validation: 0.7824435334468538]
	TIME [epoch: 5.73 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8259682383392146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8259682383392146 | validation: 1.0302684038879986]
	TIME [epoch: 5.71 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1675968320458154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1675968320458154 | validation: 2.025439978972927]
	TIME [epoch: 5.7 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2298738117467845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2298738117467845 | validation: 0.8739385216281103]
	TIME [epoch: 5.68 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9558826554140929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9558826554140929 | validation: 1.2560821516791063]
	TIME [epoch: 5.67 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2777820837294278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2777820837294278 | validation: 0.8391336152926159]
	TIME [epoch: 5.76 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984922143496243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984922143496243 | validation: 0.7978202692600289]
	TIME [epoch: 5.7 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8863888953853508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8863888953853508 | validation: 0.7209198795484107]
	TIME [epoch: 5.68 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.851765817915397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.851765817915397 | validation: 0.6987633221934983]
	TIME [epoch: 5.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331015780547876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8331015780547876 | validation: 0.7168368975571439]
	TIME [epoch: 5.75 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231791394087621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8231791394087621 | validation: 0.7378913624426793]
	TIME [epoch: 5.69 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103981579666245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103981579666245 | validation: 0.7255663575481419]
	TIME [epoch: 5.69 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097597868434312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8097597868434312 | validation: 0.7146426996629791]
	TIME [epoch: 5.66 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906533705428802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906533705428802 | validation: 0.7102961687713132]
	TIME [epoch: 5.72 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903293783425744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7903293783425744 | validation: 0.7076131212046339]
	TIME [epoch: 5.73 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7744249508394875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7744249508394875 | validation: 0.6963382425910476]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606287606900477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606287606900477 | validation: 0.713941045980986]
	TIME [epoch: 5.7 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7671654797792057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7671654797792057 | validation: 0.8795172983665343]
	TIME [epoch: 5.76 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9338746270698711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9338746270698711 | validation: 1.5650866510203534]
	TIME [epoch: 5.7 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5582468991950105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5582468991950105 | validation: 0.9874417959924833]
	TIME [epoch: 5.76 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0483694310656235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0483694310656235 | validation: 0.740527344805219]
	TIME [epoch: 5.69 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8416198627923117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8416198627923117 | validation: 0.7860514218545825]
	TIME [epoch: 5.77 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8829292455424832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8829292455424832 | validation: 0.7311765828015919]
	TIME [epoch: 5.71 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198954836880461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8198954836880461 | validation: 0.7411348775035682]
	TIME [epoch: 5.78 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413589214135212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413589214135212 | validation: 0.7084517258432947]
	TIME [epoch: 5.7 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8173077735497906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8173077735497906 | validation: 0.7144156647593779]
	TIME [epoch: 5.75 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8207423582196957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8207423582196957 | validation: 0.7167136665329328]
	TIME [epoch: 5.7 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804424986884942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804424986884942 | validation: 0.6913348194313582]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797272407501518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.797272407501518 | validation: 0.6965779154026935]
	TIME [epoch: 5.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876950296793777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7876950296793777 | validation: 0.7031935961089274]
	TIME [epoch: 5.83 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7893116261267141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7893116261267141 | validation: 0.6848273216034468]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800170399361374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800170399361374 | validation: 0.7045603701212718]
	TIME [epoch: 5.74 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777300463697626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777300463697626 | validation: 0.759042923570054]
	TIME [epoch: 5.72 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8423339949756009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8423339949756009 | validation: 0.8400867577020229]
	TIME [epoch: 5.76 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258318483849313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9258318483849313 | validation: 0.9350838975087445]
	TIME [epoch: 5.71 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9234434884736427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9234434884736427 | validation: 0.7683687255255]
	TIME [epoch: 5.78 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8409087208529243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8409087208529243 | validation: 0.6745581819194246]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592630909598331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7592630909598331 | validation: 0.665944360436566]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7282552465095828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7282552465095828 | validation: 0.6603044218174623]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976413138554287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6976413138554287 | validation: 0.6470379749863249]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6884293670341223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6884293670341223 | validation: 0.9044613070198695]
	TIME [epoch: 5.75 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444251949556718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8444251949556718 | validation: 1.560050419190981]
	TIME [epoch: 5.74 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7137421279312128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7137421279312128 | validation: 0.7110958643232488]
	TIME [epoch: 5.77 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978176337113471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978176337113471 | validation: 1.1133887668002458]
	TIME [epoch: 5.7 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1727068290056792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1727068290056792 | validation: 0.6815666588748706]
	TIME [epoch: 5.75 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881959963610266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7881959963610266 | validation: 0.8338188389795749]
	TIME [epoch: 5.75 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.916708822254938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.916708822254938 | validation: 0.6978151562810357]
	TIME [epoch: 5.77 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734403146723495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7734403146723495 | validation: 0.6839491472729637]
	TIME [epoch: 5.73 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754078319563857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7754078319563857 | validation: 0.6863754497516071]
	TIME [epoch: 5.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554237169421155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554237169421155 | validation: 0.626266593876283]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7312084425983033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7312084425983033 | validation: 0.6460826462727438]
	TIME [epoch: 5.77 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6861321065122945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6861321065122945 | validation: 0.7218039494649173]
	TIME [epoch: 5.76 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7180297861993518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7180297861993518 | validation: 1.1393400837654069]
	TIME [epoch: 5.79 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2044188183207096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2044188183207096 | validation: 0.6908943685905125]
	TIME [epoch: 5.77 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774876817932114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.774876817932114 | validation: 0.639900491406339]
	TIME [epoch: 5.79 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.698700708060434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.698700708060434 | validation: 0.6394666486890466]
	TIME [epoch: 5.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778966471576149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6778966471576149 | validation: 0.6338010479445517]
	TIME [epoch: 5.82 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6917648369681222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6917648369681222 | validation: 1.0756817903961768]
	TIME [epoch: 5.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0211136228497073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0211136228497073 | validation: 1.4381303243775514]
	TIME [epoch: 5.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6207235567736684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6207235567736684 | validation: 0.9110735442988663]
	TIME [epoch: 5.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092404658475919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.092404658475919 | validation: 0.9223515892691072]
	TIME [epoch: 5.71 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0049791572130768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0049791572130768 | validation: 0.7894042536891718]
	TIME [epoch: 5.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842289048824783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842289048824783 | validation: 0.7462433942752746]
	TIME [epoch: 5.71 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8232124530265078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8232124530265078 | validation: 0.7052116593332634]
	TIME [epoch: 5.74 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642772278961756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642772278961756 | validation: 0.6888561639310826]
	TIME [epoch: 5.71 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721392339670794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7721392339670794 | validation: 0.67943128585]
	TIME [epoch: 5.74 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.74109978123579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.74109978123579 | validation: 0.6676379932090369]
	TIME [epoch: 5.73 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123786300770348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7123786300770348 | validation: 0.629128218703448]
	TIME [epoch: 5.74 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053798451683168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053798451683168 | validation: 0.6595464971298243]
	TIME [epoch: 5.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6886398567192805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6886398567192805 | validation: 0.7001247240003545]
	TIME [epoch: 5.74 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732946572343835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.732946572343835 | validation: 1.0078674834186243]
	TIME [epoch: 5.79 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.048733173319447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.048733173319447 | validation: 0.7307807924851165]
	TIME [epoch: 5.75 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886369082312035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886369082312035 | validation: 0.6774934267352918]
	TIME [epoch: 5.78 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168827831257621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7168827831257621 | validation: 0.6667623259906623]
	TIME [epoch: 5.74 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.708926260365983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.708926260365983 | validation: 0.6587576848206432]
	TIME [epoch: 5.69 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.725133205910503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.725133205910503 | validation: 0.860568641481811]
	TIME [epoch: 5.76 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470146630031374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470146630031374 | validation: 1.0299581886644507]
	TIME [epoch: 5.75 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1180061210320071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1180061210320071 | validation: 0.6144920970566728]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933500455955809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933500455955809 | validation: 0.7600828856319031]
	TIME [epoch: 5.69 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346025688083247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346025688083247 | validation: 0.6339510156424533]
	TIME [epoch: 5.69 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7021821809839223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7021821809839223 | validation: 0.5681519857302153]
	TIME [epoch: 5.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6347852673703888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6347852673703888 | validation: 0.6014953195226211]
	TIME [epoch: 5.78 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6047914611245896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6047914611245896 | validation: 0.6217459362876664]
	TIME [epoch: 5.75 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627732161862056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.627732161862056 | validation: 0.9529029102606664]
	TIME [epoch: 5.77 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320713012793809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9320713012793809 | validation: 1.355047042298392]
	TIME [epoch: 5.81 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.479370068975386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.479370068975386 | validation: 0.8000246361972505]
	TIME [epoch: 5.78 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670037032655303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8670037032655303 | validation: 0.9201971423603829]
	TIME [epoch: 5.81 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0875073787846867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0875073787846867 | validation: 0.7308832449632019]
	TIME [epoch: 5.78 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162177346642568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8162177346642568 | validation: 0.8103733686279071]
	TIME [epoch: 5.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747865998420972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8747865998420972 | validation: 0.6722994777743416]
	TIME [epoch: 5.78 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728587089215173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7728587089215173 | validation: 0.6711969961857247]
	TIME [epoch: 5.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7273721879751562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7273721879751562 | validation: 0.6619787368888613]
	TIME [epoch: 5.81 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6949542581818758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6949542581818758 | validation: 0.5973166420814435]
	TIME [epoch: 5.81 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6564462963555798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6564462963555798 | validation: 0.6004302159847542]
	TIME [epoch: 5.81 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.607145324987736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607145324987736 | validation: 0.6539340523740469]
	TIME [epoch: 5.81 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6269186010449677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6269186010449677 | validation: 0.8262076240017022]
	TIME [epoch: 5.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617164814875572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8617164814875572 | validation: 1.083010140148326]
	TIME [epoch: 5.82 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0377352021356376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0377352021356376 | validation: 0.8365093386377609]
	TIME [epoch: 5.84 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8886952854408949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8886952854408949 | validation: 0.6017614954081271]
	TIME [epoch: 5.86 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6926255915864478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6926255915864478 | validation: 0.6655135638567935]
	TIME [epoch: 5.81 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310195298787372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310195298787372 | validation: 0.6335378179990103]
	TIME [epoch: 5.83 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6649519248656688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6649519248656688 | validation: 0.5529260253444183]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6116709721898153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6116709721898153 | validation: 0.5604270448004817]
	TIME [epoch: 5.83 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752031007401169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752031007401169 | validation: 0.5829722642755567]
	TIME [epoch: 5.84 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947486179942763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5947486179942763 | validation: 0.826948497786046]
	TIME [epoch: 5.83 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7810234770508112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7810234770508112 | validation: 1.2096675399150538]
	TIME [epoch: 5.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.283095326847479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.283095326847479 | validation: 0.5740149323745071]
	TIME [epoch: 5.82 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6217721315407515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6217721315407515 | validation: 0.8004197936350335]
	TIME [epoch: 5.81 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430026960562791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430026960562791 | validation: 0.6350597386534447]
	TIME [epoch: 5.83 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478344031992154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478344031992154 | validation: 0.5259449044248197]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_372.pth
	Model improved!!!
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710621597973622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710621597973622 | validation: 0.5469958952874984]
	TIME [epoch: 5.84 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.534108918679069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.534108918679069 | validation: 0.5520824822261642]
	TIME [epoch: 5.81 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5295712401296022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5295712401296022 | validation: 0.5528798213754161]
	TIME [epoch: 5.83 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5408733298428516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5408733298428516 | validation: 0.7899709642076591]
	TIME [epoch: 5.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572281682081499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572281682081499 | validation: 1.1143106125011937]
	TIME [epoch: 5.82 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2215388196292838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2215388196292838 | validation: 0.7999329701614913]
	TIME [epoch: 5.81 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139356085634575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9139356085634575 | validation: 0.7676998157464854]
	TIME [epoch: 5.82 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490111186467629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8490111186467629 | validation: 0.6203123511113496]
	TIME [epoch: 5.83 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7009113793006612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7009113793006612 | validation: 0.6123864843647369]
	TIME [epoch: 5.82 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.612941868040542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.612941868040542 | validation: 0.6919530891745485]
	TIME [epoch: 5.84 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6443656613595765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6443656613595765 | validation: 0.7724619764514854]
	TIME [epoch: 5.81 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907320182406241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7907320182406241 | validation: 0.566013955032201]
	TIME [epoch: 5.84 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5529554986280845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5529554986280845 | validation: 0.5739391909412104]
	TIME [epoch: 5.81 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5580834118401454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5580834118401454 | validation: 0.6159678061411175]
	TIME [epoch: 5.81 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6133044484184608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6133044484184608 | validation: 0.7206170392507585]
	TIME [epoch: 5.81 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7322973484840238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7322973484840238 | validation: 0.5438035992386782]
	TIME [epoch: 5.79 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5607950352914252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5607950352914252 | validation: 0.46794927630456]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_389.pth
	Model improved!!!
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4845059311953485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4845059311953485 | validation: 0.4992332937046702]
	TIME [epoch: 5.79 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48406621440620684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48406621440620684 | validation: 0.5626585790121695]
	TIME [epoch: 5.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.539351842149619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.539351842149619 | validation: 0.9519909871307868]
	TIME [epoch: 5.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9153896346387813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9153896346387813 | validation: 0.9648198262411585]
	TIME [epoch: 5.79 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0611233798267543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0611233798267543 | validation: 0.607489251055831]
	TIME [epoch: 5.82 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446621147913851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446621147913851 | validation: 0.6792570402199822]
	TIME [epoch: 5.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590660307644425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7590660307644425 | validation: 0.5298367944944878]
	TIME [epoch: 5.81 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5559964640015388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5559964640015388 | validation: 0.5049074751258323]
	TIME [epoch: 5.82 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027079396897913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5027079396897913 | validation: 0.5547588486538987]
	TIME [epoch: 5.8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5572132697275717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5572132697275717 | validation: 0.7696809112689245]
	TIME [epoch: 5.82 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752493121652357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752493121652357 | validation: 0.7055918498471202]
	TIME [epoch: 5.8 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6500182725406921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6500182725406921 | validation: 0.6414074929867264]
	TIME [epoch: 5.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711262471698868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6711262471698868 | validation: 0.5851876335781777]
	TIME [epoch: 5.81 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5426639442687339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5426639442687339 | validation: 0.42878786037111266]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.445109947969009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.445109947969009 | validation: 0.43533786687255405]
	TIME [epoch: 5.81 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44024445991460953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44024445991460953 | validation: 0.5148769768804029]
	TIME [epoch: 5.81 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4770993218821768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4770993218821768 | validation: 0.716833597747717]
	TIME [epoch: 5.82 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7221603498811938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7221603498811938 | validation: 0.7329851110211703]
	TIME [epoch: 5.82 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976919578798235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6976919578798235 | validation: 0.6139958608621003]
	TIME [epoch: 5.81 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201431955625213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6201431955625213 | validation: 0.47490471612408086]
	TIME [epoch: 5.82 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47527556143894273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47527556143894273 | validation: 0.4050569199512575]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_410.pth
	Model improved!!!
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4066112541723963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4066112541723963 | validation: 0.49349536252335136]
	TIME [epoch: 5.82 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41754797282966577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41754797282966577 | validation: 0.4026647760323554]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4154434392148065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4154434392148065 | validation: 0.5339112725480532]
	TIME [epoch: 5.81 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45305787881096476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45305787881096476 | validation: 0.430603004645846]
	TIME [epoch: 5.84 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49122586622401876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49122586622401876 | validation: 0.700895693310232]
	TIME [epoch: 5.83 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5810114587665839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5810114587665839 | validation: 0.7906968244015797]
	TIME [epoch: 5.83 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837818710871537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7837818710871537 | validation: 0.6989370431989955]
	TIME [epoch: 5.78 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105422185563319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8105422185563319 | validation: 0.42645738546701184]
	TIME [epoch: 5.83 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47373034214273874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47373034214273874 | validation: 0.6647663190988022]
	TIME [epoch: 5.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.581263614487459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.581263614487459 | validation: 0.4440783325766095]
	TIME [epoch: 5.83 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4253609444000537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4253609444000537 | validation: 0.5366528452595656]
	TIME [epoch: 5.79 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478828329306198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5478828329306198 | validation: 0.8190311581173533]
	TIME [epoch: 5.83 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326462596854272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326462596854272 | validation: 0.6957477170843165]
	TIME [epoch: 5.79 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309466457113715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309466457113715 | validation: 0.426718366552413]
	TIME [epoch: 5.83 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117211147598572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117211147598572 | validation: 0.42121300656899774]
	TIME [epoch: 5.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4205572821685156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4205572821685156 | validation: 0.470300187928339]
	TIME [epoch: 5.83 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41998124459762415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41998124459762415 | validation: 0.7284944582407191]
	TIME [epoch: 5.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6599863179406792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6599863179406792 | validation: 0.8692397949980948]
	TIME [epoch: 5.82 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9303297054368966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9303297054368966 | validation: 0.599096558623968]
	TIME [epoch: 5.83 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898888280312352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898888280312352 | validation: 0.5663439159466243]
	TIME [epoch: 5.82 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627013078608253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.627013078608253 | validation: 0.4263714587960145]
	TIME [epoch: 5.8 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47018804983937657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47018804983937657 | validation: 0.5902121321654819]
	TIME [epoch: 5.81 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48248746494122424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48248746494122424 | validation: 0.37902610963959926]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3756486252412135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3756486252412135 | validation: 0.4047988417861037]
	TIME [epoch: 5.81 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40058394486383153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40058394486383153 | validation: 0.490091561195009]
	TIME [epoch: 5.81 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4647446604414939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4647446604414939 | validation: 0.5397553056492498]
	TIME [epoch: 5.81 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49991439227118917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49991439227118917 | validation: 0.5837312912770607]
	TIME [epoch: 5.83 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5870205531509689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5870205531509689 | validation: 0.45896430298843144]
	TIME [epoch: 5.77 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4196332502428773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4196332502428773 | validation: 0.3356461101054977]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_439.pth
	Model improved!!!
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3482775126563772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3482775126563772 | validation: 0.45289260856279956]
	TIME [epoch: 5.76 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3733819166561379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3733819166561379 | validation: 0.45698821446539944]
	TIME [epoch: 5.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48085757937055124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48085757937055124 | validation: 0.6910173348013452]
	TIME [epoch: 5.82 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6366058635937512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6366058635937512 | validation: 0.6384454021377929]
	TIME [epoch: 5.83 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6775687440696876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6775687440696876 | validation: 0.3859329702040343]
	TIME [epoch: 5.82 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45021694304612525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45021694304612525 | validation: 0.4469397378008073]
	TIME [epoch: 5.83 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36331331223419006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36331331223419006 | validation: 0.44308913825652346]
	TIME [epoch: 5.79 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43250620298687614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43250620298687614 | validation: 0.7180879302288302]
	TIME [epoch: 5.83 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6317849947945349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6317849947945349 | validation: 0.7907883301140197]
	TIME [epoch: 5.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502522546531499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502522546531499 | validation: 0.5773714439951785]
	TIME [epoch: 5.82 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573732463322455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573732463322455 | validation: 0.5619204024463735]
	TIME [epoch: 5.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5385563353270575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5385563353270575 | validation: 0.6701796181780519]
	TIME [epoch: 5.82 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6320692604997005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6320692604997005 | validation: 0.3778047691550397]
	TIME [epoch: 5.77 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3546947758968223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3546947758968223 | validation: 0.3574515525974306]
	TIME [epoch: 5.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31026904540223377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31026904540223377 | validation: 0.33722101309590446]
	TIME [epoch: 5.81 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32850017517325597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32850017517325597 | validation: 0.41089326986485086]
	TIME [epoch: 5.83 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37823395728870496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37823395728870496 | validation: 0.5251077673771537]
	TIME [epoch: 5.79 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444504768320213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5444504768320213 | validation: 0.6176497381447494]
	TIME [epoch: 5.82 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5224226599002215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5224226599002215 | validation: 0.34676735480184706]
	TIME [epoch: 5.82 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37474459697468127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37474459697468127 | validation: 0.45770043784781544]
	TIME [epoch: 5.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37444763797513203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37444763797513203 | validation: 0.5588587112028789]
	TIME [epoch: 5.79 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5547236372578034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5547236372578034 | validation: 0.4654762845512071]
	TIME [epoch: 5.82 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42698176885705197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42698176885705197 | validation: 0.4206052678474576]
	TIME [epoch: 5.79 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4133393642568385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4133393642568385 | validation: 0.36777029187782306]
	TIME [epoch: 5.81 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32235864511494483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32235864511494483 | validation: 0.326964759890389]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_464.pth
	Model improved!!!
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3267679939887019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3267679939887019 | validation: 0.618859109868074]
	TIME [epoch: 5.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894232315333501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4894232315333501 | validation: 0.6290661800378644]
	TIME [epoch: 5.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6453466126088185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6453466126088185 | validation: 0.43839231528713224]
	TIME [epoch: 5.81 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060963585411278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5060963585411278 | validation: 0.5197719367202916]
	TIME [epoch: 5.81 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4198842083379013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4198842083379013 | validation: 0.45995774846128223]
	TIME [epoch: 5.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4646954549244109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4646954549244109 | validation: 0.6097292284486587]
	TIME [epoch: 5.81 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5416029246187806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5416029246187806 | validation: 0.5658724164624259]
	TIME [epoch: 5.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5717992349246386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5717992349246386 | validation: 0.45838178369185667]
	TIME [epoch: 5.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45931281300850063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45931281300850063 | validation: 0.3060651835840522]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_473.pth
	Model improved!!!
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34148590935204853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34148590935204853 | validation: 0.4550906045714649]
	TIME [epoch: 5.76 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3524728785613395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3524728785613395 | validation: 0.3728663920206766]
	TIME [epoch: 5.78 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35077587037331226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35077587037331226 | validation: 0.4297187497396664]
	TIME [epoch: 5.76 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42323804646953916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42323804646953916 | validation: 0.496789316061762]
	TIME [epoch: 5.77 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4804821658590434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4804821658590434 | validation: 0.4358125739411506]
	TIME [epoch: 5.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544025675418753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4544025675418753 | validation: 0.6491837036796549]
	TIME [epoch: 5.78 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5406667138628428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5406667138628428 | validation: 0.3845461254286178]
	TIME [epoch: 5.77 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3923161092014352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3923161092014352 | validation: 0.3147527027123343]
	TIME [epoch: 5.77 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2994284802698329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2994284802698329 | validation: 0.30600158796145394]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29388838202140044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29388838202140044 | validation: 0.4780023325360537]
	TIME [epoch: 5.83 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4418519377976329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4418519377976329 | validation: 0.5561610674228018]
	TIME [epoch: 5.83 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5586630969642319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5586630969642319 | validation: 0.8043981806297391]
	TIME [epoch: 5.82 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912576754282818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7912576754282818 | validation: 0.40815408768965594]
	TIME [epoch: 5.71 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5025731836500039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5025731836500039 | validation: 0.738626944133836]
	TIME [epoch: 5.81 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6328482988459955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6328482988459955 | validation: 0.3126094353329656]
	TIME [epoch: 5.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26751555874935823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26751555874935823 | validation: 0.3150494210044922]
	TIME [epoch: 5.71 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37725997757069385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37725997757069385 | validation: 0.4668173509344527]
	TIME [epoch: 5.79 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3606459457609462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3606459457609462 | validation: 0.5097579109876182]
	TIME [epoch: 5.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5104422971920797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5104422971920797 | validation: 0.34616426789565335]
	TIME [epoch: 5.81 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304424110629891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.304424110629891 | validation: 0.29153187861997804]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29362167056756294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29362167056756294 | validation: 0.4939425065058618]
	TIME [epoch: 5.84 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38834004540590755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38834004540590755 | validation: 0.5169351426046607]
	TIME [epoch: 5.82 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.548069312554496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.548069312554496 | validation: 0.39278126131649677]
	TIME [epoch: 5.82 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3430159252249848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3430159252249848 | validation: 0.28034819779803694]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_497.pth
	Model improved!!!
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2763198157311687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2763198157311687 | validation: 0.3879256138528509]
	TIME [epoch: 5.76 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.350173032104556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.350173032104556 | validation: 0.5243605150823768]
	TIME [epoch: 5.77 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43857956538800563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43857956538800563 | validation: 0.517923170097961]
	TIME [epoch: 5.78 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5987692460605564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5987692460605564 | validation: 0.3933226458126497]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4170881788087503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4170881788087503 | validation: 0.32736987578816]
	TIME [epoch: 12.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33927078768603985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33927078768603985 | validation: 0.3375526565459772]
	TIME [epoch: 12.3 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2973511797331977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2973511797331977 | validation: 0.37504305711357355]
	TIME [epoch: 12.4 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30148653252904484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30148653252904484 | validation: 0.4450629764369056]
	TIME [epoch: 12.3 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48674246936435595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48674246936435595 | validation: 0.4629341710404613]
	TIME [epoch: 12.3 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4038095691147626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4038095691147626 | validation: 0.3256378942845959]
	TIME [epoch: 12.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35458165320844787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35458165320844787 | validation: 0.5322560745786069]
	TIME [epoch: 12.3 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4189467418017263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4189467418017263 | validation: 0.28955457855264577]
	TIME [epoch: 12.3 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28230653089201047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28230653089201047 | validation: 0.2716529629790415]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2758140188243545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2758140188243545 | validation: 0.49980224802753725]
	TIME [epoch: 12.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41073388416435164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41073388416435164 | validation: 0.6947593616503482]
	TIME [epoch: 12.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551515983384193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7551515983384193 | validation: 0.29998333903551816]
	TIME [epoch: 12.4 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31597206923231375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31597206923231375 | validation: 0.48574460285028104]
	TIME [epoch: 12.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4138520754815692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4138520754815692 | validation: 0.6182302269424773]
	TIME [epoch: 12.4 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437570789531426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437570789531426 | validation: 0.34787190321292916]
	TIME [epoch: 12.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2732178129408225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2732178129408225 | validation: 0.42815728940072556]
	TIME [epoch: 12.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42997541010935836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42997541010935836 | validation: 0.4170014516311013]
	TIME [epoch: 12.4 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4286971800765766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4286971800765766 | validation: 0.40120356355094167]
	TIME [epoch: 12.3 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33328299006192624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33328299006192624 | validation: 0.2809917417830992]
	TIME [epoch: 12.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32631717911742053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32631717911742053 | validation: 0.3874955040243279]
	TIME [epoch: 12.4 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2899209882034162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2899209882034162 | validation: 0.2872987855113489]
	TIME [epoch: 12.4 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33810442151819364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33810442151819364 | validation: 0.6545113424627597]
	TIME [epoch: 12.3 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.538406550741736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.538406550741736 | validation: 0.5021138108386468]
	TIME [epoch: 12.4 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5490255461245636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5490255461245636 | validation: 0.29362685232654206]
	TIME [epoch: 12.3 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36474007962475063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36474007962475063 | validation: 0.3896206088480105]
	TIME [epoch: 12.3 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31179752137933475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31179752137933475 | validation: 0.514379830070936]
	TIME [epoch: 12.3 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4820020081978595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4820020081978595 | validation: 0.3539169247334472]
	TIME [epoch: 12.4 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29691711895339346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29691711895339346 | validation: 0.30127436301668387]
	TIME [epoch: 12.4 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29921744069286843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29921744069286843 | validation: 0.2729140434854738]
	TIME [epoch: 12.3 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25676524567565207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25676524567565207 | validation: 0.2799796145402875]
	TIME [epoch: 12.4 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716536871552632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2716536871552632 | validation: 0.4542432197903146]
	TIME [epoch: 12.4 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3783224558220214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3783224558220214 | validation: 0.5087650577222668]
	TIME [epoch: 12.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5769926794936012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5769926794936012 | validation: 0.43680046909160775]
	TIME [epoch: 12.3 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.406105736720466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.406105736720466 | validation: 0.31404050609903145]
	TIME [epoch: 12.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3399250846152975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3399250846152975 | validation: 0.35794257790635087]
	TIME [epoch: 12.3 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27966990634637845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27966990634637845 | validation: 0.2455790594458434]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_537.pth
	Model improved!!!
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23517273466973238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23517273466973238 | validation: 0.3248599222627733]
	TIME [epoch: 12.3 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2563390656491955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2563390656491955 | validation: 0.28951061186780125]
	TIME [epoch: 12.3 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3101850269497068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3101850269497068 | validation: 0.39196137580226853]
	TIME [epoch: 12.3 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.288687265244144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.288687265244144 | validation: 0.2564567425307443]
	TIME [epoch: 12.4 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31144653714919796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31144653714919796 | validation: 0.8037911786599061]
	TIME [epoch: 12.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671404532219772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671404532219772 | validation: 0.6330604846663965]
	TIME [epoch: 12.4 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7384540134634802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7384540134634802 | validation: 0.4623129137189579]
	TIME [epoch: 12.4 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5704034334202878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5704034334202878 | validation: 0.5948680362325528]
	TIME [epoch: 12.4 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.606817075144637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.606817075144637 | validation: 0.29345636553656695]
	TIME [epoch: 12.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25610665075059946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25610665075059946 | validation: 0.44743696286998164]
	TIME [epoch: 12.4 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4738340213368274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4738340213368274 | validation: 0.3926595728646547]
	TIME [epoch: 12.4 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3102702487163462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3102702487163462 | validation: 0.41279487121557895]
	TIME [epoch: 12.4 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4338989056771929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4338989056771929 | validation: 0.2860415363286797]
	TIME [epoch: 12.4 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3385544720413794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3385544720413794 | validation: 0.25833645957348017]
	TIME [epoch: 12.4 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21427622008202415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21427622008202415 | validation: 0.3229139645326778]
	TIME [epoch: 12.4 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31395786131898784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31395786131898784 | validation: 0.5985328073185425]
	TIME [epoch: 12.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48765980095886025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48765980095886025 | validation: 0.4829989744655871]
	TIME [epoch: 12.4 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5379424912946829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5379424912946829 | validation: 0.3870495441974466]
	TIME [epoch: 12.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4010250787296278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4010250787296278 | validation: 0.3179928201499853]
	TIME [epoch: 12.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.324543036873369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.324543036873369 | validation: 0.29237438611989314]
	TIME [epoch: 12.3 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30192258837813596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30192258837813596 | validation: 0.35747227359319766]
	TIME [epoch: 12.3 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2808475619676091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2808475619676091 | validation: 0.32146215920537724]
	TIME [epoch: 12.3 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38306617273590654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38306617273590654 | validation: 0.43443351427792126]
	TIME [epoch: 12.4 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3472438276593501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3472438276593501 | validation: 0.18663279015754428]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_561.pth
	Model improved!!!
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2182520654880475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2182520654880475 | validation: 0.22724190913358755]
	TIME [epoch: 12.3 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1864380361699053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1864380361699053 | validation: 0.17897192428042258]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17008646599768162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17008646599768162 | validation: 0.18467051443329274]
	TIME [epoch: 12.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1676566477294782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1676566477294782 | validation: 0.21995068837704113]
	TIME [epoch: 12.3 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17610646927066698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17610646927066698 | validation: 0.22661332504333723]
	TIME [epoch: 12.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938864476729742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2938864476729742 | validation: 0.8666141040060756]
	TIME [epoch: 12.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7267104597441736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7267104597441736 | validation: 0.6175769098468517]
	TIME [epoch: 12.3 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6961474347929886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6961474347929886 | validation: 0.37759324565098473]
	TIME [epoch: 12.3 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5029105357143232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5029105357143232 | validation: 0.41365674191482316]
	TIME [epoch: 12.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701206934479697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3701206934479697 | validation: 0.5384115253120577]
	TIME [epoch: 12.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5511702731738372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5511702731738372 | validation: 0.39685713715329496]
	TIME [epoch: 12.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4253364961549504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4253364961549504 | validation: 0.4351522616385426]
	TIME [epoch: 12.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3511502822176625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3511502822176625 | validation: 0.2046567364283848]
	TIME [epoch: 12.3 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23269999747157946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23269999747157946 | validation: 0.23304835406180838]
	TIME [epoch: 12.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21219442514847006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21219442514847006 | validation: 0.2773057411433976]
	TIME [epoch: 12.3 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612966721301204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2612966721301204 | validation: 0.3586442613841169]
	TIME [epoch: 12.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3748822500776301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3748822500776301 | validation: 0.3221326460668736]
	TIME [epoch: 12.3 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27478787417667183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27478787417667183 | validation: 0.21730999866278192]
	TIME [epoch: 12.4 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504657531652423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2504657531652423 | validation: 0.42243134218237055]
	TIME [epoch: 12.4 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32728966887130434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32728966887130434 | validation: 0.3858586578926524]
	TIME [epoch: 12.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4313074874057079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4313074874057079 | validation: 0.3356148521002686]
	TIME [epoch: 12.3 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2621388873905036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2621388873905036 | validation: 0.19468025392455798]
	TIME [epoch: 12.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2121293910167334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2121293910167334 | validation: 0.3084607633698737]
	TIME [epoch: 12.3 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26293402322142073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26293402322142073 | validation: 0.367009355521441]
	TIME [epoch: 12.3 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35810357516499464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35810357516499464 | validation: 0.2801009066649656]
	TIME [epoch: 12.3 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29474511861929353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29474511861929353 | validation: 0.29181284135117924]
	TIME [epoch: 12.3 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23355656710370906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23355656710370906 | validation: 0.23596701807238124]
	TIME [epoch: 12.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2918626589003312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2918626589003312 | validation: 0.5792776263259638]
	TIME [epoch: 12.3 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4442167116742188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4442167116742188 | validation: 0.24884881928875757]
	TIME [epoch: 12.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2813759627408856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2813759627408856 | validation: 0.28969890754466404]
	TIME [epoch: 12.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2489740777057908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2489740777057908 | validation: 0.2812387752413346]
	TIME [epoch: 12.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2705021667592168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2705021667592168 | validation: 0.26786458498413007]
	TIME [epoch: 12.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2399759824121813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2399759824121813 | validation: 0.2547591944363194]
	TIME [epoch: 12.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2746753145081983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2746753145081983 | validation: 0.3677872901788553]
	TIME [epoch: 12.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896296545272438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2896296545272438 | validation: 0.3232451215678932]
	TIME [epoch: 12.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35915783543356083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35915783543356083 | validation: 0.25477963066195314]
	TIME [epoch: 12.3 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22164233957564822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22164233957564822 | validation: 0.1752133639629968]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19074381465293339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19074381465293339 | validation: 0.41600652372406427]
	TIME [epoch: 12.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919188184121636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2919188184121636 | validation: 0.44063438404518723]
	TIME [epoch: 12.2 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5206545733071768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5206545733071768 | validation: 0.37002640889795724]
	TIME [epoch: 12.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3072727550597807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3072727550597807 | validation: 0.32791472235212366]
	TIME [epoch: 12.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3285452899644786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3285452899644786 | validation: 0.3781283571538132]
	TIME [epoch: 12.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3652110945289013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3652110945289013 | validation: 0.2509037613390927]
	TIME [epoch: 12.2 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21886732371881606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21886732371881606 | validation: 0.1727825938869244]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_605.pth
	Model improved!!!
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19539867333421426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19539867333421426 | validation: 0.32449381485713674]
	TIME [epoch: 12.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23858007520344918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23858007520344918 | validation: 0.29595175564732834]
	TIME [epoch: 12.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3615367095554798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3615367095554798 | validation: 0.24598282145998362]
	TIME [epoch: 12.4 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22567906544777805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22567906544777805 | validation: 0.22041130572042222]
	TIME [epoch: 12.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19364181146268034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19364181146268034 | validation: 0.26725783410075316]
	TIME [epoch: 12.4 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27125136514205345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27125136514205345 | validation: 0.3589906538255925]
	TIME [epoch: 12.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3179664542814905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3179664542814905 | validation: 0.19244251418588879]
	TIME [epoch: 12.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22656168966227852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22656168966227852 | validation: 0.26791834009448323]
	TIME [epoch: 12.2 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.191331260696888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.191331260696888 | validation: 0.21696382171539544]
	TIME [epoch: 12.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25484064587096766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25484064587096766 | validation: 0.656614042106637]
	TIME [epoch: 12.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5231418998359351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5231418998359351 | validation: 0.3363564384201061]
	TIME [epoch: 12.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35279734417003483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35279734417003483 | validation: 0.19605526218646485]
	TIME [epoch: 12.2 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21675470211089756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21675470211089756 | validation: 0.3211629469980987]
	TIME [epoch: 12.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23027235816567293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23027235816567293 | validation: 0.2785102789538832]
	TIME [epoch: 12.2 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31582646197598424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31582646197598424 | validation: 0.41437066371095393]
	TIME [epoch: 12.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31086666476856295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31086666476856295 | validation: 0.22066503896735307]
	TIME [epoch: 12.2 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2657348281517238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2657348281517238 | validation: 0.24620719801310376]
	TIME [epoch: 12.2 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2192693050954616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2192693050954616 | validation: 0.23371883164392737]
	TIME [epoch: 12.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24200490138951464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24200490138951464 | validation: 0.3437196551075174]
	TIME [epoch: 12.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31919844068175773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31919844068175773 | validation: 0.2209697959917144]
	TIME [epoch: 12.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23702799239023717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23702799239023717 | validation: 0.28488809200642085]
	TIME [epoch: 12.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2168048034522819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2168048034522819 | validation: 0.2596402046408563]
	TIME [epoch: 12.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.321396496301502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.321396496301502 | validation: 0.6450159651084064]
	TIME [epoch: 12.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5001316543622805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5001316543622805 | validation: 0.20393692647425]
	TIME [epoch: 12.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2155020787215802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2155020787215802 | validation: 0.24541147844787883]
	TIME [epoch: 12.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2745005092056208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2745005092056208 | validation: 0.2877902512535859]
	TIME [epoch: 12.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20567161954308855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20567161954308855 | validation: 0.14642337195097185]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15874503323802913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15874503323802913 | validation: 0.1936606410552573]
	TIME [epoch: 12.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1509364618737456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1509364618737456 | validation: 0.14913989808627823]
	TIME [epoch: 12.4 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16240953429866548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16240953429866548 | validation: 0.3618049458120877]
	TIME [epoch: 12.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24027415044857545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24027415044857545 | validation: 0.3935431856742773]
	TIME [epoch: 12.4 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4486368033782892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4486368033782892 | validation: 0.2603870578325317]
	TIME [epoch: 12.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20559110350287277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20559110350287277 | validation: 0.16251017708086107]
	TIME [epoch: 12.4 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451241697031324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1451241697031324 | validation: 0.15289527537422976]
	TIME [epoch: 12.4 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14409255879088495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14409255879088495 | validation: 0.2907054432549513]
	TIME [epoch: 12.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260839967345057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.260839967345057 | validation: 0.4997642846953536]
	TIME [epoch: 12.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.533090397883589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.533090397883589 | validation: 0.20318674526413358]
	TIME [epoch: 12.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24303167152896354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24303167152896354 | validation: 0.3831593948754676]
	TIME [epoch: 12.4 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3047154619570755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3047154619570755 | validation: 0.440005702558603]
	TIME [epoch: 12.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4353421267419975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4353421267419975 | validation: 0.25398454219805827]
	TIME [epoch: 12.4 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20879061414030445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20879061414030445 | validation: 0.18097350600511444]
	TIME [epoch: 12.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20853869971640074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20853869971640074 | validation: 0.39313952245127737]
	TIME [epoch: 12.4 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2836463051343178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2836463051343178 | validation: 0.19761138658777197]
	TIME [epoch: 12.4 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21797965917985054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21797965917985054 | validation: 0.17102740246546738]
	TIME [epoch: 12.4 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699707609559193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1699707609559193 | validation: 0.2623036223164807]
	TIME [epoch: 12.4 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19783162798456524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19783162798456524 | validation: 0.35386080856014424]
	TIME [epoch: 12.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3861892402363536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3861892402363536 | validation: 0.3337365314064911]
	TIME [epoch: 12.4 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24046633508594323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24046633508594323 | validation: 0.1729365270865953]
	TIME [epoch: 12.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19743661924155398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19743661924155398 | validation: 0.29409138302337645]
	TIME [epoch: 12.4 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21797036401263817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21797036401263817 | validation: 0.2553332676380557]
	TIME [epoch: 12.4 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29390026652003914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29390026652003914 | validation: 0.24021649244075638]
	TIME [epoch: 12.4 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18243810217627343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18243810217627343 | validation: 0.15105463742899272]
	TIME [epoch: 12.4 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16219567007475497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16219567007475497 | validation: 0.24860726366383032]
	TIME [epoch: 12.4 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17007498939879204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17007498939879204 | validation: 0.20555082494458193]
	TIME [epoch: 12.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2694003706387682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2694003706387682 | validation: 0.46765702949966176]
	TIME [epoch: 12.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334468998962041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3334468998962041 | validation: 0.2839508760613766]
	TIME [epoch: 12.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32145152252884557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32145152252884557 | validation: 0.1900711147469276]
	TIME [epoch: 12.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2112109520498642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2112109520498642 | validation: 0.3728990670293354]
	TIME [epoch: 12.4 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2706515557157198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2706515557157198 | validation: 0.2327452501874297]
	TIME [epoch: 12.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535052993922666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2535052993922666 | validation: 0.2156904666473901]
	TIME [epoch: 12.4 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2059968330759454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2059968330759454 | validation: 0.20922464076599734]
	TIME [epoch: 12.4 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16221195864573618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16221195864573618 | validation: 0.21970206938727754]
	TIME [epoch: 12.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24271221754804992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24271221754804992 | validation: 0.4882461944188844]
	TIME [epoch: 12.4 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3389951104219898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3389951104219898 | validation: 0.289968656369045]
	TIME [epoch: 12.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3454546345358594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3454546345358594 | validation: 0.20604170531389565]
	TIME [epoch: 12.4 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687252618521697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1687252618521697 | validation: 0.21602474579555012]
	TIME [epoch: 12.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16602130566826215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16602130566826215 | validation: 0.1736663860229516]
	TIME [epoch: 12.3 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19965095249034281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19965095249034281 | validation: 0.3557723234666598]
	TIME [epoch: 12.4 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27101561241102895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27101561241102895 | validation: 0.26022883304238015]
	TIME [epoch: 12.4 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29279447954926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29279447954926 | validation: 0.23292763604352112]
	TIME [epoch: 12.4 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2124199964043774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2124199964043774 | validation: 0.22419860884243192]
	TIME [epoch: 12.4 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16945310486898355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16945310486898355 | validation: 0.18735383373676098]
	TIME [epoch: 12.4 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19770405483942838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19770405483942838 | validation: 0.33857023145462356]
	TIME [epoch: 12.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2171804288505055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2171804288505055 | validation: 0.21517860093113908]
	TIME [epoch: 12.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26041749365067335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26041749365067335 | validation: 0.3721252720601384]
	TIME [epoch: 12.4 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24852210272223896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24852210272223896 | validation: 0.3610032113139809]
	TIME [epoch: 12.4 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3759489788629902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3759489788629902 | validation: 0.19916630876988028]
	TIME [epoch: 12.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23478633910814325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23478633910814325 | validation: 0.3743278169146064]
	TIME [epoch: 12.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2711097901216119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2711097901216119 | validation: 0.25094639527308077]
	TIME [epoch: 12.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23554573640749013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23554573640749013 | validation: 0.16614743448037314]
	TIME [epoch: 12.4 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18783927972278697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18783927972278697 | validation: 0.17348701558379237]
	TIME [epoch: 12.4 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14695597431683152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14695597431683152 | validation: 0.15295775237059958]
	TIME [epoch: 12.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351628583348154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1351628583348154 | validation: 0.154065065396563]
	TIME [epoch: 12.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17196334634550758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17196334634550758 | validation: 0.3998930917500119]
	TIME [epoch: 12.4 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2949945328556839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2949945328556839 | validation: 0.4635141810539118]
	TIME [epoch: 12.4 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000126334081701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000126334081701 | validation: 0.17339557843662284]
	TIME [epoch: 12.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1750040474734092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1750040474734092 | validation: 0.2539671295114099]
	TIME [epoch: 12.4 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18659587289992685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18659587289992685 | validation: 0.2712789461640943]
	TIME [epoch: 12.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2831345682187458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2831345682187458 | validation: 0.34479773403322916]
	TIME [epoch: 12.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2629451294761206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2629451294761206 | validation: 0.18913294110981752]
	TIME [epoch: 12.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20558806176954852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20558806176954852 | validation: 0.2959895664797538]
	TIME [epoch: 12.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20539560117957614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20539560117957614 | validation: 0.14441351275675524]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_697.pth
	Model improved!!!
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16931895516937265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16931895516937265 | validation: 0.21763776537224544]
	TIME [epoch: 12.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1570052021945771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1570052021945771 | validation: 0.176514639871127]
	TIME [epoch: 12.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19728019697897267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19728019697897267 | validation: 0.31962542196119137]
	TIME [epoch: 12.3 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24456666894594636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24456666894594636 | validation: 0.22751492195176093]
	TIME [epoch: 12.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22437627666610194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22437627666610194 | validation: 0.21214119327335157]
	TIME [epoch: 12.4 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2466762566836615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2466762566836615 | validation: 0.40500500688093855]
	TIME [epoch: 12.4 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2862094932652528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2862094932652528 | validation: 0.17844206698963225]
	TIME [epoch: 12.2 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19411995119058767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19411995119058767 | validation: 0.19222668343528704]
	TIME [epoch: 12.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1517189130622503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1517189130622503 | validation: 0.15933718164832752]
	TIME [epoch: 12.3 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.162461455660232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.162461455660232 | validation: 0.33943586539257015]
	TIME [epoch: 12.4 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534340176417436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2534340176417436 | validation: 0.2912787394773179]
	TIME [epoch: 12.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33826220512362865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33826220512362865 | validation: 0.20835814173364692]
	TIME [epoch: 12.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1647918333414598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1647918333414598 | validation: 0.14503096868374388]
	TIME [epoch: 12.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13989688407215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13989688407215 | validation: 0.1587557048387895]
	TIME [epoch: 12.3 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14109020186523372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14109020186523372 | validation: 0.1939506041688047]
	TIME [epoch: 12.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031468655289143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2031468655289143 | validation: 0.2875515803230339]
	TIME [epoch: 12.3 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2443768723095423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2443768723095423 | validation: 0.20887369275139356]
	TIME [epoch: 12.4 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18216045753140875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18216045753140875 | validation: 0.16845918934922477]
	TIME [epoch: 12.3 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21967908053143384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21967908053143384 | validation: 0.5744273277056332]
	TIME [epoch: 12.4 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42203098427369884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42203098427369884 | validation: 0.16248555600232117]
	TIME [epoch: 12.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1882733111967314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1882733111967314 | validation: 0.15403805610971993]
	TIME [epoch: 12.4 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13922297306887932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13922297306887932 | validation: 0.15124118444373794]
	TIME [epoch: 12.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400185108027247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1400185108027247 | validation: 0.21856256077021965]
	TIME [epoch: 12.4 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17081797147471772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17081797147471772 | validation: 0.24334973617314892]
	TIME [epoch: 12.3 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2837379281492448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2837379281492448 | validation: 0.27221679450613007]
	TIME [epoch: 12.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21019081644484192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21019081644484192 | validation: 0.17122443458023953]
	TIME [epoch: 12.3 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19637352836648853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19637352836648853 | validation: 0.3288279008027053]
	TIME [epoch: 12.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23565585149942492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23565585149942492 | validation: 0.2997789917525349]
	TIME [epoch: 12.3 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32121209454918404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32121209454918404 | validation: 0.19571416042575732]
	TIME [epoch: 12.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14278267504147465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14278267504147465 | validation: 0.2108965623174875]
	TIME [epoch: 12.3 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18686274869014807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18686274869014807 | validation: 0.16918578128133951]
	TIME [epoch: 12.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1834156569734925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1834156569734925 | validation: 0.18436727767622652]
	TIME [epoch: 12.3 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17834428094063762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17834428094063762 | validation: 0.3361830654347677]
	TIME [epoch: 12.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2256992138624994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2256992138624994 | validation: 0.23554380343000567]
	TIME [epoch: 12.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34275835230866014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34275835230866014 | validation: 0.2653940888939839]
	TIME [epoch: 12.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17200766196487102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17200766196487102 | validation: 0.1376421726687015]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_733.pth
	Model improved!!!
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14812032828031232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14812032828031232 | validation: 0.18826830602634187]
	TIME [epoch: 12.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18134008759845274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18134008759845274 | validation: 0.22513778730324213]
	TIME [epoch: 12.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2093260660418325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2093260660418325 | validation: 0.199216914601928]
	TIME [epoch: 12.5 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15250403176011973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15250403176011973 | validation: 0.13829062079683527]
	TIME [epoch: 12.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1732557768943892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1732557768943892 | validation: 0.2943797302169]
	TIME [epoch: 12.5 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20251828120283613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20251828120283613 | validation: 0.2119090317043984]
	TIME [epoch: 12.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25753701931319184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25753701931319184 | validation: 0.2729120806727488]
	TIME [epoch: 12.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19168679379146839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19168679379146839 | validation: 0.1759185872784091]
	TIME [epoch: 12.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1998151554567437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1998151554567437 | validation: 0.22033161724285294]
	TIME [epoch: 12.5 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14782488955004755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14782488955004755 | validation: 0.1050705046687587]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_743.pth
	Model improved!!!
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1214851848801936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1214851848801936 | validation: 0.1807046925635401]
	TIME [epoch: 12.4 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11565459348120481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11565459348120481 | validation: 0.10920089671838694]
	TIME [epoch: 12.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13093139920493235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13093139920493235 | validation: 0.325168166791611]
	TIME [epoch: 12.4 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23860514040603473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23860514040603473 | validation: 0.33021618656314317]
	TIME [epoch: 12.4 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3703327042956097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3703327042956097 | validation: 0.19342233470630113]
	TIME [epoch: 12.4 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15050036648801896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15050036648801896 | validation: 0.14002308899977242]
	TIME [epoch: 12.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13500722181595798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13500722181595798 | validation: 0.1890797031539572]
	TIME [epoch: 12.4 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496567921037432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1496567921037432 | validation: 0.15499739399345316]
	TIME [epoch: 12.4 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16504738610811992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16504738610811992 | validation: 0.2329038057703474]
	TIME [epoch: 12.2 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19689773521691586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19689773521691586 | validation: 0.20558418919593083]
	TIME [epoch: 12.4 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19243318509544868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19243318509544868 | validation: 0.1409530141577442]
	TIME [epoch: 12.4 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17260920531723403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17260920531723403 | validation: 0.46754761808292894]
	TIME [epoch: 12.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29874691465507974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29874691465507974 | validation: 0.2283733312308126]
	TIME [epoch: 12.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27774098133138464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27774098133138464 | validation: 0.19054759968906249]
	TIME [epoch: 12.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13796066208728092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13796066208728092 | validation: 0.11832067020641301]
	TIME [epoch: 12.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11577873196766135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11577873196766135 | validation: 0.16266962784508565]
	TIME [epoch: 12.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12969853749256086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12969853749256086 | validation: 0.16659728237105034]
	TIME [epoch: 12.4 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20325215145662534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20325215145662534 | validation: 0.40447187390984624]
	TIME [epoch: 12.4 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27213182798713165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27213182798713165 | validation: 0.22781853199460722]
	TIME [epoch: 12.4 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.270869295451877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.270869295451877 | validation: 0.16124191682880412]
	TIME [epoch: 12.4 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12475509203015012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12475509203015012 | validation: 0.1696035772909081]
	TIME [epoch: 12.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12494858876914929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12494858876914929 | validation: 0.09834634342203027]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_765.pth
	Model improved!!!
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12934120833944365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12934120833944365 | validation: 0.22169606639507242]
	TIME [epoch: 12.4 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13736005407952304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13736005407952304 | validation: 0.18333978630076966]
	TIME [epoch: 12.4 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1940985069154657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1940985069154657 | validation: 0.300801305244565]
	TIME [epoch: 12.4 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22332905146534393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22332905146534393 | validation: 0.2288497592316303]
	TIME [epoch: 12.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22470337898116627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22470337898116627 | validation: 0.12466071954312954]
	TIME [epoch: 12.5 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11989219615698506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11989219615698506 | validation: 0.1786638664085677]
	TIME [epoch: 12.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14062595508443518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14062595508443518 | validation: 0.20710849713145524]
	TIME [epoch: 12.4 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23537866241419977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23537866241419977 | validation: 0.35969324698909244]
	TIME [epoch: 12.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23638034577458134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23638034577458134 | validation: 0.2544723551084594]
	TIME [epoch: 12.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26552547879338617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26552547879338617 | validation: 0.1447734848211911]
	TIME [epoch: 12.4 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17215940605091987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17215940605091987 | validation: 0.2206429290570657]
	TIME [epoch: 12.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15669538476164754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15669538476164754 | validation: 0.10151445767830602]
	TIME [epoch: 12.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11124728743788469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11124728743788469 | validation: 0.14993294761381218]
	TIME [epoch: 12.4 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1282092652933155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1282092652933155 | validation: 0.16637238232525414]
	TIME [epoch: 12.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17808351652837012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17808351652837012 | validation: 0.5363942322394343]
	TIME [epoch: 12.4 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3602648267452983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3602648267452983 | validation: 0.23392774365604985]
	TIME [epoch: 12.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2763113385138225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2763113385138225 | validation: 0.15043643333044732]
	TIME [epoch: 12.4 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14666249611832524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14666249611832524 | validation: 0.14730349225340594]
	TIME [epoch: 12.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13280471589432258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13280471589432258 | validation: 0.15163978848396398]
	TIME [epoch: 12.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13521047498197022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13521047498197022 | validation: 0.5143741270522931]
	TIME [epoch: 12.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336356601044132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336356601044132 | validation: 0.1337251426796459]
	TIME [epoch: 12.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380640085581878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1380640085581878 | validation: 0.23865369485003496]
	TIME [epoch: 12.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1601690572974988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601690572974988 | validation: 0.2385661796551557]
	TIME [epoch: 12.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2583899318771742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2583899318771742 | validation: 0.21170134260740966]
	TIME [epoch: 12.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1464181270336626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1464181270336626 | validation: 0.125377676085078]
	TIME [epoch: 12.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12687828107069465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12687828107069465 | validation: 0.2598232838439387]
	TIME [epoch: 12.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19108717637020378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19108717637020378 | validation: 0.1639337525279715]
	TIME [epoch: 12.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20474268693421882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20474268693421882 | validation: 0.29357851527442097]
	TIME [epoch: 12.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18241636532991926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18241636532991926 | validation: 0.11831146327817701]
	TIME [epoch: 12.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15334621016000424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15334621016000424 | validation: 0.15896368977436862]
	TIME [epoch: 12.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12021949427956408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12021949427956408 | validation: 0.1634652863161807]
	TIME [epoch: 12.5 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17250048508172833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17250048508172833 | validation: 0.24854967526081123]
	TIME [epoch: 12.3 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1874902215848783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1874902215848783 | validation: 0.12492589849815922]
	TIME [epoch: 12.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15306578002405527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15306578002405527 | validation: 0.2989114958030758]
	TIME [epoch: 12.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1818292152409915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1818292152409915 | validation: 0.14999228312940868]
	TIME [epoch: 12.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18147501035028757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18147501035028757 | validation: 0.263762622187051]
	TIME [epoch: 12.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14973025930121267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14973025930121267 | validation: 0.10451251851025085]
	TIME [epoch: 12.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1313205456460086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1313205456460086 | validation: 0.16148716346501818]
	TIME [epoch: 12.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11430713838730781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11430713838730781 | validation: 0.10758406106611292]
	TIME [epoch: 12.4 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13406695246414746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13406695246414746 | validation: 0.24500112428836493]
	TIME [epoch: 12.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16588215142388557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16588215142388557 | validation: 0.23294505526911707]
	TIME [epoch: 12.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24519153292999427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24519153292999427 | validation: 0.1609259651376036]
	TIME [epoch: 12.5 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13990659029822752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13990659029822752 | validation: 0.13570006016437874]
	TIME [epoch: 12.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11266522803580138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11266522803580138 | validation: 0.10958108580045481]
	TIME [epoch: 12.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11216293536788112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11216293536788112 | validation: 0.11639972540406246]
	TIME [epoch: 12.5 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11066184588935665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11066184588935665 | validation: 0.22366771862107748]
	TIME [epoch: 12.4 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16493827459157903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16493827459157903 | validation: 0.21554254343967316]
	TIME [epoch: 12.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2561357480919444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2561357480919444 | validation: 0.5875036017184933]
	TIME [epoch: 12.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694432639923489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694432639923489 | validation: 0.31497313405662036]
	TIME [epoch: 12.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32071832996070854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32071832996070854 | validation: 0.45377582145052564]
	TIME [epoch: 12.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.370292126131381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.370292126131381 | validation: 0.20701303153580844]
	TIME [epoch: 12.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1843405400483382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1843405400483382 | validation: 0.1950057373039663]
	TIME [epoch: 12.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1880476621467608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1880476621467608 | validation: 0.15102791686479242]
	TIME [epoch: 12.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16304549734442986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16304549734442986 | validation: 0.23741177123151955]
	TIME [epoch: 12.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16915711063542432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16915711063542432 | validation: 0.16171119646916524]
	TIME [epoch: 12.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18872041007451976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18872041007451976 | validation: 0.2043492526378893]
	TIME [epoch: 12.2 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17087915897983993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17087915897983993 | validation: 0.14095785842876893]
	TIME [epoch: 12.4 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522814432065465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522814432065465 | validation: 0.18316447666104665]
	TIME [epoch: 12.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1273139126667314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1273139126667314 | validation: 0.125927039048453]
	TIME [epoch: 12.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1501473191570054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1501473191570054 | validation: 0.33407374000004514]
	TIME [epoch: 12.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21043897200465245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21043897200465245 | validation: 0.1909156812193197]
	TIME [epoch: 12.4 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23604405584174928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23604405584174928 | validation: 0.19688773204286367]
	TIME [epoch: 12.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336562828369387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336562828369387 | validation: 0.14180292897284213]
	TIME [epoch: 12.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12839956522793666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12839956522793666 | validation: 0.1623392753199481]
	TIME [epoch: 12.2 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14553822125089544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14553822125089544 | validation: 0.15461113092620618]
	TIME [epoch: 12.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14758101139150032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14758101139150032 | validation: 0.19551568936131125]
	TIME [epoch: 12.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14885646648696424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14885646648696424 | validation: 0.1363876114913436]
	TIME [epoch: 12.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16089968700872562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16089968700872562 | validation: 0.33331449227620036]
	TIME [epoch: 12.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20004111314556536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20004111314556536 | validation: 0.15295783577495864]
	TIME [epoch: 12.4 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1862738067155461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1862738067155461 | validation: 0.18003132215545434]
	TIME [epoch: 12.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11694095717604178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11694095717604178 | validation: 0.11374988458352193]
	TIME [epoch: 12.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11201564183045658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11201564183045658 | validation: 0.16821060604233665]
	TIME [epoch: 12.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1548878116415693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1548878116415693 | validation: 0.16815575933121338]
	TIME [epoch: 12.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16842058343546445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16842058343546445 | validation: 0.22541180366421099]
	TIME [epoch: 12.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15991173325444272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15991173325444272 | validation: 0.18828481419051127]
	TIME [epoch: 12.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24520908384843024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24520908384843024 | validation: 0.31644049987661543]
	TIME [epoch: 12.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1901050177165733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1901050177165733 | validation: 0.10997756686480456]
	TIME [epoch: 12.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14168474183399457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14168474183399457 | validation: 0.12993982848561575]
	TIME [epoch: 12.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11772759179743375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11772759179743375 | validation: 0.10959632985732722]
	TIME [epoch: 12.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11956868791124917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11956868791124917 | validation: 0.16176282622493973]
	TIME [epoch: 12.2 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11920529793040893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11920529793040893 | validation: 0.12673929545968227]
	TIME [epoch: 12.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1577850825883968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1577850825883968 | validation: 0.2895978642423362]
	TIME [epoch: 12.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1892959472792392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1892959472792392 | validation: 0.16940660809915567]
	TIME [epoch: 12.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19205352168944342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19205352168944342 | validation: 0.22291139630153245]
	TIME [epoch: 12.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14597731983636741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14597731983636741 | validation: 0.13861975665535917]
	TIME [epoch: 12.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16520813573063048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16520813573063048 | validation: 0.34954771783244337]
	TIME [epoch: 12.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21642628916490173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21642628916490173 | validation: 0.117601045708374]
	TIME [epoch: 12.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15356943631418943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15356943631418943 | validation: 0.14939623811796607]
	TIME [epoch: 12.4 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10742090783292653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10742090783292653 | validation: 0.1399094107530813]
	TIME [epoch: 12.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12571562277501822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12571562277501822 | validation: 0.16388587594981724]
	TIME [epoch: 12.4 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17145698795463474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17145698795463474 | validation: 0.1418062630575664]
	TIME [epoch: 12.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461923618839469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461923618839469 | validation: 0.31030825583030186]
	TIME [epoch: 12.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19876797045314046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19876797045314046 | validation: 0.16950646500456654]
	TIME [epoch: 12.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19967362634084915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19967362634084915 | validation: 0.27978826715767563]
	TIME [epoch: 12.4 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16063824558293813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16063824558293813 | validation: 0.09574559515594581]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1157129823529085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1157129823529085 | validation: 0.17663675203040927]
	TIME [epoch: 12.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11436838135015087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11436838135015087 | validation: 0.10439163481440823]
	TIME [epoch: 12.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12229394218363833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12229394218363833 | validation: 0.2187206158654901]
	TIME [epoch: 12.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1511922539542485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1511922539542485 | validation: 0.1528604620389594]
	TIME [epoch: 12.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18105899472731524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18105899472731524 | validation: 0.24139520043155616]
	TIME [epoch: 12.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1494764392210596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1494764392210596 | validation: 0.10584039082376231]
	TIME [epoch: 12.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13747827565583223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13747827565583223 | validation: 0.18167957869248944]
	TIME [epoch: 12.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11934551278642336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11934551278642336 | validation: 0.15872274065334627]
	TIME [epoch: 12.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15028010952688234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15028010952688234 | validation: 0.1965786428221891]
	TIME [epoch: 12.2 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1999435334031309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1999435334031309 | validation: 0.18928823442354847]
	TIME [epoch: 12.3 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13396099425828709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13396099425828709 | validation: 0.1235578627197739]
	TIME [epoch: 12.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09682183163271056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09682183163271056 | validation: 0.07622976742109372]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_872.pth
	Model improved!!!
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954593067211464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0954593067211464 | validation: 0.353938100441032]
	TIME [epoch: 12.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2220415011334767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2220415011334767 | validation: 0.21190069119419191]
	TIME [epoch: 12.4 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2534442825985709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2534442825985709 | validation: 0.1832568150888055]
	TIME [epoch: 12.5 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13119901860885347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13119901860885347 | validation: 0.18539050866968154]
	TIME [epoch: 12.4 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15271128550819957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15271128550819957 | validation: 0.09530721332360663]
	TIME [epoch: 12.5 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10631920913405607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10631920913405607 | validation: 0.08985310558564512]
	TIME [epoch: 12.4 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09688152224917426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09688152224917426 | validation: 0.1881503911911745]
	TIME [epoch: 12.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12955470577933503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12955470577933503 | validation: 0.2343072997507116]
	TIME [epoch: 12.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28220993766732483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28220993766732483 | validation: 0.36404801538230536]
	TIME [epoch: 12.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20367855225954798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20367855225954798 | validation: 0.13162478413070894]
	TIME [epoch: 12.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13563425562651296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13563425562651296 | validation: 0.11434643016981381]
	TIME [epoch: 12.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11961066964012601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11961066964012601 | validation: 0.15209566157758136]
	TIME [epoch: 12.3 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16535191442933708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16535191442933708 | validation: 0.1756306085821463]
	TIME [epoch: 12.2 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12869571298226126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12869571298226126 | validation: 0.2958702387406717]
	TIME [epoch: 12.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22450885483067873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22450885483067873 | validation: 0.16468108863172937]
	TIME [epoch: 12.3 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19116002722844916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19116002722844916 | validation: 0.09662351251813589]
	TIME [epoch: 12.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08930421272780205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08930421272780205 | validation: 0.16858238108999238]
	TIME [epoch: 12.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12027021893121166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12027021893121166 | validation: 0.4831065545901318]
	TIME [epoch: 12.3 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111110787648692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5111110787648692 | validation: 0.3487297866275827]
	TIME [epoch: 12.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36390479691234723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36390479691234723 | validation: 0.22446649937288893]
	TIME [epoch: 12.4 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20182212590115484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20182212590115484 | validation: 0.14918762655469486]
	TIME [epoch: 12.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13693331161899158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13693331161899158 | validation: 0.11803566528398216]
	TIME [epoch: 12.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12267800699099908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12267800699099908 | validation: 0.16416298073882862]
	TIME [epoch: 12.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546851915413281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11546851915413281 | validation: 0.12065124425236196]
	TIME [epoch: 12.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13929416094119723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13929416094119723 | validation: 0.18097356235554904]
	TIME [epoch: 12.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16558974030421092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16558974030421092 | validation: 0.12426187813715095]
	TIME [epoch: 12.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15104158036462825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15104158036462825 | validation: 0.2821513037779212]
	TIME [epoch: 12.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685916851980587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685916851980587 | validation: 0.13156873372905262]
	TIME [epoch: 12.2 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16109946890856716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16109946890856716 | validation: 0.16530693871918353]
	TIME [epoch: 12.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11676833618073836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11676833618073836 | validation: 0.0982855889802956]
	TIME [epoch: 12.4 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10537379646836456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10537379646836456 | validation: 0.1683704342585963]
	TIME [epoch: 12.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11321843251397061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11321843251397061 | validation: 0.10248757212096958]
	TIME [epoch: 12.4 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13884572891236052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13884572891236052 | validation: 0.35780599959461085]
	TIME [epoch: 12.2 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20329733697822278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20329733697822278 | validation: 0.11378577645494298]
	TIME [epoch: 12.5 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15185022409022972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15185022409022972 | validation: 0.10856352381396467]
	TIME [epoch: 12.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10790264979470303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10790264979470303 | validation: 0.11916131894021653]
	TIME [epoch: 12.5 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09968027733453078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09968027733453078 | validation: 0.08392183272573593]
	TIME [epoch: 12.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08593156806608793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08593156806608793 | validation: 0.1119607203977182]
	TIME [epoch: 12.5 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898361645142397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0898361645142397 | validation: 0.0783210961199097]
	TIME [epoch: 12.4 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09449352717150529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09449352717150529 | validation: 0.1664796646272159]
	TIME [epoch: 12.5 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10788239252526789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10788239252526789 | validation: 0.13825335873959207]
	TIME [epoch: 12.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18910487437987591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18910487437987591 | validation: 0.4726665250035373]
	TIME [epoch: 12.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3259459087797127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3259459087797127 | validation: 0.1845113398778949]
	TIME [epoch: 12.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15731574141694554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15731574141694554 | validation: 0.1631429355114293]
	TIME [epoch: 12.5 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16850351314564557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16850351314564557 | validation: 0.21606157657380154]
	TIME [epoch: 12.3 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20034176286096478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20034176286096478 | validation: 0.1533559935112186]
	TIME [epoch: 12.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12144075365786142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12144075365786142 | validation: 0.1407881549703028]
	TIME [epoch: 12.3 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16801251211049553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16801251211049553 | validation: 0.17600546308869408]
	TIME [epoch: 12.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1922722213186886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1922722213186886 | validation: 0.0910746152090467]
	TIME [epoch: 12.3 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10796494620281617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10796494620281617 | validation: 0.13623307513666427]
	TIME [epoch: 12.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09777684768346348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09777684768346348 | validation: 0.07170071689788918]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_923.pth
	Model improved!!!
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10125339940201392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10125339940201392 | validation: 0.13140515662333174]
	TIME [epoch: 12.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08612867934711815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08612867934711815 | validation: 0.09321088358260754]
	TIME [epoch: 12.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11323401084219643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11323401084219643 | validation: 0.41294292588982195]
	TIME [epoch: 12.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.285988167322531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.285988167322531 | validation: 0.2125320024204035]
	TIME [epoch: 12.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2525254267478862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2525254267478862 | validation: 0.14848473992669278]
	TIME [epoch: 12.3 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10671400305486156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10671400305486156 | validation: 0.13171880756923712]
	TIME [epoch: 12.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09408111619130681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09408111619130681 | validation: 0.08193850426478969]
	TIME [epoch: 12.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08515653755670695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08515653755670695 | validation: 0.13242823327523054]
	TIME [epoch: 12.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724754437675575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08724754437675575 | validation: 0.11199094770941385]
	TIME [epoch: 12.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15310968883630996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15310968883630996 | validation: 0.16586621523361644]
	TIME [epoch: 12.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17617384069136033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17617384069136033 | validation: 0.5044532072155514]
	TIME [epoch: 12.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3904997492087596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3904997492087596 | validation: 0.2111804992911572]
	TIME [epoch: 12.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14685025044162692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14685025044162692 | validation: 0.13920576868304396]
	TIME [epoch: 12.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15658546644398386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15658546644398386 | validation: 0.19391367979066246]
	TIME [epoch: 12.4 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1706308516318871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706308516318871 | validation: 0.17639323630849157]
	TIME [epoch: 12.3 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18449662518755236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18449662518755236 | validation: 0.2548525030218833]
	TIME [epoch: 12.4 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16391114810963595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16391114810963595 | validation: 0.1274044212908538]
	TIME [epoch: 12.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15908292205610822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15908292205610822 | validation: 0.09786636442228663]
	TIME [epoch: 12.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11690135802081826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11690135802081826 | validation: 0.19377165690945267]
	TIME [epoch: 12.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15172894686251243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15172894686251243 | validation: 0.14984999662922813]
	TIME [epoch: 12.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13486906449807798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13486906449807798 | validation: 0.14358543380609434]
	TIME [epoch: 12.3 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12767970291231504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12767970291231504 | validation: 0.18954563642857425]
	TIME [epoch: 12.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21042466636096357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21042466636096357 | validation: 0.21391493974852624]
	TIME [epoch: 12.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551199093055916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551199093055916 | validation: 0.11500233234573597]
	TIME [epoch: 12.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13133530703224058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13133530703224058 | validation: 0.1297391299274293]
	TIME [epoch: 12.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11251490574551881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11251490574551881 | validation: 0.08713290834492633]
	TIME [epoch: 12.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959550782586749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0959550782586749 | validation: 0.10136033692106224]
	TIME [epoch: 12.3 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740621901008162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09740621901008162 | validation: 0.09317698119141488]
	TIME [epoch: 12.4 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13001865554178527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13001865554178527 | validation: 0.3100176920001497]
	TIME [epoch: 12.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19331456469062847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19331456469062847 | validation: 0.14495828026158367]
	TIME [epoch: 12.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1869353511770261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1869353511770261 | validation: 0.1634820955286324]
	TIME [epoch: 12.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11779688371921865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11779688371921865 | validation: 0.08059165370776297]
	TIME [epoch: 12.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08879656939540485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08879656939540485 | validation: 0.09275663764110526]
	TIME [epoch: 12.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08430908563157058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08430908563157058 | validation: 0.20845030697814498]
	TIME [epoch: 12.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16800494218396061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16800494218396061 | validation: 0.371328588263549]
	TIME [epoch: 12.4 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43710662400913214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43710662400913214 | validation: 0.24302799389082305]
	TIME [epoch: 12.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2586913047280297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2586913047280297 | validation: 0.42862491483892884]
	TIME [epoch: 12.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3546348484495747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3546348484495747 | validation: 0.28742646849408404]
	TIME [epoch: 12.4 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611938348695487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2611938348695487 | validation: 0.1683279774791151]
	TIME [epoch: 12.4 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17274622398661926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17274622398661926 | validation: 0.12659144580125675]
	TIME [epoch: 12.4 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13542670209770416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13542670209770416 | validation: 0.20693280199372063]
	TIME [epoch: 12.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17876579571357973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17876579571357973 | validation: 0.141136311067468]
	TIME [epoch: 12.4 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1542402796042521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1542402796042521 | validation: 0.14741111235720022]
	TIME [epoch: 12.4 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13156053741536256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13156053741536256 | validation: 0.08321566215183375]
	TIME [epoch: 12.4 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10603277765258451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10603277765258451 | validation: 0.16181182126869595]
	TIME [epoch: 12.4 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11628636385979572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11628636385979572 | validation: 0.11857384772214459]
	TIME [epoch: 12.4 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446871183177142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1446871183177142 | validation: 0.22820932060499324]
	TIME [epoch: 12.4 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16062278334127755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16062278334127755 | validation: 0.10101119019048316]
	TIME [epoch: 12.4 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14070015665722863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14070015665722863 | validation: 0.11712837843097496]
	TIME [epoch: 12.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10364083829717541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10364083829717541 | validation: 0.0887285526657205]
	TIME [epoch: 12.4 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09357123363473478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09357123363473478 | validation: 0.1084621209046221]
	TIME [epoch: 12.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09097535646773369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09097535646773369 | validation: 0.42356338282973527]
	TIME [epoch: 12.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48967275341000105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48967275341000105 | validation: 0.1251127388453573]
	TIME [epoch: 12.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11134276810589047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11134276810589047 | validation: 0.194240921765318]
	TIME [epoch: 12.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13292044004405387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13292044004405387 | validation: 0.11779412159523145]
	TIME [epoch: 12.4 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09786205060587287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09786205060587287 | validation: 0.08632406974613138]
	TIME [epoch: 12.4 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975395325451466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08975395325451466 | validation: 0.12645628489880184]
	TIME [epoch: 12.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927623481841145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0927623481841145 | validation: 0.09576392072697536]
	TIME [epoch: 12.4 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09311910808112787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09311910808112787 | validation: 0.15983070458443915]
	TIME [epoch: 12.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12296754781712206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12296754781712206 | validation: 0.14921398239822353]
	TIME [epoch: 12.4 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18766723970175392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18766723970175392 | validation: 0.21094158587025158]
	TIME [epoch: 12.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1396502650424238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1396502650424238 | validation: 0.0911846005040916]
	TIME [epoch: 12.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10937130088223432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10937130088223432 | validation: 0.1290268455281649]
	TIME [epoch: 12.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09855703647492042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09855703647492042 | validation: 0.09107590684753604]
	TIME [epoch: 12.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10831153487757292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10831153487757292 | validation: 0.17051674925440016]
	TIME [epoch: 12.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11810779196233527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11810779196233527 | validation: 0.16215993192094114]
	TIME [epoch: 12.4 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19620189134942898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19620189134942898 | validation: 0.14283821388609344]
	TIME [epoch: 12.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11018017354949407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11018017354949407 | validation: 0.11389085064969956]
	TIME [epoch: 12.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08814061451985214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08814061451985214 | validation: 0.0657032527708497]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08505360773720597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08505360773720597 | validation: 0.09400711701616532]
	TIME [epoch: 12.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07679546820654104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07679546820654104 | validation: 0.07798736766675428]
	TIME [epoch: 12.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103578699421069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07103578699421069 | validation: 0.07791481410734258]
	TIME [epoch: 12.4 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11629879245706543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11629879245706543 | validation: 0.3027627219740477]
	TIME [epoch: 12.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18089278936733486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18089278936733486 | validation: 0.18540792456682764]
	TIME [epoch: 12.4 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19668033301686488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19668033301686488 | validation: 0.18393958126528956]
	TIME [epoch: 12.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18030708533325773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18030708533325773 | validation: 0.10859375142315873]
	TIME [epoch: 12.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11694723996170676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11694723996170676 | validation: 0.10259613585668753]
	TIME [epoch: 12.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08507593562206861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08507593562206861 | validation: 0.06444168383293021]
	TIME [epoch: 193 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_1001.pth
	Model improved!!!
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738081989424989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0738081989424989 | validation: 0.062194974920619484]
	TIME [epoch: 25.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_1002.pth
	Model improved!!!
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07819314983981139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07819314983981139 | validation: 0.23211122884000807]
	TIME [epoch: 25.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14192822871506514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14192822871506514 | validation: 0.09606697659187678]
	TIME [epoch: 25.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11037209590648797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11037209590648797 | validation: 0.2168422383359941]
	TIME [epoch: 25.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16470058791710668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16470058791710668 | validation: 0.1387023171103384]
	TIME [epoch: 25.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1551691699993104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1551691699993104 | validation: 0.22205262524008137]
	TIME [epoch: 25.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14643840937044697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14643840937044697 | validation: 0.1031222498581613]
	TIME [epoch: 25.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1499951111406726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1499951111406726 | validation: 0.13616060949766595]
	TIME [epoch: 25.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10267654717509825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10267654717509825 | validation: 0.1389309174129102]
	TIME [epoch: 25.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11269069944225336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11269069944225336 | validation: 0.09472159320926521]
	TIME [epoch: 25.6 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241717460415092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10241717460415092 | validation: 0.08002128071945008]
	TIME [epoch: 25.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09325149197895766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09325149197895766 | validation: 0.23315946918646724]
	TIME [epoch: 25.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13791831244352457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13791831244352457 | validation: 0.12937903671434847]
	TIME [epoch: 25.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1891442192987573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1891442192987573 | validation: 0.1907947926676773]
	TIME [epoch: 25.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12716051332581824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12716051332581824 | validation: 0.10425481924759457]
	TIME [epoch: 25.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09693721707865742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09693721707865742 | validation: 0.08932336981627245]
	TIME [epoch: 25.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08817061028046748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08817061028046748 | validation: 0.07110831969842178]
	TIME [epoch: 25.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09172770760088333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09172770760088333 | validation: 0.18244095319941442]
	TIME [epoch: 25.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11848877477571453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11848877477571453 | validation: 0.1233406512163775]
	TIME [epoch: 25.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16119165683224007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16119165683224007 | validation: 1.1581416404517977]
	TIME [epoch: 25.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9594512799830899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9594512799830899 | validation: 0.6634095906857456]
	TIME [epoch: 25.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581852460857762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5581852460857762 | validation: 0.3315484795085421]
	TIME [epoch: 25.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3952768182067938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3952768182067938 | validation: 0.20941333316366278]
	TIME [epoch: 25.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26913011213176125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26913011213176125 | validation: 0.20121873609032814]
	TIME [epoch: 25.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23695156492304706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23695156492304706 | validation: 0.13090971501473941]
	TIME [epoch: 25.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467604365252731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1467604365252731 | validation: 0.11991765971564153]
	TIME [epoch: 25.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10985450512374666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10985450512374666 | validation: 0.10596462491421509]
	TIME [epoch: 25.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1011723341100656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1011723341100656 | validation: 0.18867196220361426]
	TIME [epoch: 25.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292464201435129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1292464201435129 | validation: 0.1050652476396138]
	TIME [epoch: 25.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1343132608491436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1343132608491436 | validation: 0.32355338289249147]
	TIME [epoch: 25.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19238445606327226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19238445606327226 | validation: 0.10984160095692655]
	TIME [epoch: 25.6 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12679071335769368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12679071335769368 | validation: 0.12523713009235338]
	TIME [epoch: 25.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10599506902691012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10599506902691012 | validation: 0.0944019295663216]
	TIME [epoch: 25.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08917402648434784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08917402648434784 | validation: 0.10310464707091896]
	TIME [epoch: 25.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08138604923032818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08138604923032818 | validation: 0.07386782880300051]
	TIME [epoch: 25.6 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09777580344304147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09777580344304147 | validation: 0.16867416459783044]
	TIME [epoch: 25.6 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11432879111079029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11432879111079029 | validation: 0.08989789060525191]
	TIME [epoch: 25.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11908803466111405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11908803466111405 | validation: 0.17030275644948645]
	TIME [epoch: 25.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1714325381335748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1714325381335748 | validation: 0.07786248434558307]
	TIME [epoch: 25.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08818068535722644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08818068535722644 | validation: 0.11757821862183238]
	TIME [epoch: 25.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08067250019679634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08067250019679634 | validation: 0.07432396329834183]
	TIME [epoch: 25.6 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066169202557234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09066169202557234 | validation: 0.16158756746779496]
	TIME [epoch: 25.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11274483696838988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11274483696838988 | validation: 0.09433328119567712]
	TIME [epoch: 25.6 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12961312183702764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12961312183702764 | validation: 0.2174435309173311]
	TIME [epoch: 25.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13123135885714063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13123135885714063 | validation: 0.09316483130819735]
	TIME [epoch: 25.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12174276311804377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12174276311804377 | validation: 0.13427994373967658]
	TIME [epoch: 25.6 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09206344486879228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09206344486879228 | validation: 0.07839271321820258]
	TIME [epoch: 25.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09197394795826569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09197394795826569 | validation: 0.11375806912241684]
	TIME [epoch: 25.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09669797240417487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09669797240417487 | validation: 0.08697398127349557]
	TIME [epoch: 25.6 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09674655524056253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09674655524056253 | validation: 0.13284289084018594]
	TIME [epoch: 25.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09247478715254369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09247478715254369 | validation: 0.08183619436019096]
	TIME [epoch: 25.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09548136163243498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09548136163243498 | validation: 0.20394898810135867]
	TIME [epoch: 25.6 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12377973947414607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12377973947414607 | validation: 0.08779643329425796]
	TIME [epoch: 25.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13382997069050304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13382997069050304 | validation: 0.22009127134223833]
	TIME [epoch: 25.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1283934891602687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1283934891602687 | validation: 0.08580512533985392]
	TIME [epoch: 25.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10344981539820283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10344981539820283 | validation: 0.11903073880573155]
	TIME [epoch: 25.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1001099607638886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1001099607638886 | validation: 0.07984378960210572]
	TIME [epoch: 25.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08917108382808905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08917108382808905 | validation: 0.10956972335618875]
	TIME [epoch: 25.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08794216769779123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08794216769779123 | validation: 0.07578984848287645]
	TIME [epoch: 25.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10001779027290734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10001779027290734 | validation: 0.17986908949993866]
	TIME [epoch: 25.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12397769193090913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12397769193090913 | validation: 0.6730073365501247]
	TIME [epoch: 25.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5564942716689951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5564942716689951 | validation: 0.3960072103803575]
	TIME [epoch: 25.6 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3649917568642211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3649917568642211 | validation: 0.22376594468490457]
	TIME [epoch: 25.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2136043077268105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2136043077268105 | validation: 0.13207149348979152]
	TIME [epoch: 25.6 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16502397320032877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16502397320032877 | validation: 0.14099187001419416]
	TIME [epoch: 25.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14732340648797962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14732340648797962 | validation: 0.19123263461231696]
	TIME [epoch: 25.6 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1306500029110677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1306500029110677 | validation: 0.15094382849654933]
	TIME [epoch: 25.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1724814447457093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1724814447457093 | validation: 0.24792000760900912]
	TIME [epoch: 25.6 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21270807524585172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21270807524585172 | validation: 0.09356892603645145]
	TIME [epoch: 25.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12375305822021819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12375305822021819 | validation: 0.14510073329257953]
	TIME [epoch: 25.6 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1069627394094381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1069627394094381 | validation: 0.07383310738901197]
	TIME [epoch: 27.5 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08504602631182005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08504602631182005 | validation: 0.1143049717415587]
	TIME [epoch: 25.6 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0969067796762133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0969067796762133 | validation: 0.08942610396721763]
	TIME [epoch: 25.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11557743924546662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11557743924546662 | validation: 0.15925734455833057]
	TIME [epoch: 25.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11776570230197202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11776570230197202 | validation: 0.11563389273114884]
	TIME [epoch: 25.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13442755066780815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13442755066780815 | validation: 0.16198429896947553]
	TIME [epoch: 25.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11896061460885708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11896061460885708 | validation: 0.08306334600923088]
	TIME [epoch: 25.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09407748788706523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09407748788706523 | validation: 0.13990790752539495]
	TIME [epoch: 25.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09952158592182259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09952158592182259 | validation: 0.40117597156228674]
	TIME [epoch: 25.6 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3578165223890095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3578165223890095 | validation: 0.25909506553190986]
	TIME [epoch: 25.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23015983223390465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23015983223390465 | validation: 0.08930260758304293]
	TIME [epoch: 25.4 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14997255863575887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14997255863575887 | validation: 0.474737750456052]
	TIME [epoch: 25.6 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34495919830440935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34495919830440935 | validation: 0.21149678737526037]
	TIME [epoch: 25.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16850772731603056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16850772731603056 | validation: 0.07680557884914563]
	TIME [epoch: 25.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11088770740660892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11088770740660892 | validation: 0.09821101828608819]
	TIME [epoch: 25.4 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08403837713032812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08403837713032812 | validation: 0.06500174451268502]
	TIME [epoch: 25.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08024463954157858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08024463954157858 | validation: 0.08874043493433413]
	TIME [epoch: 25.4 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462996143801696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08462996143801696 | validation: 0.08408279356819413]
	TIME [epoch: 25.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11169653366377812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11169653366377812 | validation: 0.24978583968057683]
	TIME [epoch: 25.4 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15706203035712224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15706203035712224 | validation: 0.10539904728031435]
	TIME [epoch: 25.6 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13066328550222858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13066328550222858 | validation: 0.12753668389839626]
	TIME [epoch: 25.6 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09405232342447127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09405232342447127 | validation: 0.07475611569687661]
	TIME [epoch: 25.6 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08297791084048381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08297791084048381 | validation: 0.10355140284612513]
	TIME [epoch: 25.6 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724320941117444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08724320941117444 | validation: 0.5009032537561885]
	TIME [epoch: 25.6 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5086195495896633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5086195495896633 | validation: 0.4123642049598939]
	TIME [epoch: 25.6 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4073256511485977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4073256511485977 | validation: 0.15900079919319332]
	TIME [epoch: 25.6 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18529073950563665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18529073950563665 | validation: 0.15689052745033916]
	TIME [epoch: 25.6 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14100240573721973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14100240573721973 | validation: 0.12692321632915307]
	TIME [epoch: 25.6 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13102373663145148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13102373663145148 | validation: 0.14401435024291886]
	TIME [epoch: 25.6 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1292943826724009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1292943826724009 | validation: 0.09005137942924907]
	TIME [epoch: 25.4 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10361461820217795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10361461820217795 | validation: 0.10293716165538186]
	TIME [epoch: 25.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09053516315178513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09053516315178513 | validation: 0.10807070588043328]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_203629/states/model_phi1_4c_v_mmd2_1103.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 11867.952 seconds.
