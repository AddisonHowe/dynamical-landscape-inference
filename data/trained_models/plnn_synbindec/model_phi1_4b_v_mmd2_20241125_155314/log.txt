Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/basic/data_phi1_4b/training', validation_data='data/training_data/basic/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3638558091

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.372438722728972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.372438722728972 | validation: 5.353659904678597]
	TIME [epoch: 175 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.084815611018468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.084815611018468 | validation: 4.569604661519637]
	TIME [epoch: 1.44 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.575284020836718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.575284020836718 | validation: 4.226155938568496]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.153690390775413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.153690390775413 | validation: 4.778008870773507]
	TIME [epoch: 1.42 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.790850505984017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.790850505984017 | validation: 4.116593944510581]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025977075972191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025977075972191 | validation: 4.107893999112703]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1759004361070895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1759004361070895 | validation: 3.98633422800541]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.40390008320576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.40390008320576 | validation: 4.601961909207672]
	TIME [epoch: 1.41 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.4940876422208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.4940876422208 | validation: 3.8125660785292794]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8722309009416676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8722309009416676 | validation: 3.7619790282494843]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9496475255185555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9496475255185555 | validation: 4.068326757716172]
	TIME [epoch: 1.41 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9970157638622115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9970157638622115 | validation: 3.7018847788161353]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.816837102131754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.816837102131754 | validation: 3.7113549850524294]
	TIME [epoch: 1.46 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.764250051704723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.764250051704723 | validation: 3.723259050310892]
	TIME [epoch: 1.41 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7470291882066147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7470291882066147 | validation: 3.6208007254030274]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.733123803163999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.733123803163999 | validation: 3.694497620163555]
	TIME [epoch: 1.41 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7267851743564124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7267851743564124 | validation: 3.551682103728462]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.730089825589424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.730089825589424 | validation: 3.7144600431361177]
	TIME [epoch: 1.41 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7646301733818803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7646301733818803 | validation: 3.5023839820944405]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7389352463194587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7389352463194587 | validation: 3.614619809099205]
	TIME [epoch: 1.41 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6865261653219843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6865261653219843 | validation: 3.4469023914296453]
	TIME [epoch: 1.53 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6254261607440164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6254261607440164 | validation: 3.4865016542174736]
	TIME [epoch: 1.42 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.593016966777174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.593016966777174 | validation: 3.397142755305661]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5709156385664005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5709156385664005 | validation: 3.4400541424871274]
	TIME [epoch: 1.42 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5648713243865586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5648713243865586 | validation: 3.351306045029139]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.573233675917042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.573233675917042 | validation: 3.482947038241942]
	TIME [epoch: 1.42 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6198361677372692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6198361677372692 | validation: 3.334132276875788]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5943861508459047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5943861508459047 | validation: 3.377682017648276]
	TIME [epoch: 1.42 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5167795819192946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5167795819192946 | validation: 3.258296465957406]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.459478695605313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.459478695605313 | validation: 3.257494950534539]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4236722703149383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4236722703149383 | validation: 3.233801948245409]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.357527326417159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.357527326417159 | validation: 3.249667338714959]
	TIME [epoch: 1.41 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0739889413159918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0739889413159918 | validation: 3.003662031935588]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.537614206021503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.537614206021503 | validation: 2.49196661729561]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1662281482797163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1662281482797163 | validation: 1.8446184301043183]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5641842176643028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5641842176643028 | validation: 2.078346631472906]
	TIME [epoch: 1.42 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5489277756609676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5489277756609676 | validation: 3.51480947886096]
	TIME [epoch: 1.42 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3427079783244853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3427079783244853 | validation: 2.190832602566182]
	TIME [epoch: 1.41 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.327963949690937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.327963949690937 | validation: 2.0735143562065987]
	TIME [epoch: 1.41 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.514475536953952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.514475536953952 | validation: 1.4288313744433137]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0185550619953565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0185550619953565 | validation: 1.2040769791712247]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9937294604378232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9937294604378232 | validation: 1.1453302631965465]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853699388514507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8853699388514507 | validation: 1.1181883099213443]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8215431717428794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8215431717428794 | validation: 1.0258647268842318]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051686681069317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8051686681069317 | validation: 0.9124453689147949]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973443719101525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973443719101525 | validation: 0.9498887253722277]
	TIME [epoch: 1.41 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7813098906850163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7813098906850163 | validation: 0.9907332588780072]
	TIME [epoch: 1.41 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7802626428542805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7802626428542805 | validation: 0.9469521433776724]
	TIME [epoch: 1.41 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795506817210331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7795506817210331 | validation: 0.9364916082982235]
	TIME [epoch: 1.41 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757330212651351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757330212651351 | validation: 1.0014717595979021]
	TIME [epoch: 1.41 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759913114110691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7759913114110691 | validation: 0.9268602757550646]
	TIME [epoch: 1.41 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7707617686526581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7707617686526581 | validation: 0.9199575497507704]
	TIME [epoch: 1.41 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688604323781445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688604323781445 | validation: 0.9680488425979714]
	TIME [epoch: 1.41 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670833397153771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7670833397153771 | validation: 0.9049933374145408]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736580720176961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736580720176961 | validation: 0.9784358207469412]
	TIME [epoch: 1.41 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7780482950629926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7780482950629926 | validation: 0.8932351608850756]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937531174969107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937531174969107 | validation: 1.086874309432621]
	TIME [epoch: 1.42 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8698501972721482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8698501972721482 | validation: 1.0215437014905233]
	TIME [epoch: 1.41 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897432546074121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.897432546074121 | validation: 1.060770481500695]
	TIME [epoch: 1.41 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880042529110775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8880042529110775 | validation: 0.9697348301622295]
	TIME [epoch: 1.41 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700172089245361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700172089245361 | validation: 0.8773439682689714]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618378158059318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618378158059318 | validation: 0.988449160207796]
	TIME [epoch: 1.42 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7793495749229717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7793495749229717 | validation: 0.8675240915817402]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7828159425359879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7828159425359879 | validation: 1.1421566447148859]
	TIME [epoch: 1.42 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8272225069678442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8272225069678442 | validation: 0.8622817781172518]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7934650620176563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7934650620176563 | validation: 1.148244135813902]
	TIME [epoch: 1.41 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8235900162007466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8235900162007466 | validation: 0.9287686565355848]
	TIME [epoch: 1.41 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283674225778482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8283674225778482 | validation: 1.0435032258021069]
	TIME [epoch: 1.41 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025194821973834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025194821973834 | validation: 0.9139124032307872]
	TIME [epoch: 1.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794242749784713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794242749784713 | validation: 0.891041653124397]
	TIME [epoch: 1.4 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629183455701325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629183455701325 | validation: 0.9947164041947851]
	TIME [epoch: 1.4 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7785058156892538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7785058156892538 | validation: 0.8549506598528507]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229753434542749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229753434542749 | validation: 1.2199916085485816]
	TIME [epoch: 1.41 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9093786379167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9093786379167 | validation: 0.8752766320554386]
	TIME [epoch: 1.41 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527919400232409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7527919400232409 | validation: 0.8635383963318255]
	TIME [epoch: 1.41 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649256300448113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649256300448113 | validation: 0.9656181176760438]
	TIME [epoch: 1.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7848758190466584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7848758190466584 | validation: 0.87706501248121]
	TIME [epoch: 1.4 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827445798283066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7827445798283066 | validation: 0.9551627602485522]
	TIME [epoch: 1.4 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8338769640179663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8338769640179663 | validation: 1.1086713604994076]
	TIME [epoch: 1.4 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474285024320097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8474285024320097 | validation: 0.9785221424872553]
	TIME [epoch: 1.41 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9135624143901351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9135624143901351 | validation: 1.1660425709126139]
	TIME [epoch: 1.41 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8585301481459655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8585301481459655 | validation: 0.9349940213409212]
	TIME [epoch: 1.41 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720122415417524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720122415417524 | validation: 0.8853609064697463]
	TIME [epoch: 1.41 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097933953963161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8097933953963161 | validation: 1.0231286724994821]
	TIME [epoch: 1.41 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821594043255143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7821594043255143 | validation: 0.8450548296141212]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584724226434011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584724226434011 | validation: 0.922308928674584]
	TIME [epoch: 1.41 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619742740772065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619742740772065 | validation: 0.8569674108572376]
	TIME [epoch: 1.41 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626070314986805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7626070314986805 | validation: 0.9795613474688647]
	TIME [epoch: 1.4 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7865173232532103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7865173232532103 | validation: 0.8473488913053395]
	TIME [epoch: 1.4 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769004509964529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.769004509964529 | validation: 0.9671839242925657]
	TIME [epoch: 1.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.769673015996034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.769673015996034 | validation: 0.84079172641977]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562770463073712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562770463073712 | validation: 0.9108858543468623]
	TIME [epoch: 1.41 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449043823498517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449043823498517 | validation: 0.8357996195177805]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426710172714163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426710172714163 | validation: 0.9544301137282187]
	TIME [epoch: 1.41 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7731310649498532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7731310649498532 | validation: 0.9350910195258133]
	TIME [epoch: 1.41 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8217667942043696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8217667942043696 | validation: 1.0977378458614002]
	TIME [epoch: 1.4 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603761368945695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9603761368945695 | validation: 0.9504694035640245]
	TIME [epoch: 1.4 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653196066763872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653196066763872 | validation: 0.8187887109736084]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551480532036411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7551480532036411 | validation: 1.178593095147283]
	TIME [epoch: 1.41 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8648524030362993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8648524030362993 | validation: 0.8391360158686347]
	TIME [epoch: 1.41 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264662201081191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8264662201081191 | validation: 1.0956993009647817]
	TIME [epoch: 1.41 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114888757328623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114888757328623 | validation: 0.8473533524023211]
	TIME [epoch: 1.4 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514696126954065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514696126954065 | validation: 0.8364765342162173]
	TIME [epoch: 1.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7407380920286403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7407380920286403 | validation: 0.9692118756221797]
	TIME [epoch: 1.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590812169223874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7590812169223874 | validation: 0.8026185418718099]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737102448289454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737102448289454 | validation: 0.8932700689804683]
	TIME [epoch: 1.42 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7202673793602135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7202673793602135 | validation: 0.8103277144589184]
	TIME [epoch: 1.41 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285805068073282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7285805068073282 | validation: 0.9788389371155022]
	TIME [epoch: 1.41 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600257088250335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600257088250335 | validation: 0.9468933726805556]
	TIME [epoch: 1.41 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038112027003686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038112027003686 | validation: 1.0128083200807019]
	TIME [epoch: 1.41 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7709284488052883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7709284488052883 | validation: 0.8776077700001079]
	TIME [epoch: 1.41 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226154762796506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226154762796506 | validation: 0.8066660335394163]
	TIME [epoch: 1.41 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162886431022298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162886431022298 | validation: 1.020804735715015]
	TIME [epoch: 1.41 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7691031797897098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7691031797897098 | validation: 0.8522483100791768]
	TIME [epoch: 1.41 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.903798151909208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.903798151909208 | validation: 1.2717588217322242]
	TIME [epoch: 1.41 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9568810227372917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9568810227372917 | validation: 0.9086866752081975]
	TIME [epoch: 1.42 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7214317416400005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7214317416400005 | validation: 0.8275154627780824]
	TIME [epoch: 1.41 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8417833619018441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8417833619018441 | validation: 0.8985544125115035]
	TIME [epoch: 1.41 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.770707710715395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.770707710715395 | validation: 1.101753659749981]
	TIME [epoch: 1.41 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248172668198452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8248172668198452 | validation: 0.9185694669280887]
	TIME [epoch: 1.41 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8895016973185141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8895016973185141 | validation: 0.8324193470975572]
	TIME [epoch: 1.41 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6985048816502493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6985048816502493 | validation: 0.9204834895202719]
	TIME [epoch: 1.41 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173605432517681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173605432517681 | validation: 0.7962952047971805]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303112287439426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7303112287439426 | validation: 0.8285620218059702]
	TIME [epoch: 1.41 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710236586483365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.710236586483365 | validation: 0.8600862925461622]
	TIME [epoch: 1.41 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6969862008577395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6969862008577395 | validation: 0.7648883180068663]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913778712729498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913778712729498 | validation: 0.8251574028534144]
	TIME [epoch: 1.41 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085290911445865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7085290911445865 | validation: 0.9731481059690232]
	TIME [epoch: 1.41 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599278629313902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599278629313902 | validation: 1.0262546065895444]
	TIME [epoch: 1.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9591230438026506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9591230438026506 | validation: 0.83101880995943]
	TIME [epoch: 1.41 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700003282454187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700003282454187 | validation: 0.8992411152121836]
	TIME [epoch: 1.41 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6794268688343033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6794268688343033 | validation: 0.8063353804158118]
	TIME [epoch: 1.41 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052794231841486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052794231841486 | validation: 0.8828551423017119]
	TIME [epoch: 1.41 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6691545419235183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6691545419235183 | validation: 0.8496774397839538]
	TIME [epoch: 1.41 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6501716516712921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6501716516712921 | validation: 0.7549869197143027]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537213911920559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537213911920559 | validation: 1.0261220250099623]
	TIME [epoch: 1.42 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7663426776191585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7663426776191585 | validation: 0.9163016791434722]
	TIME [epoch: 1.42 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0288982855653241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0288982855653241 | validation: 0.9598597412799699]
	TIME [epoch: 1.42 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6992409554233449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6992409554233449 | validation: 0.81303506125222]
	TIME [epoch: 1.42 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6403512830139129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6403512830139129 | validation: 0.7550741431668471]
	TIME [epoch: 1.42 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6995294754285053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6995294754285053 | validation: 0.8509108096987325]
	TIME [epoch: 1.42 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728405361274458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.728405361274458 | validation: 1.06277703447179]
	TIME [epoch: 1.42 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799574831240065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799574831240065 | validation: 0.809940779872894]
	TIME [epoch: 1.42 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6698987782329878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6698987782329878 | validation: 0.8085901617401682]
	TIME [epoch: 1.42 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6106373394537898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6106373394537898 | validation: 0.8290424604225948]
	TIME [epoch: 1.42 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6157029937637154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6157029937637154 | validation: 0.7611150105025727]
	TIME [epoch: 1.42 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6570289301433839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6570289301433839 | validation: 0.9821954541588629]
	TIME [epoch: 1.42 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447747309750408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7447747309750408 | validation: 0.8417546553064617]
	TIME [epoch: 1.41 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590188421239771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6590188421239771 | validation: 0.753828224589754]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5791881976989071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5791881976989071 | validation: 0.7549761506802576]
	TIME [epoch: 1.41 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5558325889041853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5558325889041853 | validation: 0.7234815383761858]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6157518626075723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6157518626075723 | validation: 1.0664529876385593]
	TIME [epoch: 1.41 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8054342662139538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8054342662139538 | validation: 0.9207515944558153]
	TIME [epoch: 1.42 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923860343545282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923860343545282 | validation: 0.7984970084097667]
	TIME [epoch: 1.42 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862120712694449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5862120712694449 | validation: 0.8614122914591755]
	TIME [epoch: 1.41 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908304614612717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5908304614612717 | validation: 0.7631201934664971]
	TIME [epoch: 1.42 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6276239733643302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6276239733643302 | validation: 0.8616335928517813]
	TIME [epoch: 1.42 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6533501991414028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6533501991414028 | validation: 0.8035854820173882]
	TIME [epoch: 1.42 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239530592885424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6239530592885424 | validation: 0.6752345109903297]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5864734199957622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5864734199957622 | validation: 0.7555778514230528]
	TIME [epoch: 1.42 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5430122408527139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5430122408527139 | validation: 0.5988941003895502]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5471308509130479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5471308509130479 | validation: 0.8207009454893737]
	TIME [epoch: 1.42 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5514200796834647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5514200796834647 | validation: 0.7381746174308387]
	TIME [epoch: 1.41 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262988395455037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262988395455037 | validation: 1.2074735814325614]
	TIME [epoch: 1.41 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317473955846617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317473955846617 | validation: 0.8398747585854567]
	TIME [epoch: 1.41 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553378940710181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.553378940710181 | validation: 0.7234432103641576]
	TIME [epoch: 1.41 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5078840890501679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5078840890501679 | validation: 0.7170158235195583]
	TIME [epoch: 1.41 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5027129185553301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5027129185553301 | validation: 0.559986471722642]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140598736963078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5140598736963078 | validation: 0.7570055421572234]
	TIME [epoch: 1.41 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5250628203493952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5250628203493952 | validation: 0.5705695771408131]
	TIME [epoch: 1.41 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5516094892972159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5516094892972159 | validation: 0.7148693550835321]
	TIME [epoch: 1.41 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48737859854039883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48737859854039883 | validation: 0.6570154800016956]
	TIME [epoch: 1.41 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542003977167405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5542003977167405 | validation: 1.0695141673681874]
	TIME [epoch: 1.42 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715816852084474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715816852084474 | validation: 1.0267965716769147]
	TIME [epoch: 1.42 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6111846109412608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6111846109412608 | validation: 0.8286902852881474]
	TIME [epoch: 1.42 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5302449548736226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5302449548736226 | validation: 0.8246388221737475]
	TIME [epoch: 1.42 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5061183738621832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5061183738621832 | validation: 0.5790720050252863]
	TIME [epoch: 1.42 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4633415053102191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4633415053102191 | validation: 0.6484093054925266]
	TIME [epoch: 1.42 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49227088033109506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49227088033109506 | validation: 0.5313741506984931]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673572794123867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5673572794123867 | validation: 0.5516420324039544]
	TIME [epoch: 1.42 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5177412639163691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5177412639163691 | validation: 1.0121054556699787]
	TIME [epoch: 1.42 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935932200520708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6935932200520708 | validation: 0.614477551206308]
	TIME [epoch: 1.41 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5334979726699934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5334979726699934 | validation: 0.7432228885270096]
	TIME [epoch: 1.41 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4675885437399086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4675885437399086 | validation: 0.769659286826949]
	TIME [epoch: 1.41 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4863201603425198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4863201603425198 | validation: 0.5235341295967413]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4290565986506097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4290565986506097 | validation: 0.543524705924397]
	TIME [epoch: 1.42 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47090394903382093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47090394903382093 | validation: 1.0227799784318445]
	TIME [epoch: 1.41 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5915453837429742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5915453837429742 | validation: 0.757577323463295]
	TIME [epoch: 1.41 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5385352545114249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5385352545114249 | validation: 0.6623932870353528]
	TIME [epoch: 1.41 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40579932304303273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40579932304303273 | validation: 0.547109915514049]
	TIME [epoch: 1.41 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41857306290979085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41857306290979085 | validation: 0.48867412056364296]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3832588275005103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3832588275005103 | validation: 0.6281601795716619]
	TIME [epoch: 1.41 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4024554873998073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4024554873998073 | validation: 0.4583265766433077]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3907511476979211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3907511476979211 | validation: 0.6456505603341548]
	TIME [epoch: 1.41 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5124719667853397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5124719667853397 | validation: 0.9872008261528052]
	TIME [epoch: 1.41 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5965965239744605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5965965239744605 | validation: 0.7944663743186685]
	TIME [epoch: 1.41 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6055638328108088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6055638328108088 | validation: 0.7360425841496144]
	TIME [epoch: 1.41 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40504584394553184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40504584394553184 | validation: 0.48202699544383987]
	TIME [epoch: 1.41 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37225298676330154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37225298676330154 | validation: 0.57436548530007]
	TIME [epoch: 1.41 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3644398339949562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3644398339949562 | validation: 0.5869911479474885]
	TIME [epoch: 1.41 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4106959225788486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4106959225788486 | validation: 0.611144463688916]
	TIME [epoch: 182 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46254375943032167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46254375943032167 | validation: 0.6044600432083546]
	TIME [epoch: 2.82 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5333745302756203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5333745302756203 | validation: 1.0795493432314363]
	TIME [epoch: 2.8 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6052574167432818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6052574167432818 | validation: 0.6111231736188558]
	TIME [epoch: 2.79 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5497557098532092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5497557098532092 | validation: 0.6738713920400834]
	TIME [epoch: 2.79 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41235400050930254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41235400050930254 | validation: 0.5734315872494936]
	TIME [epoch: 2.79 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3892548964916484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3892548964916484 | validation: 0.4532120697961389]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948855595136645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3948855595136645 | validation: 0.6110181332289425]
	TIME [epoch: 2.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3731924911750376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3731924911750376 | validation: 0.4882629584715087]
	TIME [epoch: 2.8 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32441704528183934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32441704528183934 | validation: 0.4538371141378541]
	TIME [epoch: 2.8 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3182588546843927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3182588546843927 | validation: 0.4861644327610568]
	TIME [epoch: 2.8 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.320270327761158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.320270327761158 | validation: 0.4847590063052283]
	TIME [epoch: 2.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3446231473539801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3446231473539801 | validation: 0.8210447324812228]
	TIME [epoch: 2.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8486191787881557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8486191787881557 | validation: 1.055575484149857]
	TIME [epoch: 2.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6262714221029123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6262714221029123 | validation: 0.5542434241713391]
	TIME [epoch: 2.79 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5629417476528144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5629417476528144 | validation: 0.5447602757314666]
	TIME [epoch: 2.79 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38527316241098486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38527316241098486 | validation: 0.5936992377634465]
	TIME [epoch: 2.79 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3482086257760018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3482086257760018 | validation: 0.4347060708495839]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3646470138741458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3646470138741458 | validation: 0.7455882741813897]
	TIME [epoch: 2.81 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.411431950706108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.411431950706108 | validation: 0.5440831190590393]
	TIME [epoch: 2.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39499248954507177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39499248954507177 | validation: 0.4226053703555085]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31822132087753613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31822132087753613 | validation: 0.47011737275865456]
	TIME [epoch: 2.81 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3021833255728942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3021833255728942 | validation: 0.4464935644099449]
	TIME [epoch: 2.8 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.280168439432603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.280168439432603 | validation: 0.4275710530470919]
	TIME [epoch: 2.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2650729807668706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2650729807668706 | validation: 0.42207356025727133]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26189663979397304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26189663979397304 | validation: 0.42371505565598433]
	TIME [epoch: 2.8 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25749362060703734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25749362060703734 | validation: 0.46223975679657425]
	TIME [epoch: 2.79 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25674453376490347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25674453376490347 | validation: 0.4001572679810627]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3512810429358846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3512810429358846 | validation: 1.2241790064825542]
	TIME [epoch: 2.81 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923175426758392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923175426758392 | validation: 0.5155214617087965]
	TIME [epoch: 2.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4425900667459905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4425900667459905 | validation: 0.4679342768741669]
	TIME [epoch: 2.8 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3920791660150254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3920791660150254 | validation: 0.7788099129399928]
	TIME [epoch: 2.81 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47671070235780805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47671070235780805 | validation: 0.7306909893248877]
	TIME [epoch: 2.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42826757572651686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42826757572651686 | validation: 0.4950821282843718]
	TIME [epoch: 2.81 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2838133199994238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2838133199994238 | validation: 0.42952136630535276]
	TIME [epoch: 2.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2841118611877142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2841118611877142 | validation: 0.41700830469937317]
	TIME [epoch: 2.8 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2535056140971128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2535056140971128 | validation: 0.4262663279054187]
	TIME [epoch: 2.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26596175372827935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26596175372827935 | validation: 0.5714427882272315]
	TIME [epoch: 2.8 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38226397629241077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38226397629241077 | validation: 0.46802664991187326]
	TIME [epoch: 2.8 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30436187243115664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30436187243115664 | validation: 0.4254533996191736]
	TIME [epoch: 2.8 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27573630608284516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27573630608284516 | validation: 0.5201884812004904]
	TIME [epoch: 2.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26129858004529044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26129858004529044 | validation: 0.3487484988074589]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2523748749600952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2523748749600952 | validation: 0.6052557417209886]
	TIME [epoch: 2.81 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2969264893201423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2969264893201423 | validation: 0.3699899148208392]
	TIME [epoch: 2.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2242324189011061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2242324189011061 | validation: 0.41658882658461976]
	TIME [epoch: 2.79 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26054264377146097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26054264377146097 | validation: 0.5414677087859117]
	TIME [epoch: 2.79 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33610635580884024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33610635580884024 | validation: 0.3401041764305719]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3048911614692279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3048911614692279 | validation: 0.5919299186925512]
	TIME [epoch: 2.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31692263344516414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31692263344516414 | validation: 0.36175808089462974]
	TIME [epoch: 2.81 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28092217488790155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28092217488790155 | validation: 0.803238481520245]
	TIME [epoch: 2.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.460159183677862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.460159183677862 | validation: 0.6546452086652779]
	TIME [epoch: 2.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3650814830749873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3650814830749873 | validation: 0.4242220314627533]
	TIME [epoch: 2.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.275434270999705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.275434270999705 | validation: 0.44718398915579877]
	TIME [epoch: 2.8 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25296434213318514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25296434213318514 | validation: 0.47464585025222555]
	TIME [epoch: 2.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28338060022451184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28338060022451184 | validation: 0.45091204689168835]
	TIME [epoch: 2.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22848941896031832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22848941896031832 | validation: 0.32894003614891926]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21067459353291226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21067459353291226 | validation: 0.6165178610890276]
	TIME [epoch: 2.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26018152344274653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26018152344274653 | validation: 0.3212860843830082]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811261436183405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2811261436183405 | validation: 0.7023372970529764]
	TIME [epoch: 2.81 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38883095002449325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38883095002449325 | validation: 0.34325169292415825]
	TIME [epoch: 2.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21677204243439832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21677204243439832 | validation: 0.36594200457944576]
	TIME [epoch: 2.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18573731846735408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18573731846735408 | validation: 0.45933792251228384]
	TIME [epoch: 2.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1949932568976363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1949932568976363 | validation: 0.3429338888514815]
	TIME [epoch: 2.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1955078301217236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1955078301217236 | validation: 0.5370380758047224]
	TIME [epoch: 2.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22045745383743406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22045745383743406 | validation: 0.3299982214954129]
	TIME [epoch: 2.79 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24279340313039982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24279340313039982 | validation: 0.6748903706248897]
	TIME [epoch: 2.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3186145974697191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3186145974697191 | validation: 0.4057371382801524]
	TIME [epoch: 2.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16358110440594822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16358110440594822 | validation: 0.3055015250813798]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28017811191792563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28017811191792563 | validation: 0.7597984909217829]
	TIME [epoch: 2.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6084653624410313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6084653624410313 | validation: 0.6302526283940034]
	TIME [epoch: 2.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2684778283653247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2684778283653247 | validation: 0.3222799787574491]
	TIME [epoch: 2.81 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3491783125400748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3491783125400748 | validation: 0.4058077184603033]
	TIME [epoch: 2.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21386706008759981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21386706008759981 | validation: 0.5441491665565471]
	TIME [epoch: 2.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21102371822335722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21102371822335722 | validation: 0.3323785440672029]
	TIME [epoch: 2.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16908764160330667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16908764160330667 | validation: 0.3512929984308702]
	TIME [epoch: 2.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16966559647128462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16966559647128462 | validation: 0.4443339443229755]
	TIME [epoch: 2.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17739596278281355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17739596278281355 | validation: 0.34042763479087834]
	TIME [epoch: 2.79 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19174056249105917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19174056249105917 | validation: 0.5134239770726835]
	TIME [epoch: 2.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21251848589458547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21251848589458547 | validation: 0.3720226399594365]
	TIME [epoch: 2.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17326835672239252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17326835672239252 | validation: 0.39305561387055515]
	TIME [epoch: 2.81 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27897797083630826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27897797083630826 | validation: 0.59810822902444]
	TIME [epoch: 2.81 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4511919805195667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4511919805195667 | validation: 0.7710340555828478]
	TIME [epoch: 2.81 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3710901814429483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3710901814429483 | validation: 0.37785202458583617]
	TIME [epoch: 2.81 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30310702515271004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30310702515271004 | validation: 0.3544157332604738]
	TIME [epoch: 2.81 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1564115572859638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1564115572859638 | validation: 0.4397944393266707]
	TIME [epoch: 2.81 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20554999718723732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20554999718723732 | validation: 0.32304334214476194]
	TIME [epoch: 2.81 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22896630475541435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22896630475541435 | validation: 0.6550459290580307]
	TIME [epoch: 2.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.282033713924324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.282033713924324 | validation: 0.3332809722306924]
	TIME [epoch: 2.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18898325207221348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18898325207221348 | validation: 0.4295126566143479]
	TIME [epoch: 2.81 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1829220421392579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1829220421392579 | validation: 0.3098252413820145]
	TIME [epoch: 2.81 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16011096610186495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16011096610186495 | validation: 0.43649388702251035]
	TIME [epoch: 2.81 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17177160070073133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17177160070073133 | validation: 0.30358359278947455]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20167850980893254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20167850980893254 | validation: 0.5694267109832594]
	TIME [epoch: 2.79 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21812569668790283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21812569668790283 | validation: 0.2648724054209743]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33561991441884437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33561991441884437 | validation: 0.5409010441414328]
	TIME [epoch: 2.81 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.366675595221731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.366675595221731 | validation: 0.5058384587266588]
	TIME [epoch: 2.81 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2138405583300812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2138405583300812 | validation: 0.30958665294809146]
	TIME [epoch: 2.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21010442305931062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21010442305931062 | validation: 0.413712352396803]
	TIME [epoch: 2.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1790293523530399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1790293523530399 | validation: 0.39342419682558627]
	TIME [epoch: 2.8 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21341201660751444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21341201660751444 | validation: 0.41823645106434015]
	TIME [epoch: 2.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19568649585870687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19568649585870687 | validation: 0.33664162450320534]
	TIME [epoch: 2.8 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15977894205418638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15977894205418638 | validation: 0.3545572085877431]
	TIME [epoch: 2.81 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12418453802648198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12418453802648198 | validation: 0.317574762772069]
	TIME [epoch: 2.81 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1207922455888162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1207922455888162 | validation: 0.45823105977051015]
	TIME [epoch: 2.81 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16209511799504625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16209511799504625 | validation: 0.525997151297281]
	TIME [epoch: 2.81 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728511816687707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4728511816687707 | validation: 0.9668859106590237]
	TIME [epoch: 2.81 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5299378524973386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5299378524973386 | validation: 0.7544683853349543]
	TIME [epoch: 2.8 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4245752056344667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4245752056344667 | validation: 0.5550605088267966]
	TIME [epoch: 2.8 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20379881791659224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20379881791659224 | validation: 0.4021398444617707]
	TIME [epoch: 2.8 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2432731929699822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2432731929699822 | validation: 0.4136040932995928]
	TIME [epoch: 2.8 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18656200548627136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18656200548627136 | validation: 0.41185160674241006]
	TIME [epoch: 2.81 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18497754775051553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18497754775051553 | validation: 0.3909460430462928]
	TIME [epoch: 2.81 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17310071878041716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17310071878041716 | validation: 0.4053924939051209]
	TIME [epoch: 2.81 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18514074829239735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18514074829239735 | validation: 0.3191058379170185]
	TIME [epoch: 2.8 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1947028930263369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1947028930263369 | validation: 0.6664600264529011]
	TIME [epoch: 2.81 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29950100999534185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29950100999534185 | validation: 0.28710620491352723]
	TIME [epoch: 2.81 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15462717933085127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15462717933085127 | validation: 0.3768902419534902]
	TIME [epoch: 2.81 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13279906726486754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13279906726486754 | validation: 0.3773128397795708]
	TIME [epoch: 2.8 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16260936600102635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16260936600102635 | validation: 0.4553881757376983]
	TIME [epoch: 2.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20098754872674007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20098754872674007 | validation: 0.36790607505409345]
	TIME [epoch: 2.8 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16590997226972667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16590997226972667 | validation: 0.36274007082313786]
	TIME [epoch: 2.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13118904167186768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13118904167186768 | validation: 0.34768986349361675]
	TIME [epoch: 2.8 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12446480037903182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12446480037903182 | validation: 0.2953273028450301]
	TIME [epoch: 2.81 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1645518464144378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1645518464144378 | validation: 0.8077843270215279]
	TIME [epoch: 2.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5069290764284593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5069290764284593 | validation: 0.34827280995688464]
	TIME [epoch: 2.81 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18224177690340848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18224177690340848 | validation: 0.5865084609836901]
	TIME [epoch: 2.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3514115907006735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3514115907006735 | validation: 0.6453858637429549]
	TIME [epoch: 2.81 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26470695190328486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26470695190328486 | validation: 0.4278532058940437]
	TIME [epoch: 2.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2738100982739685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2738100982739685 | validation: 0.49217661431942494]
	TIME [epoch: 2.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1734648519971474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1734648519971474 | validation: 0.4400035238599218]
	TIME [epoch: 2.79 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181847977371716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181847977371716 | validation: 0.40085575325303413]
	TIME [epoch: 2.8 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15156051723446454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15156051723446454 | validation: 0.3518335520272122]
	TIME [epoch: 2.81 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12599533320742623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12599533320742623 | validation: 0.38681668280418435]
	TIME [epoch: 2.81 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11371208590814857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11371208590814857 | validation: 0.3405788737548981]
	TIME [epoch: 2.8 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11791524763313023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11791524763313023 | validation: 0.43779128222484137]
	TIME [epoch: 2.8 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1505335243418173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1505335243418173 | validation: 0.29841243819860147]
	TIME [epoch: 2.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14699051205855726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14699051205855726 | validation: 0.700608797744439]
	TIME [epoch: 2.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26056997983664665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26056997983664665 | validation: 0.27021456397281235]
	TIME [epoch: 2.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2449273887637091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2449273887637091 | validation: 0.44917233947367263]
	TIME [epoch: 2.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1807201390161881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1807201390161881 | validation: 0.3673380421477077]
	TIME [epoch: 2.79 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15834548479060456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15834548479060456 | validation: 0.3643697154867687]
	TIME [epoch: 2.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18891247587659288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18891247587659288 | validation: 0.38871643331519956]
	TIME [epoch: 2.79 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17325971672839885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17325971672839885 | validation: 0.3533238188293137]
	TIME [epoch: 2.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11860060349902533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11860060349902533 | validation: 0.29388244960082904]
	TIME [epoch: 2.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11570693158747589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11570693158747589 | validation: 0.4457130416361791]
	TIME [epoch: 2.81 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14388038320716823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14388038320716823 | validation: 0.2495081788908312]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_346.pth
	Model improved!!!
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2817507751483897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2817507751483897 | validation: 0.6628494066030663]
	TIME [epoch: 2.79 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24824269345355504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24824269345355504 | validation: 0.3658246521386892]
	TIME [epoch: 2.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19670037203621646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19670037203621646 | validation: 0.6353904057586006]
	TIME [epoch: 2.78 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29791404685762407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29791404685762407 | validation: 0.42251236022434996]
	TIME [epoch: 2.78 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15027887058973796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15027887058973796 | validation: 0.3589065666408902]
	TIME [epoch: 2.79 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19166334657009365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19166334657009365 | validation: 0.39875718065435817]
	TIME [epoch: 2.78 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12235108690716757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12235108690716757 | validation: 0.3801422975904955]
	TIME [epoch: 2.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247909843145196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1247909843145196 | validation: 0.3343951591310108]
	TIME [epoch: 2.78 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1302322104039669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1302322104039669 | validation: 0.3377398797465418]
	TIME [epoch: 2.79 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13776576513980981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13776576513980981 | validation: 0.4262705524258098]
	TIME [epoch: 2.79 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1520102127153131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1520102127153131 | validation: 0.3690277213538085]
	TIME [epoch: 2.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17513221711073595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17513221711073595 | validation: 0.44054464750664063]
	TIME [epoch: 2.79 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16618657982082247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16618657982082247 | validation: 0.30354758726742176]
	TIME [epoch: 2.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14235252036360735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14235252036360735 | validation: 0.35793127408299724]
	TIME [epoch: 2.79 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11539213510934224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11539213510934224 | validation: 0.2955085973261335]
	TIME [epoch: 2.79 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09836425321182922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09836425321182922 | validation: 0.3862305314410808]
	TIME [epoch: 2.79 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12052408261341863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12052408261341863 | validation: 0.2531049711551449]
	TIME [epoch: 2.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17447759369511545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17447759369511545 | validation: 0.8887026505724865]
	TIME [epoch: 2.79 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41748619998090036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41748619998090036 | validation: 0.32106883184265245]
	TIME [epoch: 2.79 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13135101903199264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13135101903199264 | validation: 0.2919673099080519]
	TIME [epoch: 2.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18091835083163177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18091835083163177 | validation: 0.558334081119184]
	TIME [epoch: 2.79 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849072510431509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849072510431509 | validation: 0.33698616329367176]
	TIME [epoch: 2.79 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1117316111119003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1117316111119003 | validation: 0.28614533671542075]
	TIME [epoch: 2.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13540154866115572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13540154866115572 | validation: 0.5760779339287533]
	TIME [epoch: 2.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17734680010455095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17734680010455095 | validation: 0.3127704299751247]
	TIME [epoch: 2.79 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327622003525428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12327622003525428 | validation: 0.27800664545152026]
	TIME [epoch: 2.78 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10244677565618432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10244677565618432 | validation: 0.3565696415693066]
	TIME [epoch: 2.79 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257173091766685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09257173091766685 | validation: 0.29866954811240265]
	TIME [epoch: 2.78 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08986055152284901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08986055152284901 | validation: 0.3592999884017079]
	TIME [epoch: 2.79 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09692427543770778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09692427543770778 | validation: 0.3343835222165655]
	TIME [epoch: 2.81 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13036592331304037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13036592331304037 | validation: 0.6214867636300365]
	TIME [epoch: 2.79 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22224917644619116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22224917644619116 | validation: 0.32942488985981505]
	TIME [epoch: 2.79 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15525390247859186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15525390247859186 | validation: 0.4505771491458983]
	TIME [epoch: 2.79 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699004208360658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1699004208360658 | validation: 0.37569186414611444]
	TIME [epoch: 2.79 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20203307382079114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20203307382079114 | validation: 0.3327231694848794]
	TIME [epoch: 2.79 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1507835698632541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1507835698632541 | validation: 0.2989373612601773]
	TIME [epoch: 2.79 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09549184170468632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09549184170468632 | validation: 0.3929972649059434]
	TIME [epoch: 2.79 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13880698637685981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13880698637685981 | validation: 0.26778623649111477]
	TIME [epoch: 2.79 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20026282302944032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20026282302944032 | validation: 0.5997030496752206]
	TIME [epoch: 2.77 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24141205495562595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24141205495562595 | validation: 0.28181552798800985]
	TIME [epoch: 2.78 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11898407686790868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11898407686790868 | validation: 0.2688784706777522]
	TIME [epoch: 2.79 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15191986355578982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15191986355578982 | validation: 0.36371599589544623]
	TIME [epoch: 2.78 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14896607596703587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14896607596703587 | validation: 0.2540464885951164]
	TIME [epoch: 2.78 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09841762516007524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09841762516007524 | validation: 0.32579578768912]
	TIME [epoch: 2.79 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10949465082352677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10949465082352677 | validation: 0.2606883457666837]
	TIME [epoch: 2.78 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10622409252224752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10622409252224752 | validation: 0.361746110306556]
	TIME [epoch: 2.78 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08991800156246603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08991800156246603 | validation: 0.2319948735386538]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_393.pth
	Model improved!!!
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10330070457299222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10330070457299222 | validation: 0.5160367107647365]
	TIME [epoch: 2.79 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17174746344347733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17174746344347733 | validation: 0.2740440173916904]
	TIME [epoch: 2.79 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1588785038125832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1588785038125832 | validation: 0.471400233879403]
	TIME [epoch: 2.79 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13221575193371637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13221575193371637 | validation: 0.2643472067252478]
	TIME [epoch: 2.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711068427936511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08711068427936511 | validation: 0.250278645772868]
	TIME [epoch: 2.79 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438252837697413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09438252837697413 | validation: 0.4983314231946624]
	TIME [epoch: 2.79 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15825918468409542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15825918468409542 | validation: 0.31999297316006126]
	TIME [epoch: 2.79 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19011347282293403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19011347282293403 | validation: 0.7326143774890436]
	TIME [epoch: 2.79 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3208185053804033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3208185053804033 | validation: 0.41607333776914257]
	TIME [epoch: 2.78 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12954107861761854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12954107861761854 | validation: 0.27548484495692527]
	TIME [epoch: 2.78 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582823120150007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1582823120150007 | validation: 0.4101728142959772]
	TIME [epoch: 2.77 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11488904569068734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11488904569068734 | validation: 0.4055630916954325]
	TIME [epoch: 2.77 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364127186943776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1364127186943776 | validation: 0.2378539071345748]
	TIME [epoch: 2.77 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12668270601481157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12668270601481157 | validation: 0.35003226873389237]
	TIME [epoch: 2.78 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556579669421908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1556579669421908 | validation: 0.3248108362454407]
	TIME [epoch: 2.78 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959302061578454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0959302061578454 | validation: 0.2724157764368852]
	TIME [epoch: 2.77 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13118990999844726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13118990999844726 | validation: 0.29832703952281064]
	TIME [epoch: 2.77 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12312850107358443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12312850107358443 | validation: 0.25304282587659166]
	TIME [epoch: 2.78 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08336473000610971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08336473000610971 | validation: 0.28321015146851386]
	TIME [epoch: 2.77 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11415734298825797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11415734298825797 | validation: 0.3129700053855719]
	TIME [epoch: 2.77 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11892670006722082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11892670006722082 | validation: 0.2626723555893061]
	TIME [epoch: 2.77 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09391579447241048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09391579447241048 | validation: 0.3495042691952735]
	TIME [epoch: 2.77 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373751315624712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373751315624712 | validation: 1.146464476887093]
	TIME [epoch: 2.77 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4102268174562704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4102268174562704 | validation: 0.940250976518655]
	TIME [epoch: 2.77 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3015205519223332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3015205519223332 | validation: 0.5209302501648282]
	TIME [epoch: 2.78 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693002062815655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5693002062815655 | validation: 0.6901030663091747]
	TIME [epoch: 2.78 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39700608670992454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39700608670992454 | validation: 0.5410843147022348]
	TIME [epoch: 2.78 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33096644050717755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33096644050717755 | validation: 0.4359318451870451]
	TIME [epoch: 2.77 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1836784036842976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1836784036842976 | validation: 0.32308139461064506]
	TIME [epoch: 2.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15254046978861052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15254046978861052 | validation: 0.2691997118756288]
	TIME [epoch: 2.77 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12416142098567504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12416142098567504 | validation: 0.3255654838524734]
	TIME [epoch: 2.78 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264183833945463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11264183833945463 | validation: 0.28649628837411156]
	TIME [epoch: 2.78 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09621749466125547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09621749466125547 | validation: 0.25049983740330406]
	TIME [epoch: 2.77 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257882609251578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09257882609251578 | validation: 0.26302149885298104]
	TIME [epoch: 2.77 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09795814682213917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09795814682213917 | validation: 0.2769095573068944]
	TIME [epoch: 2.78 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13911764087040893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13911764087040893 | validation: 0.5055046282323995]
	TIME [epoch: 2.78 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38454024814176635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38454024814176635 | validation: 0.35094844882205933]
	TIME [epoch: 2.78 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20579676758033316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20579676758033316 | validation: 0.23749751008035971]
	TIME [epoch: 2.77 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09625581570509884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09625581570509884 | validation: 0.2499531844234368]
	TIME [epoch: 2.77 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07030100203397631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07030100203397631 | validation: 0.20878707767638313]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07283504171154435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07283504171154435 | validation: 0.24634767257936896]
	TIME [epoch: 2.79 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697372784502416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0697372784502416 | validation: 0.2209542162512922]
	TIME [epoch: 2.79 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095892826690768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07095892826690768 | validation: 0.2508981246663547]
	TIME [epoch: 2.79 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265181649115032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265181649115032 | validation: 0.2474437932979212]
	TIME [epoch: 2.79 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07952009310456946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07952009310456946 | validation: 0.2898082239349042]
	TIME [epoch: 2.79 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10919893809279922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10919893809279922 | validation: 0.2737513430982135]
	TIME [epoch: 2.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13401818989019507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13401818989019507 | validation: 0.31271412771680573]
	TIME [epoch: 2.79 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1960948756988497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1960948756988497 | validation: 0.2752903056026505]
	TIME [epoch: 2.78 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11320442458651421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11320442458651421 | validation: 0.25435674172302347]
	TIME [epoch: 2.79 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07010675342994115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07010675342994115 | validation: 0.1981287437941351]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846291394939376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06846291394939376 | validation: 0.44713493151277073]
	TIME [epoch: 2.78 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10136978931848646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10136978931848646 | validation: 0.18258284715726816]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1309223405602979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1309223405602979 | validation: 0.5903432416028284]
	TIME [epoch: 2.79 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23996759220455133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23996759220455133 | validation: 0.22151230393044155]
	TIME [epoch: 2.79 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11192520780188422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11192520780188422 | validation: 0.28232451225067906]
	TIME [epoch: 2.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1829765318979566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1829765318979566 | validation: 0.27615477729120974]
	TIME [epoch: 2.79 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1268742790551905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1268742790551905 | validation: 0.25380172491029757]
	TIME [epoch: 2.79 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10226897614449247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10226897614449247 | validation: 0.2577765900775248]
	TIME [epoch: 2.79 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14582646150804734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14582646150804734 | validation: 0.251705663142375]
	TIME [epoch: 2.79 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640131610159794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07640131610159794 | validation: 0.19874906042442497]
	TIME [epoch: 2.79 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06117681738166688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06117681738166688 | validation: 0.20726567423191034]
	TIME [epoch: 2.78 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06731219787620159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06731219787620159 | validation: 0.2267625060700995]
	TIME [epoch: 2.78 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08120138118977036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08120138118977036 | validation: 0.19813514621285147]
	TIME [epoch: 2.78 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08017807839046309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08017807839046309 | validation: 0.24519961519926614]
	TIME [epoch: 2.78 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10690731207586518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10690731207586518 | validation: 0.2604682576527773]
	TIME [epoch: 2.78 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13448752997695668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13448752997695668 | validation: 0.2635056877871246]
	TIME [epoch: 2.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12049721015879515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12049721015879515 | validation: 0.19693578619634222]
	TIME [epoch: 2.78 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871922555726111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11871922555726111 | validation: 0.6382493461911585]
	TIME [epoch: 2.79 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20519756261276625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20519756261276625 | validation: 0.21792160961235504]
	TIME [epoch: 2.79 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10465775830845266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10465775830845266 | validation: 0.30203004159441316]
	TIME [epoch: 2.79 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09618250176960397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09618250176960397 | validation: 0.24279428621063617]
	TIME [epoch: 2.79 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09944305549777412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09944305549777412 | validation: 0.18244848431735058]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07697068221273813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07697068221273813 | validation: 0.236551774904975]
	TIME [epoch: 2.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813468653897692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0813468653897692 | validation: 0.17557278699771156]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08616522893136193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08616522893136193 | validation: 0.2291049715780769]
	TIME [epoch: 2.78 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10948965808028856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10948965808028856 | validation: 0.19834940428528913]
	TIME [epoch: 2.77 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06778172990602088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06778172990602088 | validation: 0.2225102552168902]
	TIME [epoch: 2.77 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07177454341130479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07177454341130479 | validation: 0.23232702661649418]
	TIME [epoch: 2.77 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11635665369824662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11635665369824662 | validation: 0.22517065222624333]
	TIME [epoch: 2.78 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07668873018495564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07668873018495564 | validation: 0.1938828996062837]
	TIME [epoch: 2.78 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08407217156999339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08407217156999339 | validation: 0.3715304484657666]
	TIME [epoch: 2.78 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15721423669253687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15721423669253687 | validation: 0.21624914528882744]
	TIME [epoch: 2.77 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12603392012701087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12603392012701087 | validation: 0.2834519908863204]
	TIME [epoch: 2.78 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903572159222738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11903572159222738 | validation: 0.19854618443628597]
	TIME [epoch: 2.77 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06123627983248863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06123627983248863 | validation: 0.17589726439068631]
	TIME [epoch: 2.77 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08652104723005899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08652104723005899 | validation: 0.6074756367907157]
	TIME [epoch: 2.78 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17682970708028556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17682970708028556 | validation: 0.21703792800636307]
	TIME [epoch: 2.78 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130133565475667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.130133565475667 | validation: 0.25009541346147474]
	TIME [epoch: 2.79 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305765615435929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305765615435929 | validation: 0.1858319551273655]
	TIME [epoch: 2.87 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321762635710148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07321762635710148 | validation: 0.2581471884632382]
	TIME [epoch: 2.77 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728366378123103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0728366378123103 | validation: 0.20006752741804867]
	TIME [epoch: 2.77 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08061352729363201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08061352729363201 | validation: 0.2066601250129228]
	TIME [epoch: 2.78 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06461024682008829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06461024682008829 | validation: 0.1849451356263947]
	TIME [epoch: 2.78 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532194129617495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07532194129617495 | validation: 0.26607679663677575]
	TIME [epoch: 2.78 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09548680443303034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09548680443303034 | validation: 0.21885864532882848]
	TIME [epoch: 2.78 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11098483008819614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11098483008819614 | validation: 0.549533918277848]
	TIME [epoch: 2.77 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11381318606128826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11381318606128826 | validation: 0.14898305780230658]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05210989156442041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05210989156442041 | validation: 0.2201851773615321]
	TIME [epoch: 2.78 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06587717080210834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06587717080210834 | validation: 0.19266995567302275]
	TIME [epoch: 2.78 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11894135983863802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11894135983863802 | validation: 0.4023829763051612]
	TIME [epoch: 2.78 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23165339861313314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23165339861313314 | validation: 0.19239065047122492]
	TIME [epoch: 2.78 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.112372484537364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.112372484537364 | validation: 0.20279817663019417]
	TIME [epoch: 2.78 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07559470552066921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07559470552066921 | validation: 0.47527791137066144]
	TIME [epoch: 2.78 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08980085480312167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08980085480312167 | validation: 0.1814562183647204]
	TIME [epoch: 2.78 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05343483969171407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05343483969171407 | validation: 0.20353641827483507]
	TIME [epoch: 2.78 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.073980798310675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.073980798310675 | validation: 0.24169649675498792]
	TIME [epoch: 2.78 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135750925551511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05135750925551511 | validation: 0.16636566654542032]
	TIME [epoch: 2.78 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03658415896508853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03658415896508853 | validation: 0.16770354146125155]
	TIME [epoch: 183 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03268580874456086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03268580874456086 | validation: 0.16736610150384548]
	TIME [epoch: 5.98 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036344066714353425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036344066714353425 | validation: 0.17211443564614412]
	TIME [epoch: 5.96 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046250538940498354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046250538940498354 | validation: 0.2485037807765127]
	TIME [epoch: 5.96 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08713010210350142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08713010210350142 | validation: 0.26664655317032565]
	TIME [epoch: 5.96 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23408622511265975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23408622511265975 | validation: 0.3367678202527043]
	TIME [epoch: 5.95 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862322538649105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0862322538649105 | validation: 0.18513955562435236]
	TIME [epoch: 5.96 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13420865980989347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13420865980989347 | validation: 0.5373068151448719]
	TIME [epoch: 5.96 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19230151451647204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19230151451647204 | validation: 0.1953604121847676]
	TIME [epoch: 5.97 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12232253129750735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12232253129750735 | validation: 0.26657201719477563]
	TIME [epoch: 5.96 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11939649449966211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11939649449966211 | validation: 0.23846725608793584]
	TIME [epoch: 5.95 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06261052072449798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06261052072449798 | validation: 0.2235703781382723]
	TIME [epoch: 5.95 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08689781657034247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08689781657034247 | validation: 0.46423965060552164]
	TIME [epoch: 5.96 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10844123439815867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10844123439815867 | validation: 0.20527131351179728]
	TIME [epoch: 5.96 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10322106401601909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10322106401601909 | validation: 0.24210542016595707]
	TIME [epoch: 5.95 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09457203609660449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09457203609660449 | validation: 0.16494342977983661]
	TIME [epoch: 5.95 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05482857429314863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05482857429314863 | validation: 0.24370382149860836]
	TIME [epoch: 5.96 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04217396336296858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04217396336296858 | validation: 0.14718056416018463]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0411572033348914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0411572033348914 | validation: 0.19842246251862777]
	TIME [epoch: 5.96 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03439931355830545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03439931355830545 | validation: 0.15842275779885068]
	TIME [epoch: 5.96 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04550314120927888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04550314120927888 | validation: 0.15131491608975534]
	TIME [epoch: 5.96 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05692676230696765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05692676230696765 | validation: 0.24929158331302534]
	TIME [epoch: 5.98 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1126594008528447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1126594008528447 | validation: 0.1323776699216227]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507136655136691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507136655136691 | validation: 0.3303004758930245]
	TIME [epoch: 5.95 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08356583088245474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08356583088245474 | validation: 0.166117171864293]
	TIME [epoch: 5.96 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08870407147242329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08870407147242329 | validation: 0.4461322748076968]
	TIME [epoch: 5.96 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13270031339531113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13270031339531113 | validation: 0.20547327231455048]
	TIME [epoch: 5.96 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10437019878523719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10437019878523719 | validation: 0.23839631100461223]
	TIME [epoch: 5.96 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163250448628406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.163250448628406 | validation: 0.49999714473911877]
	TIME [epoch: 5.96 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20146430732062942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20146430732062942 | validation: 0.3081460457436509]
	TIME [epoch: 5.96 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399519916582919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06399519916582919 | validation: 0.11859887754981827]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227683795547364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07227683795547364 | validation: 0.22159169137175036]
	TIME [epoch: 5.98 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08599950419778037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08599950419778037 | validation: 0.6918581988255633]
	TIME [epoch: 5.96 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24718719705084408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24718719705084408 | validation: 0.18640055566820518]
	TIME [epoch: 5.97 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185644327354886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09185644327354886 | validation: 0.22038867486640157]
	TIME [epoch: 5.96 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09293367436049582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09293367436049582 | validation: 0.25323671720861257]
	TIME [epoch: 5.95 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05673760279312436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05673760279312436 | validation: 0.29129573284080923]
	TIME [epoch: 5.96 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0596434590791516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0596434590791516 | validation: 0.21172636016125282]
	TIME [epoch: 5.96 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662597928733904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04662597928733904 | validation: 0.15437401609654794]
	TIME [epoch: 5.96 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04698744725451652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04698744725451652 | validation: 0.1880122015041581]
	TIME [epoch: 5.96 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06301083274868366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06301083274868366 | validation: 0.15603137596599315]
	TIME [epoch: 5.96 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056632263337934356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056632263337934356 | validation: 0.14756683415732572]
	TIME [epoch: 5.96 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049276998136931836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049276998136931836 | validation: 0.12712853734151458]
	TIME [epoch: 5.96 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03738939846727994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03738939846727994 | validation: 0.13183770830705993]
	TIME [epoch: 5.96 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046942871025650396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046942871025650396 | validation: 0.1725621673781737]
	TIME [epoch: 5.95 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08140841667713353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08140841667713353 | validation: 0.18234645508886954]
	TIME [epoch: 5.95 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12251593650837307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12251593650837307 | validation: 0.17676348845509118]
	TIME [epoch: 5.96 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08785427403781221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08785427403781221 | validation: 0.12960911592005506]
	TIME [epoch: 5.96 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05649290852601306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05649290852601306 | validation: 0.26096100483384743]
	TIME [epoch: 5.97 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05227524420478668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05227524420478668 | validation: 0.11355417138728513]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520169642952143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0520169642952143 | validation: 0.1602742941011393]
	TIME [epoch: 5.96 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12568546339019895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12568546339019895 | validation: 0.2756857445464882]
	TIME [epoch: 5.96 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14343033741999267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14343033741999267 | validation: 0.2786855452413474]
	TIME [epoch: 5.96 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.160209149295082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.160209149295082 | validation: 0.27391696270375976]
	TIME [epoch: 5.95 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.129599435083384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.129599435083384 | validation: 0.4932989976374124]
	TIME [epoch: 5.96 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11969846406123737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11969846406123737 | validation: 0.33660147443878397]
	TIME [epoch: 5.96 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06251579447976177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06251579447976177 | validation: 0.15971029088296484]
	TIME [epoch: 5.97 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05429184395463985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05429184395463985 | validation: 0.14803892903956006]
	TIME [epoch: 5.97 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048753985955558354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048753985955558354 | validation: 0.198942421013385]
	TIME [epoch: 5.96 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048379503009675244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048379503009675244 | validation: 0.13061813839132677]
	TIME [epoch: 5.96 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04183809311300245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04183809311300245 | validation: 0.1383623951342484]
	TIME [epoch: 5.95 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0480014057432549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0480014057432549 | validation: 0.17715414029500612]
	TIME [epoch: 5.95 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07311465047450941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07311465047450941 | validation: 0.1765433661553051]
	TIME [epoch: 5.95 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07799432947784951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07799432947784951 | validation: 0.13772087351201878]
	TIME [epoch: 5.95 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705407860989368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0705407860989368 | validation: 0.1403661282482009]
	TIME [epoch: 5.95 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05627770004156929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05627770004156929 | validation: 0.1365742011417682]
	TIME [epoch: 5.95 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061035917232824984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061035917232824984 | validation: 0.10890270561717799]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060856894594233824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060856894594233824 | validation: 0.22617132441037158]
	TIME [epoch: 5.96 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727002842504902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1727002842504902 | validation: 0.13234384407646024]
	TIME [epoch: 5.98 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945507664302283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06945507664302283 | validation: 0.14409985432780142]
	TIME [epoch: 5.96 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849245514879003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08849245514879003 | validation: 0.12209551364250162]
	TIME [epoch: 5.95 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04721388777989111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04721388777989111 | validation: 0.18780122363289328]
	TIME [epoch: 5.95 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07986277470600371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07986277470600371 | validation: 0.1365761267728513]
	TIME [epoch: 5.96 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047744598261787945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047744598261787945 | validation: 0.17535038055260435]
	TIME [epoch: 5.96 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04684363957567454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04684363957567454 | validation: 0.12581412721113314]
	TIME [epoch: 5.95 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05502332528117958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05502332528117958 | validation: 0.2500250587427497]
	TIME [epoch: 5.95 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05857923238010472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05857923238010472 | validation: 0.13323715995582602]
	TIME [epoch: 5.96 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06531791620284962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06531791620284962 | validation: 0.5396943994916876]
	TIME [epoch: 5.96 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1241835417995281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1241835417995281 | validation: 0.24680355308315446]
	TIME [epoch: 5.96 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14746760452613492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14746760452613492 | validation: 0.42097054851555477]
	TIME [epoch: 5.95 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.129022139854651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.129022139854651 | validation: 0.3845767664656211]
	TIME [epoch: 5.95 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727901936045461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07727901936045461 | validation: 0.17192401186684161]
	TIME [epoch: 5.97 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06660498898576052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06660498898576052 | validation: 0.24305614239047246]
	TIME [epoch: 5.96 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07968883964764889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07968883964764889 | validation: 0.18481887249613102]
	TIME [epoch: 5.96 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09052052216564965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09052052216564965 | validation: 0.18154282141503508]
	TIME [epoch: 5.96 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07475246906430331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07475246906430331 | validation: 0.11703192064479953]
	TIME [epoch: 5.96 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04103652659262257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04103652659262257 | validation: 0.145592722609136]
	TIME [epoch: 5.96 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04000260336164141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04000260336164141 | validation: 0.1120633941384376]
	TIME [epoch: 5.95 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902256825506172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03902256825506172 | validation: 0.11152225593582815]
	TIME [epoch: 5.95 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443205974582757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04443205974582757 | validation: 0.14048402597722706]
	TIME [epoch: 5.95 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267694155507769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04267694155507769 | validation: 0.08878650727741774]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_591.pth
	Model improved!!!
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05062842498244402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05062842498244402 | validation: 0.226199421327701]
	TIME [epoch: 5.96 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08155393760871597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08155393760871597 | validation: 0.08614748504194074]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041702198067341247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041702198067341247 | validation: 0.14930450186514]
	TIME [epoch: 5.96 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03939057978125622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03939057978125622 | validation: 0.12305202043549097]
	TIME [epoch: 5.95 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062351211249381804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062351211249381804 | validation: 0.14617609597904088]
	TIME [epoch: 5.95 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07769109714004459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07769109714004459 | validation: 0.18599916727428725]
	TIME [epoch: 5.96 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12845075334832037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12845075334832037 | validation: 0.24240528434079947]
	TIME [epoch: 5.95 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16906794558172028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16906794558172028 | validation: 0.43509411073107507]
	TIME [epoch: 5.95 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09097903684633632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09097903684633632 | validation: 0.15210954150570705]
	TIME [epoch: 5.95 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729832178331584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04729832178331584 | validation: 0.10746220191704584]
	TIME [epoch: 5.95 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046109068679041645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046109068679041645 | validation: 0.1048542423461957]
	TIME [epoch: 5.96 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053120863153492924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053120863153492924 | validation: 0.17411390982877264]
	TIME [epoch: 5.95 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04477201205930152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04477201205930152 | validation: 0.08859510288584749]
	TIME [epoch: 5.96 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033982639789286305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033982639789286305 | validation: 0.13607564496647306]
	TIME [epoch: 5.95 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03223853496660766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03223853496660766 | validation: 0.08977143002738915]
	TIME [epoch: 5.96 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03710542767973678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03710542767973678 | validation: 0.19981930165627657]
	TIME [epoch: 5.97 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0788409133156563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0788409133156563 | validation: 0.12446361295157299]
	TIME [epoch: 5.96 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06881075186023444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06881075186023444 | validation: 0.13681191967766304]
	TIME [epoch: 5.96 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04188946277178936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04188946277178936 | validation: 0.09504848434881591]
	TIME [epoch: 5.95 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03161727866748399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03161727866748399 | validation: 0.10241964789874208]
	TIME [epoch: 5.96 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04219882243534404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04219882243534404 | validation: 0.30703741641640714]
	TIME [epoch: 5.97 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10025273684881628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10025273684881628 | validation: 0.11695836791721011]
	TIME [epoch: 5.96 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07474761470552976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07474761470552976 | validation: 0.09661524691706302]
	TIME [epoch: 5.97 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06451202985984407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06451202985984407 | validation: 0.34693359974376675]
	TIME [epoch: 5.95 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07752536381305701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07752536381305701 | validation: 0.16329606769303814]
	TIME [epoch: 5.95 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07098404551970847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07098404551970847 | validation: 0.47176007470975245]
	TIME [epoch: 5.95 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10299073995094137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10299073995094137 | validation: 0.1703297583237413]
	TIME [epoch: 5.96 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11356997465433885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11356997465433885 | validation: 0.11884736513076902]
	TIME [epoch: 5.96 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062385041152226935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062385041152226935 | validation: 0.12598664734395035]
	TIME [epoch: 5.96 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05816106918353188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05816106918353188 | validation: 0.1232787330289419]
	TIME [epoch: 5.95 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0534113697286408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0534113697286408 | validation: 0.129638438040856]
	TIME [epoch: 5.97 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03156813791935776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03156813791935776 | validation: 0.09647368089420565]
	TIME [epoch: 5.95 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03823550563562707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03823550563562707 | validation: 0.1220500879965542]
	TIME [epoch: 5.95 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054122701807569625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054122701807569625 | validation: 0.1070563136621548]
	TIME [epoch: 5.95 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05309028645482933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05309028645482933 | validation: 0.11163551653381112]
	TIME [epoch: 5.96 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05700388784889773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05700388784889773 | validation: 0.10477705076691808]
	TIME [epoch: 5.95 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06168259458794305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06168259458794305 | validation: 0.34860913176217134]
	TIME [epoch: 5.96 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08264491772456926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08264491772456926 | validation: 0.1584041849616813]
	TIME [epoch: 5.96 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05500907386305265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05500907386305265 | validation: 0.1360572808627426]
	TIME [epoch: 5.95 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046307269558109064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046307269558109064 | validation: 0.35702125539856133]
	TIME [epoch: 5.95 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768895481081767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0768895481081767 | validation: 0.1138115309862735]
	TIME [epoch: 5.96 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044411520068944416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044411520068944416 | validation: 0.28968639276004426]
	TIME [epoch: 5.95 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09996982614582517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09996982614582517 | validation: 0.1779445234566978]
	TIME [epoch: 5.96 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12805682436364904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12805682436364904 | validation: 0.2618615893520714]
	TIME [epoch: 5.95 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1471110966656479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1471110966656479 | validation: 0.1599267717709946]
	TIME [epoch: 5.96 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05067546594047851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05067546594047851 | validation: 0.21387515916582878]
	TIME [epoch: 5.97 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835324760742822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10835324760742822 | validation: 0.37860803532361864]
	TIME [epoch: 5.97 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09759136866264384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09759136866264384 | validation: 0.34423298003903335]
	TIME [epoch: 5.97 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10834203124984733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10834203124984733 | validation: 0.2564912832164459]
	TIME [epoch: 5.95 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05062189607693252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05062189607693252 | validation: 0.2828007041434248]
	TIME [epoch: 5.95 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05946816734144115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05946816734144115 | validation: 0.3247132678334579]
	TIME [epoch: 5.96 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1318558871216842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1318558871216842 | validation: 0.7168837388647105]
	TIME [epoch: 5.96 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.251766186155303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.251766186155303 | validation: 0.22909482731623731]
	TIME [epoch: 5.96 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12975031380202204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12975031380202204 | validation: 0.29956392256835596]
	TIME [epoch: 5.95 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12352861491610366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12352861491610366 | validation: 0.2846150814631314]
	TIME [epoch: 5.96 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06114096273969832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06114096273969832 | validation: 0.3936329870821291]
	TIME [epoch: 5.96 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339755481011837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07339755481011837 | validation: 0.22490780229580817]
	TIME [epoch: 5.96 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340130562946725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04340130562946725 | validation: 0.12919679790280278]
	TIME [epoch: 5.96 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038599333810313705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038599333810313705 | validation: 0.12623104326131496]
	TIME [epoch: 5.96 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041310434777976975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041310434777976975 | validation: 0.12599400292305044]
	TIME [epoch: 5.95 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051345461278148416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051345461278148416 | validation: 0.13523444043306002]
	TIME [epoch: 5.96 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04561988005186859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04561988005186859 | validation: 0.11952169500622489]
	TIME [epoch: 5.96 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04376729557347135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04376729557347135 | validation: 0.10603290840809909]
	TIME [epoch: 5.96 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719710649176155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03719710649176155 | validation: 0.10295364104662481]
	TIME [epoch: 5.96 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416393918139098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03416393918139098 | validation: 0.09470208366809067]
	TIME [epoch: 5.96 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0369487606948302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0369487606948302 | validation: 0.13790721933331734]
	TIME [epoch: 5.97 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04379478751168987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04379478751168987 | validation: 0.11743908049898305]
	TIME [epoch: 5.96 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04904372287009384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04904372287009384 | validation: 0.11570821186472267]
	TIME [epoch: 5.97 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06541204987015674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06541204987015674 | validation: 0.09113168723840365]
	TIME [epoch: 5.95 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047884041521083845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047884041521083845 | validation: 0.10107804423575127]
	TIME [epoch: 5.95 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03421073077363821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03421073077363821 | validation: 0.08177591492142751]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_662.pth
	Model improved!!!
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029056706545952073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029056706545952073 | validation: 0.15212142808487764]
	TIME [epoch: 5.99 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155060255304305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155060255304305 | validation: 0.12243683690918186]
	TIME [epoch: 6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432323636371605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04432323636371605 | validation: 0.09209095945113999]
	TIME [epoch: 5.99 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802889593647679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06802889593647679 | validation: 0.209412424234415]
	TIME [epoch: 5.99 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741160012988091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09741160012988091 | validation: 0.33946856729654024]
	TIME [epoch: 6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10589747387730235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10589747387730235 | validation: 0.0900835060184299]
	TIME [epoch: 5.99 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04041188539860702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04041188539860702 | validation: 0.10384163977662833]
	TIME [epoch: 5.95 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615089297670055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04615089297670055 | validation: 0.11800457985035112]
	TIME [epoch: 5.95 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044729046400616125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044729046400616125 | validation: 0.07506291687866062]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02454510091254372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02454510091254372 | validation: 0.08586584342419351]
	TIME [epoch: 6.01 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03080577925410693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03080577925410693 | validation: 0.12099174217323685]
	TIME [epoch: 5.99 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04836041550832082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04836041550832082 | validation: 0.13054646075899418]
	TIME [epoch: 5.99 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08080155017925603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08080155017925603 | validation: 0.17169423151428856]
	TIME [epoch: 5.99 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07932888235107932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07932888235107932 | validation: 0.0826960107064653]
	TIME [epoch: 5.95 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05109080145145677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05109080145145677 | validation: 0.0847046299003018]
	TIME [epoch: 5.96 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048993857592537146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048993857592537146 | validation: 0.16740872963980657]
	TIME [epoch: 5.96 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963085071337742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04963085071337742 | validation: 0.10929469928818579]
	TIME [epoch: 5.96 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461516881330313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05461516881330313 | validation: 0.2589723081730592]
	TIME [epoch: 5.95 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042011527780931646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042011527780931646 | validation: 0.08451984050873222]
	TIME [epoch: 5.96 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03229731381729049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03229731381729049 | validation: 0.07547956761887827]
	TIME [epoch: 5.97 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038066861635691986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038066861635691986 | validation: 0.0918124200392152]
	TIME [epoch: 5.95 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059832242455561326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059832242455561326 | validation: 0.06486839459581319]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029644458014308466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029644458014308466 | validation: 0.07081406727839781]
	TIME [epoch: 5.99 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0344584828594574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0344584828594574 | validation: 0.09360731861905089]
	TIME [epoch: 5.98 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474661931964225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04474661931964225 | validation: 0.204866527906899]
	TIME [epoch: 5.99 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05573694808082772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05573694808082772 | validation: 0.16439815109210676]
	TIME [epoch: 5.98 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0768933205970076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0768933205970076 | validation: 0.22391967178517647]
	TIME [epoch: 5.99 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015295462750254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07015295462750254 | validation: 0.09788559113523132]
	TIME [epoch: 5.99 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04732119189486872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04732119189486872 | validation: 0.0984099192650244]
	TIME [epoch: 5.99 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06289808528700039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06289808528700039 | validation: 0.09098875234090659]
	TIME [epoch: 6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715232053383867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05715232053383867 | validation: 0.10027369512826412]
	TIME [epoch: 5.99 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04087910188313549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04087910188313549 | validation: 0.07447412691594708]
	TIME [epoch: 5.99 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05797185060222561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05797185060222561 | validation: 0.10598531121284349]
	TIME [epoch: 5.99 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06954514647542277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06954514647542277 | validation: 0.351598566995377]
	TIME [epoch: 5.99 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323542925726088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08323542925726088 | validation: 0.09998039340207983]
	TIME [epoch: 6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08324379044587188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08324379044587188 | validation: 0.12280182028349591]
	TIME [epoch: 5.99 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07974293224903538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07974293224903538 | validation: 0.19436850757043975]
	TIME [epoch: 5.99 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06450820634635458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06450820634635458 | validation: 0.05417482386904106]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027685408311599585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027685408311599585 | validation: 0.0703692169889373]
	TIME [epoch: 5.96 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02967908823782671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02967908823782671 | validation: 0.07781866899731482]
	TIME [epoch: 5.96 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033278361527800254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033278361527800254 | validation: 0.15530121297278388]
	TIME [epoch: 5.95 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791346485314872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03791346485314872 | validation: 0.14188504527947324]
	TIME [epoch: 5.96 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05275763220346082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05275763220346082 | validation: 0.3878087402224762]
	TIME [epoch: 5.98 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06598737415676845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06598737415676845 | validation: 0.09126032518730212]
	TIME [epoch: 5.99 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03638136789449814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03638136789449814 | validation: 0.1017636007629931]
	TIME [epoch: 6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04154138358585131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04154138358585131 | validation: 0.16871514961841563]
	TIME [epoch: 6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042863763970105356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042863763970105356 | validation: 0.11993193544235653]
	TIME [epoch: 6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047090437576174086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047090437576174086 | validation: 0.10004889300056968]
	TIME [epoch: 5.99 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059631866254848856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059631866254848856 | validation: 0.1405923591410208]
	TIME [epoch: 5.98 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10559290358618585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10559290358618585 | validation: 0.1414043337023871]
	TIME [epoch: 5.99 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06747633491299207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06747633491299207 | validation: 0.08082530253220552]
	TIME [epoch: 6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047483778525037434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047483778525037434 | validation: 0.1756618761285204]
	TIME [epoch: 5.99 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07322462128987807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07322462128987807 | validation: 0.1264259466619567]
	TIME [epoch: 5.99 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052380464778309016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052380464778309016 | validation: 0.18111967307666058]
	TIME [epoch: 5.99 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06264842832538872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06264842832538872 | validation: 0.09088339827112009]
	TIME [epoch: 6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05578725248664666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05578725248664666 | validation: 0.07218752272331258]
	TIME [epoch: 5.99 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03144622458795995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03144622458795995 | validation: 0.06228407963365386]
	TIME [epoch: 5.99 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04462421580696477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04462421580696477 | validation: 0.11315756092689871]
	TIME [epoch: 5.99 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0460084327488433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0460084327488433 | validation: 0.1662144767932018]
	TIME [epoch: 5.99 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043532581695892365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043532581695892365 | validation: 0.08989743142696732]
	TIME [epoch: 5.99 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04836115412015983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04836115412015983 | validation: 0.07990265545560812]
	TIME [epoch: 5.99 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03384662081446993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03384662081446993 | validation: 0.09671292994078766]
	TIME [epoch: 5.99 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765816404097036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04765816404097036 | validation: 0.12375730876593308]
	TIME [epoch: 5.99 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054321304056594466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054321304056594466 | validation: 0.07325769045224505]
	TIME [epoch: 5.99 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04584857095407949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04584857095407949 | validation: 0.06886378785990475]
	TIME [epoch: 5.99 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03632502343103647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03632502343103647 | validation: 0.06824445600782407]
	TIME [epoch: 5.99 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022574543472157628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022574543472157628 | validation: 0.056675878107803605]
	TIME [epoch: 6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02090536699766655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02090536699766655 | validation: 0.16555329186779622]
	TIME [epoch: 5.99 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03190336097842567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03190336097842567 | validation: 0.14184497336687502]
	TIME [epoch: 5.99 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05995906164637365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05995906164637365 | validation: 0.44847001557910215]
	TIME [epoch: 6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11310618950993702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11310618950993702 | validation: 0.12804127896375314]
	TIME [epoch: 5.97 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07955092613451638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07955092613451638 | validation: 0.09119835938264333]
	TIME [epoch: 5.99 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05418607601135773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05418607601135773 | validation: 0.16240253707998314]
	TIME [epoch: 5.96 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03394729329304771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03394729329304771 | validation: 0.06108544006893657]
	TIME [epoch: 5.98 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03265493262459822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03265493262459822 | validation: 0.06658752425515302]
	TIME [epoch: 5.96 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0393191342550778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0393191342550778 | validation: 0.11820468675132947]
	TIME [epoch: 5.98 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1215899915641618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1215899915641618 | validation: 0.2792084323052239]
	TIME [epoch: 5.95 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06300174638779955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06300174638779955 | validation: 0.14136633318175265]
	TIME [epoch: 5.98 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718488997149697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718488997149697 | validation: 0.1434248559074901]
	TIME [epoch: 5.96 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953480959665314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07953480959665314 | validation: 0.4060511108667688]
	TIME [epoch: 5.99 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09898005863745973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09898005863745973 | validation: 0.10977135076998734]
	TIME [epoch: 5.97 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04412570891229324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04412570891229324 | validation: 0.06935398696073039]
	TIME [epoch: 5.99 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03964345708997253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03964345708997253 | validation: 0.08370846269035392]
	TIME [epoch: 6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05827637413621589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05827637413621589 | validation: 0.08184814490423853]
	TIME [epoch: 5.99 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039456665484613664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039456665484613664 | validation: 0.07222977973541092]
	TIME [epoch: 5.99 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023635310113527996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023635310113527996 | validation: 0.060645899133732674]
	TIME [epoch: 5.99 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028009861274660316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028009861274660316 | validation: 0.0919715946984591]
	TIME [epoch: 5.94 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03303123995519111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03303123995519111 | validation: 0.08829115436359967]
	TIME [epoch: 5.98 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04289670548115004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04289670548115004 | validation: 0.06921580203491642]
	TIME [epoch: 5.95 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049605786117341184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049605786117341184 | validation: 0.05665178530576921]
	TIME [epoch: 5.99 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0388165036720717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0388165036720717 | validation: 0.08159124287821595]
	TIME [epoch: 5.96 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03703084729155501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03703084729155501 | validation: 0.09214001394541028]
	TIME [epoch: 5.99 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04780142702889804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04780142702889804 | validation: 0.09462202345065412]
	TIME [epoch: 5.96 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06318495886935889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06318495886935889 | validation: 0.07939260875030807]
	TIME [epoch: 5.98 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05632615331224894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05632615331224894 | validation: 0.05246867141369521]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_757.pth
	Model improved!!!
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023292851712153288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023292851712153288 | validation: 0.08921020982758676]
	TIME [epoch: 5.95 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026211696724299252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026211696724299252 | validation: 0.10770704234409174]
	TIME [epoch: 5.96 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537342100796253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03537342100796253 | validation: 0.3826595119716927]
	TIME [epoch: 5.98 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06982370392196206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06982370392196206 | validation: 0.2054402913533187]
	TIME [epoch: 5.99 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0919294380340062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0919294380340062 | validation: 0.16709236111845296]
	TIME [epoch: 6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09381679629610475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09381679629610475 | validation: 0.3102118454304636]
	TIME [epoch: 5.99 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057823529497039595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057823529497039595 | validation: 0.33770043431568825]
	TIME [epoch: 5.98 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2770791433643536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2770791433643536 | validation: 0.1962868691723059]
	TIME [epoch: 5.98 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11318745223387998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11318745223387998 | validation: 0.30366862980147125]
	TIME [epoch: 5.98 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053049155261112604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053049155261112604 | validation: 0.15196312488516323]
	TIME [epoch: 5.98 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04389820378067002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04389820378067002 | validation: 0.22631782708886228]
	TIME [epoch: 5.98 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028938059480469232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028938059480469232 | validation: 0.12227927707223762]
	TIME [epoch: 5.99 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025830346996998346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025830346996998346 | validation: 0.15975693152762918]
	TIME [epoch: 5.98 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029322297712522084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029322297712522084 | validation: 0.10168474125493282]
	TIME [epoch: 5.98 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02831498507204869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02831498507204869 | validation: 0.17140979396108644]
	TIME [epoch: 5.98 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03758580214538608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03758580214538608 | validation: 0.15297288411869656]
	TIME [epoch: 5.98 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04956513451233201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04956513451233201 | validation: 0.37346438005208704]
	TIME [epoch: 5.98 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09669836453998251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09669836453998251 | validation: 0.1024223936065742]
	TIME [epoch: 5.97 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037877687964368544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037877687964368544 | validation: 0.08167695737890823]
	TIME [epoch: 5.98 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02198926543765924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02198926543765924 | validation: 0.10171730718559996]
	TIME [epoch: 5.97 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022625478810135366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022625478810135366 | validation: 0.08465394949199419]
	TIME [epoch: 5.98 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03785105012803234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03785105012803234 | validation: 0.09709126211940025]
	TIME [epoch: 5.98 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06238944188221963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06238944188221963 | validation: 0.09026681044483494]
	TIME [epoch: 5.98 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05453293325617332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05453293325617332 | validation: 0.09139638896221715]
	TIME [epoch: 5.98 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052559968009389425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052559968009389425 | validation: 0.06904426116444963]
	TIME [epoch: 5.99 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024978916713956717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024978916713956717 | validation: 0.06078684960047198]
	TIME [epoch: 6.01 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026975754193463344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026975754193463344 | validation: 0.06049944926023382]
	TIME [epoch: 5.98 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0405625845732948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0405625845732948 | validation: 0.10801445829893781]
	TIME [epoch: 5.98 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04581393066558293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04581393066558293 | validation: 0.1132054592971606]
	TIME [epoch: 5.98 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04117683463144033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04117683463144033 | validation: 0.13887634395943424]
	TIME [epoch: 5.98 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594082044241644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04594082044241644 | validation: 0.07220852011633588]
	TIME [epoch: 5.98 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031096093624336647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031096093624336647 | validation: 0.08374279821029991]
	TIME [epoch: 5.98 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01952241779012473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01952241779012473 | validation: 0.08174877276382891]
	TIME [epoch: 5.98 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01995355801849292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01995355801849292 | validation: 0.11365350252128761]
	TIME [epoch: 5.97 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021226366094568867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021226366094568867 | validation: 0.050380544623726875]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018194902770868816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018194902770868816 | validation: 0.06420357804764647]
	TIME [epoch: 5.94 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026427586340579925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026427586340579925 | validation: 0.11248935719200803]
	TIME [epoch: 5.95 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0534368653007331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0534368653007331 | validation: 0.1349380929804463]
	TIME [epoch: 5.95 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08923998865346718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08923998865346718 | validation: 0.14239064527903353]
	TIME [epoch: 5.95 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10609333467297248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10609333467297248 | validation: 0.10966140194882734]
	TIME [epoch: 5.95 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03962220443260052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03962220443260052 | validation: 0.10348073696821092]
	TIME [epoch: 5.95 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028309314014100768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028309314014100768 | validation: 0.1982946293721001]
	TIME [epoch: 5.95 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041356684579099696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041356684579099696 | validation: 0.04810202045645965]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022350348827753993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022350348827753993 | validation: 0.090254644111972]
	TIME [epoch: 6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035279494963063825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035279494963063825 | validation: 0.30547876169626875]
	TIME [epoch: 5.97 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654109426782106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09654109426782106 | validation: 0.4488700899414586]
	TIME [epoch: 5.98 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08100289576674921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08100289576674921 | validation: 0.14509380588356638]
	TIME [epoch: 5.97 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053154430444465585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053154430444465585 | validation: 0.07735595107182647]
	TIME [epoch: 5.97 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049467215651386134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049467215651386134 | validation: 0.06361399215810642]
	TIME [epoch: 5.97 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03840906531582314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03840906531582314 | validation: 0.0549146629489383]
	TIME [epoch: 5.98 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013802266425975944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013802266425975944 | validation: 0.052418733665978025]
	TIME [epoch: 5.98 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01878019186865533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01878019186865533 | validation: 0.0715566595660741]
	TIME [epoch: 5.98 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021916202800794836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021916202800794836 | validation: 0.0513778415903125]
	TIME [epoch: 5.95 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025447118466406266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025447118466406266 | validation: 0.0940810068552661]
	TIME [epoch: 5.95 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04375307420781173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04375307420781173 | validation: 0.11707880020684042]
	TIME [epoch: 5.95 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05764196389654317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05764196389654317 | validation: 0.3164104621596143]
	TIME [epoch: 5.96 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059731280567008334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059731280567008334 | validation: 0.06415584532528512]
	TIME [epoch: 5.96 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040653855160593375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040653855160593375 | validation: 0.056065415497615005]
	TIME [epoch: 5.95 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035668051538890945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035668051538890945 | validation: 0.07187938940654032]
	TIME [epoch: 5.95 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03729014051963355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03729014051963355 | validation: 0.1371842262244883]
	TIME [epoch: 5.95 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707233962774175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09707233962774175 | validation: 0.11785359874136657]
	TIME [epoch: 5.99 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874539089281889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07874539089281889 | validation: 0.05664917242847459]
	TIME [epoch: 5.95 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02498976181057067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02498976181057067 | validation: 0.04710032536162259]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_820.pth
	Model improved!!!
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025964832530430314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025964832530430314 | validation: 0.09825595479269458]
	TIME [epoch: 5.96 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025346761917283248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025346761917283248 | validation: 0.06563163101506435]
	TIME [epoch: 5.96 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021222552871738465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021222552871738465 | validation: 0.06914656863516291]
	TIME [epoch: 5.96 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0240772944613995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0240772944613995 | validation: 0.04534386534851795]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025125536574216643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025125536574216643 | validation: 0.07526325876596192]
	TIME [epoch: 5.95 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218424616893367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06218424616893367 | validation: 0.10676772234500764]
	TIME [epoch: 5.95 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038056408113671426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038056408113671426 | validation: 0.09231513799883426]
	TIME [epoch: 5.95 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0325653808053986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0325653808053986 | validation: 0.15672248062620356]
	TIME [epoch: 5.95 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007469307989839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04007469307989839 | validation: 0.0961785082537799]
	TIME [epoch: 5.95 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04933993373085596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04933993373085596 | validation: 0.09969527268562291]
	TIME [epoch: 5.95 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04422347975528581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04422347975528581 | validation: 0.05131270108392171]
	TIME [epoch: 5.97 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023022615992104933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023022615992104933 | validation: 0.1932961099711189]
	TIME [epoch: 5.95 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10182986705921789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10182986705921789 | validation: 0.2068742719426776]
	TIME [epoch: 5.96 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16275891376342724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16275891376342724 | validation: 0.2160478768710279]
	TIME [epoch: 5.95 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09871519164375393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09871519164375393 | validation: 0.5991568483087216]
	TIME [epoch: 5.96 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14696500917980765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14696500917980765 | validation: 0.24163147102927596]
	TIME [epoch: 5.96 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09209872701187616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09209872701187616 | validation: 0.10338888092873595]
	TIME [epoch: 5.95 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881602273949738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05881602273949738 | validation: 0.06664954450436288]
	TIME [epoch: 5.96 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04169876578080169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04169876578080169 | validation: 0.11970848368019255]
	TIME [epoch: 5.95 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608140476101312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06608140476101312 | validation: 0.21005461011776957]
	TIME [epoch: 5.95 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05390630076887147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05390630076887147 | validation: 0.08031971431631814]
	TIME [epoch: 5.96 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0264118123618972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0264118123618972 | validation: 0.06775973796139093]
	TIME [epoch: 5.95 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01988626728124451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01988626728124451 | validation: 0.04355173244778489]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018387698601256434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018387698601256434 | validation: 0.055668537508607056]
	TIME [epoch: 5.95 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018813587776368518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018813587776368518 | validation: 0.09547376625395515]
	TIME [epoch: 5.95 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02205326188742239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02205326188742239 | validation: 0.06031577667572724]
	TIME [epoch: 5.96 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024612181895888714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024612181895888714 | validation: 0.05161975247920835]
	TIME [epoch: 5.96 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02813969606288797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02813969606288797 | validation: 0.06193229948133325]
	TIME [epoch: 5.98 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03834445819441439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03834445819441439 | validation: 0.10646390256630811]
	TIME [epoch: 5.96 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455292926452946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455292926452946 | validation: 0.11478664442123732]
	TIME [epoch: 5.97 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21645627033470705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21645627033470705 | validation: 0.08913730664405567]
	TIME [epoch: 5.96 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057791024258696554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057791024258696554 | validation: 0.10114622353189047]
	TIME [epoch: 5.97 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09391498522429942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09391498522429942 | validation: 0.11966321938218477]
	TIME [epoch: 5.95 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06453032539547823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06453032539547823 | validation: 0.2620593797673962]
	TIME [epoch: 5.95 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06717859210129193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06717859210129193 | validation: 0.07587936348579905]
	TIME [epoch: 5.96 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031053316854813984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031053316854813984 | validation: 0.06109383241149833]
	TIME [epoch: 5.96 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03202342056606031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03202342056606031 | validation: 0.04389345984450438]
	TIME [epoch: 5.96 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02289542087762867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02289542087762867 | validation: 0.046688987501006765]
	TIME [epoch: 5.97 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024929391323055466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024929391323055466 | validation: 0.04801128106826907]
	TIME [epoch: 5.95 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02143691127502226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02143691127502226 | validation: 0.042745022675539625]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_860.pth
	Model improved!!!
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02248570881022744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02248570881022744 | validation: 0.054241717297592466]
	TIME [epoch: 5.95 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025585327660172137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025585327660172137 | validation: 0.06496287333627353]
	TIME [epoch: 5.97 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043916009776799346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043916009776799346 | validation: 0.0847796960477642]
	TIME [epoch: 5.96 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06542547077585839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06542547077585839 | validation: 0.07525465211020625]
	TIME [epoch: 5.96 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05598081945275385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05598081945275385 | validation: 0.06451313027498416]
	TIME [epoch: 5.95 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02635900940568754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02635900940568754 | validation: 0.0440196913204656]
	TIME [epoch: 5.97 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019154828634897418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019154828634897418 | validation: 0.07620417040299698]
	TIME [epoch: 5.96 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02975633516998662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02975633516998662 | validation: 0.05674061220865998]
	TIME [epoch: 5.97 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022335127292923633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022335127292923633 | validation: 0.07617361771981399]
	TIME [epoch: 5.95 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023486475726290965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023486475726290965 | validation: 0.04856576223897877]
	TIME [epoch: 5.96 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021105435763362827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021105435763362827 | validation: 0.040254442217331314]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_871.pth
	Model improved!!!
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016566630175409842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016566630175409842 | validation: 0.06325965910416305]
	TIME [epoch: 5.96 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027623643793519177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027623643793519177 | validation: 0.06855983589512375]
	TIME [epoch: 5.97 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05952915239167048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05952915239167048 | validation: 0.17745275642309788]
	TIME [epoch: 5.96 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08360007014356277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08360007014356277 | validation: 0.15074587275599388]
	TIME [epoch: 5.95 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555611903323983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07555611903323983 | validation: 0.10577077477250825]
	TIME [epoch: 5.97 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08544669569184198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08544669569184198 | validation: 0.08161951476142765]
	TIME [epoch: 5.97 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050137706317711694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050137706317711694 | validation: 0.0696641912096784]
	TIME [epoch: 5.96 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026812460063241755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026812460063241755 | validation: 0.1594159546864325]
	TIME [epoch: 5.96 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03825257523739977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03825257523739977 | validation: 0.075464876960441]
	TIME [epoch: 5.95 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029766652861638425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029766652861638425 | validation: 0.034893899195408196]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_881.pth
	Model improved!!!
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016721328613357157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016721328613357157 | validation: 0.03394061419857535]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_882.pth
	Model improved!!!
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016104686590447635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016104686590447635 | validation: 0.046118967983675624]
	TIME [epoch: 5.96 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01960405924174965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01960405924174965 | validation: 0.04984462292461073]
	TIME [epoch: 5.96 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026519711035087257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026519711035087257 | validation: 0.1029950446901367]
	TIME [epoch: 5.95 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045092460688022855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045092460688022855 | validation: 0.09490253727114151]
	TIME [epoch: 5.95 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1363064199280478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1363064199280478 | validation: 0.06399997747851828]
	TIME [epoch: 5.95 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04310137517071849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04310137517071849 | validation: 0.06887869676783769]
	TIME [epoch: 5.96 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05918819900014408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05918819900014408 | validation: 0.1061573464891334]
	TIME [epoch: 5.96 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04703887507288599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04703887507288599 | validation: 0.3137362756079662]
	TIME [epoch: 5.96 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055067235632554556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055067235632554556 | validation: 0.0999351272732948]
	TIME [epoch: 5.96 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0316049307820571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0316049307820571 | validation: 0.07101211190920469]
	TIME [epoch: 5.96 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040699358439739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040699358439739 | validation: 0.05227304312435867]
	TIME [epoch: 5.96 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026474401708546404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026474401708546404 | validation: 0.0725103573778816]
	TIME [epoch: 5.96 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020063935653382936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020063935653382936 | validation: 0.03802406258920979]
	TIME [epoch: 5.95 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020916379252873055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020916379252873055 | validation: 0.048606838536406266]
	TIME [epoch: 5.96 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020393655283720297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020393655283720297 | validation: 0.17011814974232642]
	TIME [epoch: 5.96 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06773169404533819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06773169404533819 | validation: 0.5086267533558838]
	TIME [epoch: 5.96 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373043134700095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373043134700095 | validation: 0.24096011829665215]
	TIME [epoch: 5.96 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10310125510767108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10310125510767108 | validation: 0.0791847419871983]
	TIME [epoch: 5.95 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055469203381603716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055469203381603716 | validation: 0.07562971277645347]
	TIME [epoch: 5.97 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04248949678176408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04248949678176408 | validation: 0.07559300230559862]
	TIME [epoch: 5.96 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04277285436146583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04277285436146583 | validation: 0.039421987202858756]
	TIME [epoch: 5.96 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016971024636841834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016971024636841834 | validation: 0.04375811120545408]
	TIME [epoch: 5.95 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011636207661595758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011636207661595758 | validation: 0.04785989104134778]
	TIME [epoch: 5.95 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012693615761897316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012693615761897316 | validation: 0.044785149552441846]
	TIME [epoch: 5.95 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014245997273907206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014245997273907206 | validation: 0.04637599853369356]
	TIME [epoch: 5.95 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02545434926952597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02545434926952597 | validation: 0.10108360234935132]
	TIME [epoch: 5.95 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08835308567037792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08835308567037792 | validation: 0.0798264200091341]
	TIME [epoch: 5.96 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055047922261437544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055047922261437544 | validation: 0.047079584144999104]
	TIME [epoch: 5.96 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03158208005575125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03158208005575125 | validation: 0.054891519843998565]
	TIME [epoch: 5.96 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02156540582422096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02156540582422096 | validation: 0.044190811675379076]
	TIME [epoch: 5.96 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026427795542480998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026427795542480998 | validation: 0.11817599004610502]
	TIME [epoch: 5.97 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03542411860051324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03542411860051324 | validation: 0.111041437611295]
	TIME [epoch: 5.96 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404585088506126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0404585088506126 | validation: 0.21536350581719807]
	TIME [epoch: 5.95 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042240261343971436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042240261343971436 | validation: 0.04735490749454622]
	TIME [epoch: 5.95 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019321818352855336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019321818352855336 | validation: 0.1384228223537727]
	TIME [epoch: 5.96 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043600749999194284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043600749999194284 | validation: 0.1488590731692971]
	TIME [epoch: 5.95 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03284534898951908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03284534898951908 | validation: 0.041756321814159]
	TIME [epoch: 5.96 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01911124408386978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01911124408386978 | validation: 0.07261519967084838]
	TIME [epoch: 5.95 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04189984359741688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04189984359741688 | validation: 0.08364968701623919]
	TIME [epoch: 5.96 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086455599855429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086455599855429 | validation: 0.15606499462055404]
	TIME [epoch: 5.95 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08288818296579292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08288818296579292 | validation: 0.05787653014084644]
	TIME [epoch: 5.95 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029285984762585343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029285984762585343 | validation: 0.049762338995605615]
	TIME [epoch: 5.95 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020041291541669636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020041291541669636 | validation: 0.09593549623942775]
	TIME [epoch: 5.95 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02382215091741281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02382215091741281 | validation: 0.04960722881339046]
	TIME [epoch: 5.95 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018628122318710746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018628122318710746 | validation: 0.04118030505206698]
	TIME [epoch: 5.96 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020838363212746715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020838363212746715 | validation: 0.044576303623177174]
	TIME [epoch: 5.96 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951117733089391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01951117733089391 | validation: 0.033202353400898166]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_929.pth
	Model improved!!!
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016113117161109643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016113117161109643 | validation: 0.042127425014649666]
	TIME [epoch: 5.95 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020113638687978157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020113638687978157 | validation: 0.061644589572420444]
	TIME [epoch: 5.96 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03529240848188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03529240848188 | validation: 0.1389416652108798]
	TIME [epoch: 5.95 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06824096539119355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06824096539119355 | validation: 0.15725174144610546]
	TIME [epoch: 5.96 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08157462224011518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08157462224011518 | validation: 0.15768041715131464]
	TIME [epoch: 5.95 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0366434522857709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0366434522857709 | validation: 0.14794836790465787]
	TIME [epoch: 5.95 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033080440678728215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033080440678728215 | validation: 0.10010961641293369]
	TIME [epoch: 5.96 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927459508453739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03927459508453739 | validation: 0.07357069431344887]
	TIME [epoch: 5.96 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332223623972354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332223623972354 | validation: 0.08150793726041404]
	TIME [epoch: 5.95 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02631062376353664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02631062376353664 | validation: 0.0696287623725894]
	TIME [epoch: 5.96 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035786027993449855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035786027993449855 | validation: 0.10541782642174798]
	TIME [epoch: 5.98 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04119857908082974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04119857908082974 | validation: 0.08213118454481569]
	TIME [epoch: 5.96 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400311079706287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03400311079706287 | validation: 0.05878505222678252]
	TIME [epoch: 5.96 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025769622348974425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025769622348974425 | validation: 0.04584518842509239]
	TIME [epoch: 5.96 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02353961031251993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02353961031251993 | validation: 0.06321346905069285]
	TIME [epoch: 5.95 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0911692454871324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0911692454871324 | validation: 0.052512187451492325]
	TIME [epoch: 5.95 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168868689716985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03168868689716985 | validation: 0.08980797004908574]
	TIME [epoch: 5.95 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045463320805480115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045463320805480115 | validation: 0.138133638325266]
	TIME [epoch: 5.95 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08117902935929121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08117902935929121 | validation: 0.21470887999173863]
	TIME [epoch: 5.95 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07369436824594294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07369436824594294 | validation: 0.17654245608402125]
	TIME [epoch: 5.96 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06735720815638964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06735720815638964 | validation: 0.20984943331135952]
	TIME [epoch: 5.95 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05324153146723381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05324153146723381 | validation: 0.18042986024744592]
	TIME [epoch: 5.96 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031028601895162444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031028601895162444 | validation: 0.14889435830154416]
	TIME [epoch: 5.95 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03082685971627668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03082685971627668 | validation: 0.08311701397821776]
	TIME [epoch: 5.96 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0233214718504005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0233214718504005 | validation: 0.12837617666361725]
	TIME [epoch: 5.96 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029168261162628304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029168261162628304 | validation: 0.1395907325783057]
	TIME [epoch: 5.95 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03840649725839756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03840649725839756 | validation: 0.15898706479837332]
	TIME [epoch: 5.95 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044330145781845916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044330145781845916 | validation: 0.07002756140670252]
	TIME [epoch: 5.96 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041940111239441795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041940111239441795 | validation: 0.05327546463673127]
	TIME [epoch: 5.96 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025229491580700487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025229491580700487 | validation: 0.04101911038236264]
	TIME [epoch: 5.95 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02263568227579097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02263568227579097 | validation: 0.04527822403354078]
	TIME [epoch: 5.95 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01949889532112131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01949889532112131 | validation: 0.08555967772254584]
	TIME [epoch: 5.96 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019616940757711733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019616940757711733 | validation: 0.2691529294688861]
	TIME [epoch: 5.96 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1301951410824542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1301951410824542 | validation: 0.3064437726579177]
	TIME [epoch: 5.97 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054788653591208866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054788653591208866 | validation: 0.2841771774691965]
	TIME [epoch: 5.96 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0524445656531868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0524445656531868 | validation: 0.42220025801691197]
	TIME [epoch: 5.95 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06604468221446413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06604468221446413 | validation: 0.17738914829023847]
	TIME [epoch: 5.95 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625396689205092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625396689205092 | validation: 0.3487381752938341]
	TIME [epoch: 5.95 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06941710141196375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06941710141196375 | validation: 0.114687675528636]
	TIME [epoch: 5.96 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02234775963556209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02234775963556209 | validation: 0.07904156062880931]
	TIME [epoch: 5.96 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02361193873993981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02361193873993981 | validation: 0.07546043201068627]
	TIME [epoch: 5.95 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03197590924194191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03197590924194191 | validation: 0.04898803259828737]
	TIME [epoch: 5.96 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020692658899384108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020692658899384108 | validation: 0.05884744209840183]
	TIME [epoch: 5.96 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026441998077610673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026441998077610673 | validation: 0.0815981212257052]
	TIME [epoch: 5.95 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043547353854443624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043547353854443624 | validation: 0.06872943447286617]
	TIME [epoch: 5.95 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050612487889258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05050612487889258 | validation: 0.07549909536645041]
	TIME [epoch: 5.95 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031178673084101806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031178673084101806 | validation: 0.04478010453036483]
	TIME [epoch: 5.95 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015592100167508432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015592100167508432 | validation: 0.03397985406214974]
	TIME [epoch: 5.95 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013706908222500192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013706908222500192 | validation: 0.05085018631289315]
	TIME [epoch: 5.95 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013173022511842988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013173022511842988 | validation: 0.049333067309356785]
	TIME [epoch: 5.96 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018786634350743854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018786634350743854 | validation: 0.17806320689605354]
	TIME [epoch: 5.95 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439929098624045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04439929098624045 | validation: 0.16916721839373944]
	TIME [epoch: 5.97 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524493002301114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524493002301114 | validation: 0.10651962192963094]
	TIME [epoch: 5.96 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04062632356731836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04062632356731836 | validation: 0.04337666844555722]
	TIME [epoch: 5.96 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019086119123033266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019086119123033266 | validation: 0.09651814867448298]
	TIME [epoch: 5.95 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031846118622371196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031846118622371196 | validation: 0.19784665799391654]
	TIME [epoch: 5.97 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04470171217652924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04470171217652924 | validation: 0.07664888099521933]
	TIME [epoch: 5.96 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048238789404005424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048238789404005424 | validation: 0.07847164857041142]
	TIME [epoch: 5.95 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06585133061944586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06585133061944586 | validation: 0.08348860049379118]
	TIME [epoch: 5.96 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310700015462559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06310700015462559 | validation: 0.0698437276056832]
	TIME [epoch: 5.95 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028093039859548453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028093039859548453 | validation: 0.05609527528485269]
	TIME [epoch: 5.95 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01619495818442237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01619495818442237 | validation: 0.050581473852830766]
	TIME [epoch: 5.96 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01776763589883682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01776763589883682 | validation: 0.05572535740228614]
	TIME [epoch: 5.96 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02191549485654601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02191549485654601 | validation: 0.042837765348071634]
	TIME [epoch: 5.95 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013883271571419775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013883271571419775 | validation: 0.04175793488637211]
	TIME [epoch: 5.95 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.008671228461346858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.008671228461346858 | validation: 0.04571981581300913]
	TIME [epoch: 5.95 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011398962273890076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011398962273890076 | validation: 0.03781035245959455]
	TIME [epoch: 5.96 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014830929212273244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014830929212273244 | validation: 0.06723012112281312]
	TIME [epoch: 5.96 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034552859637524504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034552859637524504 | validation: 0.15122537809755518]
	TIME [epoch: 5.95 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08657397844629326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08657397844629326 | validation: 0.08861083245816898]
	TIME [epoch: 5.96 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0628551569835963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0628551569835963 | validation: 0.0808183465348416]
	TIME [epoch: 5.95 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556100701125487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04556100701125487 | validation: 0.05960458497609908]
	TIME [epoch: 192 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05304236940129135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05304236940129135 | validation: 0.0555080028738298]
	TIME [epoch: 12.6 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034789797603423225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034789797603423225 | validation: 0.05695539837325822]
	TIME [epoch: 12.6 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0372664180013656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0372664180013656 | validation: 0.04884796060904983]
	TIME [epoch: 12.6 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022551786163940372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022551786163940372 | validation: 0.05106946197253801]
	TIME [epoch: 12.6 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020083355530556807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020083355530556807 | validation: 0.06462450640935484]
	TIME [epoch: 12.6 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01716266248037691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01716266248037691 | validation: 0.05025992952893593]
	TIME [epoch: 12.6 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014543463724422251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014543463724422251 | validation: 0.08665422379065675]
	TIME [epoch: 12.6 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017807837833800914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017807837833800914 | validation: 0.03406763729851208]
	TIME [epoch: 12.6 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031284513617466245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031284513617466245 | validation: 0.0366626384291668]
	TIME [epoch: 12.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03227340792859466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03227340792859466 | validation: 0.053878751359547034]
	TIME [epoch: 12.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02736675510030871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02736675510030871 | validation: 0.1463521281607066]
	TIME [epoch: 12.6 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03261423001927546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03261423001927546 | validation: 0.2502376401514894]
	TIME [epoch: 12.6 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1368824082945812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1368824082945812 | validation: 0.11815612129987328]
	TIME [epoch: 12.6 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07216480813426122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07216480813426122 | validation: 0.526515888830386]
	TIME [epoch: 12.6 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11497071329676384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11497071329676384 | validation: 0.2511944697994977]
	TIME [epoch: 12.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031726413448189994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031726413448189994 | validation: 0.12423539728459952]
	TIME [epoch: 12.6 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03171039775917305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03171039775917305 | validation: 0.08080713017398285]
	TIME [epoch: 12.6 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030107053958679957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030107053958679957 | validation: 0.0744611449405246]
	TIME [epoch: 12.6 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09562707166284877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09562707166284877 | validation: 0.07565886402848153]
	TIME [epoch: 12.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02796695058262599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02796695058262599 | validation: 0.09968305861128261]
	TIME [epoch: 12.6 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823393339615151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823393339615151 | validation: 0.13816358257277092]
	TIME [epoch: 12.6 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08893906619318977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08893906619318977 | validation: 0.1372838426763841]
	TIME [epoch: 12.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07752562278477611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07752562278477611 | validation: 0.06878714158605635]
	TIME [epoch: 12.6 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04356600193473132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04356600193473132 | validation: 0.03925304977741665]
	TIME [epoch: 12.6 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019436390481503925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019436390481503925 | validation: 0.04170522745312858]
	TIME [epoch: 12.6 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012207596187380439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012207596187380439 | validation: 0.04464063837330687]
	TIME [epoch: 12.6 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017879144666964702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017879144666964702 | validation: 0.040309207199742784]
	TIME [epoch: 12.6 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014221753971298384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014221753971298384 | validation: 0.037229811403747995]
	TIME [epoch: 12.6 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013401707828767572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013401707828767572 | validation: 0.041150776344261224]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_155314/states/model_phi1_4b_v_mmd2_1030.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5277.538 seconds.
