Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 4261848397

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.565458850876933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.565458850876933 | validation: 4.651404565476805]
	TIME [epoch: 172 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.857692547260963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.857692547260963 | validation: 5.502405652388117]
	TIME [epoch: 2.83 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.433830023561394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.433830023561394 | validation: 4.377877281483721]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.227489251090052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.227489251090052 | validation: 5.317367635508164]
	TIME [epoch: 2.8 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.537810957689369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.537810957689369 | validation: 4.331861000507671]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.597840004139164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.597840004139164 | validation: 4.166050498527636]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.47123253609921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.47123253609921 | validation: 4.086865485902459]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.390086596397915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.390086596397915 | validation: 3.8645816737687984]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.226516841592831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.226516841592831 | validation: 3.625486488473729]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.090754985252463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.090754985252463 | validation: 3.478721615967013]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9553413414120002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9553413414120002 | validation: 3.399089224681058]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.821167464836946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.821167464836946 | validation: 3.143342142863089]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6855491810078993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6855491810078993 | validation: 2.986715203795659]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5311420732741783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5311420732741783 | validation: 2.693034354907084]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.335972295631754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.335972295631754 | validation: 2.019906702626702]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9408663878236783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9408663878236783 | validation: 2.635388778519328]
	TIME [epoch: 2.81 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1683413706390606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1683413706390606 | validation: 2.39002446001491]
	TIME [epoch: 2.81 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1976286417354225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1976286417354225 | validation: 1.8744346360662367]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.431095859961952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.431095859961952 | validation: 1.700547777785827]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.08780627155726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.08780627155726 | validation: 1.6024978184818484]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.897856198527565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.897856198527565 | validation: 1.6232309616066012]
	TIME [epoch: 2.81 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0886055433112847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0886055433112847 | validation: 2.0455256451418555]
	TIME [epoch: 2.81 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.665027210469711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.665027210469711 | validation: 1.9974328359430524]
	TIME [epoch: 2.81 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4076951692920114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4076951692920114 | validation: 1.9071544566110825]
	TIME [epoch: 2.81 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3372365330983955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3372365330983955 | validation: 1.4055483287140245]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8905063788208927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8905063788208927 | validation: 1.265356446404897]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6463438242248694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6463438242248694 | validation: 1.2937011121797806]
	TIME [epoch: 2.81 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.623454081626556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.623454081626556 | validation: 1.0614267744475163]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.403769475856418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.403769475856418 | validation: 1.016751736010882]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3556064195303419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3556064195303419 | validation: 0.965617549584173]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.323722603935165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.323722603935165 | validation: 0.9685746979709642]
	TIME [epoch: 2.81 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3498961444538093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3498961444538093 | validation: 1.0533429057404677]
	TIME [epoch: 2.8 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3800914774800976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3800914774800976 | validation: 0.871352864705157]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.197856957749456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.197856957749456 | validation: 0.9707044355355907]
	TIME [epoch: 2.82 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2350157819264034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2350157819264034 | validation: 0.9410262464502694]
	TIME [epoch: 2.82 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2163981199776146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2163981199776146 | validation: 0.8584790489677012]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1158584416134918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1158584416134918 | validation: 0.9030770441778627]
	TIME [epoch: 2.8 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1591239442088153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1591239442088153 | validation: 0.8717814680406998]
	TIME [epoch: 2.8 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1003924121892055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1003924121892055 | validation: 0.8184763269829877]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0635619212644545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0635619212644545 | validation: 0.8176322168708868]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.058426213245177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.058426213245177 | validation: 0.8614500255419753]
	TIME [epoch: 2.81 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2111990828040664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2111990828040664 | validation: 0.8813017908087559]
	TIME [epoch: 2.8 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0426665065032559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0426665065032559 | validation: 0.8891412793191544]
	TIME [epoch: 2.81 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1657312780733464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1657312780733464 | validation: 0.793351706200017]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0318205617117082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0318205617117082 | validation: 0.8021356081939013]
	TIME [epoch: 2.8 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0027439921636674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0027439921636674 | validation: 0.846979953911803]
	TIME [epoch: 2.8 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0346886881644104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0346886881644104 | validation: 0.7743830079359733]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9955110460588571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9955110460588571 | validation: 0.8155292796510543]
	TIME [epoch: 2.81 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0220714851001265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0220714851001265 | validation: 0.8744562854293685]
	TIME [epoch: 2.81 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0281212385516056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0281212385516056 | validation: 0.7627798653839124]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9891327420874829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9891327420874829 | validation: 0.8481795102599643]
	TIME [epoch: 2.8 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0348621259232842		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 1.0348621259232842 | validation: 0.7610882358897426]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9754442075039426		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 0.9754442075039426 | validation: 0.9173036773757737]
	TIME [epoch: 2.81 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1489248530859233		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 1.1489248530859233 | validation: 0.8430892041111546]
	TIME [epoch: 2.81 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0322869349366561		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 1.0322869349366561 | validation: 0.7693908351685561]
	TIME [epoch: 2.81 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.948439670566966		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 0.948439670566966 | validation: 0.7628005230736904]
	TIME [epoch: 2.81 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9459097068652742		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 0.9459097068652742 | validation: 0.7945023187689112]
	TIME [epoch: 2.81 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9396697175370357		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 0.9396697175370357 | validation: 0.7491094774610798]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9065509066767194		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 0.9065509066767194 | validation: 0.7441107147689754]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9146837559367275		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 0.9146837559367275 | validation: 0.8123261736694016]
	TIME [epoch: 2.83 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9286409240998919		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 0.9286409240998919 | validation: 0.7973730452377341]
	TIME [epoch: 2.82 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9349308349689794		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 0.9349308349689794 | validation: 0.7555706054011622]
	TIME [epoch: 2.82 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9506464152298348		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 0.9506464152298348 | validation: 0.7781895815026151]
	TIME [epoch: 2.82 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9165495177325778		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 0.9165495177325778 | validation: 0.8106221056786581]
	TIME [epoch: 2.82 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.083551164858122		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 1.083551164858122 | validation: 0.8295898863123329]
	TIME [epoch: 2.82 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9424645276076034		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 0.9424645276076034 | validation: 0.7208939769402118]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9017210866659667		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 0.9017210866659667 | validation: 0.7873804285215061]
	TIME [epoch: 2.8 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917348925012842		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 0.917348925012842 | validation: 0.8088366171083365]
	TIME [epoch: 2.8 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0144683109255763		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 1.0144683109255763 | validation: 0.7258575941467842]
	TIME [epoch: 2.8 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8856601417513812		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 0.8856601417513812 | validation: 0.7567416293616565]
	TIME [epoch: 2.81 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0469049211453945		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.0469049211453945 | validation: 0.7868914768173632]
	TIME [epoch: 2.81 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9482487345188428		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 0.9482487345188428 | validation: 0.8263263268464848]
	TIME [epoch: 2.82 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9950730577528721		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 0.9950730577528721 | validation: 0.7527119771023916]
	TIME [epoch: 2.81 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9543262886012067		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 0.9543262886012067 | validation: 0.7146509241892313]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830738891599584		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 0.8830738891599584 | validation: 0.716776427465494]
	TIME [epoch: 2.81 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873850751342805		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 0.873850751342805 | validation: 0.7695200449473556]
	TIME [epoch: 2.8 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8982383728976885		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 0.8982383728976885 | validation: 0.7289922608047299]
	TIME [epoch: 2.8 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8720471182041447		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 0.8720471182041447 | validation: 0.7061814062439528]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.886200770856826		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 0.886200770856826 | validation: 0.7436943215699704]
	TIME [epoch: 2.8 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011440609648839		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.9011440609648839 | validation: 0.724151655619392]
	TIME [epoch: 2.8 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717616423991951		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 0.8717616423991951 | validation: 0.7195434160712972]
	TIME [epoch: 2.8 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038069296008959		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.9038069296008959 | validation: 0.796244394749757]
	TIME [epoch: 2.8 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9410667507953983		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 0.9410667507953983 | validation: 0.7169738658883699]
	TIME [epoch: 2.8 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8603830233338642		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.8603830233338642 | validation: 0.7401199130939136]
	TIME [epoch: 2.81 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8835475641693057		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.8835475641693057 | validation: 0.7552539237329137]
	TIME [epoch: 2.81 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9326689720714884		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 0.9326689720714884 | validation: 0.8264356682686821]
	TIME [epoch: 2.81 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422871141192455		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.9422871141192455 | validation: 0.8406401709342666]
	TIME [epoch: 2.8 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0834175737024414		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 1.0834175737024414 | validation: 0.761850470825279]
	TIME [epoch: 2.8 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9180429426064284		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.9180429426064284 | validation: 0.8217373022384147]
	TIME [epoch: 2.8 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419852228450876		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9419852228450876 | validation: 0.7007170429454213]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964136509499048		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.8964136509499048 | validation: 0.6998875662119427]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746671647519044		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.8746671647519044 | validation: 0.7260669040706476]
	TIME [epoch: 2.82 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8764926316817312		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8764926316817312 | validation: 0.7695296484624652]
	TIME [epoch: 2.82 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778005878957173		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.9778005878957173 | validation: 0.6835070121147075]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85537126826601		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.85537126826601 | validation: 0.7562380666385637]
	TIME [epoch: 2.83 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891178575756714		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.891178575756714 | validation: 0.7097743168191812]
	TIME [epoch: 2.83 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756945139607243		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.8756945139607243 | validation: 0.6990445250414373]
	TIME [epoch: 2.83 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500368995062254		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8500368995062254 | validation: 0.769434319305841]
	TIME [epoch: 2.83 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9290873692326213		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.9290873692326213 | validation: 0.7102799240992708]
	TIME [epoch: 2.83 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574187306150011		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8574187306150011 | validation: 0.6886359635336293]
	TIME [epoch: 2.83 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8457478529418049		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.8457478529418049 | validation: 0.7073127028734796]
	TIME [epoch: 2.8 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526527512226497		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.8526527512226497 | validation: 0.6929754031453051]
	TIME [epoch: 2.8 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710103469036704		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8710103469036704 | validation: 0.7815220744851787]
	TIME [epoch: 2.8 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9097874947933366		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.9097874947933366 | validation: 0.7659893910699703]
	TIME [epoch: 2.8 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9529027016620389		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.9529027016620389 | validation: 0.7016423806087289]
	TIME [epoch: 2.8 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8548952343027355		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8548952343027355 | validation: 0.6744974618730402]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8379933184439873		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8379933184439873 | validation: 0.6636009730834245]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_107.pth
	Model improved!!!
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8335737257470213		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8335737257470213 | validation: 0.6959073651993699]
	TIME [epoch: 2.83 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8344096503825764		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8344096503825764 | validation: 0.7107547371769704]
	TIME [epoch: 2.82 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8924151898505295		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.8924151898505295 | validation: 0.8223559762103719]
	TIME [epoch: 2.82 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9344385320499453		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.9344385320499453 | validation: 0.9319000298301611]
	TIME [epoch: 2.83 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0194162001432292		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 1.0194162001432292 | validation: 0.6670018082610192]
	TIME [epoch: 2.82 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018908654900057		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8018908654900057 | validation: 0.7912923183686983]
	TIME [epoch: 2.82 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905971738989039		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.8905971738989039 | validation: 0.7347852864892097]
	TIME [epoch: 2.82 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8342409079929357		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8342409079929357 | validation: 0.6556032947794782]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587193629372684		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.7587193629372684 | validation: 0.6374002024256873]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549229186202131		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.7549229186202131 | validation: 0.961698020905775]
	TIME [epoch: 2.83 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0772099464608866		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 1.0772099464608866 | validation: 0.7889942938368454]
	TIME [epoch: 2.8 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8480062076623245		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.8480062076623245 | validation: 0.7572860099548819]
	TIME [epoch: 2.8 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425402963193213		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8425402963193213 | validation: 0.6697082881547506]
	TIME [epoch: 2.8 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858750021263237		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.7858750021263237 | validation: 0.643450524934067]
	TIME [epoch: 2.8 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665906676643697		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.7665906676643697 | validation: 0.5962599674952103]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7153102628003307		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.7153102628003307 | validation: 0.6811771243652802]
	TIME [epoch: 2.8 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859123895538187		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.6859123895538187 | validation: 0.9338559792084418]
	TIME [epoch: 2.8 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0100141726387952		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0100141726387952 | validation: 0.6850147100602396]
	TIME [epoch: 2.8 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745811204772813		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.745811204772813 | validation: 0.6349290564374228]
	TIME [epoch: 2.8 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.707173030222925		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.707173030222925 | validation: 0.6330595953340853]
	TIME [epoch: 2.8 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6965866130397319		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.6965866130397319 | validation: 1.2047105744844537]
	TIME [epoch: 2.8 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.107164137453357		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.107164137453357 | validation: 1.0334422438910267]
	TIME [epoch: 2.8 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2070621963479007		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 1.2070621963479007 | validation: 0.7915824981323287]
	TIME [epoch: 2.8 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9725809533591223		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9725809533591223 | validation: 0.7778516535013117]
	TIME [epoch: 2.79 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8949047283730385		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8949047283730385 | validation: 0.8126635810490769]
	TIME [epoch: 2.8 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9208598192119364		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9208598192119364 | validation: 0.705973267684237]
	TIME [epoch: 2.8 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8275255862105426		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8275255862105426 | validation: 0.6937198584920198]
	TIME [epoch: 2.8 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8253181119148718		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.8253181119148718 | validation: 0.6558487634630936]
	TIME [epoch: 2.8 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017078791785235		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.8017078791785235 | validation: 0.6239393386035442]
	TIME [epoch: 2.8 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7578367888128708		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.7578367888128708 | validation: 0.608364625864105]
	TIME [epoch: 2.8 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7107087128776622		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.7107087128776622 | validation: 0.5842242103298763]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417907423328473		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.6417907423328473 | validation: 0.6287985144895906]
	TIME [epoch: 2.8 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6310792949906151		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.6310792949906151 | validation: 0.6703543205605438]
	TIME [epoch: 2.8 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262354090851515		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7262354090851515 | validation: 0.8593277480984363]
	TIME [epoch: 2.8 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462233895053775		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8462233895053775 | validation: 0.7464628912758735]
	TIME [epoch: 2.8 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352146005159702		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8352146005159702 | validation: 0.6890687719444432]
	TIME [epoch: 2.8 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7881984869656857		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.7881984869656857 | validation: 0.7008997432330384]
	TIME [epoch: 2.8 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778920826780911		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7778920826780911 | validation: 0.6431280757714278]
	TIME [epoch: 2.79 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263712608203509		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.7263712608203509 | validation: 0.5923355238165391]
	TIME [epoch: 2.8 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.638462725913349		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.638462725913349 | validation: 0.6109876432180341]
	TIME [epoch: 2.79 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6346952994828997		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.6346952994828997 | validation: 0.7216340335620031]
	TIME [epoch: 2.8 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975942970270742		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7975942970270742 | validation: 0.5723059469445898]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6012000075212395		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6012000075212395 | validation: 0.5393619389923647]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860959282870021		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.5860959282870021 | validation: 0.5734588421974375]
	TIME [epoch: 2.8 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6062304729607898		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.6062304729607898 | validation: 0.7424273776667022]
	TIME [epoch: 2.8 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7293788409398695		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7293788409398695 | validation: 0.7776124941316072]
	TIME [epoch: 2.8 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9248632979396625		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9248632979396625 | validation: 0.6764263961245072]
	TIME [epoch: 2.8 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849809608163527		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.7849809608163527 | validation: 0.6765499914915927]
	TIME [epoch: 2.8 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638590111089275		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7638590111089275 | validation: 0.5796975466573125]
	TIME [epoch: 2.8 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6629399525872492		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.6629399525872492 | validation: 0.5173010645015674]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5468365738582541		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.5468365738582541 | validation: 0.5857021830889558]
	TIME [epoch: 2.82 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592295439570837		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.592295439570837 | validation: 0.5451951423658491]
	TIME [epoch: 2.82 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5574408278447273		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.5574408278447273 | validation: 0.6601743233400725]
	TIME [epoch: 2.81 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6939251667794623		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6939251667794623 | validation: 0.69561670207806]
	TIME [epoch: 2.81 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026417966850151		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8026417966850151 | validation: 0.6000609037831907]
	TIME [epoch: 2.81 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6942494142830081		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.6942494142830081 | validation: 0.5917146563039085]
	TIME [epoch: 2.81 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6718253173907873		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.6718253173907873 | validation: 0.5262682396116215]
	TIME [epoch: 2.81 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5528014145973357		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.5528014145973357 | validation: 0.4939271122030891]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5379224045681567		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.5379224045681567 | validation: 0.48358493835391886]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5138210966539228		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.5138210966539228 | validation: 0.47928268171583516]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5066977123056862		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.5066977123056862 | validation: 0.4871468787675204]
	TIME [epoch: 2.8 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.493238739222687		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.493238739222687 | validation: 0.48149591709556]
	TIME [epoch: 2.8 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49874499646666764		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.49874499646666764 | validation: 0.5489810558305531]
	TIME [epoch: 2.8 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5380315464213458		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.5380315464213458 | validation: 0.6886026539333536]
	TIME [epoch: 2.8 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788224694846731		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.788224694846731 | validation: 0.560127152476126]
	TIME [epoch: 2.8 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6119436432868797		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6119436432868797 | validation: 0.7726494962267433]
	TIME [epoch: 2.8 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891670251089892		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7891670251089892 | validation: 0.6990682740104055]
	TIME [epoch: 2.8 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041530119130527		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.8041530119130527 | validation: 0.5912567216919514]
	TIME [epoch: 2.8 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772799850666053		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6772799850666053 | validation: 0.5052476014059101]
	TIME [epoch: 2.8 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5787714357846947		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.5787714357846947 | validation: 0.533912133708187]
	TIME [epoch: 2.82 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5621526006658053		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.5621526006658053 | validation: 0.5980066237962053]
	TIME [epoch: 2.82 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356432753496862		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6356432753496862 | validation: 0.47816797098334174]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5120552881397468		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.5120552881397468 | validation: 0.7069337215631379]
	TIME [epoch: 2.8 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6762875307458102		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.6762875307458102 | validation: 0.5834594384357351]
	TIME [epoch: 2.8 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6558570024877257		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.6558570024877257 | validation: 0.4656754854080181]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_182.pth
	Model improved!!!
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5232190103425043		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.5232190103425043 | validation: 0.6129302445828262]
	TIME [epoch: 2.81 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6070799463546556		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6070799463546556 | validation: 0.5397207324451764]
	TIME [epoch: 2.81 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5512709094666106		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.5512709094666106 | validation: 0.4382167871200083]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48741196645668394		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.48741196645668394 | validation: 0.6182362994212977]
	TIME [epoch: 2.82 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6067098541936085		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6067098541936085 | validation: 0.5538251676489127]
	TIME [epoch: 2.81 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5859222412032429		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.5859222412032429 | validation: 0.4966077505862043]
	TIME [epoch: 2.82 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5227941966470488		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.5227941966470488 | validation: 0.6055406605035597]
	TIME [epoch: 2.81 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592527035395557		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.592527035395557 | validation: 0.4750956121160401]
	TIME [epoch: 2.81 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5109497798494518		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5109497798494518 | validation: 0.42396427118515456]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4479660215424847		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.4479660215424847 | validation: 0.4459507420695317]
	TIME [epoch: 2.82 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46480320838132666		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.46480320838132666 | validation: 0.4429593222965693]
	TIME [epoch: 2.81 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48472508260324143		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.48472508260324143 | validation: 0.4135450264226078]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4332321919115824		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.4332321919115824 | validation: 0.41189799254702797]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_195.pth
	Model improved!!!
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43392810307475743		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.43392810307475743 | validation: 0.4420972714653692]
	TIME [epoch: 2.82 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4693413528196036		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.4693413528196036 | validation: 0.489520994995486]
	TIME [epoch: 2.81 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4771089487703759		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.4771089487703759 | validation: 0.5152417476026749]
	TIME [epoch: 2.81 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.575088710486241		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.575088710486241 | validation: 0.394576086965044]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42823921738363857		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.42823921738363857 | validation: 0.5196881660245453]
	TIME [epoch: 2.81 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5096970490354764		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.5096970490354764 | validation: 0.5538360377437436]
	TIME [epoch: 183 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6208943684027277		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.6208943684027277 | validation: 0.4061183760377085]
	TIME [epoch: 6.05 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.445160753220574		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.445160753220574 | validation: 0.6930982577876794]
	TIME [epoch: 6.04 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679657426453964		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.6679657426453964 | validation: 0.5232502128224977]
	TIME [epoch: 6.04 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5861385043888958		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5861385043888958 | validation: 0.455801941174507]
	TIME [epoch: 6.05 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48901216832612665		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.48901216832612665 | validation: 0.4592869934008945]
	TIME [epoch: 6.04 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4680593725326539		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.4680593725326539 | validation: 0.37958713674866523]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41013064589443005		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.41013064589443005 | validation: 0.3654680180690869]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39187771631395096		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.39187771631395096 | validation: 0.3724612370898295]
	TIME [epoch: 6.04 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3872327657025412		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.3872327657025412 | validation: 0.35553282035458394]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3833088984557903		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.3833088984557903 | validation: 0.35084140551333787]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3783566067780247		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.3783566067780247 | validation: 0.3457172313839529]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3760250158059599		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.3760250158059599 | validation: 0.3525308007220949]
	TIME [epoch: 6.04 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3696189782562413		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.3696189782562413 | validation: 0.36857548618925473]
	TIME [epoch: 6.05 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35822848465987095		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.35822848465987095 | validation: 0.36243119679638625]
	TIME [epoch: 6.05 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36982148436444306		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.36982148436444306 | validation: 0.450122407068909]
	TIME [epoch: 6.05 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42792098944288215		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.42792098944288215 | validation: 0.5057442622133189]
	TIME [epoch: 6.04 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120776209353308		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.6120776209353308 | validation: 0.35658807074823856]
	TIME [epoch: 6.04 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3698675925521785		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.3698675925521785 | validation: 0.42411411416024447]
	TIME [epoch: 6.03 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4256818385833573		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.4256818385833573 | validation: 0.5032055456557262]
	TIME [epoch: 6.03 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571338399579872		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5571338399579872 | validation: 0.349221814965376]
	TIME [epoch: 6.04 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37873182520520343		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.37873182520520343 | validation: 0.5176890668423375]
	TIME [epoch: 6.04 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5077407151966257		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.5077407151966257 | validation: 0.46026158524257915]
	TIME [epoch: 6.07 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5121959175217731		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.5121959175217731 | validation: 0.3671819641637699]
	TIME [epoch: 6.05 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239905958306672		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.4239905958306672 | validation: 0.377491826199026]
	TIME [epoch: 6.04 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39746125457284487		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.39746125457284487 | validation: 0.31350692512615536]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34497117599484073		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.34497117599484073 | validation: 0.2992739682723889]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32093626343267767		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.32093626343267767 | validation: 0.3175450150626862]
	TIME [epoch: 6.02 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32460691023047106		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.32460691023047106 | validation: 0.3148366289191473]
	TIME [epoch: 6.01 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353120442212557		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.3353120442212557 | validation: 0.301796920812377]
	TIME [epoch: 6.01 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3177224036944847		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.3177224036944847 | validation: 0.294232552496789]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3185799938525846		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.3185799938525846 | validation: 0.3973920967565082]
	TIME [epoch: 6.03 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38179815958850827		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.38179815958850827 | validation: 0.42971372208589487]
	TIME [epoch: 6.03 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5118086610778093		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.5118086610778093 | validation: 0.2915177252722215]
	TIME [epoch: 6.03 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34098390028175657		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.34098390028175657 | validation: 0.4893534522907825]
	TIME [epoch: 6.06 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4733661070945677		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.4733661070945677 | validation: 0.41069373309384005]
	TIME [epoch: 6.05 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46338137883255626		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.46338137883255626 | validation: 0.3188675782529282]
	TIME [epoch: 6.06 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3517505799233249		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.3517505799233249 | validation: 0.3957989022568017]
	TIME [epoch: 6.05 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38245891626679046		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.38245891626679046 | validation: 0.29318789082886587]
	TIME [epoch: 6.06 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33857215244937594		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.33857215244937594 | validation: 0.27006704010135785]
	TIME [epoch: 6.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2873724522731808		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.2873724522731808 | validation: 0.28697179668137174]
	TIME [epoch: 6.05 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878031011977559		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.2878031011977559 | validation: 0.2519574258603406]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2939267512485832		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.2939267512485832 | validation: 0.28759156159871474]
	TIME [epoch: 6.01 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28668988372238374		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.28668988372238374 | validation: 0.2671119964898823]
	TIME [epoch: 6.06 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2922286373654303		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.2922286373654303 | validation: 0.2836027728594937]
	TIME [epoch: 6.06 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864159970576592		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.2864159970576592 | validation: 0.2722218384647396]
	TIME [epoch: 6.06 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29145312016194175		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.29145312016194175 | validation: 0.238819870518879]
	TIME [epoch: 6.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25330956688328027		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.25330956688328027 | validation: 0.23930249751541452]
	TIME [epoch: 6.06 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.253220838537949		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.253220838537949 | validation: 0.2729972115061547]
	TIME [epoch: 6.05 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2730840754270534		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.2730840754270534 | validation: 0.2913059664078006]
	TIME [epoch: 6.05 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3241952827261829		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.3241952827261829 | validation: 0.3011932410010607]
	TIME [epoch: 6.05 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29678631841499364		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.29678631841499364 | validation: 0.34280349241411656]
	TIME [epoch: 6.05 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37988760384854686		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.37988760384854686 | validation: 0.26900958801474567]
	TIME [epoch: 6.05 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2473284196160423		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.2473284196160423 | validation: 0.20815956576456554]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2345389529758833		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.2345389529758833 | validation: 0.21141701114844225]
	TIME [epoch: 6.05 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23175498048731066		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.23175498048731066 | validation: 0.2002250079849978]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22642622515935024		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.22642622515935024 | validation: 0.21778332380192192]
	TIME [epoch: 6.05 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22100278652829233		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.22100278652829233 | validation: 0.20448666612775798]
	TIME [epoch: 6.05 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22998604637581085		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.22998604637581085 | validation: 0.3653816870589252]
	TIME [epoch: 6.05 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33619934867995943		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.33619934867995943 | validation: 0.43864338646572354]
	TIME [epoch: 6.04 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5190132092212454		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.5190132092212454 | validation: 0.4281483311044994]
	TIME [epoch: 6.04 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5131014616578186		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.5131014616578186 | validation: 0.24861284200152514]
	TIME [epoch: 6.04 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.279012480434101		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.279012480434101 | validation: 0.6715556076942134]
	TIME [epoch: 6.04 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5304244298573326		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.5304244298573326 | validation: 0.3008049283300183]
	TIME [epoch: 6.04 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33242283337664086		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.33242283337664086 | validation: 0.2521535546300863]
	TIME [epoch: 6.05 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28025886695311264		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.28025886695311264 | validation: 0.2605219309680895]
	TIME [epoch: 6.04 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24989780923079666		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.24989780923079666 | validation: 0.1993912657811471]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21648987028430813		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.21648987028430813 | validation: 0.18318682527346794]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20454439911872005		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.20454439911872005 | validation: 0.17797247191552307]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20666696205379367		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.20666696205379367 | validation: 0.17954754083684948]
	TIME [epoch: 6.05 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19991995164944734		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.19991995164944734 | validation: 0.19070509493200657]
	TIME [epoch: 6.04 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19534867319789403		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.19534867319789403 | validation: 0.16906154226890036]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19240596542212374		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.19240596542212374 | validation: 0.20096123610220304]
	TIME [epoch: 6.01 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20408232369074733		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.20408232369074733 | validation: 0.20853808242967073]
	TIME [epoch: 6.01 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24393373286668465		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.24393373286668465 | validation: 0.3265371763104199]
	TIME [epoch: 6.01 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25671369232095886		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.25671369232095886 | validation: 0.27794323333600063]
	TIME [epoch: 6.01 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32990762814674407		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.32990762814674407 | validation: 0.1672168042859441]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18245900807344		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.18245900807344 | validation: 0.40698038847355716]
	TIME [epoch: 6.06 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35075114187876905		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.35075114187876905 | validation: 0.32184924838102663]
	TIME [epoch: 6.06 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3963419883020765		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.3963419883020765 | validation: 0.3122622293439011]
	TIME [epoch: 6.07 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3635778435966019		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.3635778435966019 | validation: 0.20252998446200934]
	TIME [epoch: 6.05 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19662047753986223		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.19662047753986223 | validation: 0.18382362269922337]
	TIME [epoch: 6.06 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20087568601522557		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.20087568601522557 | validation: 0.17656704370918844]
	TIME [epoch: 6.05 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.224677111639548		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.224677111639548 | validation: 0.17745635092699943]
	TIME [epoch: 6.06 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18159405889026903		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.18159405889026903 | validation: 0.1679880970907168]
	TIME [epoch: 6.06 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773420808052475		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.1773420808052475 | validation: 0.1619242003770828]
	TIME [epoch: 6.05 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21087526416251798		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.21087526416251798 | validation: 0.2777727988701273]
	TIME [epoch: 6.03 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22910127859040394		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.22910127859040394 | validation: 0.23210216026744673]
	TIME [epoch: 6.02 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26888487485696394		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.26888487485696394 | validation: 0.18264753898558805]
	TIME [epoch: 6.02 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17312127641578745		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.17312127641578745 | validation: 0.1584163966959582]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649044584300844		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.1649044584300844 | validation: 0.16127016686720205]
	TIME [epoch: 6.02 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17458355310940143		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.17458355310940143 | validation: 0.2702870176746641]
	TIME [epoch: 6.02 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20540615288399458		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.20540615288399458 | validation: 0.20058644980896012]
	TIME [epoch: 6.02 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25900498463665733		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.25900498463665733 | validation: 0.20531252410097736]
	TIME [epoch: 6.02 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170322875806815		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.170322875806815 | validation: 0.15891341739869744]
	TIME [epoch: 6.02 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15934027937556686		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.15934027937556686 | validation: 0.1572870964232515]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17900420284798144		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.17900420284798144 | validation: 0.22515030698169125]
	TIME [epoch: 6.06 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1953808682620933		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.1953808682620933 | validation: 0.2134262720162104]
	TIME [epoch: 6.04 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612828309521734		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.2612828309521734 | validation: 0.15841178821415758]
	TIME [epoch: 6.04 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15461555759077292		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.15461555759077292 | validation: 0.27891153033032]
	TIME [epoch: 6.04 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23152964956612643		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.23152964956612643 | validation: 0.26289800192776863]
	TIME [epoch: 6.04 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3200207151465163		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3200207151465163 | validation: 0.14491862624452648]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16848760068918722		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.16848760068918722 | validation: 0.5894254678660155]
	TIME [epoch: 6.01 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45363198315287906		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.45363198315287906 | validation: 0.19813360970930083]
	TIME [epoch: 6.02 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23747709121048347		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.23747709121048347 | validation: 0.1679648334084809]
	TIME [epoch: 6.02 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.218011854641176		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.218011854641176 | validation: 0.2355240762141595]
	TIME [epoch: 6.02 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21002287579582404		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.21002287579582404 | validation: 0.15663572760953584]
	TIME [epoch: 6.02 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622328678713499		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.1622328678713499 | validation: 0.1331193146694761]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15804566301022052		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.15804566301022052 | validation: 0.1518271724101378]
	TIME [epoch: 6.07 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16643350127753626		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.16643350127753626 | validation: 0.12833236157995298]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1523144961114457		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.1523144961114457 | validation: 0.13628630486937682]
	TIME [epoch: 6.05 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14589438994322998		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.14589438994322998 | validation: 0.12627889824470795]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13833907190690595		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.13833907190690595 | validation: 0.12938147607291814]
	TIME [epoch: 6.02 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14478764309686856		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.14478764309686856 | validation: 0.15910359566771637]
	TIME [epoch: 6.01 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15256097918370615		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.15256097918370615 | validation: 0.14179995300605347]
	TIME [epoch: 6.01 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18765233195366546		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.18765233195366546 | validation: 0.27428266190190265]
	TIME [epoch: 6.01 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20135995665853024		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.20135995665853024 | validation: 0.19063073763805025]
	TIME [epoch: 6.01 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23330324773221833		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.23330324773221833 | validation: 0.14161196516856436]
	TIME [epoch: 6.01 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422916018547498		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.1422916018547498 | validation: 0.14419541051164292]
	TIME [epoch: 6.01 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14818382269972938		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.14818382269972938 | validation: 0.1317059860875802]
	TIME [epoch: 6.01 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15190875067357565		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.15190875067357565 | validation: 0.20448267063337583]
	TIME [epoch: 6.02 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1664034340580384		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.1664034340580384 | validation: 0.13425660507225276]
	TIME [epoch: 6.01 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1722622388874474		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.1722622388874474 | validation: 0.14746204268609656]
	TIME [epoch: 6.02 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1426106332259942		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.1426106332259942 | validation: 0.13845252011283157]
	TIME [epoch: 6.01 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15089719752705816		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.15089719752705816 | validation: 0.17448371383585834]
	TIME [epoch: 6.01 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15051690128092765		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.15051690128092765 | validation: 0.13811499664198326]
	TIME [epoch: 6.01 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17295433954446918		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.17295433954446918 | validation: 0.22930082214804492]
	TIME [epoch: 6.01 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875808458073119		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.1875808458073119 | validation: 0.18868034895441022]
	TIME [epoch: 6.02 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2100062254358491		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.2100062254358491 | validation: 0.14008638920939773]
	TIME [epoch: 6.01 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431540125383793		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.1431540125383793 | validation: 0.28705205341595885]
	TIME [epoch: 6.01 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22263277793787467		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.22263277793787467 | validation: 0.20097908445755452]
	TIME [epoch: 6.01 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23743239429451293		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.23743239429451293 | validation: 0.15537033065557537]
	TIME [epoch: 6.01 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377269885414282		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.1377269885414282 | validation: 0.23251122937767696]
	TIME [epoch: 6.02 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17856067259949412		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.17856067259949412 | validation: 0.13288513323339382]
	TIME [epoch: 6.01 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17665425906174412		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.17665425906174412 | validation: 0.13686826839226007]
	TIME [epoch: 6.01 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12638565556469908		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.12638565556469908 | validation: 0.15893818124042808]
	TIME [epoch: 6.01 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.132657500257969		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.132657500257969 | validation: 0.10962729059222814]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14154249543764522		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.14154249543764522 | validation: 0.15355032293424506]
	TIME [epoch: 6.01 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13157636882374038		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.13157636882374038 | validation: 0.12802224601238738]
	TIME [epoch: 6.01 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13599234333046986		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.13599234333046986 | validation: 0.13742363017666695]
	TIME [epoch: 6.01 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13418416972705166		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.13418416972705166 | validation: 0.13339340582845893]
	TIME [epoch: 6.02 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1553734484261531		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.1553734484261531 | validation: 0.20546988805349775]
	TIME [epoch: 6.02 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14754435916464362		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.14754435916464362 | validation: 0.11935001190614294]
	TIME [epoch: 6.01 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15980369519092436		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.15980369519092436 | validation: 0.15579994488139948]
	TIME [epoch: 6.02 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13369462049725403		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.13369462049725403 | validation: 0.14123607041505673]
	TIME [epoch: 6.01 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15981147304429352		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.15981147304429352 | validation: 0.17382558141334165]
	TIME [epoch: 6.02 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477226862695946		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.1477226862695946 | validation: 0.10813866077838918]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420615531433409		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.1420615531433409 | validation: 0.13928049382847282]
	TIME [epoch: 6.02 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13438057496786968		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.13438057496786968 | validation: 0.11496267709491517]
	TIME [epoch: 6.01 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11764432051256081		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.11764432051256081 | validation: 0.14146862876651187]
	TIME [epoch: 6.04 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11952935005561913		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.11952935005561913 | validation: 0.12474184607542882]
	TIME [epoch: 6.01 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11361030880416173		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.11361030880416173 | validation: 0.11666492596772154]
	TIME [epoch: 6.01 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11657233501786828		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.11657233501786828 | validation: 0.10003509178744414]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_353.pth
	Model improved!!!
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11805439548242579		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.11805439548242579 | validation: 0.18042638710539471]
	TIME [epoch: 6.02 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14003514903683023		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.14003514903683023 | validation: 0.15390387526018454]
	TIME [epoch: 6.01 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20795196866959714		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.20795196866959714 | validation: 0.410215668086676]
	TIME [epoch: 6.01 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27252422744329474		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.27252422744329474 | validation: 0.14662805965870465]
	TIME [epoch: 6.01 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1682298935445253		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.1682298935445253 | validation: 0.0898625581550026]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12082580087477497		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.12082580087477497 | validation: 0.12461182947072054]
	TIME [epoch: 6.01 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13435682169750757		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.13435682169750757 | validation: 0.12044610091515329]
	TIME [epoch: 6.01 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.126649479552201		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.126649479552201 | validation: 0.12834436716453465]
	TIME [epoch: 6.01 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11496006968423243		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.11496006968423243 | validation: 0.11087090341761617]
	TIME [epoch: 6.01 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327827506413166		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.1327827506413166 | validation: 0.41285270935007895]
	TIME [epoch: 6.02 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728749461137798		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.2728749461137798 | validation: 0.12733220301434434]
	TIME [epoch: 6.01 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377902509786164		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.1377902509786164 | validation: 0.10811692170695984]
	TIME [epoch: 6.02 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12808286949475484		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.12808286949475484 | validation: 0.1772981319441911]
	TIME [epoch: 6.01 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15039874395566583		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.15039874395566583 | validation: 0.11277288325980495]
	TIME [epoch: 6.02 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12387018784883556		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.12387018784883556 | validation: 0.12574639002300522]
	TIME [epoch: 6.01 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10771186142937626		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.10771186142937626 | validation: 0.09912843483436816]
	TIME [epoch: 6.01 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10554426244832399		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.10554426244832399 | validation: 0.11385141847681686]
	TIME [epoch: 6.01 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11041494101337346		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.11041494101337346 | validation: 0.12223341543749977]
	TIME [epoch: 6.01 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13174149970913335		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.13174149970913335 | validation: 0.19785877035609278]
	TIME [epoch: 6.01 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13617562177947012		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.13617562177947012 | validation: 0.0902983151973058]
	TIME [epoch: 6.02 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12560143765002763		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.12560143765002763 | validation: 0.15796924971349796]
	TIME [epoch: 6.01 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15389141471003268		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.15389141471003268 | validation: 0.10112222988452667]
	TIME [epoch: 6.01 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.112655565978895		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.112655565978895 | validation: 0.1257078795971794]
	TIME [epoch: 6.01 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1062754998469604		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.1062754998469604 | validation: 0.12520474050223318]
	TIME [epoch: 6.01 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11034340302131813		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.11034340302131813 | validation: 0.09573670504261543]
	TIME [epoch: 6.01 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12055277150892826		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.12055277150892826 | validation: 0.1521551205768828]
	TIME [epoch: 6.01 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10954627352189594		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.10954627352189594 | validation: 0.08973859371841583]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_380.pth
	Model improved!!!
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10199357940251805		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.10199357940251805 | validation: 0.12879508160321856]
	TIME [epoch: 6.01 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1116154560608322		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.1116154560608322 | validation: 0.11114703640657426]
	TIME [epoch: 6.01 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14037066150999386		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.14037066150999386 | validation: 0.24412474092861355]
	TIME [epoch: 6.01 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1631475994663952		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1631475994663952 | validation: 0.13431937266858163]
	TIME [epoch: 6.01 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17831922722812096		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.17831922722812096 | validation: 0.12845027439348572]
	TIME [epoch: 6.01 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10734742511090659		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.10734742511090659 | validation: 0.12123747887550523]
	TIME [epoch: 6.02 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10078901955199018		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.10078901955199018 | validation: 0.10404089756969802]
	TIME [epoch: 6.01 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11842813970802887		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.11842813970802887 | validation: 0.2747627656101278]
	TIME [epoch: 6.01 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16898882190439662		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.16898882190439662 | validation: 0.1370471162437816]
	TIME [epoch: 6.01 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17151993506351526		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.17151993506351526 | validation: 0.1006566770061054]
	TIME [epoch: 6 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10282965616438958		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.10282965616438958 | validation: 0.20607355238852884]
	TIME [epoch: 6.01 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12207757114998367		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.12207757114998367 | validation: 0.11179811437819241]
	TIME [epoch: 6 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1178273304318688		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.1178273304318688 | validation: 0.1414765989981536]
	TIME [epoch: 6.01 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12908413021851417		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.12908413021851417 | validation: 0.08753060221280219]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_394.pth
	Model improved!!!
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10723524262675299		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.10723524262675299 | validation: 0.13033351304563393]
	TIME [epoch: 6.02 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257705207243469		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.09257705207243469 | validation: 0.09225696044965247]
	TIME [epoch: 6.02 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373975991168231		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.10373975991168231 | validation: 0.11773955120793285]
	TIME [epoch: 6.01 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796075153293377		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.09796075153293377 | validation: 0.14149798659412732]
	TIME [epoch: 6.01 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09986714238435715		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.09986714238435715 | validation: 0.08564155106205136]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10383830167506325		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.10383830167506325 | validation: 0.2324089757119303]
	TIME [epoch: 6.01 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14192010539717206		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.14192010539717206 | validation: 0.15754253477619057]
	TIME [epoch: 6.02 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19390065399050382		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.19390065399050382 | validation: 0.11929289059551068]
	TIME [epoch: 6.01 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09320751375683288		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.09320751375683288 | validation: 0.20932554182359225]
	TIME [epoch: 6.01 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279342373029273		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.1279342373029273 | validation: 0.10054111297663626]
	TIME [epoch: 6.01 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12833754950816118		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.12833754950816118 | validation: 0.27728702058745724]
	TIME [epoch: 6.01 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605505577041906		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.1605505577041906 | validation: 0.0945514219737583]
	TIME [epoch: 6.01 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10198657963214253		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.10198657963214253 | validation: 0.08601498945382556]
	TIME [epoch: 6.01 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102510954991828		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.10102510954991828 | validation: 0.16240951844904342]
	TIME [epoch: 6.01 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13982558700957834		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.13982558700957834 | validation: 0.09540513697347688]
	TIME [epoch: 6.02 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11831513924206151		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.11831513924206151 | validation: 0.1150282627562269]
	TIME [epoch: 6.01 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0945004906208401		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.0945004906208401 | validation: 0.15405123801377096]
	TIME [epoch: 6.01 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09879295107138485		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.09879295107138485 | validation: 0.09853244760542668]
	TIME [epoch: 6.01 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10432209482424476		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.10432209482424476 | validation: 0.10548937511533987]
	TIME [epoch: 6.01 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09212036391523187		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.09212036391523187 | validation: 0.08777752238678611]
	TIME [epoch: 6.01 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09190708876169368		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.09190708876169368 | validation: 0.09412049319924737]
	TIME [epoch: 6.01 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08827330433174577		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.08827330433174577 | validation: 0.12222007120947183]
	TIME [epoch: 6.01 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08997008197547292		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.08997008197547292 | validation: 0.10447331162608658]
	TIME [epoch: 6.02 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09311900904014539		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.09311900904014539 | validation: 0.08158476778771656]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10079011255598262		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.10079011255598262 | validation: 0.23781720389198543]
	TIME [epoch: 6.01 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13760254060112107		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.13760254060112107 | validation: 0.12032563534728889]
	TIME [epoch: 6.02 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543730022420828		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.1543730022420828 | validation: 0.17988828922411532]
	TIME [epoch: 6.02 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11513657393535595		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.11513657393535595 | validation: 0.07441543177095782]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09350636969068016		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.09350636969068016 | validation: 0.14376119774081286]
	TIME [epoch: 6.08 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09449684271606672		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.09449684271606672 | validation: 0.08285492698799589]
	TIME [epoch: 6.06 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09139159547096767		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.09139159547096767 | validation: 0.13723879052947768]
	TIME [epoch: 6.07 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10773668898951645		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.10773668898951645 | validation: 0.13646256711679447]
	TIME [epoch: 6.07 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17649770441862864		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.17649770441862864 | validation: 0.10920078791848736]
	TIME [epoch: 6.07 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365117347360506		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.09365117347360506 | validation: 0.2187741583033881]
	TIME [epoch: 6.09 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12279792979305018		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.12279792979305018 | validation: 0.15800317763049396]
	TIME [epoch: 6.06 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375018036038507		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.1375018036038507 | validation: 0.13660423796194884]
	TIME [epoch: 6.09 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09878404092385995		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.09878404092385995 | validation: 0.11922726825254681]
	TIME [epoch: 6.09 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256376657249827		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.10256376657249827 | validation: 0.07913657799760608]
	TIME [epoch: 6.08 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260004424453868		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.10260004424453868 | validation: 0.20386856402555165]
	TIME [epoch: 6.06 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12565054549423596		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.12565054549423596 | validation: 0.11454692350951848]
	TIME [epoch: 6.06 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11736756710107284		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.11736756710107284 | validation: 0.39391408141991646]
	TIME [epoch: 6.06 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170006189999456		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.2170006189999456 | validation: 0.10341519859496646]
	TIME [epoch: 6.06 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0985916042399181		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.0985916042399181 | validation: 0.07807167544101629]
	TIME [epoch: 6.06 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09782310942466335		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.09782310942466335 | validation: 0.1783476243823175]
	TIME [epoch: 6.06 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11275580950496032		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.11275580950496032 | validation: 0.08537093560197816]
	TIME [epoch: 6.06 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08354183251482585		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.08354183251482585 | validation: 0.07572361225025968]
	TIME [epoch: 6.06 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09457777700515266		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.09457777700515266 | validation: 0.09005008347249367]
	TIME [epoch: 6.06 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08688272753116148		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.08688272753116148 | validation: 0.13618286652557007]
	TIME [epoch: 6.06 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11342418793355809		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.11342418793355809 | validation: 0.07798499621210658]
	TIME [epoch: 6.06 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08286559838146129		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.08286559838146129 | validation: 0.0702728818255054]
	TIME [epoch: 6.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_444.pth
	Model improved!!!
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08964998726900479		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.08964998726900479 | validation: 0.08249343864049183]
	TIME [epoch: 6.05 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08884520785898228		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.08884520785898228 | validation: 0.08354960110379259]
	TIME [epoch: 6.04 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08345561074751025		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.08345561074751025 | validation: 0.16499521400849956]
	TIME [epoch: 6.05 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12099009081581287		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.12099009081581287 | validation: 0.0760397221310681]
	TIME [epoch: 6.04 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10893709806593745		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.10893709806593745 | validation: 0.10874998295601357]
	TIME [epoch: 6.04 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09030417710633902		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.09030417710633902 | validation: 0.09718781673576836]
	TIME [epoch: 6.04 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09068069173158474		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.09068069173158474 | validation: 0.09268843432233537]
	TIME [epoch: 6.04 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972449820102984		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.07972449820102984 | validation: 0.08454922081571332]
	TIME [epoch: 6.03 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040092954185188		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.08040092954185188 | validation: 0.09460573596616177]
	TIME [epoch: 6.04 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08103935781438068		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.08103935781438068 | validation: 0.09679334347944149]
	TIME [epoch: 6.04 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10291738764964503		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.10291738764964503 | validation: 0.19762874443091596]
	TIME [epoch: 6.04 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12207720374331621		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.12207720374331621 | validation: 0.10191018768008271]
	TIME [epoch: 6.04 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15225008440066137		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.15225008440066137 | validation: 0.14529736539246332]
	TIME [epoch: 6.04 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09640221103176642		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.09640221103176642 | validation: 0.12980500910944612]
	TIME [epoch: 6.04 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09218793193081062		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.09218793193081062 | validation: 0.07975666795041823]
	TIME [epoch: 6.04 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09939833451193743		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.09939833451193743 | validation: 0.139516469014209]
	TIME [epoch: 6.04 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15943758582862066		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.15943758582862066 | validation: 0.10653943103423896]
	TIME [epoch: 6.04 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12818252687811316		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.12818252687811316 | validation: 0.08159790383491634]
	TIME [epoch: 6.03 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1118105243468972		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.1118105243468972 | validation: 0.1449051273185329]
	TIME [epoch: 6.03 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882215345868516		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.0882215345868516 | validation: 0.14371259500247596]
	TIME [epoch: 6.03 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935460615829934		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.0935460615829934 | validation: 0.0815520923402511]
	TIME [epoch: 6.03 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08674348732130156		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.08674348732130156 | validation: 0.08577759380998135]
	TIME [epoch: 6.04 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08173606579022646		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.08173606579022646 | validation: 0.06915599246234101]
	TIME [epoch: 6.04 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08097850207525717		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.08097850207525717 | validation: 0.12706562612662584]
	TIME [epoch: 6.04 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0846590916608805		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.0846590916608805 | validation: 0.08832573676097669]
	TIME [epoch: 6.02 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10127801097131761		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.10127801097131761 | validation: 0.1783398390648951]
	TIME [epoch: 6.03 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10815939860618187		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.10815939860618187 | validation: 0.08003542732265581]
	TIME [epoch: 6.03 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10805771282586533		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.10805771282586533 | validation: 0.11007995199629357]
	TIME [epoch: 6.04 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08245712882292663		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.08245712882292663 | validation: 0.08420098371856283]
	TIME [epoch: 6.03 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07346335797343058		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.07346335797343058 | validation: 0.0927236143515459]
	TIME [epoch: 6.04 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569969157935258		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.07569969157935258 | validation: 0.08407135326456101]
	TIME [epoch: 6.01 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07601918624168184		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.07601918624168184 | validation: 0.09572721939873485]
	TIME [epoch: 6.04 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07346693009414088		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.07346693009414088 | validation: 0.09300638644458076]
	TIME [epoch: 6.03 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776792296517044		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.0776792296517044 | validation: 0.0728393878248385]
	TIME [epoch: 6.05 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09608631293504649		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.09608631293504649 | validation: 0.23206737132699262]
	TIME [epoch: 6.04 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12347045419313561		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.12347045419313561 | validation: 0.10282188180020024]
	TIME [epoch: 6.04 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13022754058067867		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.13022754058067867 | validation: 0.104565944499489]
	TIME [epoch: 6.04 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08488260903594745		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.08488260903594745 | validation: 0.09497721224316473]
	TIME [epoch: 6.06 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08552073470920439		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.08552073470920439 | validation: 0.07010483198462433]
	TIME [epoch: 6.04 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09224679792600247		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.09224679792600247 | validation: 0.16580320460806308]
	TIME [epoch: 6.08 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09407858417010634		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.09407858417010634 | validation: 0.09403384403519749]
	TIME [epoch: 6.04 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09657218170555604		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.09657218170555604 | validation: 0.2879968257706268]
	TIME [epoch: 6.04 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23514380341979946		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.23514380341979946 | validation: 0.12002954851352313]
	TIME [epoch: 6.03 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10642480358484463		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.10642480358484463 | validation: 0.08320664078832324]
	TIME [epoch: 6.04 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11334913157267862		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.11334913157267862 | validation: 0.10097756889669202]
	TIME [epoch: 6.03 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08140961537701062		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.08140961537701062 | validation: 0.17454731428517928]
	TIME [epoch: 6.04 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08528006654930184		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.08528006654930184 | validation: 0.09335840122669792]
	TIME [epoch: 6.03 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07610187672390636		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.07610187672390636 | validation: 0.06999580137475929]
	TIME [epoch: 6.04 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07551090671011433		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.07551090671011433 | validation: 0.09732738217747491]
	TIME [epoch: 6.04 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07326022582862107		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.07326022582862107 | validation: 0.0826770058830631]
	TIME [epoch: 6.05 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07224380908165927		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.07224380908165927 | validation: 0.0856038398986661]
	TIME [epoch: 6.04 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07096906614596246		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.07096906614596246 | validation: 0.07747055065611824]
	TIME [epoch: 6.04 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741435663897953		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.0741435663897953 | validation: 0.08009920054556613]
	TIME [epoch: 6.04 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739560587466578		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.0739560587466578 | validation: 0.15516469049033413]
	TIME [epoch: 6.04 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08624493791723768		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.08624493791723768 | validation: 0.07642889472989431]
	TIME [epoch: 6.04 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09231737813759164		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.09231737813759164 | validation: 0.08700252083233131]
	TIME [epoch: 6 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07682950776194879		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.07682950776194879 | validation: 0.08070789952955564]
	TIME [epoch: 192 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0707075018039286		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.0707075018039286 | validation: 0.12082752006919671]
	TIME [epoch: 12.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07289332764092309		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.07289332764092309 | validation: 0.07566359257139825]
	TIME [epoch: 12.8 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08659854833141627		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.08659854833141627 | validation: 0.1920977357219237]
	TIME [epoch: 12.8 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10607732469857922		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.10607732469857922 | validation: 0.0657485249337469]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08883035608172563		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.08883035608172563 | validation: 0.08553635866213886]
	TIME [epoch: 12.8 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07358654211555377		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.07358654211555377 | validation: 0.09538366594074003]
	TIME [epoch: 12.8 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07218377771974838		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.07218377771974838 | validation: 0.07355090511600552]
	TIME [epoch: 12.8 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07080562491674475		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.07080562491674475 | validation: 0.08128537994727483]
	TIME [epoch: 12.8 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06998274387099623		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.06998274387099623 | validation: 0.06462290781312306]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_510.pth
	Model improved!!!
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354663276083576		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.07354663276083576 | validation: 0.10804545893297335]
	TIME [epoch: 12.8 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746182299456564		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.0746182299456564 | validation: 0.06849239186144174]
	TIME [epoch: 12.8 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953348148669519		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.07953348148669519 | validation: 0.2036947344515462]
	TIME [epoch: 12.8 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11523249140772177		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.11523249140772177 | validation: 0.08687599897344245]
	TIME [epoch: 12.8 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12174074731150394		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.12174074731150394 | validation: 0.11195998498805992]
	TIME [epoch: 12.8 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07946646065115927		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.07946646065115927 | validation: 0.2361245953167427]
	TIME [epoch: 12.8 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14084835068189835		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.14084835068189835 | validation: 0.07650796719501343]
	TIME [epoch: 12.8 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10349990035412336		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.10349990035412336 | validation: 0.06775289313058144]
	TIME [epoch: 12.8 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992499045884074		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.06992499045884074 | validation: 0.12203020224104151]
	TIME [epoch: 12.8 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07830887428597492		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.07830887428597492 | validation: 0.09389099610874108]
	TIME [epoch: 12.8 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08466655842320218		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.08466655842320218 | validation: 0.09471887547135795]
	TIME [epoch: 12.8 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07012299011681832		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.07012299011681832 | validation: 0.09765440181608201]
	TIME [epoch: 12.8 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07023457579590414		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.07023457579590414 | validation: 0.0635731059368742]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449520933285189		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.07449520933285189 | validation: 0.12427401376636667]
	TIME [epoch: 12.8 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341180434232408		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.1341180434232408 | validation: 0.08345242793059737]
	TIME [epoch: 12.8 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10898916032651475		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.10898916032651475 | validation: 0.09136351875911924]
	TIME [epoch: 12.8 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0713157824570136		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.0713157824570136 | validation: 0.10862885137286207]
	TIME [epoch: 12.8 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074508877884357		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.07074508877884357 | validation: 0.08965049063884555]
	TIME [epoch: 12.8 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877024586487925		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.0877024586487925 | validation: 0.21232787613590354]
	TIME [epoch: 12.8 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10526758763124168		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.10526758763124168 | validation: 0.08147295737789756]
	TIME [epoch: 12.8 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09884986249899776		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.09884986249899776 | validation: 0.1036054552739947]
	TIME [epoch: 12.8 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07485415574094116		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.07485415574094116 | validation: 0.25539612382157983]
	TIME [epoch: 12.8 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12081804798766346		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.12081804798766346 | validation: 0.14427299178962794]
	TIME [epoch: 12.8 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276766349085615		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1276766349085615 | validation: 0.1507488118638062]
	TIME [epoch: 12.8 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09077869832985404		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.09077869832985404 | validation: 0.08564432105131065]
	TIME [epoch: 12.8 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07139300960077336		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.07139300960077336 | validation: 0.05695880256039435]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07948657404589897		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.07948657404589897 | validation: 0.07023797870908709]
	TIME [epoch: 12.8 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185306616798104		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.07185306616798104 | validation: 0.06451851112572729]
	TIME [epoch: 12.8 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.065264987164053		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.065264987164053 | validation: 0.07138918270794595]
	TIME [epoch: 12.8 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413182468737413		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.06413182468737413 | validation: 0.1286384517660472]
	TIME [epoch: 12.8 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0720512972759018		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.0720512972759018 | validation: 0.07873410187695194]
	TIME [epoch: 12.8 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111268007765501		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.111268007765501 | validation: 0.13756511678880076]
	TIME [epoch: 12.8 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08718026430630997		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.08718026430630997 | validation: 0.06674734567762851]
	TIME [epoch: 12.8 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07031574515919058		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.07031574515919058 | validation: 0.06333976273310647]
	TIME [epoch: 12.8 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859103897549652		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.06859103897549652 | validation: 0.08222452511143717]
	TIME [epoch: 12.8 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227817060706487		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.07227817060706487 | validation: 0.13438296060372595]
	TIME [epoch: 12.8 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08645655681250773		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.08645655681250773 | validation: 0.06167295156871852]
	TIME [epoch: 12.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065444779167869		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.07065444779167869 | validation: 0.09449215726961066]
	TIME [epoch: 12.8 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068600580171563		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.068600580171563 | validation: 0.07872930610042624]
	TIME [epoch: 12.8 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074299608645564		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.07074299608645564 | validation: 0.09986358877170455]
	TIME [epoch: 12.8 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07112807424465369		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.07112807424465369 | validation: 0.07128453029838551]
	TIME [epoch: 12.8 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729619761441909		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.0729619761441909 | validation: 0.09421663245178787]
	TIME [epoch: 12.8 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07206348524090943		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.07206348524090943 | validation: 0.05861128306901884]
	TIME [epoch: 12.8 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07293105915713745		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.07293105915713745 | validation: 0.10474640057816614]
	TIME [epoch: 12.8 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07236601887635927		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.07236601887635927 | validation: 0.06100135398911363]
	TIME [epoch: 12.8 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06837086479974891		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.06837086479974891 | validation: 0.1348093053589286]
	TIME [epoch: 12.8 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07457008341394797		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.07457008341394797 | validation: 0.07360848774283497]
	TIME [epoch: 12.8 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07694378080426852		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.07694378080426852 | validation: 0.11238498703059077]
	TIME [epoch: 12.8 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07329536927975747		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.07329536927975747 | validation: 0.061402159002790405]
	TIME [epoch: 12.8 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09123310409117232		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.09123310409117232 | validation: 0.10059498420148305]
	TIME [epoch: 12.8 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08164180104226247		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.08164180104226247 | validation: 0.13088078204270603]
	TIME [epoch: 12.8 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14226778055207098		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.14226778055207098 | validation: 0.08907581434770835]
	TIME [epoch: 12.8 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185941966444074		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.09185941966444074 | validation: 0.2102832950296513]
	TIME [epoch: 12.8 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10233318335466589		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.10233318335466589 | validation: 0.06268597891974981]
	TIME [epoch: 12.8 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06834317687898507		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.06834317687898507 | validation: 0.060265345600805334]
	TIME [epoch: 12.8 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839917715739055		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.0839917715739055 | validation: 0.09585345164754208]
	TIME [epoch: 12.8 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07071353742767383		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.07071353742767383 | validation: 0.06613242357949527]
	TIME [epoch: 12.8 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06313511414058602		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.06313511414058602 | validation: 0.08033574764767704]
	TIME [epoch: 12.8 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06401476207015548		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.06401476207015548 | validation: 0.07982796012546574]
	TIME [epoch: 12.8 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06797147555692712		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.06797147555692712 | validation: 0.0836269168535302]
	TIME [epoch: 12.8 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06489143108818853		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.06489143108818853 | validation: 0.06773508388046674]
	TIME [epoch: 12.8 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06219942371196037		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.06219942371196037 | validation: 0.07021725977579707]
	TIME [epoch: 12.8 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06108325823003044		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.06108325823003044 | validation: 0.13501511181206385]
	TIME [epoch: 12.8 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10227541936893196		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.10227541936893196 | validation: 0.06285422904737564]
	TIME [epoch: 12.8 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10078983816543412		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.10078983816543412 | validation: 0.1291330039963813]
	TIME [epoch: 12.8 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07701386243018092		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.07701386243018092 | validation: 0.08679365117802869]
	TIME [epoch: 12.8 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973367805161848		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.05973367805161848 | validation: 0.06804867282690188]
	TIME [epoch: 12.8 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06960512091857983		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.06960512091857983 | validation: 0.10434884117019304]
	TIME [epoch: 12.8 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015831717894631		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.07015831717894631 | validation: 0.06405477959606587]
	TIME [epoch: 12.8 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06956826519536559		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.06956826519536559 | validation: 0.06001586326315838]
	TIME [epoch: 12.8 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715360126698784		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.06715360126698784 | validation: 0.07442418264653698]
	TIME [epoch: 12.8 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07623028993695607		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.07623028993695607 | validation: 0.17838272747996106]
	TIME [epoch: 12.8 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09437280372611294		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.09437280372611294 | validation: 0.15364831906098578]
	TIME [epoch: 12.8 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1256495251584962		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1256495251584962 | validation: 0.16959136846780606]
	TIME [epoch: 12.8 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09478570737636298		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.09478570737636298 | validation: 0.1175297700701913]
	TIME [epoch: 12.8 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07818000215251929		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.07818000215251929 | validation: 0.06735956300368998]
	TIME [epoch: 12.8 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08903016659752591		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.08903016659752591 | validation: 0.305105985964509]
	TIME [epoch: 12.8 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2283096987232453		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.2283096987232453 | validation: 0.26494100699106155]
	TIME [epoch: 12.8 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23181572168674525		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.23181572168674525 | validation: 0.14539617720551176]
	TIME [epoch: 12.8 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629571209059486		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.1629571209059486 | validation: 0.10683432178272319]
	TIME [epoch: 12.8 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12796438361899629		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.12796438361899629 | validation: 0.09036178608311861]
	TIME [epoch: 12.8 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10603025018953705		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.10603025018953705 | validation: 0.09589853598133218]
	TIME [epoch: 12.8 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08475088334600564		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.08475088334600564 | validation: 0.09939986706556073]
	TIME [epoch: 12.8 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07758036584843422		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.07758036584843422 | validation: 0.09796679108158321]
	TIME [epoch: 12.8 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07153865662432915		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.07153865662432915 | validation: 0.11991427483942974]
	TIME [epoch: 12.8 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06724562053313371		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.06724562053313371 | validation: 0.1002069895326744]
	TIME [epoch: 12.8 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06575240890891425		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.06575240890891425 | validation: 0.08885475779920489]
	TIME [epoch: 12.8 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06674074375224719		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.06674074375224719 | validation: 0.08350636978400737]
	TIME [epoch: 12.8 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0652543147139519		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.0652543147139519 | validation: 0.07314831625187573]
	TIME [epoch: 12.8 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642644288173449		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0642644288173449 | validation: 0.08166478687980407]
	TIME [epoch: 12.8 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635437033337838		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.0635437033337838 | validation: 0.09862709705851946]
	TIME [epoch: 12.8 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442907862412389		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.06442907862412389 | validation: 0.07971532793650735]
	TIME [epoch: 12.8 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0620527669737922		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.0620527669737922 | validation: 0.0719816935880178]
	TIME [epoch: 12.8 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062436197347500964		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.062436197347500964 | validation: 0.09577096493547012]
	TIME [epoch: 12.8 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061055592002163143		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.061055592002163143 | validation: 0.088122114623469]
	TIME [epoch: 12.8 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196591114662041		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.06196591114662041 | validation: 0.06473149228479792]
	TIME [epoch: 12.8 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059976888682585425		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.059976888682585425 | validation: 0.08151115320508544]
	TIME [epoch: 12.8 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0610672348367708		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.0610672348367708 | validation: 0.08629052498333373]
	TIME [epoch: 12.8 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06297759931213155		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.06297759931213155 | validation: 0.06713346968189458]
	TIME [epoch: 12.8 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0628087794125069		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.0628087794125069 | validation: 0.052690161720113086]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0758169171855979		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.0758169171855979 | validation: 0.10963806315398231]
	TIME [epoch: 12.8 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08616868737332521		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.08616868737332521 | validation: 0.060263677997544875]
	TIME [epoch: 12.8 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651934736185577		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.0651934736185577 | validation: 0.06793044464489745]
	TIME [epoch: 12.8 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06369486357957124		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.06369486357957124 | validation: 0.06707441299306792]
	TIME [epoch: 12.8 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061916219597008326		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.061916219597008326 | validation: 0.3231836424969312]
	TIME [epoch: 12.8 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14608274505773827		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.14608274505773827 | validation: 0.09741650732814049]
	TIME [epoch: 12.8 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856030446854824		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.06856030446854824 | validation: 0.059881437271992945]
	TIME [epoch: 12.8 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07343687020234739		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.07343687020234739 | validation: 0.07714596746777626]
	TIME [epoch: 12.8 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06130214422658246		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.06130214422658246 | validation: 0.08170679757041777]
	TIME [epoch: 12.8 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06445648929094974		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.06445648929094974 | validation: 0.053780321133580125]
	TIME [epoch: 12.8 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06215325980195433		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.06215325980195433 | validation: 0.06547548620100492]
	TIME [epoch: 12.8 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0588113675482889		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.0588113675482889 | validation: 0.05953318804890955]
	TIME [epoch: 12.8 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05802586839960292		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.05802586839960292 | validation: 0.05799050774440078]
	TIME [epoch: 12.8 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057480600800679994		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.057480600800679994 | validation: 0.08613401225146368]
	TIME [epoch: 12.8 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05917991694199811		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.05917991694199811 | validation: 0.06626070095619417]
	TIME [epoch: 12.8 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060475626540565264		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.060475626540565264 | validation: 0.08738098284175623]
	TIME [epoch: 12.8 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06074852149784833		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.06074852149784833 | validation: 0.0612685185440584]
	TIME [epoch: 12.8 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236445339405041		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.06236445339405041 | validation: 0.08982201443317012]
	TIME [epoch: 12.8 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06106087640116229		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.06106087640116229 | validation: 0.06641358307050325]
	TIME [epoch: 12.8 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058450757216091456		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.058450757216091456 | validation: 0.07013713886916717]
	TIME [epoch: 12.8 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061645283989513903		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.061645283989513903 | validation: 0.06570392288506409]
	TIME [epoch: 12.8 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0589665435392075		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.0589665435392075 | validation: 0.11011892265438808]
	TIME [epoch: 12.8 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281249024867758		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.06281249024867758 | validation: 0.07159191488335327]
	TIME [epoch: 12.8 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141722240631266		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.09141722240631266 | validation: 0.12087848753699713]
	TIME [epoch: 12.8 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06982366825557211		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.06982366825557211 | validation: 0.07013944338428123]
	TIME [epoch: 12.8 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06069328673663869		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.06069328673663869 | validation: 0.05913668703967839]
	TIME [epoch: 12.8 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05859554253232242		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.05859554253232242 | validation: 0.05952299900002518]
	TIME [epoch: 12.8 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057574079939443586		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.057574079939443586 | validation: 0.05451620739761977]
	TIME [epoch: 12.8 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057345836784215364		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.057345836784215364 | validation: 0.09523116199502246]
	TIME [epoch: 12.8 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06503763844352797		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.06503763844352797 | validation: 0.06480321083766075]
	TIME [epoch: 12.8 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129927474745444		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.08129927474745444 | validation: 0.13969099130167847]
	TIME [epoch: 12.8 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08077282905533707		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.08077282905533707 | validation: 0.058893557040898936]
	TIME [epoch: 12.8 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061408810196222026		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.061408810196222026 | validation: 0.06485533287587936]
	TIME [epoch: 12.8 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05793586551768035		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.05793586551768035 | validation: 0.08723058711855385]
	TIME [epoch: 12.8 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06099738840900377		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.06099738840900377 | validation: 0.06933904032148022]
	TIME [epoch: 12.8 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057681448412812045		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.057681448412812045 | validation: 0.05780562534221702]
	TIME [epoch: 12.8 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05957057415896563		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.05957057415896563 | validation: 0.08691479391129062]
	TIME [epoch: 12.8 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055439747558848555		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.055439747558848555 | validation: 0.07039389913344722]
	TIME [epoch: 12.8 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0652573404644172		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.0652573404644172 | validation: 0.08895073048079674]
	TIME [epoch: 12.8 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343145384954822		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.06343145384954822 | validation: 0.053670187281147766]
	TIME [epoch: 12.8 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060897703634375695		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.060897703634375695 | validation: 0.07588565569814995]
	TIME [epoch: 12.8 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05735900058330176		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.05735900058330176 | validation: 0.08979891734460642]
	TIME [epoch: 12.8 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07636814126595252		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.07636814126595252 | validation: 0.14271343645544404]
	TIME [epoch: 12.8 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08312736519017865		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.08312736519017865 | validation: 0.06486325716868722]
	TIME [epoch: 12.8 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08611809701640026		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.08611809701640026 | validation: 0.1068108411308145]
	TIME [epoch: 12.8 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735571919882032		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.0735571919882032 | validation: 0.061530549710925635]
	TIME [epoch: 12.8 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057852103474131715		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.057852103474131715 | validation: 0.05190123047630124]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_657.pth
	Model improved!!!
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05666909196088855		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.05666909196088855 | validation: 0.08589904827225371]
	TIME [epoch: 12.8 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056706259707387345		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.056706259707387345 | validation: 0.07756011244162755]
	TIME [epoch: 12.8 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052155140622312864		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.052155140622312864 | validation: 0.0571240770336791]
	TIME [epoch: 12.8 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0546584905016336		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.0546584905016336 | validation: 0.06996440755805938]
	TIME [epoch: 12.8 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05422152126308892		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.05422152126308892 | validation: 0.09185628636469689]
	TIME [epoch: 12.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06895904832034815		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.06895904832034815 | validation: 0.05455009381886831]
	TIME [epoch: 12.8 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08144143588836776		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.08144143588836776 | validation: 0.12414941101447641]
	TIME [epoch: 12.8 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07266602943597984		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.07266602943597984 | validation: 0.059566955600591026]
	TIME [epoch: 12.8 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05902152906397399		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.05902152906397399 | validation: 0.06795629584068526]
	TIME [epoch: 12.8 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054464528455996176		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.054464528455996176 | validation: 0.08183271386193222]
	TIME [epoch: 12.8 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05873223692845857		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.05873223692845857 | validation: 0.0540310084408156]
	TIME [epoch: 12.8 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379848089991753		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.06379848089991753 | validation: 0.08313679175839035]
	TIME [epoch: 12.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059887719215748605		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.059887719215748605 | validation: 0.05589061687044288]
	TIME [epoch: 12.8 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06044205644359215		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.06044205644359215 | validation: 0.08740632476539908]
	TIME [epoch: 12.8 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05920216639876486		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.05920216639876486 | validation: 0.053209008328869094]
	TIME [epoch: 12.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05551329293826129		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.05551329293826129 | validation: 0.06793717584995773]
	TIME [epoch: 12.8 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05413257683556188		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.05413257683556188 | validation: 0.06089839069662006]
	TIME [epoch: 12.8 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319586902604757		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.05319586902604757 | validation: 0.05675785523788555]
	TIME [epoch: 12.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06933635085598433		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.06933635085598433 | validation: 0.13420537708663424]
	TIME [epoch: 12.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879956975528655		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.06879956975528655 | validation: 0.07268184388175596]
	TIME [epoch: 12.8 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0754364531133142		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.0754364531133142 | validation: 0.08283323528841816]
	TIME [epoch: 12.8 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05809239845858196		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.05809239845858196 | validation: 0.07996200875985089]
	TIME [epoch: 12.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05929905773827056		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.05929905773827056 | validation: 0.04901315698661223]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_680.pth
	Model improved!!!
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05781312769497045		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.05781312769497045 | validation: 0.07476224936967532]
	TIME [epoch: 12.8 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05629716087401249		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.05629716087401249 | validation: 0.06937816278125321]
	TIME [epoch: 12.8 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05600841938586313		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.05600841938586313 | validation: 0.0661712563391222]
	TIME [epoch: 12.8 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05438456365817354		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.05438456365817354 | validation: 0.06826504345176351]
	TIME [epoch: 12.8 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05385151726047168		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.05385151726047168 | validation: 0.06573544315648776]
	TIME [epoch: 12.8 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052004643996823766		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.052004643996823766 | validation: 0.07018388495190629]
	TIME [epoch: 12.8 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09362653961820282		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.09362653961820282 | validation: 0.13949944254103044]
	TIME [epoch: 12.8 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12171021818436895		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.12171021818436895 | validation: 0.06726951647258227]
	TIME [epoch: 12.8 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07019685916423171		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.07019685916423171 | validation: 0.052438708997850796]
	TIME [epoch: 12.8 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056394252949911225		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.056394252949911225 | validation: 0.06339889960922615]
	TIME [epoch: 12.8 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503017870112308		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.05503017870112308 | validation: 0.06412383100979917]
	TIME [epoch: 12.8 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052699509349528606		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.052699509349528606 | validation: 0.05367334527919446]
	TIME [epoch: 12.8 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051651095691325324		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.051651095691325324 | validation: 0.060155617061696365]
	TIME [epoch: 12.8 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052825144397774956		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.052825144397774956 | validation: 0.05415073291600904]
	TIME [epoch: 12.8 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051385362987551135		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.051385362987551135 | validation: 0.057484750440141674]
	TIME [epoch: 12.8 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052611859817783525		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.052611859817783525 | validation: 0.1011059789564912]
	TIME [epoch: 12.8 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06273845684002678		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.06273845684002678 | validation: 0.05780764200467853]
	TIME [epoch: 12.8 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06976601720805121		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.06976601720805121 | validation: 0.10678144116856768]
	TIME [epoch: 12.8 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813916374222305		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.06813916374222305 | validation: 0.05166285324247032]
	TIME [epoch: 12.8 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357772661433829		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.05357772661433829 | validation: 0.04500694341439802]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05307560286981335		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.05307560286981335 | validation: 0.06879706032725498]
	TIME [epoch: 12.8 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05075511346793426		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.05075511346793426 | validation: 0.05782155470633556]
	TIME [epoch: 12.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05259751713827797		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.05259751713827797 | validation: 0.07376057831777445]
	TIME [epoch: 12.8 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05532994122280403		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.05532994122280403 | validation: 0.04768129268394076]
	TIME [epoch: 12.8 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05466939992147653		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.05466939992147653 | validation: 0.09879303304179378]
	TIME [epoch: 12.8 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05982052977371109		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.05982052977371109 | validation: 0.0511664941940233]
	TIME [epoch: 12.8 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06388291572366395		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.06388291572366395 | validation: 0.07778413694247077]
	TIME [epoch: 12.8 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05962243250356138		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.05962243250356138 | validation: 0.04813423888330712]
	TIME [epoch: 12.8 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052029402830812865		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.052029402830812865 | validation: 0.05435085900940263]
	TIME [epoch: 12.8 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052806035771497		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.052806035771497 | validation: 0.08587951098569113]
	TIME [epoch: 12.8 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05575428770001299		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.05575428770001299 | validation: 0.06376577337480414]
	TIME [epoch: 12.8 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477651533860719		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.07477651533860719 | validation: 0.12791596386151638]
	TIME [epoch: 12.8 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06675075099162832		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.06675075099162832 | validation: 0.061203429286512234]
	TIME [epoch: 12.8 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05181826751442742		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.05181826751442742 | validation: 0.05617267853405712]
	TIME [epoch: 12.8 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287344964252871		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.05287344964252871 | validation: 0.09139338854612414]
	TIME [epoch: 12.8 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05498623537805961		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.05498623537805961 | validation: 0.0534000007961191]
	TIME [epoch: 12.8 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301594590240904		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.05301594590240904 | validation: 0.05513615770214306]
	TIME [epoch: 12.8 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052863490556276815		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.052863490556276815 | validation: 0.08174405548330108]
	TIME [epoch: 12.8 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05178079857695314		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.05178079857695314 | validation: 0.05104768966287981]
	TIME [epoch: 12.8 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05730123012976104		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05730123012976104 | validation: 0.08427981539414652]
	TIME [epoch: 12.8 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05304011572435892		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.05304011572435892 | validation: 0.051525358900986785]
	TIME [epoch: 12.8 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052109986144555906		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.052109986144555906 | validation: 0.05980105730726309]
	TIME [epoch: 12.8 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04873754530974063		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.04873754530974063 | validation: 0.04821307414673442]
	TIME [epoch: 12.8 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05018587435392564		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.05018587435392564 | validation: 0.08248064834900734]
	TIME [epoch: 12.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05063748150573852		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.05063748150573852 | validation: 0.07563527438322204]
	TIME [epoch: 12.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05541498653260305		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.05541498653260305 | validation: 0.053098644423476275]
	TIME [epoch: 12.8 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04964358546314763		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.04964358546314763 | validation: 0.06199629124138232]
	TIME [epoch: 12.8 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04807775269655581		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.04807775269655581 | validation: 0.04759746306300962]
	TIME [epoch: 12.8 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04895454690563808		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.04895454690563808 | validation: 0.07794452962892609]
	TIME [epoch: 12.8 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710225309300865		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.04710225309300865 | validation: 0.05893264376606699]
	TIME [epoch: 12.8 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048830103830541746		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.048830103830541746 | validation: 0.047645761702002444]
	TIME [epoch: 12.8 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048306227949822665		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.048306227949822665 | validation: 0.057191639098916704]
	TIME [epoch: 12.8 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05358409048239818		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.05358409048239818 | validation: 0.140651719917256]
	TIME [epoch: 12.8 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07133619722094679		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.07133619722094679 | validation: 0.08130020662779049]
	TIME [epoch: 12.8 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10544584410533674		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.10544584410533674 | validation: 0.07403898744312694]
	TIME [epoch: 12.8 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237312881893488		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.05237312881893488 | validation: 0.0882182562250818]
	TIME [epoch: 12.8 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055076006989836335		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.055076006989836335 | validation: 0.04641652461752946]
	TIME [epoch: 12.8 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060017260029251104		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.060017260029251104 | validation: 0.08420414826336257]
	TIME [epoch: 12.8 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05207948730979633		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.05207948730979633 | validation: 0.06749286762785361]
	TIME [epoch: 12.8 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050378838591687077		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.050378838591687077 | validation: 0.05519913462282762]
	TIME [epoch: 12.8 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05159145001359208		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.05159145001359208 | validation: 0.048525774722105744]
	TIME [epoch: 12.8 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057473693148935814		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.057473693148935814 | validation: 0.07141516398306884]
	TIME [epoch: 12.8 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06920901764888068		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.06920901764888068 | validation: 0.04891429535408647]
	TIME [epoch: 12.8 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06241063300134872		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.06241063300134872 | validation: 0.07905384380588794]
	TIME [epoch: 12.8 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05449814224875516		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.05449814224875516 | validation: 0.06529850243064052]
	TIME [epoch: 12.8 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04580021308147477		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.04580021308147477 | validation: 0.053537938247247174]
	TIME [epoch: 12.8 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051061769508541785		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.051061769508541785 | validation: 0.09051475905734985]
	TIME [epoch: 12.8 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050885208787529466		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.050885208787529466 | validation: 0.06262016192806606]
	TIME [epoch: 12.8 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788586032132117		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.04788586032132117 | validation: 0.05597921350836541]
	TIME [epoch: 12.8 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047679363891176635		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.047679363891176635 | validation: 0.08092987728212821]
	TIME [epoch: 12.8 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04890816808682928		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.04890816808682928 | validation: 0.050992589376024404]
	TIME [epoch: 12.8 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05218987455902813		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.05218987455902813 | validation: 0.06731982706491431]
	TIME [epoch: 12.8 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050586888902627435		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.050586888902627435 | validation: 0.0711320105304178]
	TIME [epoch: 12.8 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049779801455963106		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.049779801455963106 | validation: 0.058324674000338285]
	TIME [epoch: 12.8 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04901567349690588		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.04901567349690588 | validation: 0.06128169542871009]
	TIME [epoch: 12.8 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04746312539874814		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.04746312539874814 | validation: 0.05955801778025436]
	TIME [epoch: 12.8 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04725710008432603		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.04725710008432603 | validation: 0.06851780947462786]
	TIME [epoch: 12.8 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048971245637488704		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.048971245637488704 | validation: 0.0551011840512659]
	TIME [epoch: 12.8 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06726164283174266		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.06726164283174266 | validation: 0.13267473237627955]
	TIME [epoch: 12.8 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666705385257117		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.0666705385257117 | validation: 0.05713209635346733]
	TIME [epoch: 12.8 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062165281820027264		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.062165281820027264 | validation: 0.060866392229309146]
	TIME [epoch: 12.8 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058615936014439		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.05058615936014439 | validation: 0.05481417685079512]
	TIME [epoch: 12.8 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05328516189324708		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.05328516189324708 | validation: 0.1170062988923077]
	TIME [epoch: 12.8 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07411340528593013		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.07411340528593013 | validation: 0.1711049307623318]
	TIME [epoch: 12.8 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07853543897436446		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.07853543897436446 | validation: 0.08423631133884257]
	TIME [epoch: 12.8 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07184761947034769		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.07184761947034769 | validation: 0.0706149109536277]
	TIME [epoch: 12.8 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714688317694126		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.05714688317694126 | validation: 0.07616536923396598]
	TIME [epoch: 12.8 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05643106511434729		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.05643106511434729 | validation: 0.054123857408751475]
	TIME [epoch: 12.8 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05234966549712108		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.05234966549712108 | validation: 0.08057863462454005]
	TIME [epoch: 12.8 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05140421308632903		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.05140421308632903 | validation: 0.0678672071896168]
	TIME [epoch: 12.8 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050133497936745		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.050133497936745 | validation: 0.05863931384375492]
	TIME [epoch: 12.8 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054495473712498195		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.054495473712498195 | validation: 0.12021255766390149]
	TIME [epoch: 12.8 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06909897316046514		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.06909897316046514 | validation: 0.05967705961928427]
	TIME [epoch: 12.8 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05757103219326478		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.05757103219326478 | validation: 0.07142947645877125]
	TIME [epoch: 12.8 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054964315909576594		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.054964315909576594 | validation: 0.056814822161572937]
	TIME [epoch: 12.8 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934939014355859		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.04934939014355859 | validation: 0.0807423829625962]
	TIME [epoch: 12.8 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046981682674143904		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.046981682674143904 | validation: 0.06402621656474547]
	TIME [epoch: 12.8 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05182391716650038		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.05182391716650038 | validation: 0.06703246253455895]
	TIME [epoch: 12.8 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049519420480760544		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.049519420480760544 | validation: 0.05451638897264759]
	TIME [epoch: 12.8 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045606511735352		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.045606511735352 | validation: 0.05809955941872354]
	TIME [epoch: 12.8 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05071184081465443		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.05071184081465443 | validation: 0.08379832182182523]
	TIME [epoch: 12.8 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189807024977016		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.05189807024977016 | validation: 0.049698579800686496]
	TIME [epoch: 12.8 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531449656432082		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.05531449656432082 | validation: 0.1071217470972989]
	TIME [epoch: 12.8 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06645413035701705		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.06645413035701705 | validation: 0.04673808777022163]
	TIME [epoch: 12.8 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05150450934021952		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.05150450934021952 | validation: 0.056144254698921106]
	TIME [epoch: 12.8 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04969003007143894		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.04969003007143894 | validation: 0.05365276417076628]
	TIME [epoch: 12.8 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594830246827271		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.04594830246827271 | validation: 0.05173513242449628]
	TIME [epoch: 12.8 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0442667105314086		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.0442667105314086 | validation: 0.05658787861291958]
	TIME [epoch: 12.8 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046935925349099857		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.046935925349099857 | validation: 0.05139521571423772]
	TIME [epoch: 12.8 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04633166464106244		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.04633166464106244 | validation: 0.06865737515012298]
	TIME [epoch: 12.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04928573236910595		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.04928573236910595 | validation: 0.061993878166849904]
	TIME [epoch: 12.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06614758028873526		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06614758028873526 | validation: 0.10078905198448779]
	TIME [epoch: 12.8 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06167053668270649		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.06167053668270649 | validation: 0.04869601287199751]
	TIME [epoch: 12.8 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051963632977653884		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.051963632977653884 | validation: 0.0460854097511892]
	TIME [epoch: 12.8 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05175742018448057		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.05175742018448057 | validation: 0.07114026384513095]
	TIME [epoch: 12.8 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04995121471275617		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.04995121471275617 | validation: 0.04604847216876654]
	TIME [epoch: 12.8 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945742343426341		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.04945742343426341 | validation: 0.05851555279257943]
	TIME [epoch: 12.8 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541146865082384		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.04541146865082384 | validation: 0.06656684451548489]
	TIME [epoch: 12.8 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734687230379855		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.04734687230379855 | validation: 0.04824380065862262]
	TIME [epoch: 12.8 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05142119277617864		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.05142119277617864 | validation: 0.07246206976985799]
	TIME [epoch: 12.8 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05121835149607156		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.05121835149607156 | validation: 0.04771487143559349]
	TIME [epoch: 12.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_132908/states/model_phi1_4c_v_mmd1_801.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6840.086 seconds.
