Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1001277344

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.54164114977194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.54164114977194 | validation: 6.219100351172506]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.089337839807979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.089337839807979 | validation: 5.596645553291347]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.752479562989179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.752479562989179 | validation: 5.106460906350427]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.226631400078385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.226631400078385 | validation: 5.228706315369806]
	TIME [epoch: 2.79 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.250628971076581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.250628971076581 | validation: 4.669629975339229]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.708164157877593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.708164157877593 | validation: 4.847118940262975]
	TIME [epoch: 2.78 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.016836162764349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.016836162764349 | validation: 4.5039323649597724]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.617784158548691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.617784158548691 | validation: 4.648726875657709]
	TIME [epoch: 2.77 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.709852302290037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.709852302290037 | validation: 4.386914810872631]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.473747982076465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.473747982076465 | validation: 4.340050842357268]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.510782439603689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.510782439603689 | validation: 4.2177308518421]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.321609154254021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.321609154254021 | validation: 4.229875770994477]
	TIME [epoch: 2.78 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.327044732633501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.327044732633501 | validation: 4.1079391610155405]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.253008003094507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.253008003094507 | validation: 4.038576971350956]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.195308481320803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.195308481320803 | validation: 4.015108437141765]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1597823759174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1597823759174 | validation: 3.9281969619608796]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.120720644829038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.120720644829038 | validation: 3.9326715120261113]
	TIME [epoch: 2.78 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.110933796522175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.110933796522175 | validation: 3.9418359205280895]
	TIME [epoch: 2.77 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.163614933132328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.163614933132328 | validation: 3.976441654440987]
	TIME [epoch: 2.77 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1930063481917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1930063481917 | validation: 3.9073148036746477]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.116753467894565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.116753467894565 | validation: 3.75863519170995]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.000239252694632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.000239252694632 | validation: 3.748616527900383]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9725236241763993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9725236241763993 | validation: 3.6952018740901966]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9525340380786393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9525340380786393 | validation: 3.6920735546533936]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.935969974459458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.935969974459458 | validation: 3.661005274958578]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.938580499940235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.938580499940235 | validation: 3.710436443703719]
	TIME [epoch: 2.78 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9760604546564884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9760604546564884 | validation: 3.7549684932854777]
	TIME [epoch: 2.78 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.0608868209132325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.0608868209132325 | validation: 3.610642952136004]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8857745947028715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8857745947028715 | validation: 3.540246227065854]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.842924202212496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.842924202212496 | validation: 3.546787347106445]
	TIME [epoch: 2.77 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.829202884398402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.829202884398402 | validation: 3.509240182884905]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8410110908684203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8410110908684203 | validation: 3.587384279813264]
	TIME [epoch: 2.78 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8625027775780585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8625027775780585 | validation: 3.486273251565535]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8181544323799086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8181544323799086 | validation: 3.4865025878512728]
	TIME [epoch: 2.79 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7955692167640804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7955692167640804 | validation: 3.4519709990056158]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7692402577344524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7692402577344524 | validation: 3.432408879528966]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.790008575683229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.790008575683229 | validation: 3.4974184529154004]
	TIME [epoch: 2.77 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7951765791729226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7951765791729226 | validation: 3.3887882102731375]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7482469270048306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7482469270048306 | validation: 3.353613788029336]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6922630445331923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6922630445331923 | validation: 3.350723576023379]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.679123154623076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.679123154623076 | validation: 3.298565014213476]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6513614277661395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6513614277661395 | validation: 3.2866040502932456]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6341168879192742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6341168879192742 | validation: 3.2644731075811353]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.615427590243162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.615427590243162 | validation: 3.2486390554298126]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.607459990284153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.607459990284153 | validation: 3.248628917076991]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.599913401884669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599913401884669 | validation: 3.2622854335127793]
	TIME [epoch: 2.77 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6179011659434788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6179011659434788 | validation: 3.2341027355056355]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5897357262019627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5897357262019627 | validation: 3.2298832789252887]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.579643234492689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.579643234492689 | validation: 3.172712289275548]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5268922866658317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5268922866658317 | validation: 3.1295779830754475]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.495880588674793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.495880588674793 | validation: 3.1165073789986475]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4753910212684134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4753910212684134 | validation: 3.0833766549007455]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.456782767508877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.456782767508877 | validation: 3.095013560152951]
	TIME [epoch: 2.78 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.437588873348468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.437588873348468 | validation: 3.0794662806309727]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4354192324903505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4354192324903505 | validation: 3.123592964378125]
	TIME [epoch: 2.78 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.472946728976219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.472946728976219 | validation: 3.158924155188155]
	TIME [epoch: 2.78 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4930011156195766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4930011156195766 | validation: 3.0823966185587284]
	TIME [epoch: 2.78 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4363215379753345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4363215379753345 | validation: 2.9902073168692844]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.345550918165155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.345550918165155 | validation: 3.0156887578436717]
	TIME [epoch: 2.77 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3671861050183196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3671861050183196 | validation: 2.989799942465414]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.332480274116865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.332480274116865 | validation: 2.926549470127914]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.263691142989412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.263691142989412 | validation: 2.856617487478519]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.181215306627062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.181215306627062 | validation: 2.8115547884610312]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1264947064661035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1264947064661035 | validation: 2.7848763140706048]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.08564741739577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.08564741739577 | validation: 2.785494884442924]
	TIME [epoch: 2.78 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.167814070972872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.167814070972872 | validation: 1.9265845377856676]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.238556599846399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.238556599846399 | validation: 2.4354540357134447]
	TIME [epoch: 2.79 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.074501008522717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.074501008522717 | validation: 1.2344918700783045]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6049693328985657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6049693328985657 | validation: 0.9825641787649101]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3172855677026996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3172855677026996 | validation: 0.7786279033976087]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0586869522579383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0586869522579383 | validation: 1.3295705237293665]
	TIME [epoch: 2.77 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.749908001582019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.749908001582019 | validation: 1.2122023056737632]
	TIME [epoch: 2.78 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5659421886710017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5659421886710017 | validation: 1.1410868272645758]
	TIME [epoch: 2.78 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.398256295463406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.398256295463406 | validation: 0.9281940689421364]
	TIME [epoch: 2.77 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1848129538559875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1848129538559875 | validation: 0.7690110700311297]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9546210845904941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9546210845904941 | validation: 0.8056907062576066]
	TIME [epoch: 2.78 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.067366676400981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.067366676400981 | validation: 0.8922424281117296]
	TIME [epoch: 2.78 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1859935141416227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1859935141416227 | validation: 0.7771281535404819]
	TIME [epoch: 2.78 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.004739812359066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.004739812359066 | validation: 0.7729265634824292]
	TIME [epoch: 2.77 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9667934717974551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9667934717974551 | validation: 0.7794108426264569]
	TIME [epoch: 2.78 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262207218855827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9262207218855827 | validation: 0.7458847004774678]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152360412377977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152360412377977 | validation: 0.7318640074407631]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966276192181803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966276192181803 | validation: 0.779646262723734]
	TIME [epoch: 2.78 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9954552087775693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9954552087775693 | validation: 0.7229174795516242]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092619539393312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9092619539393312 | validation: 0.7354818882748024]
	TIME [epoch: 2.78 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889782527768756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889782527768756 | validation: 0.7319049841720873]
	TIME [epoch: 2.78 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036661624782388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9036661624782388 | validation: 0.7247895813917333]
	TIME [epoch: 2.78 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891086435852887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891086435852887 | validation: 0.7147809290110083]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774297402445792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774297402445792 | validation: 0.715553367000725]
	TIME [epoch: 2.77 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994260379901977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8994260379901977 | validation: 0.7180876754077308]
	TIME [epoch: 2.77 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760718740694169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760718740694169 | validation: 0.7164587906652418]
	TIME [epoch: 2.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871000247171144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871000247171144 | validation: 0.7017136608189537]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893893597389265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893893597389265 | validation: 0.7022917593042504]
	TIME [epoch: 2.77 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8939666485640702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8939666485640702 | validation: 0.8586216194172931]
	TIME [epoch: 2.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.109053821508328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.109053821508328 | validation: 1.0060680482652076]
	TIME [epoch: 2.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2643162654408935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2643162654408935 | validation: 0.7757460656764066]
	TIME [epoch: 2.77 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9645371531956716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9645371531956716 | validation: 0.8087755056565863]
	TIME [epoch: 2.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9664224073612332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9664224073612332 | validation: 0.8999380894871898]
	TIME [epoch: 2.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1437365488337814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1437365488337814 | validation: 0.7395810950110073]
	TIME [epoch: 2.77 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9260999916392217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9260999916392217 | validation: 0.7298416520617386]
	TIME [epoch: 2.77 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842113438184755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842113438184755 | validation: 0.7294184835515364]
	TIME [epoch: 2.77 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9427115103210827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9427115103210827 | validation: 0.7620773446479353]
	TIME [epoch: 2.77 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.935884756470154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.935884756470154 | validation: 0.6955740870383947]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860945920647265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860945920647265 | validation: 0.7397371224782128]
	TIME [epoch: 2.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865964179492449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8865964179492449 | validation: 0.7315017548862123]
	TIME [epoch: 2.77 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8900042833278571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8900042833278571 | validation: 0.6993624908416408]
	TIME [epoch: 2.78 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741700232539905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741700232539905 | validation: 0.7847816091959272]
	TIME [epoch: 2.77 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9484646599337792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9484646599337792 | validation: 0.759001388447038]
	TIME [epoch: 2.77 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9402601451964294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9402601451964294 | validation: 0.7253724482774528]
	TIME [epoch: 2.77 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678406113130299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8678406113130299 | validation: 0.6982353206594303]
	TIME [epoch: 2.78 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84910370889206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84910370889206 | validation: 1.006423398587446]
	TIME [epoch: 2.77 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3376372424232819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3376372424232819 | validation: 0.9088291299199857]
	TIME [epoch: 2.77 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0626855474138586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0626855474138586 | validation: 0.9837172350660558]
	TIME [epoch: 2.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1664476745270296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1664476745270296 | validation: 0.7733783519196042]
	TIME [epoch: 2.77 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9257649498508085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9257649498508085 | validation: 0.7162929387661947]
	TIME [epoch: 2.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906974866241032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906974866241032 | validation: 0.7364249147075004]
	TIME [epoch: 2.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986244470678051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8986244470678051 | validation: 0.7281073818530741]
	TIME [epoch: 2.77 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791367585764667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8791367585764667 | validation: 0.7198251706603334]
	TIME [epoch: 2.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591603738269147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591603738269147 | validation: 0.7371013323405191]
	TIME [epoch: 2.77 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8594730605591705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8594730605591705 | validation: 0.716136921072938]
	TIME [epoch: 2.77 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456236108365559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456236108365559 | validation: 0.7060264483410233]
	TIME [epoch: 2.77 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358823355380055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358823355380055 | validation: 0.7076503245131269]
	TIME [epoch: 2.77 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433349934133341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8433349934133341 | validation: 0.7639263151888699]
	TIME [epoch: 2.77 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9126342231882023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9126342231882023 | validation: 0.8440234987468106]
	TIME [epoch: 2.77 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0214168062742222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0214168062742222 | validation: 0.7996956728552236]
	TIME [epoch: 2.77 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9906681783389175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9906681783389175 | validation: 0.7008335389395276]
	TIME [epoch: 2.77 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329907686434723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329907686434723 | validation: 0.721868858859624]
	TIME [epoch: 2.78 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8756161910499958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8756161910499958 | validation: 0.7890032507124701]
	TIME [epoch: 2.77 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9379366803565762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9379366803565762 | validation: 0.7159554376587836]
	TIME [epoch: 2.77 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848201490968548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848201490968548 | validation: 0.6965103442229568]
	TIME [epoch: 2.77 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8485789348517571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8485789348517571 | validation: 0.7359559965891691]
	TIME [epoch: 2.77 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011157183925217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9011157183925217 | validation: 0.7915511937002542]
	TIME [epoch: 2.77 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9563251894216899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9563251894216899 | validation: 0.7220680000151417]
	TIME [epoch: 2.77 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413236884124409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413236884124409 | validation: 0.7625153417964698]
	TIME [epoch: 2.78 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712943476667456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712943476667456 | validation: 0.9439100307324086]
	TIME [epoch: 2.78 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0473562661983429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0473562661983429 | validation: 0.7097971919810886]
	TIME [epoch: 2.77 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8429610131841554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8429610131841554 | validation: 0.7122658046470728]
	TIME [epoch: 2.77 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8302644664723764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8302644664723764 | validation: 0.7869235139821888]
	TIME [epoch: 2.77 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9141244246124515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9141244246124515 | validation: 0.7656324029511716]
	TIME [epoch: 2.77 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519127246767869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519127246767869 | validation: 0.7483309613197533]
	TIME [epoch: 2.77 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8813370033983849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8813370033983849 | validation: 0.7654957965389783]
	TIME [epoch: 2.78 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8984202681892188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8984202681892188 | validation: 0.8820385542648137]
	TIME [epoch: 2.78 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0759954258435207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0759954258435207 | validation: 0.7531741967476696]
	TIME [epoch: 2.77 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8584522461577763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8584522461577763 | validation: 0.8223359299714975]
	TIME [epoch: 2.77 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9218698598348852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9218698598348852 | validation: 0.7440828898476611]
	TIME [epoch: 2.77 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152007708388785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152007708388785 | validation: 0.6825359864388145]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_146.pth
	Model improved!!!
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7896575714519277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7896575714519277 | validation: 0.7219804051315771]
	TIME [epoch: 2.77 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8214452393122236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8214452393122236 | validation: 0.8218382011994367]
	TIME [epoch: 2.78 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889242164447076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889242164447076 | validation: 0.8062109088727086]
	TIME [epoch: 2.78 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9119846421409861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9119846421409861 | validation: 0.8794342822592752]
	TIME [epoch: 2.77 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9373890178963978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9373890178963978 | validation: 0.7157104059674797]
	TIME [epoch: 2.78 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491902630813871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8491902630813871 | validation: 0.7634769539330656]
	TIME [epoch: 2.77 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575208633536687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8575208633536687 | validation: 0.7607968601606471]
	TIME [epoch: 2.77 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8995599576615382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8995599576615382 | validation: 0.7421774414620836]
	TIME [epoch: 2.77 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7999361880739398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7999361880739398 | validation: 0.6852439188361027]
	TIME [epoch: 2.77 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981077763225846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7981077763225846 | validation: 0.8042373290520373]
	TIME [epoch: 2.77 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506086182773743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8506086182773743 | validation: 0.7649331728525077]
	TIME [epoch: 2.77 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8776885403022722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8776885403022722 | validation: 0.8568650756353797]
	TIME [epoch: 2.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8741337025812171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8741337025812171 | validation: 0.6764396656088323]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7675773651365478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7675773651365478 | validation: 0.6970776236455788]
	TIME [epoch: 2.78 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498326102607235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7498326102607235 | validation: 0.8722707301585791]
	TIME [epoch: 2.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9715003852708988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9715003852708988 | validation: 0.9937166333399896]
	TIME [epoch: 2.78 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1091154758186152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1091154758186152 | validation: 0.6961153848872673]
	TIME [epoch: 2.78 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858293919027004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858293919027004 | validation: 0.6442069284584482]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640745160118709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7640745160118709 | validation: 0.6961744126218874]
	TIME [epoch: 2.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77914716632655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77914716632655 | validation: 0.7363039127482756]
	TIME [epoch: 2.78 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788131862150226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788131862150226 | validation: 0.7517903835745321]
	TIME [epoch: 2.77 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8834214674490076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8834214674490076 | validation: 0.968710697636467]
	TIME [epoch: 2.78 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.983855788376803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.983855788376803 | validation: 0.728645341653673]
	TIME [epoch: 2.78 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.849184394960624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.849184394960624 | validation: 0.7518991298276787]
	TIME [epoch: 2.78 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079856303586438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8079856303586438 | validation: 0.6999655550927687]
	TIME [epoch: 2.78 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588623945465802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588623945465802 | validation: 0.6648629564581259]
	TIME [epoch: 2.78 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7074071632776224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7074071632776224 | validation: 0.6255248992913993]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.702820842556516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.702820842556516 | validation: 0.7067461014969276]
	TIME [epoch: 2.78 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7416802571241243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7416802571241243 | validation: 0.8159683617700356]
	TIME [epoch: 2.78 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9802754010733028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9802754010733028 | validation: 0.8508463321256177]
	TIME [epoch: 2.78 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9608669182971343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9608669182971343 | validation: 0.6654203068040246]
	TIME [epoch: 2.78 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617464831862714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617464831862714 | validation: 0.6187903960586612]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7357618236646818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7357618236646818 | validation: 0.6903747654502479]
	TIME [epoch: 2.78 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7377099200121898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7377099200121898 | validation: 0.6410428240474554]
	TIME [epoch: 2.78 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7277921065658163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7277921065658163 | validation: 0.6843778412235266]
	TIME [epoch: 2.78 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019420387684789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7019420387684789 | validation: 0.6297597359643712]
	TIME [epoch: 2.78 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7174271103221238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7174271103221238 | validation: 0.7513331839691575]
	TIME [epoch: 2.78 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562419721829539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562419721829539 | validation: 0.7815967093364272]
	TIME [epoch: 2.78 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9723875861824138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9723875861824138 | validation: 0.956607997678236]
	TIME [epoch: 2.78 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9358727206642009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9358727206642009 | validation: 0.6673866667471314]
	TIME [epoch: 2.78 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215942849221184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215942849221184 | validation: 0.658787560534047]
	TIME [epoch: 2.78 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358842300876264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358842300876264 | validation: 0.7048068410565964]
	TIME [epoch: 2.78 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186087507277087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186087507277087 | validation: 0.6096357962925743]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_189.pth
	Model improved!!!
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66436035261824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66436035261824 | validation: 0.6656639120141885]
	TIME [epoch: 2.78 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6848379010406128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6848379010406128 | validation: 0.8443394673395958]
	TIME [epoch: 2.78 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8747087715441714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8747087715441714 | validation: 0.9031138511193902]
	TIME [epoch: 2.78 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.035347486591687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.035347486591687 | validation: 0.6204178292665523]
	TIME [epoch: 2.77 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903127610228937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6903127610228937 | validation: 0.7482050773502398]
	TIME [epoch: 2.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790670520524591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790670520524591 | validation: 0.64175208201667]
	TIME [epoch: 2.77 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7994058979970791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7994058979970791 | validation: 0.6108032764869066]
	TIME [epoch: 2.77 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6566884505352414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6566884505352414 | validation: 0.6910413515831305]
	TIME [epoch: 2.77 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7297353273245034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7297353273245034 | validation: 0.7683766797330516]
	TIME [epoch: 2.78 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.81116074645754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.81116074645754 | validation: 0.7491817289771076]
	TIME [epoch: 2.77 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963357049542674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963357049542674 | validation: 0.7527473936041003]
	TIME [epoch: 2.78 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430338614616702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7430338614616702 | validation: 0.5739259741212562]
	TIME [epoch: 173 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6432513960250873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6432513960250873 | validation: 0.5908458443222646]
	TIME [epoch: 5.97 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6353167333933041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6353167333933041 | validation: 0.6693058762955634]
	TIME [epoch: 5.96 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176338996307282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7176338996307282 | validation: 0.770946952947186]
	TIME [epoch: 5.96 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861065158330197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861065158330197 | validation: 0.6729357881385707]
	TIME [epoch: 5.96 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8342290317786977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8342290317786977 | validation: 0.6288819710636919]
	TIME [epoch: 5.96 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6587938157922665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6587938157922665 | validation: 0.6075425862491615]
	TIME [epoch: 5.96 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6293101454213781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6293101454213781 | validation: 0.6385820907015711]
	TIME [epoch: 5.96 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6710088145409993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6710088145409993 | validation: 0.6833506465315443]
	TIME [epoch: 5.97 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7019164902418282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7019164902418282 | validation: 0.700148114512735]
	TIME [epoch: 5.96 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7151874528049476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7151874528049476 | validation: 0.6329512845771261]
	TIME [epoch: 5.96 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6782485117897892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6782485117897892 | validation: 0.5811558226418372]
	TIME [epoch: 5.96 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248651628793244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6248651628793244 | validation: 0.5729090093658481]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5782170165306343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5782170165306343 | validation: 0.5440177298606033]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043183448178244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043183448178244 | validation: 0.8024877435829643]
	TIME [epoch: 5.97 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528104632530377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528104632530377 | validation: 0.7806644605442834]
	TIME [epoch: 5.97 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9716507243205189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9716507243205189 | validation: 0.5732866228024344]
	TIME [epoch: 5.97 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163120036753813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6163120036753813 | validation: 0.6905138908667642]
	TIME [epoch: 5.96 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.700286469322075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.700286469322075 | validation: 0.5857408442949309]
	TIME [epoch: 5.97 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843050139570039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6843050139570039 | validation: 0.5495659185157052]
	TIME [epoch: 5.97 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5846823781040678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5846823781040678 | validation: 0.667134695816907]
	TIME [epoch: 5.97 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6362216378630128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6362216378630128 | validation: 0.7214870907371678]
	TIME [epoch: 5.97 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915324302810846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7915324302810846 | validation: 0.6905900343759455]
	TIME [epoch: 5.97 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.661321285074696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.661321285074696 | validation: 0.5088779201497095]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5609173566116501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5609173566116501 | validation: 0.5608944715221775]
	TIME [epoch: 5.97 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5804619607153451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5804619607153451 | validation: 0.6111201053772899]
	TIME [epoch: 5.97 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401894099631739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401894099631739 | validation: 0.7709763927422548]
	TIME [epoch: 5.97 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8477340594826848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8477340594826848 | validation: 0.5826460569205069]
	TIME [epoch: 5.97 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7059444219594347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7059444219594347 | validation: 0.5302374492300043]
	TIME [epoch: 5.97 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5575918541039354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5575918541039354 | validation: 0.5408538854118611]
	TIME [epoch: 5.97 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559313956744798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.559313956744798 | validation: 0.6442940734387609]
	TIME [epoch: 5.97 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64330594733725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.64330594733725 | validation: 0.7804807053747121]
	TIME [epoch: 5.97 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456915801162717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456915801162717 | validation: 0.6174837467676805]
	TIME [epoch: 5.97 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7311858854990888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7311858854990888 | validation: 0.5289615412564462]
	TIME [epoch: 5.97 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5583331299787397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5583331299787397 | validation: 0.5649198947137404]
	TIME [epoch: 5.98 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5851606879677754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5851606879677754 | validation: 0.6466362189961021]
	TIME [epoch: 5.97 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796304437161345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6796304437161345 | validation: 0.6749781585914367]
	TIME [epoch: 5.98 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.688301241366082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.688301241366082 | validation: 0.5385073975477287]
	TIME [epoch: 5.97 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5956498512826334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5956498512826334 | validation: 0.49831203517572864]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5663325472796112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5663325472796112 | validation: 0.6241543014103563]
	TIME [epoch: 5.95 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6094108441232201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6094108441232201 | validation: 0.5699289193204294]
	TIME [epoch: 5.95 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.654226216511604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.654226216511604 | validation: 0.6472872474976932]
	TIME [epoch: 5.96 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437576491503066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437576491503066 | validation: 0.5101360337642044]
	TIME [epoch: 5.95 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6134471218960124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6134471218960124 | validation: 0.5728741380356989]
	TIME [epoch: 5.96 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5640498949916153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5640498949916153 | validation: 0.5523527204038557]
	TIME [epoch: 5.95 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5749084270855919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5749084270855919 | validation: 0.5627429044594437]
	TIME [epoch: 5.94 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5838104161126297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5838104161126297 | validation: 0.4855784977341655]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5220047526002499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5220047526002499 | validation: 0.5029139309271393]
	TIME [epoch: 5.98 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5018390352551122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5018390352551122 | validation: 0.5075119490230445]
	TIME [epoch: 5.98 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5640659725210408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5640659725210408 | validation: 0.6997487802051765]
	TIME [epoch: 5.98 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888517869644952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7888517869644952 | validation: 0.5169027802769937]
	TIME [epoch: 5.98 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5846584382842324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5846584382842324 | validation: 0.4412966427389147]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48795667026986933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48795667026986933 | validation: 0.5858127705031103]
	TIME [epoch: 5.98 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5498491399487853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5498491399487853 | validation: 0.5486263022932292]
	TIME [epoch: 5.98 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6233762212023991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6233762212023991 | validation: 0.6341661177661096]
	TIME [epoch: 5.98 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730334004408022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730334004408022 | validation: 0.479897456667835]
	TIME [epoch: 5.98 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5481879143161988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5481879143161988 | validation: 0.44820221440850966]
	TIME [epoch: 5.99 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.473048273754486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.473048273754486 | validation: 0.5123430902375015]
	TIME [epoch: 5.98 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.517034183287079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.517034183287079 | validation: 0.5138888521621257]
	TIME [epoch: 5.99 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326298435369411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5326298435369411 | validation: 0.5915381764202792]
	TIME [epoch: 5.98 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6381359081239433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6381359081239433 | validation: 0.490538982592871]
	TIME [epoch: 5.99 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5422760689228339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5422760689228339 | validation: 0.4136988850686623]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44695522557139206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44695522557139206 | validation: 0.5525772957036671]
	TIME [epoch: 5.99 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4956322237606292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4956322237606292 | validation: 0.5344124798798738]
	TIME [epoch: 5.98 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5830159068794517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5830159068794517 | validation: 0.6182886204424163]
	TIME [epoch: 5.99 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359603574460835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359603574460835 | validation: 0.45290016353560764]
	TIME [epoch: 5.98 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5340020090892346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5340020090892346 | validation: 0.49431270849177605]
	TIME [epoch: 5.99 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4948081294251779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4948081294251779 | validation: 0.488159646167142]
	TIME [epoch: 5.98 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46735795180295603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46735795180295603 | validation: 0.43739311115230095]
	TIME [epoch: 5.98 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4587597661543339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4587597661543339 | validation: 0.4658249829833633]
	TIME [epoch: 5.98 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47266902484297996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47266902484297996 | validation: 0.5894893285477482]
	TIME [epoch: 5.99 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6290190072668591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6290190072668591 | validation: 0.6982463019615638]
	TIME [epoch: 5.99 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9499753200957767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9499753200957767 | validation: 0.4548779020073219]
	TIME [epoch: 5.99 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4807659072986152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4807659072986152 | validation: 0.5225965538925683]
	TIME [epoch: 5.99 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5608812609956125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5608812609956125 | validation: 0.42741284029091625]
	TIME [epoch: 5.99 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4673893655773646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4673893655773646 | validation: 0.37365914408057854]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088090014273948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4088090014273948 | validation: 0.4484215311772978]
	TIME [epoch: 5.98 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42928755330114676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42928755330114676 | validation: 0.4143120515876129]
	TIME [epoch: 5.99 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4502587405224209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4502587405224209 | validation: 0.49936838751958956]
	TIME [epoch: 5.99 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46115155755647524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46115155755647524 | validation: 0.3996507620241568]
	TIME [epoch: 5.99 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4163595625475213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4163595625475213 | validation: 0.40542868016927747]
	TIME [epoch: 5.98 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4034374998744923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4034374998744923 | validation: 0.5102460063692158]
	TIME [epoch: 5.99 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4935444516804985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4935444516804985 | validation: 0.5032464375218793]
	TIME [epoch: 5.99 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5879223735141613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5879223735141613 | validation: 0.5154139401021584]
	TIME [epoch: 5.99 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5102680820436903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5102680820436903 | validation: 0.4294651057045504]
	TIME [epoch: 5.98 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060281170324208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5060281170324208 | validation: 0.3996692618983979]
	TIME [epoch: 5.99 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3904977424554101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3904977424554101 | validation: 0.40252249758329867]
	TIME [epoch: 5.99 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4167151567707212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4167151567707212 | validation: 0.40377274932161766]
	TIME [epoch: 5.98 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41055501853020815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41055501853020815 | validation: 0.4028567849947637]
	TIME [epoch: 5.98 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41581327538854995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41581327538854995 | validation: 0.3238080688615089]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37869486019213566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37869486019213566 | validation: 0.3584754661013645]
	TIME [epoch: 5.97 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.323632951758773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.323632951758773 | validation: 0.3664838037722231]
	TIME [epoch: 5.99 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38397698234848165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38397698234848165 | validation: 0.5533577573218494]
	TIME [epoch: 5.98 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5551007542722952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5551007542722952 | validation: 0.45051341297397474]
	TIME [epoch: 5.98 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5579055579246698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5579055579246698 | validation: 0.47062969053970716]
	TIME [epoch: 5.97 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4147688319068761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4147688319068761 | validation: 0.4410277575973675]
	TIME [epoch: 5.99 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45397113640427633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45397113640427633 | validation: 0.5009552683927704]
	TIME [epoch: 5.98 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4464217074364524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4464217074364524 | validation: 0.36361800865809174]
	TIME [epoch: 6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38700755988380126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38700755988380126 | validation: 0.28326414158931484]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33010795286867606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33010795286867606 | validation: 0.29478008963772595]
	TIME [epoch: 5.99 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2900214039113232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2900214039113232 | validation: 0.28337605862959947]
	TIME [epoch: 5.97 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2872623346066249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2872623346066249 | validation: 0.3154798594516516]
	TIME [epoch: 5.98 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473697508550766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3473697508550766 | validation: 0.5968957571890984]
	TIME [epoch: 5.98 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6256428527852813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6256428527852813 | validation: 0.42669465698462067]
	TIME [epoch: 5.98 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5282556369204838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5282556369204838 | validation: 0.41336590146118407]
	TIME [epoch: 5.98 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37085055078064094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37085055078064094 | validation: 0.37146080246156726]
	TIME [epoch: 5.98 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3921656265857736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3921656265857736 | validation: 0.32360680017245064]
	TIME [epoch: 5.98 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30649832334852406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30649832334852406 | validation: 0.3211890942206471]
	TIME [epoch: 5.98 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31643133952110775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31643133952110775 | validation: 0.2472703941133988]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.275923194853867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.275923194853867 | validation: 0.3738973249086091]
	TIME [epoch: 5.98 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29534090996564777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29534090996564777 | validation: 0.45397673728545435]
	TIME [epoch: 5.98 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49753310714705967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49753310714705967 | validation: 0.5708967997499155]
	TIME [epoch: 6 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.621214934376924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.621214934376924 | validation: 0.5286975230541]
	TIME [epoch: 5.99 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.63013225228068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.63013225228068 | validation: 0.2598558154502065]
	TIME [epoch: 6 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2887497594924129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2887497594924129 | validation: 0.5649994490034599]
	TIME [epoch: 5.99 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4842467284990748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4842467284990748 | validation: 0.24828586372854566]
	TIME [epoch: 5.99 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28154697773243514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28154697773243514 | validation: 0.3035304606579776]
	TIME [epoch: 5.98 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2997615382887819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2997615382887819 | validation: 0.33414796659552465]
	TIME [epoch: 5.98 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3257108035870539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3257108035870539 | validation: 0.3610034496862808]
	TIME [epoch: 5.98 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32164721816811814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32164721816811814 | validation: 0.35935239091444293]
	TIME [epoch: 5.99 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41481168969806476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41481168969806476 | validation: 0.6773567977242844]
	TIME [epoch: 5.98 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5557014548667552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5557014548667552 | validation: 0.2433740694165712]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22701487477770946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22701487477770946 | validation: 0.30310500731796175]
	TIME [epoch: 5.98 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3659004244099968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3659004244099968 | validation: 0.48383917057027087]
	TIME [epoch: 5.98 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40192831321900147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40192831321900147 | validation: 0.33004718483926765]
	TIME [epoch: 5.99 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3204473262045978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3204473262045978 | validation: 0.4320701387744473]
	TIME [epoch: 5.99 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46936542138887305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46936542138887305 | validation: 0.2785654915479118]
	TIME [epoch: 5.98 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35111426618868313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35111426618868313 | validation: 0.3388476461619465]
	TIME [epoch: 5.98 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28281563884828287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28281563884828287 | validation: 0.3133477436714429]
	TIME [epoch: 5.98 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31127563226019883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31127563226019883 | validation: 0.3705185498972036]
	TIME [epoch: 5.98 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3915473357638596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3915473357638596 | validation: 0.4277478528649734]
	TIME [epoch: 5.98 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40413119011908705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40413119011908705 | validation: 0.2676902520521288]
	TIME [epoch: 5.98 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37476577926516386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37476577926516386 | validation: 0.4006385036543124]
	TIME [epoch: 5.98 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3228622965374395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3228622965374395 | validation: 0.2572562795235277]
	TIME [epoch: 5.98 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27492232984730075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27492232984730075 | validation: 0.22478342803873128]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24866195951915335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24866195951915335 | validation: 0.28829299237360495]
	TIME [epoch: 5.98 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28126030003630414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28126030003630414 | validation: 0.3080296013232229]
	TIME [epoch: 5.98 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38750272973280125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38750272973280125 | validation: 0.5170699543551626]
	TIME [epoch: 5.98 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3790979823296204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3790979823296204 | validation: 0.22222006879406142]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_339.pth
	Model improved!!!
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24291366392418964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24291366392418964 | validation: 0.23109786887094336]
	TIME [epoch: 5.94 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25615287705810724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25615287705810724 | validation: 0.2629487556950456]
	TIME [epoch: 5.94 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23581434062097179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23581434062097179 | validation: 0.22143890901790156]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25177512763922777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25177512763922777 | validation: 0.35153861223825444]
	TIME [epoch: 5.94 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313859748354384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.313859748354384 | validation: 0.3884008820196747]
	TIME [epoch: 5.95 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.393994568211955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.393994568211955 | validation: 0.5295360055983084]
	TIME [epoch: 5.96 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7283804799444243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7283804799444243 | validation: 0.40379369016164635]
	TIME [epoch: 5.97 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3594094290072633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3594094290072633 | validation: 0.4578673039229944]
	TIME [epoch: 5.99 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39768414441114586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39768414441114586 | validation: 0.3390477815759192]
	TIME [epoch: 5.97 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40356344858059645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40356344858059645 | validation: 0.3161468715444654]
	TIME [epoch: 5.99 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29239637747316843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29239637747316843 | validation: 0.33306398120807273]
	TIME [epoch: 5.97 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28765162518117243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28765162518117243 | validation: 0.19964124488054724]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23146933694221436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23146933694221436 | validation: 0.20643722079115998]
	TIME [epoch: 5.94 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19117284114650268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19117284114650268 | validation: 0.20422290096124832]
	TIME [epoch: 5.95 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19212253635505247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19212253635505247 | validation: 0.1840030544552388]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19604177895546548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19604177895546548 | validation: 0.30664541665349754]
	TIME [epoch: 5.98 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25026494362568225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25026494362568225 | validation: 0.4156598634927507]
	TIME [epoch: 5.97 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4607718506813535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4607718506813535 | validation: 0.39904291366283373]
	TIME [epoch: 5.97 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33151465577832484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33151465577832484 | validation: 0.16074211616806497]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22557983246095334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22557983246095334 | validation: 0.39314715956195256]
	TIME [epoch: 5.96 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886441531706197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2886441531706197 | validation: 0.33569436465813807]
	TIME [epoch: 5.95 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3978103845084429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3978103845084429 | validation: 0.3563419580311705]
	TIME [epoch: 5.95 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28911160755741483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28911160755741483 | validation: 0.24955643888148274]
	TIME [epoch: 5.95 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2915815877404914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2915815877404914 | validation: 0.4447981313983402]
	TIME [epoch: 5.95 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41215337829767107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41215337829767107 | validation: 0.22104745649393107]
	TIME [epoch: 5.95 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29596910891614536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29596910891614536 | validation: 0.3409198043247641]
	TIME [epoch: 5.95 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2697304898585789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2697304898585789 | validation: 0.18280401846814961]
	TIME [epoch: 5.97 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19702428228645244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19702428228645244 | validation: 0.16710412745323805]
	TIME [epoch: 5.97 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20624223456154472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20624223456154472 | validation: 0.36088296903356565]
	TIME [epoch: 5.95 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27920333563170924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27920333563170924 | validation: 0.3198703009999655]
	TIME [epoch: 5.97 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.359586075282729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.359586075282729 | validation: 0.3091705700414269]
	TIME [epoch: 5.97 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28331801484131197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28331801484131197 | validation: 0.14957373372773874]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19388562982086396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19388562982086396 | validation: 0.24319598114940033]
	TIME [epoch: 5.98 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19050158667344783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19050158667344783 | validation: 0.168321453956227]
	TIME [epoch: 5.97 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2066068121229565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2066068121229565 | validation: 0.38673739919654054]
	TIME [epoch: 5.97 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2902532036489621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2902532036489621 | validation: 0.21517885294076877]
	TIME [epoch: 5.97 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2954501306149192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2954501306149192 | validation: 0.3789879609013958]
	TIME [epoch: 5.98 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28029796891434744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28029796891434744 | validation: 0.33309163154529675]
	TIME [epoch: 5.98 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35356815674011566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35356815674011566 | validation: 0.46563123310409527]
	TIME [epoch: 5.98 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5067423985950024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5067423985950024 | validation: 0.26524824109034606]
	TIME [epoch: 5.97 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4060465975666239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4060465975666239 | validation: 0.32474612832555616]
	TIME [epoch: 5.98 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28261874283412186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28261874283412186 | validation: 0.26117366300351774]
	TIME [epoch: 5.98 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23729488567006501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23729488567006501 | validation: 0.22759693196770342]
	TIME [epoch: 5.98 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2702495136755415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2702495136755415 | validation: 0.3088006403181471]
	TIME [epoch: 5.98 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25655518690756923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25655518690756923 | validation: 0.17356589225052843]
	TIME [epoch: 5.98 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21762169397305428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21762169397305428 | validation: 0.15580160389470005]
	TIME [epoch: 5.98 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16503094865729515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16503094865729515 | validation: 0.19307641178179802]
	TIME [epoch: 5.98 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15760263178876277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15760263178876277 | validation: 0.13402743307709947]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_387.pth
	Model improved!!!
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15630432042101625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15630432042101625 | validation: 0.27470763848214]
	TIME [epoch: 5.98 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18378604030081597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18378604030081597 | validation: 0.38097598493907203]
	TIME [epoch: 5.98 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4242216637507412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4242216637507412 | validation: 0.4788267792747828]
	TIME [epoch: 5.98 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3480888502052737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3480888502052737 | validation: 0.2778594345340397]
	TIME [epoch: 5.98 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27093808114021506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27093808114021506 | validation: 0.19579145687019736]
	TIME [epoch: 5.98 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23515783613162974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23515783613162974 | validation: 0.288842978362928]
	TIME [epoch: 5.97 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22364641247277445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22364641247277445 | validation: 0.14409532180085904]
	TIME [epoch: 5.99 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17701929610880118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17701929610880118 | validation: 0.17021993653106446]
	TIME [epoch: 5.97 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14452660704354447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14452660704354447 | validation: 0.1338296602543151]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_396.pth
	Model improved!!!
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13976982012869943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13976982012869943 | validation: 0.16818852744135482]
	TIME [epoch: 5.98 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13661650545254653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13661650545254653 | validation: 0.21340495624282033]
	TIME [epoch: 5.98 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24503824919523146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24503824919523146 | validation: 0.6408221495961984]
	TIME [epoch: 5.98 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5162657857707508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5162657857707508 | validation: 0.16353548387643532]
	TIME [epoch: 5.99 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1951639404317455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1951639404317455 | validation: 0.1423999446554518]
	TIME [epoch: 5.98 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14041723001455858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14041723001455858 | validation: 0.2172381026977417]
	TIME [epoch: 5.98 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16246143083297668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16246143083297668 | validation: 0.18244689331932637]
	TIME [epoch: 5.98 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2714337597790173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2714337597790173 | validation: 0.5077189785774071]
	TIME [epoch: 5.98 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36014564630958457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36014564630958457 | validation: 0.1635807152647408]
	TIME [epoch: 5.98 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23431425229404082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23431425229404082 | validation: 0.21157953489959783]
	TIME [epoch: 5.99 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15760553892482843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15760553892482843 | validation: 0.20574518795921187]
	TIME [epoch: 5.98 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1722167066955804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1722167066955804 | validation: 0.2841543678677976]
	TIME [epoch: 5.99 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.329943384537612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.329943384537612 | validation: 0.4855868876792615]
	TIME [epoch: 5.98 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3948610754440033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3948610754440033 | validation: 0.14369954105849242]
	TIME [epoch: 5.99 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16945274382879974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16945274382879974 | validation: 0.11886971217662701]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1496323089555975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1496323089555975 | validation: 0.21264629064767454]
	TIME [epoch: 5.99 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16349251218399732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16349251218399732 | validation: 0.12578855134705136]
	TIME [epoch: 5.98 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17854981466480277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17854981466480277 | validation: 0.3430925795442198]
	TIME [epoch: 5.98 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2410715602796986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2410715602796986 | validation: 0.34150938956307664]
	TIME [epoch: 5.97 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3754761687334466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3754761687334466 | validation: 0.20065686359065626]
	TIME [epoch: 5.98 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1841341692949425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1841341692949425 | validation: 0.1722190782580848]
	TIME [epoch: 5.97 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14527514780276898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14527514780276898 | validation: 0.13913394360147688]
	TIME [epoch: 5.98 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15141459423209966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15141459423209966 | validation: 0.21846394340941303]
	TIME [epoch: 5.97 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17678485603009997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17678485603009997 | validation: 0.12137251485442686]
	TIME [epoch: 5.98 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680341157524976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1680341157524976 | validation: 0.5023250763202936]
	TIME [epoch: 5.97 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3335729055901697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3335729055901697 | validation: 0.14009019527061764]
	TIME [epoch: 5.98 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19955014857012432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19955014857012432 | validation: 0.22912295041207972]
	TIME [epoch: 5.97 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21141943057826382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21141943057826382 | validation: 0.33590734758148344]
	TIME [epoch: 5.98 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2945507907670564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2945507907670564 | validation: 0.17914450192623055]
	TIME [epoch: 5.98 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21172838012096143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21172838012096143 | validation: 0.23118685445879225]
	TIME [epoch: 5.98 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637891527513635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1637891527513635 | validation: 0.12831499370022897]
	TIME [epoch: 5.98 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13649229123443754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13649229123443754 | validation: 0.16755445565565288]
	TIME [epoch: 5.98 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13551800782582846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13551800782582846 | validation: 0.2049688020749222]
	TIME [epoch: 5.98 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19856476455878372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19856476455878372 | validation: 0.31593821075736184]
	TIME [epoch: 5.98 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26846125169392837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26846125169392837 | validation: 0.20097687997917146]
	TIME [epoch: 5.98 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2953981763821517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2953981763821517 | validation: 0.5592212063013532]
	TIME [epoch: 5.98 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39846763138773783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39846763138773783 | validation: 0.21133361776100099]
	TIME [epoch: 5.98 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17553211662843063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17553211662843063 | validation: 0.19736513544136866]
	TIME [epoch: 5.98 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2791318030481536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2791318030481536 | validation: 0.14649470456582933]
	TIME [epoch: 5.98 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14604209461024625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14604209461024625 | validation: 0.2212661757674285]
	TIME [epoch: 5.98 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15537392084129514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15537392084129514 | validation: 0.11598584459405158]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14738153235139298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14738153235139298 | validation: 0.252196513450356]
	TIME [epoch: 5.98 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21326516121608025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21326516121608025 | validation: 0.26770100413629877]
	TIME [epoch: 5.97 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2501310180425401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2501310180425401 | validation: 0.15486893004655947]
	TIME [epoch: 5.98 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19280627838990122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19280627838990122 | validation: 0.3464824672243738]
	TIME [epoch: 5.97 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2263012097374679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2263012097374679 | validation: 0.13583105124641534]
	TIME [epoch: 5.98 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15217555394138246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15217555394138246 | validation: 0.15934670387101768]
	TIME [epoch: 5.98 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16029535571240533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16029535571240533 | validation: 0.18918217259948622]
	TIME [epoch: 5.97 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741165647831864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1741165647831864 | validation: 0.19519807605050246]
	TIME [epoch: 5.97 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21288331245638414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21288331245638414 | validation: 0.3146526615434471]
	TIME [epoch: 5.97 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.211109685963591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.211109685963591 | validation: 0.12856350817767787]
	TIME [epoch: 5.98 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17399237618081703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17399237618081703 | validation: 0.23767311687540602]
	TIME [epoch: 5.98 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14393065970733587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14393065970733587 | validation: 0.09266274576582985]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_449.pth
	Model improved!!!
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14625663382414378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14625663382414378 | validation: 0.26365078949112186]
	TIME [epoch: 5.98 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16024208080186428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16024208080186428 | validation: 0.14835890934319737]
	TIME [epoch: 5.98 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23003082846552622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23003082846552622 | validation: 0.6904116351516452]
	TIME [epoch: 5.98 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5629383963878718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5629383963878718 | validation: 0.4091503289360908]
	TIME [epoch: 5.97 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.314612600332039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.314612600332039 | validation: 0.12629954454117764]
	TIME [epoch: 5.98 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21075114207281537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21075114207281537 | validation: 0.1011566215949107]
	TIME [epoch: 5.98 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16850164832596526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16850164832596526 | validation: 0.14002876791832689]
	TIME [epoch: 5.98 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14679822145921817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14679822145921817 | validation: 0.13007047649176137]
	TIME [epoch: 5.98 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13444547618901742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13444547618901742 | validation: 0.12044530004161427]
	TIME [epoch: 5.98 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12022090578466463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12022090578466463 | validation: 0.13317018078949514]
	TIME [epoch: 5.98 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11255297461620309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11255297461620309 | validation: 0.1666133189494084]
	TIME [epoch: 5.98 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16149407986386524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16149407986386524 | validation: 0.5504346330092503]
	TIME [epoch: 5.99 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251671825986444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5251671825986444 | validation: 0.14557426721453864]
	TIME [epoch: 5.98 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20850905341099613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20850905341099613 | validation: 0.16842813084069017]
	TIME [epoch: 5.99 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14216597088768437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14216597088768437 | validation: 0.14536540081963492]
	TIME [epoch: 5.98 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13338121947793763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13338121947793763 | validation: 0.12420765797772265]
	TIME [epoch: 5.98 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14664978289221867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14664978289221867 | validation: 0.18947275687084036]
	TIME [epoch: 5.98 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15469870156413293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15469870156413293 | validation: 0.21900263303626724]
	TIME [epoch: 5.99 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23923430530982479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23923430530982479 | validation: 0.2765297048936424]
	TIME [epoch: 5.97 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25021407184652833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25021407184652833 | validation: 0.1357613963886041]
	TIME [epoch: 5.99 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2034714473739276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2034714473739276 | validation: 0.34198125653098005]
	TIME [epoch: 5.98 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21085232319859237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21085232319859237 | validation: 0.14288973824287907]
	TIME [epoch: 5.98 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17545093922527252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17545093922527252 | validation: 0.17937548939713854]
	TIME [epoch: 5.98 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15768049232469072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15768049232469072 | validation: 0.09658937336791536]
	TIME [epoch: 5.98 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1250761905058838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1250761905058838 | validation: 0.21828202757639292]
	TIME [epoch: 5.97 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14590114898527348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14590114898527348 | validation: 0.21866256020734343]
	TIME [epoch: 5.98 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20921588763280233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20921588763280233 | validation: 0.29333183078969194]
	TIME [epoch: 5.98 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37801226418526734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37801226418526734 | validation: 0.41093475203064894]
	TIME [epoch: 5.98 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24371972470730144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24371972470730144 | validation: 0.15086804308239157]
	TIME [epoch: 5.98 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13634530876476894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13634530876476894 | validation: 0.14011715939464062]
	TIME [epoch: 5.98 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781055226456013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1781055226456013 | validation: 0.2512340473479098]
	TIME [epoch: 5.98 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21115938636719134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21115938636719134 | validation: 0.21550364427823193]
	TIME [epoch: 5.97 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18881359156765473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18881359156765473 | validation: 0.11104850568483056]
	TIME [epoch: 5.98 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16772205635543153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16772205635543153 | validation: 0.14799727743658825]
	TIME [epoch: 5.98 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1398691393152616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1398691393152616 | validation: 0.11262187620504394]
	TIME [epoch: 5.97 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12567403918304712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12567403918304712 | validation: 0.12878924345272189]
	TIME [epoch: 5.98 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11079724695696223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11079724695696223 | validation: 0.09295136224453165]
	TIME [epoch: 5.98 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10484523288752481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10484523288752481 | validation: 0.21448248894973407]
	TIME [epoch: 5.98 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12968253686473233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12968253686473233 | validation: 0.44299292161467574]
	TIME [epoch: 5.98 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4747729619490373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4747729619490373 | validation: 0.42054209460703595]
	TIME [epoch: 5.97 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33947564640036254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33947564640036254 | validation: 0.25729438905840096]
	TIME [epoch: 5.97 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20568970307983922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20568970307983922 | validation: 0.12596830413151214]
	TIME [epoch: 5.97 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2057713962701412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2057713962701412 | validation: 0.21548477944342084]
	TIME [epoch: 5.98 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18324706280260927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18324706280260927 | validation: 0.09519039612612584]
	TIME [epoch: 5.97 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11531213370981873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11531213370981873 | validation: 0.09961442033382327]
	TIME [epoch: 5.97 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11613097520669369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11613097520669369 | validation: 0.16456531646610542]
	TIME [epoch: 5.98 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11317912624965037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11317912624965037 | validation: 0.09704078956653998]
	TIME [epoch: 5.98 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10379396683399757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10379396683399757 | validation: 0.2900408429047278]
	TIME [epoch: 5.97 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19433492329260227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19433492329260227 | validation: 0.26303565550616104]
	TIME [epoch: 5.98 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26906371667328716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26906371667328716 | validation: 0.25529427469451854]
	TIME [epoch: 5.97 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22165820805092762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22165820805092762 | validation: 0.28969075258896043]
	TIME [epoch: 5.98 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18278823287778656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18278823287778656 | validation: 0.15844671445003963]
	TIME [epoch: 181 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21362955292754693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21362955292754693 | validation: 0.3299543720390619]
	TIME [epoch: 12.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20287116168837643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20287116168837643 | validation: 0.09471303388731041]
	TIME [epoch: 12.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10245172491258432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10245172491258432 | validation: 0.16629401303761993]
	TIME [epoch: 12.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16226477366981001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16226477366981001 | validation: 0.3737411767825239]
	TIME [epoch: 12.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24927599145487328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24927599145487328 | validation: 0.24024848317935854]
	TIME [epoch: 12.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19793848792031363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19793848792031363 | validation: 0.12861978962953607]
	TIME [epoch: 12.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17615392493269028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17615392493269028 | validation: 0.309648385367868]
	TIME [epoch: 12.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18088284554078152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18088284554078152 | validation: 0.07969518859926515]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_509.pth
	Model improved!!!
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10754014281944763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10754014281944763 | validation: 0.10521958310168547]
	TIME [epoch: 12.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09861717575769056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09861717575769056 | validation: 0.16853162682420952]
	TIME [epoch: 12.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10389386727396961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10389386727396961 | validation: 0.1251806677623964]
	TIME [epoch: 12.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15743551085419788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15743551085419788 | validation: 0.21579719631184974]
	TIME [epoch: 12.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13110741468265835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13110741468265835 | validation: 0.2798299330437142]
	TIME [epoch: 12.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2650280145874199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2650280145874199 | validation: 0.6112679413691744]
	TIME [epoch: 12.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4873998238085832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4873998238085832 | validation: 0.4166788281972827]
	TIME [epoch: 12.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3298631296387477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3298631296387477 | validation: 0.16140741961340788]
	TIME [epoch: 12.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20389448048013323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20389448048013323 | validation: 0.13733423395736907]
	TIME [epoch: 12.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21513646093843328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21513646093843328 | validation: 0.25321278007168885]
	TIME [epoch: 12.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17883602431581733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17883602431581733 | validation: 0.23071609859823347]
	TIME [epoch: 12.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15172097928945735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15172097928945735 | validation: 0.11546989406512057]
	TIME [epoch: 12.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11695155160500856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11695155160500856 | validation: 0.11681658913518667]
	TIME [epoch: 12.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11212876781551198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11212876781551198 | validation: 0.12761616861213262]
	TIME [epoch: 12.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1055383482830966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1055383482830966 | validation: 0.13698640716615337]
	TIME [epoch: 12.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11400942107351242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11400942107351242 | validation: 0.12418158950804728]
	TIME [epoch: 12.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1449972269565823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1449972269565823 | validation: 0.33162018312590935]
	TIME [epoch: 12.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.243664420803662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.243664420803662 | validation: 0.2254102159818114]
	TIME [epoch: 12.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2990094250078267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2990094250078267 | validation: 0.25115411095005646]
	TIME [epoch: 12.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2201843550643806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2201843550643806 | validation: 0.14424796325929465]
	TIME [epoch: 12.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14929310779667038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14929310779667038 | validation: 0.10344620918989583]
	TIME [epoch: 12.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14290299908266077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14290299908266077 | validation: 0.08677289401758628]
	TIME [epoch: 12.6 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09390464592653473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09390464592653473 | validation: 0.12664133163478036]
	TIME [epoch: 12.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09211937296681852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09211937296681852 | validation: 0.08443018697878196]
	TIME [epoch: 12.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11523629660902764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11523629660902764 | validation: 0.17988968172609265]
	TIME [epoch: 12.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09812187356432663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09812187356432663 | validation: 0.19302317211943074]
	TIME [epoch: 12.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20256489312133089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20256489312133089 | validation: 0.34143948046642225]
	TIME [epoch: 12.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23457452394413503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23457452394413503 | validation: 0.2946639487129665]
	TIME [epoch: 12.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28650373985170596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28650373985170596 | validation: 0.2608330575730921]
	TIME [epoch: 12.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1997686445319624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1997686445319624 | validation: 0.1683392086718951]
	TIME [epoch: 12.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14166796967714862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14166796967714862 | validation: 0.13825861201351178]
	TIME [epoch: 12.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176820462788523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.176820462788523 | validation: 0.3008090751256224]
	TIME [epoch: 12.6 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18233990484325877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18233990484325877 | validation: 0.19680136824781608]
	TIME [epoch: 12.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13556280377831242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13556280377831242 | validation: 0.08887279701089046]
	TIME [epoch: 12.6 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.125063182605676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.125063182605676 | validation: 0.20332187248458988]
	TIME [epoch: 12.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356118415608221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1356118415608221 | validation: 0.15453752483193922]
	TIME [epoch: 12.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16905147497667272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16905147497667272 | validation: 0.16062024542024253]
	TIME [epoch: 12.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1251201607990564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1251201607990564 | validation: 0.10589831496685483]
	TIME [epoch: 12.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10793455406255027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10793455406255027 | validation: 0.1358363308277196]
	TIME [epoch: 12.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10186588104828782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10186588104828782 | validation: 0.20325734144040128]
	TIME [epoch: 12.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20014305969473029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20014305969473029 | validation: 0.4105946500141493]
	TIME [epoch: 12.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29907079291150285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29907079291150285 | validation: 0.37059742992033184]
	TIME [epoch: 12.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45058479322289047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45058479322289047 | validation: 0.3791477177616388]
	TIME [epoch: 12.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3817616889340006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3817616889340006 | validation: 0.26269434483191917]
	TIME [epoch: 12.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23319793646176137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23319793646176137 | validation: 0.11622053404906332]
	TIME [epoch: 12.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16502511047247218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16502511047247218 | validation: 0.19954681060070223]
	TIME [epoch: 12.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17139769962839269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17139769962839269 | validation: 0.21688010306696964]
	TIME [epoch: 12.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538275760536307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538275760536307 | validation: 0.1275371103371646]
	TIME [epoch: 12.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12362950813954911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12362950813954911 | validation: 0.13027851951450928]
	TIME [epoch: 12.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1168170112780051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1168170112780051 | validation: 0.09128113849454629]
	TIME [epoch: 12.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850142987955246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11850142987955246 | validation: 0.28107464697990076]
	TIME [epoch: 12.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1637217114786233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1637217114786233 | validation: 0.14100504626353258]
	TIME [epoch: 12.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15263898399068698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15263898399068698 | validation: 0.18684217397591735]
	TIME [epoch: 12.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13647351251645318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13647351251645318 | validation: 0.08827643407243031]
	TIME [epoch: 12.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11069432509999956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11069432509999956 | validation: 0.13012091979219892]
	TIME [epoch: 12.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10259435309696788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10259435309696788 | validation: 0.07895471617777752]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_565.pth
	Model improved!!!
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10131788113189247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10131788113189247 | validation: 0.35809597679901156]
	TIME [epoch: 12.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19250235392392948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19250235392392948 | validation: 0.1423487926109324]
	TIME [epoch: 12.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2084996685691222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2084996685691222 | validation: 0.21614879001090528]
	TIME [epoch: 12.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20464786028730586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20464786028730586 | validation: 0.2838296770307349]
	TIME [epoch: 12.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16690998409521793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16690998409521793 | validation: 0.2455377536342724]
	TIME [epoch: 12.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15989078780282867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15989078780282867 | validation: 0.07374347564353148]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09605496735977453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09605496735977453 | validation: 0.08393719454345716]
	TIME [epoch: 12.7 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08739501516346848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08739501516346848 | validation: 0.09551285102113975]
	TIME [epoch: 12.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08033927161089258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08033927161089258 | validation: 0.08497989587215433]
	TIME [epoch: 12.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07505103372823431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07505103372823431 | validation: 0.08292669481900222]
	TIME [epoch: 12.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07788460685288032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07788460685288032 | validation: 0.12178194723676153]
	TIME [epoch: 12.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08724733690692965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08724733690692965 | validation: 0.1131631490691575]
	TIME [epoch: 12.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10662118325966358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10662118325966358 | validation: 0.2465916814453764]
	TIME [epoch: 12.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17781394785323854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17781394785323854 | validation: 0.18562966754480148]
	TIME [epoch: 12.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1832876017631444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1832876017631444 | validation: 0.09202365397161144]
	TIME [epoch: 12.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10675232852911429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10675232852911429 | validation: 0.11230325651713674]
	TIME [epoch: 12.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11616974709902382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11616974709902382 | validation: 0.20464420805246475]
	TIME [epoch: 12.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476206201559824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1476206201559824 | validation: 0.10084069887511644]
	TIME [epoch: 12.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09486742485188782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09486742485188782 | validation: 0.11503943978604157]
	TIME [epoch: 12.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1452890234078703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1452890234078703 | validation: 0.30861550827361794]
	TIME [epoch: 12.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16869296425548605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16869296425548605 | validation: 0.08484795249779792]
	TIME [epoch: 12.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12591168854253254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12591168854253254 | validation: 0.18872294513613144]
	TIME [epoch: 12.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09407545606301082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09407545606301082 | validation: 0.11741040320426438]
	TIME [epoch: 12.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12203359859612838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12203359859612838 | validation: 0.23277969336718105]
	TIME [epoch: 12.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11982094106460948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11982094106460948 | validation: 0.20895172998134598]
	TIME [epoch: 12.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12125329928012533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12125329928012533 | validation: 0.21691356430179043]
	TIME [epoch: 12.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2286189348290393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2286189348290393 | validation: 0.12783606540978462]
	TIME [epoch: 12.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10663070928241203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10663070928241203 | validation: 0.07701551776987899]
	TIME [epoch: 12.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09139460662668185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09139460662668185 | validation: 0.15037717768327089]
	TIME [epoch: 12.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12615732205253177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12615732205253177 | validation: 0.30807756173606715]
	TIME [epoch: 12.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25703171759825494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25703171759825494 | validation: 0.11889540358013945]
	TIME [epoch: 12.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11009229526385754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11009229526385754 | validation: 0.07321748128893472]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08027359417392377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08027359417392377 | validation: 0.11607051943920302]
	TIME [epoch: 12.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08886259383087833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08886259383087833 | validation: 0.06248442982669072]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_599.pth
	Model improved!!!
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08399576655317426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08399576655317426 | validation: 0.21565169217908975]
	TIME [epoch: 12.7 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09940877185681819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09940877185681819 | validation: 0.06934759162331527]
	TIME [epoch: 12.7 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09302375432256992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09302375432256992 | validation: 0.21382369955803204]
	TIME [epoch: 12.7 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14209394981164203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14209394981164203 | validation: 0.12326635397350133]
	TIME [epoch: 12.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247690810795213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1247690810795213 | validation: 0.08425050754121294]
	TIME [epoch: 12.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11026187222205928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11026187222205928 | validation: 0.16560964131817763]
	TIME [epoch: 12.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12353566507863109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12353566507863109 | validation: 0.16443941971234777]
	TIME [epoch: 12.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20936547397377972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20936547397377972 | validation: 0.4505861830986233]
	TIME [epoch: 12.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23084431657089816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23084431657089816 | validation: 0.12045730674140903]
	TIME [epoch: 12.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10793214558186524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10793214558186524 | validation: 0.1394023455996016]
	TIME [epoch: 12.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15361261570452941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15361261570452941 | validation: 0.08324815621543478]
	TIME [epoch: 12.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09293879501661742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09293879501661742 | validation: 0.19444030476993202]
	TIME [epoch: 12.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12271019922697297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12271019922697297 | validation: 0.06110575730593133]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_612.pth
	Model improved!!!
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07512647459534968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07512647459534968 | validation: 0.3280718539336205]
	TIME [epoch: 12.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17471773929377313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17471773929377313 | validation: 0.1789562087536979]
	TIME [epoch: 12.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11357188158292662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11357188158292662 | validation: 0.0743747256963653]
	TIME [epoch: 12.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11664997733322591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11664997733322591 | validation: 0.13819102475672201]
	TIME [epoch: 12.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11681402051266959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11681402051266959 | validation: 0.07750985490729806]
	TIME [epoch: 12.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08923783308551479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08923783308551479 | validation: 0.09711375246384794]
	TIME [epoch: 12.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671784339485781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07671784339485781 | validation: 0.0688382876434341]
	TIME [epoch: 12.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07020721020425452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07020721020425452 | validation: 0.10043158744629764]
	TIME [epoch: 12.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07939884636059297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07939884636059297 | validation: 0.17023495169590813]
	TIME [epoch: 12.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15233176371761667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15233176371761667 | validation: 0.17095337055903664]
	TIME [epoch: 12.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1083389396239991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1083389396239991 | validation: 0.06880690232819472]
	TIME [epoch: 12.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09358824509444916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09358824509444916 | validation: 0.13515937771034547]
	TIME [epoch: 12.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936038768155921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08936038768155921 | validation: 0.10732645121136719]
	TIME [epoch: 12.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13499034077619954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13499034077619954 | validation: 0.2573879131833556]
	TIME [epoch: 12.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14236238865088394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14236238865088394 | validation: 0.09133620676561108]
	TIME [epoch: 12.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08942215923352428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08942215923352428 | validation: 0.15720622585040667]
	TIME [epoch: 12.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13014030388824968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13014030388824968 | validation: 0.15893876363260537]
	TIME [epoch: 12.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13658814595500915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13658814595500915 | validation: 0.07972517245641225]
	TIME [epoch: 12.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09359686592500156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09359686592500156 | validation: 0.10604901404734675]
	TIME [epoch: 12.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458460837202186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09458460837202186 | validation: 0.29098521675628963]
	TIME [epoch: 12.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19046652055014085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19046652055014085 | validation: 0.076327302475132]
	TIME [epoch: 12.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06681195965615146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06681195965615146 | validation: 0.04810259241982306]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07841509961350372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07841509961350372 | validation: 0.1522593552996751]
	TIME [epoch: 12.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08403504308940166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08403504308940166 | validation: 0.11064642648178552]
	TIME [epoch: 12.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13365489459017313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13365489459017313 | validation: 0.24552502248219038]
	TIME [epoch: 12.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12013349799973767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12013349799973767 | validation: 0.07919453097311223]
	TIME [epoch: 12.7 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0930955105169123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0930955105169123 | validation: 0.13972597173805174]
	TIME [epoch: 12.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12218573432941073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12218573432941073 | validation: 0.28612052196039217]
	TIME [epoch: 12.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20277123085857748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20277123085857748 | validation: 0.1599125960570765]
	TIME [epoch: 12.7 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1205324733743942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1205324733743942 | validation: 0.06709437155341212]
	TIME [epoch: 12.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09755561425738449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09755561425738449 | validation: 0.14036583916959286]
	TIME [epoch: 12.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954395278184452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07954395278184452 | validation: 0.053704543281427354]
	TIME [epoch: 12.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553116746278943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06553116746278943 | validation: 0.139576311980287]
	TIME [epoch: 12.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587591431261167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08587591431261167 | validation: 0.16260854927722138]
	TIME [epoch: 12.7 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1345301681990907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1345301681990907 | validation: 0.08391779769115049]
	TIME [epoch: 12.7 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09497646930058042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09497646930058042 | validation: 0.09675013807029112]
	TIME [epoch: 12.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07655147237130798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07655147237130798 | validation: 0.061193133030842534]
	TIME [epoch: 12.7 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412533350571003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07412533350571003 | validation: 0.1833065715529709]
	TIME [epoch: 12.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12047352155237413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12047352155237413 | validation: 0.10864994396883415]
	TIME [epoch: 12.7 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296169808294953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1296169808294953 | validation: 0.4344183646328313]
	TIME [epoch: 12.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23787762624744027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23787762624744027 | validation: 0.16216005919561985]
	TIME [epoch: 12.7 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1085853027078495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1085853027078495 | validation: 0.12336617520459936]
	TIME [epoch: 12.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14834492890368522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14834492890368522 | validation: 0.10054633332926453]
	TIME [epoch: 12.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10157634392325048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10157634392325048 | validation: 0.06303140216532642]
	TIME [epoch: 12.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08034487184548128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08034487184548128 | validation: 0.10550416073726288]
	TIME [epoch: 12.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09614902653302666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09614902653302666 | validation: 0.16759851565453876]
	TIME [epoch: 12.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11632103774414056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11632103774414056 | validation: 0.09528226722094718]
	TIME [epoch: 12.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09006653086832588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09006653086832588 | validation: 0.07027405570414642]
	TIME [epoch: 12.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06830145256292613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06830145256292613 | validation: 0.05320981055494452]
	TIME [epoch: 12.7 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055641416387257385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055641416387257385 | validation: 0.06879789413078506]
	TIME [epoch: 12.7 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06322458819996897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06322458819996897 | validation: 0.12493280077548444]
	TIME [epoch: 12.7 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1409334902099145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1409334902099145 | validation: 0.20856625029502363]
	TIME [epoch: 12.7 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11561069460101908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11561069460101908 | validation: 0.08570423446269566]
	TIME [epoch: 12.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10268335103439649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10268335103439649 | validation: 0.270570911273007]
	TIME [epoch: 12.7 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1641163359603772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1641163359603772 | validation: 0.17297920091774494]
	TIME [epoch: 12.7 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1822019507754883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1822019507754883 | validation: 0.06412534976837189]
	TIME [epoch: 12.7 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06842770070422372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06842770070422372 | validation: 0.10081014771265827]
	TIME [epoch: 12.7 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08210089118943004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08210089118943004 | validation: 0.10064306810557336]
	TIME [epoch: 12.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116225874848231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07116225874848231 | validation: 0.04831643685477011]
	TIME [epoch: 12.7 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05143690373125576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05143690373125576 | validation: 0.08465101398425108]
	TIME [epoch: 12.7 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420220561506425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06420220561506425 | validation: 0.47442283185306905]
	TIME [epoch: 12.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35285063578453724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35285063578453724 | validation: 0.2042987309358646]
	TIME [epoch: 12.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2372778329238128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2372778329238128 | validation: 0.2202880101126677]
	TIME [epoch: 12.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605560677786566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2605560677786566 | validation: 0.17643053282207433]
	TIME [epoch: 12.7 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15701202652931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15701202652931 | validation: 0.06650077073098112]
	TIME [epoch: 12.7 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10340620912136697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10340620912136697 | validation: 0.1813311656610853]
	TIME [epoch: 12.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10069542916723652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10069542916723652 | validation: 0.08931639445419862]
	TIME [epoch: 12.7 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.095171047630648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.095171047630648 | validation: 0.09181397330018032]
	TIME [epoch: 12.7 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880307966323045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0880307966323045 | validation: 0.07175392898447806]
	TIME [epoch: 12.7 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08028127187480312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08028127187480312 | validation: 0.07385149052524977]
	TIME [epoch: 12.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07133744583347128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07133744583347128 | validation: 0.07719239907366593]
	TIME [epoch: 12.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09154816748423197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09154816748423197 | validation: 0.24320928735137617]
	TIME [epoch: 12.7 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12979789960746974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12979789960746974 | validation: 0.05178209120914467]
	TIME [epoch: 12.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060032670999643496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060032670999643496 | validation: 0.044245861731129425]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_686.pth
	Model improved!!!
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05149151431223119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05149151431223119 | validation: 0.06302637550705584]
	TIME [epoch: 12.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664793635273483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05664793635273483 | validation: 0.04501457454961104]
	TIME [epoch: 12.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0546877230274828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0546877230274828 | validation: 0.5405825431603811]
	TIME [epoch: 12.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6184008126856265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6184008126856265 | validation: 0.6259091950727562]
	TIME [epoch: 12.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5717072877671483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5717072877671483 | validation: 0.6694728266032203]
	TIME [epoch: 12.7 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43782455624297756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43782455624297756 | validation: 0.23272504810372788]
	TIME [epoch: 12.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431272467452395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1431272467452395 | validation: 0.1487027559926337]
	TIME [epoch: 12.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11693794116509316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11693794116509316 | validation: 0.08576370490183459]
	TIME [epoch: 12.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08330828936178175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08330828936178175 | validation: 0.0818568917342957]
	TIME [epoch: 12.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07490231184788075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07490231184788075 | validation: 0.06559779351951804]
	TIME [epoch: 12.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091401953320271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08091401953320271 | validation: 0.07703686655071502]
	TIME [epoch: 12.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06808661325354408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06808661325354408 | validation: 0.06310390778994239]
	TIME [epoch: 12.7 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07553923515727393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07553923515727393 | validation: 0.10907526236722705]
	TIME [epoch: 12.7 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0836553065180841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0836553065180841 | validation: 0.05893489252144485]
	TIME [epoch: 12.7 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07332767269003417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07332767269003417 | validation: 0.12402620860288023]
	TIME [epoch: 12.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040679527612135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08040679527612135 | validation: 0.0447205263286801]
	TIME [epoch: 12.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051009662604164126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051009662604164126 | validation: 0.04741896850202792]
	TIME [epoch: 12.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152429837542756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04152429837542756 | validation: 0.041124549224731036]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_704.pth
	Model improved!!!
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03605511547064262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03605511547064262 | validation: 0.0486760855285327]
	TIME [epoch: 12.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03977016429579483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03977016429579483 | validation: 0.0439299538915868]
	TIME [epoch: 12.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06127730267154744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06127730267154744 | validation: 0.27444199833872024]
	TIME [epoch: 12.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13602281214790893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13602281214790893 | validation: 0.06648966586088045]
	TIME [epoch: 12.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555024382910392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08555024382910392 | validation: 0.07720045463708572]
	TIME [epoch: 12.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651119280345566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0651119280345566 | validation: 0.03387362589411385]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_710.pth
	Model improved!!!
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042646020044810204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042646020044810204 | validation: 0.0654340930470985]
	TIME [epoch: 12.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04018396043372506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04018396043372506 | validation: 0.06374694854501901]
	TIME [epoch: 12.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054050329608447606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054050329608447606 | validation: 0.12540941894385998]
	TIME [epoch: 12.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12866385736878042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12866385736878042 | validation: 0.21609338064784078]
	TIME [epoch: 12.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1567673727575505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1567673727575505 | validation: 0.08662872485030683]
	TIME [epoch: 12.7 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11534865215866322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11534865215866322 | validation: 0.06675620714046375]
	TIME [epoch: 12.7 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05257576447503933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05257576447503933 | validation: 0.7000549272922831]
	TIME [epoch: 12.7 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9908662255340499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9908662255340499 | validation: 0.6307411605175095]
	TIME [epoch: 12.7 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9492629174695668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9492629174695668 | validation: 0.5390002743832533]
	TIME [epoch: 12.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5607531148345172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5607531148345172 | validation: 0.4213018424696582]
	TIME [epoch: 12.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3725978445557058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3725978445557058 | validation: 0.22957333872207017]
	TIME [epoch: 12.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22398455759415176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22398455759415176 | validation: 0.37393796195731827]
	TIME [epoch: 12.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4079564979146643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4079564979146643 | validation: 0.21439827672408898]
	TIME [epoch: 12.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2640050141246398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2640050141246398 | validation: 0.14524436703454408]
	TIME [epoch: 12.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1742409950633005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1742409950633005 | validation: 0.11122707701613273]
	TIME [epoch: 12.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10654220858301536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10654220858301536 | validation: 0.12484340914217343]
	TIME [epoch: 12.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1111106068694995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1111106068694995 | validation: 0.08366048883408195]
	TIME [epoch: 12.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09552571093695676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09552571093695676 | validation: 0.0860655330192275]
	TIME [epoch: 12.7 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10443553404977372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10443553404977372 | validation: 0.05188106382023077]
	TIME [epoch: 12.7 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06748748349110299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06748748349110299 | validation: 0.12826353241183577]
	TIME [epoch: 12.7 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11183718743122406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11183718743122406 | validation: 0.13501718114592096]
	TIME [epoch: 12.7 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11428125804402822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11428125804402822 | validation: 0.11183933015496361]
	TIME [epoch: 12.7 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1096080080301157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1096080080301157 | validation: 0.06657717212619672]
	TIME [epoch: 12.7 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07151866467280237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07151866467280237 | validation: 0.035834463925640575]
	TIME [epoch: 12.7 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053099907377661244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053099907377661244 | validation: 0.10856409839606185]
	TIME [epoch: 12.7 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06393950887663345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06393950887663345 | validation: 0.15643245189653504]
	TIME [epoch: 12.7 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14074853223360395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14074853223360395 | validation: 0.05345743814276732]
	TIME [epoch: 12.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06789766571463696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06789766571463696 | validation: 0.050960701134328934]
	TIME [epoch: 12.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05243058373378471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05243058373378471 | validation: 0.07545524617499705]
	TIME [epoch: 12.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06275222248777909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06275222248777909 | validation: 0.04198926223759522]
	TIME [epoch: 12.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0477863476982135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0477863476982135 | validation: 0.051395350318652944]
	TIME [epoch: 12.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046570002331918534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046570002331918534 | validation: 0.04742723368556809]
	TIME [epoch: 12.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424467308999101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04424467308999101 | validation: 0.05801312577421987]
	TIME [epoch: 12.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490519823790767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04490519823790767 | validation: 0.04439970462385267]
	TIME [epoch: 12.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05782924581079968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05782924581079968 | validation: 0.20316341053909417]
	TIME [epoch: 12.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11545193314693297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11545193314693297 | validation: 0.10037132487877815]
	TIME [epoch: 12.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06583675499195024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06583675499195024 | validation: 0.30437314700371687]
	TIME [epoch: 12.7 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41752343634637823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41752343634637823 | validation: 0.27579745454722654]
	TIME [epoch: 12.7 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29043618492508416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29043618492508416 | validation: 0.2649228622443916]
	TIME [epoch: 12.7 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17705237873368265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17705237873368265 | validation: 0.15872648011115065]
	TIME [epoch: 12.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088542947741112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.088542947741112 | validation: 0.07076796947535675]
	TIME [epoch: 12.7 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056568722611084185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056568722611084185 | validation: 0.10084822995199494]
	TIME [epoch: 12.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09474974238816099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09474974238816099 | validation: 0.19650805582957576]
	TIME [epoch: 12.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1414886489194455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1414886489194455 | validation: 0.10877848767592452]
	TIME [epoch: 12.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10855053096795185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10855053096795185 | validation: 0.04098393268181947]
	TIME [epoch: 12.7 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07162191113745547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07162191113745547 | validation: 0.04715862234040008]
	TIME [epoch: 12.7 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319510755048146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05319510755048146 | validation: 0.053626275399945994]
	TIME [epoch: 12.7 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04888928237466542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04888928237466542 | validation: 0.05424352518668586]
	TIME [epoch: 12.7 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07555969674104328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07555969674104328 | validation: 0.0987549375386823]
	TIME [epoch: 12.7 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059676925082806365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059676925082806365 | validation: 0.1880414652410063]
	TIME [epoch: 12.7 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09958417165726662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09958417165726662 | validation: 0.19736207103048448]
	TIME [epoch: 12.7 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12448821317679638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12448821317679638 | validation: 0.0812513422824362]
	TIME [epoch: 12.7 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09092015811730886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09092015811730886 | validation: 0.05358164682372252]
	TIME [epoch: 12.7 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057061554308313445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057061554308313445 | validation: 0.0511228077189057]
	TIME [epoch: 12.7 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0513884041448967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0513884041448967 | validation: 0.058637829341282977]
	TIME [epoch: 12.7 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06171310867082454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06171310867082454 | validation: 0.05798215787799871]
	TIME [epoch: 12.7 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05498427619479864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05498427619479864 | validation: 0.13113365536743313]
	TIME [epoch: 12.7 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13448515951444187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13448515951444187 | validation: 0.08840948190352027]
	TIME [epoch: 12.7 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05228793077790133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05228793077790133 | validation: 0.045468149478459424]
	TIME [epoch: 12.7 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047730783872856064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047730783872856064 | validation: 0.05674109070504684]
	TIME [epoch: 12.7 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666325494427453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666325494427453 | validation: 0.04215681214341508]
	TIME [epoch: 12.7 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039482496752040235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039482496752040235 | validation: 0.030439352581449444]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030666609703482364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030666609703482364 | validation: 0.03472763529109076]
	TIME [epoch: 12.7 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0318887729948519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0318887729948519 | validation: 0.5015718412818234]
	TIME [epoch: 12.7 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41121860865383086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41121860865383086 | validation: 0.35551071042527777]
	TIME [epoch: 12.7 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347237728426617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2347237728426617 | validation: 0.06359475688538917]
	TIME [epoch: 12.7 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679608252088012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09679608252088012 | validation: 0.07573310631958141]
	TIME [epoch: 12.7 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11011986481165671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11011986481165671 | validation: 0.21147082445411877]
	TIME [epoch: 12.7 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12827009920991297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12827009920991297 | validation: 0.06971220245937639]
	TIME [epoch: 12.7 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054960422557183385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054960422557183385 | validation: 0.038012857423437074]
	TIME [epoch: 12.7 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06316871529805473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06316871529805473 | validation: 0.14332870307643095]
	TIME [epoch: 12.7 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061344292653342444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061344292653342444 | validation: 0.05318985642827048]
	TIME [epoch: 12.7 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04405021034167751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04405021034167751 | validation: 0.03310403852256854]
	TIME [epoch: 12.7 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043538902286447335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043538902286447335 | validation: 0.045642577074496794]
	TIME [epoch: 12.7 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040501983718307565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040501983718307565 | validation: 0.028587564124073397]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_785.pth
	Model improved!!!
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03994817138371142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03994817138371142 | validation: 0.03942175727113204]
	TIME [epoch: 12.7 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03275027640353622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03275027640353622 | validation: 0.03123535148446285]
	TIME [epoch: 12.7 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03397907073159955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03397907073159955 | validation: 0.03924759572067885]
	TIME [epoch: 12.7 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03179882877247642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03179882877247642 | validation: 0.04404665585302936]
	TIME [epoch: 12.7 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04726119759061644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04726119759061644 | validation: 0.09402835706864782]
	TIME [epoch: 12.7 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0597207879801122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0597207879801122 | validation: 0.06113513370343178]
	TIME [epoch: 12.7 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08117272842843647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08117272842843647 | validation: 0.13624405285192798]
	TIME [epoch: 12.7 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10099597536392253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10099597536392253 | validation: 0.34192104079999724]
	TIME [epoch: 12.7 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847199563412601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2847199563412601 | validation: 0.3307046134170887]
	TIME [epoch: 12.7 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2542889052279109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2542889052279109 | validation: 0.1293471242209448]
	TIME [epoch: 12.7 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12237290123407128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12237290123407128 | validation: 0.04133019616007525]
	TIME [epoch: 12.7 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662799807705348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0662799807705348 | validation: 0.026268100524702076]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525356527693134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03525356527693134 | validation: 0.3980318492516286]
	TIME [epoch: 12.7 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23722286463409004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23722286463409004 | validation: 0.4200060574785952]
	TIME [epoch: 12.7 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2797124713452751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2797124713452751 | validation: 0.1775883442974749]
	TIME [epoch: 12.7 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15065556569604124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15065556569604124 | validation: 0.08917211118542157]
	TIME [epoch: 12.7 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10494969753855717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10494969753855717 | validation: 0.05076695950703219]
	TIME [epoch: 12.7 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06002032642266439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06002032642266439 | validation: 0.07112252984714978]
	TIME [epoch: 12.7 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054705887485404905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054705887485404905 | validation: 0.06796596995550079]
	TIME [epoch: 12.7 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04660418303186705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04660418303186705 | validation: 0.05351891703053277]
	TIME [epoch: 12.7 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04186797328908575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04186797328908575 | validation: 0.034243920076791494]
	TIME [epoch: 12.7 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03667054164057755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03667054164057755 | validation: 0.033791174849817696]
	TIME [epoch: 12.7 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502081053813524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03502081053813524 | validation: 0.051444967867941005]
	TIME [epoch: 12.7 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04577816112216742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04577816112216742 | validation: 0.07216140569377129]
	TIME [epoch: 12.7 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339903529393967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07339903529393967 | validation: 0.0900060822907617]
	TIME [epoch: 12.7 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958617448451918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07958617448451918 | validation: 0.04130707205709412]
	TIME [epoch: 12.7 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04585963717417601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04585963717417601 | validation: 0.046061833888273035]
	TIME [epoch: 12.7 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03619946366390113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03619946366390113 | validation: 0.021478698986476863]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_813.pth
	Model improved!!!
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028115639966103202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028115639966103202 | validation: 0.023369208273017007]
	TIME [epoch: 12.7 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025368519639713406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025368519639713406 | validation: 0.1306693555985618]
	TIME [epoch: 12.7 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10313607259635324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10313607259635324 | validation: 0.10586218673995279]
	TIME [epoch: 12.7 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09710930533326596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09710930533326596 | validation: 0.22908938284092548]
	TIME [epoch: 12.7 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2781585798279053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2781585798279053 | validation: 0.16139292892836699]
	TIME [epoch: 12.7 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10700600398246185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10700600398246185 | validation: 0.1738059692413606]
	TIME [epoch: 12.7 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.102905920299742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.102905920299742 | validation: 0.05294265485575494]
	TIME [epoch: 12.7 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414086899999875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06414086899999875 | validation: 0.03670025264203437]
	TIME [epoch: 12.7 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286298307577222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04286298307577222 | validation: 0.05898817527433539]
	TIME [epoch: 12.7 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044944662963745115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044944662963745115 | validation: 0.061157385906079026]
	TIME [epoch: 12.7 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04136785264518926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04136785264518926 | validation: 0.03600381354685957]
	TIME [epoch: 12.7 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03656459781161832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03656459781161832 | validation: 0.048518761130038945]
	TIME [epoch: 12.7 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049069397006100635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049069397006100635 | validation: 0.11293852202151262]
	TIME [epoch: 12.7 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08717965948427413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08717965948427413 | validation: 0.05390607805206672]
	TIME [epoch: 12.7 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05655086611705178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05655086611705178 | validation: 0.05923052205611671]
	TIME [epoch: 12.7 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05038655838403645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05038655838403645 | validation: 0.028960937542379595]
	TIME [epoch: 12.7 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601335976918132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03601335976918132 | validation: 0.043364226245395404]
	TIME [epoch: 12.7 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033848743016618686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033848743016618686 | validation: 0.15907241103073685]
	TIME [epoch: 12.7 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.178228351458955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.178228351458955 | validation: 0.12321397238806996]
	TIME [epoch: 12.7 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14015123318151182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14015123318151182 | validation: 0.09954419964743101]
	TIME [epoch: 12.7 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12415373980810218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12415373980810218 | validation: 0.18844277127634176]
	TIME [epoch: 12.7 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07723588482670285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07723588482670285 | validation: 0.040761709779341786]
	TIME [epoch: 12.7 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045810750992717315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045810750992717315 | validation: 0.03701450881393244]
	TIME [epoch: 12.7 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804391282536214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03804391282536214 | validation: 0.04856995997305657]
	TIME [epoch: 12.7 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05460178590988937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05460178590988937 | validation: 0.07780350259983373]
	TIME [epoch: 12.7 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07090012871075733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07090012871075733 | validation: 0.06941533149076322]
	TIME [epoch: 12.7 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06790291939690798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06790291939690798 | validation: 0.06450090330939305]
	TIME [epoch: 12.7 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07016922673986004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07016922673986004 | validation: 0.03541210614405214]
	TIME [epoch: 12.7 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04164991735628247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04164991735628247 | validation: 0.043419271901374984]
	TIME [epoch: 12.7 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026798333381685583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026798333381685583 | validation: 0.015430644898210211]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022711306356809926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022711306356809926 | validation: 0.039497100352006966]
	TIME [epoch: 12.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03376793102105597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03376793102105597 | validation: 0.047837162487815665]
	TIME [epoch: 12.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05373665753240266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05373665753240266 | validation: 0.1328925190596708]
	TIME [epoch: 12.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09086032915140234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09086032915140234 | validation: 0.12137740300858596]
	TIME [epoch: 12.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1721153184466177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1721153184466177 | validation: 0.07474475370875067]
	TIME [epoch: 12.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05958737386332051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05958737386332051 | validation: 0.0653448488589367]
	TIME [epoch: 12.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03848357311243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03848357311243 | validation: 0.027594734770456122]
	TIME [epoch: 12.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03153075852274492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03153075852274492 | validation: 0.047040291163453735]
	TIME [epoch: 12.7 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047487015964268825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047487015964268825 | validation: 0.07373691337545933]
	TIME [epoch: 12.7 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05041795750234061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05041795750234061 | validation: 0.06144961862837109]
	TIME [epoch: 12.7 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05859835330080029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05859835330080029 | validation: 0.0665527054875401]
	TIME [epoch: 12.7 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048123588798030345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048123588798030345 | validation: 0.04343206731202478]
	TIME [epoch: 12.7 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041918357631229364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041918357631229364 | validation: 0.021529041152962804]
	TIME [epoch: 12.7 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029939640900405903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029939640900405903 | validation: 0.030703786809418144]
	TIME [epoch: 12.7 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034453801888342114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034453801888342114 | validation: 0.05136160989482907]
	TIME [epoch: 12.7 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05075551151062742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05075551151062742 | validation: 0.08467271139646944]
	TIME [epoch: 12.7 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08695487106523489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08695487106523489 | validation: 0.10795383513560222]
	TIME [epoch: 12.7 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11418165279660626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11418165279660626 | validation: 0.035019322457937783]
	TIME [epoch: 12.7 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04900986615686741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04900986615686741 | validation: 0.0927700334152772]
	TIME [epoch: 12.7 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09408353887570303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09408353887570303 | validation: 0.08173951650754546]
	TIME [epoch: 12.7 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513108409331562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05513108409331562 | validation: 0.03913648407377484]
	TIME [epoch: 12.7 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0488279390851623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0488279390851623 | validation: 0.08094403975649146]
	TIME [epoch: 12.7 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07901953647513933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07901953647513933 | validation: 0.04422307840400657]
	TIME [epoch: 12.7 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745619433910103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04745619433910103 | validation: 0.1734110992681943]
	TIME [epoch: 12.7 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12876142478421304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12876142478421304 | validation: 0.03229160082580053]
	TIME [epoch: 12.7 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0376900315465942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0376900315465942 | validation: 0.03833751657591137]
	TIME [epoch: 12.7 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04619196680476288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04619196680476288 | validation: 0.1891042970404605]
	TIME [epoch: 12.7 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10566982978909778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10566982978909778 | validation: 0.14000628257479505]
	TIME [epoch: 12.7 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09221878434097942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09221878434097942 | validation: 0.0852425139845045]
	TIME [epoch: 12.7 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717955137730325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09717955137730325 | validation: 0.13940097420778527]
	TIME [epoch: 12.7 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060377868409006975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060377868409006975 | validation: 0.1259672141700739]
	TIME [epoch: 12.7 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07490681784827258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07490681784827258 | validation: 0.0435701509480126]
	TIME [epoch: 12.7 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03576682301192613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03576682301192613 | validation: 0.037810654463995445]
	TIME [epoch: 12.7 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034173969392398196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034173969392398196 | validation: 0.02748407092965918]
	TIME [epoch: 12.7 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028150118365428148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028150118365428148 | validation: 0.03416475948567567]
	TIME [epoch: 12.7 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032069079434059414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032069079434059414 | validation: 0.05983731359777116]
	TIME [epoch: 12.7 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059883890991382326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059883890991382326 | validation: 0.13904800406404264]
	TIME [epoch: 12.7 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14122733470737311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14122733470737311 | validation: 0.11032519364294634]
	TIME [epoch: 12.7 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09644349020693362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09644349020693362 | validation: 0.15827357295478758]
	TIME [epoch: 12.7 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08660922495516558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08660922495516558 | validation: 0.03179961424237321]
	TIME [epoch: 12.7 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04363980636320234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04363980636320234 | validation: 0.024788963892720884]
	TIME [epoch: 12.7 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03964163300643333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03964163300643333 | validation: 0.1545867323000241]
	TIME [epoch: 12.7 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07884154341760334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07884154341760334 | validation: 0.16780480233028863]
	TIME [epoch: 12.7 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09543711115636655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09543711115636655 | validation: 0.033841149997351674]
	TIME [epoch: 12.7 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04439476488890038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04439476488890038 | validation: 0.03638661881728774]
	TIME [epoch: 12.7 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04955289913136727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04955289913136727 | validation: 0.07319377202935265]
	TIME [epoch: 12.7 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05366808239843934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05366808239843934 | validation: 0.041254415606916076]
	TIME [epoch: 12.7 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03248666266425701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03248666266425701 | validation: 0.019746185055536047]
	TIME [epoch: 12.7 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030031676725185027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030031676725185027 | validation: 0.028283033655873882]
	TIME [epoch: 12.7 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021902638320772046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021902638320772046 | validation: 0.04782913518783286]
	TIME [epoch: 12.7 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04767111634528097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04767111634528097 | validation: 0.05896961519578467]
	TIME [epoch: 12.7 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055926175739266365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055926175739266365 | validation: 0.1039767847159371]
	TIME [epoch: 12.7 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0876782981886562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0876782981886562 | validation: 0.05397561920164087]
	TIME [epoch: 12.7 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057033390217787056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057033390217787056 | validation: 0.35997573328184373]
	TIME [epoch: 12.7 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3243683434349823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3243683434349823 | validation: 0.27409217897147825]
	TIME [epoch: 12.7 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17865087700118198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17865087700118198 | validation: 0.24193576869689243]
	TIME [epoch: 12.7 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2455845812444752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2455845812444752 | validation: 0.21771605789029383]
	TIME [epoch: 12.7 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22732709511074933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22732709511074933 | validation: 0.10446695617386742]
	TIME [epoch: 12.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756451828146194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09756451828146194 | validation: 0.07604300620224905]
	TIME [epoch: 12.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07786869387418639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07786869387418639 | validation: 0.05579816954881152]
	TIME [epoch: 12.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06110206503109228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06110206503109228 | validation: 0.05765900334650059]
	TIME [epoch: 12.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04233585614234759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04233585614234759 | validation: 0.1397187868799896]
	TIME [epoch: 12.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05481082582253967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05481082582253967 | validation: 0.04748328253128107]
	TIME [epoch: 12.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035025490854018335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035025490854018335 | validation: 0.02423871059843683]
	TIME [epoch: 12.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031252683451045184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031252683451045184 | validation: 0.03328250619044115]
	TIME [epoch: 12.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031117275165873766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031117275165873766 | validation: 0.03333974512557301]
	TIME [epoch: 12.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616051965419375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03616051965419375 | validation: 0.048437720517517546]
	TIME [epoch: 12.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048450119956578795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048450119956578795 | validation: 0.06876010952315385]
	TIME [epoch: 12.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186950446785924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07186950446785924 | validation: 0.06494148864575264]
	TIME [epoch: 12.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557772329372679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05557772329372679 | validation: 0.03001801870496732]
	TIME [epoch: 12.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033666180756537746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033666180756537746 | validation: 0.030535984149189656]
	TIME [epoch: 12.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02749136873372092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02749136873372092 | validation: 0.026196976254128215]
	TIME [epoch: 12.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024119222769431542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024119222769431542 | validation: 0.019696610728235277]
	TIME [epoch: 12.6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025675305284006286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025675305284006286 | validation: 0.44469971456977864]
	TIME [epoch: 12.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3733356824547151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3733356824547151 | validation: 0.5682302563423193]
	TIME [epoch: 12.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3197770945925135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3197770945925135 | validation: 0.28514647733125315]
	TIME [epoch: 12.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17018913809964514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17018913809964514 | validation: 0.15082851782623863]
	TIME [epoch: 12.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446744236444935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1446744236444935 | validation: 0.08470963913384662]
	TIME [epoch: 12.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09318487032717211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09318487032717211 | validation: 0.05056563041915449]
	TIME [epoch: 12.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729512951745529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04729512951745529 | validation: 0.06979565154183046]
	TIME [epoch: 12.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0481285456781617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0481285456781617 | validation: 0.060706887734046235]
	TIME [epoch: 12.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043213566265074874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043213566265074874 | validation: 0.040496870121062005]
	TIME [epoch: 12.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032909642389189156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032909642389189156 | validation: 0.05537627414528506]
	TIME [epoch: 12.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05711871518756894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05711871518756894 | validation: 0.10983324955996526]
	TIME [epoch: 12.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455576034571197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05455576034571197 | validation: 0.09039460575211596]
	TIME [epoch: 12.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05648486499120967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05648486499120967 | validation: 0.06243651587832351]
	TIME [epoch: 12.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07322021224045233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07322021224045233 | validation: 0.060869942884432815]
	TIME [epoch: 12.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061438143314161024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061438143314161024 | validation: 0.04668557573150026]
	TIME [epoch: 12.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626044735266783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03626044735266783 | validation: 0.06699799902758767]
	TIME [epoch: 12.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042514403120952886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042514403120952886 | validation: 0.026673520833927846]
	TIME [epoch: 12.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029302770522592947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029302770522592947 | validation: 0.013516854987466444]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_934.pth
	Model improved!!!
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019617492538924784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019617492538924784 | validation: 0.018545940562073395]
	TIME [epoch: 12.7 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019824017521886565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019824017521886565 | validation: 0.04036825573202058]
	TIME [epoch: 12.7 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048133199485937424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048133199485937424 | validation: 0.03306793411995906]
	TIME [epoch: 12.7 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02428237380243478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02428237380243478 | validation: 0.027641101142441837]
	TIME [epoch: 12.7 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023357767853251703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023357767853251703 | validation: 0.02903436430647932]
	TIME [epoch: 12.7 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03154029043505926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03154029043505926 | validation: 0.05834141647475299]
	TIME [epoch: 12.7 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038406669215336836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038406669215336836 | validation: 0.05423240487670342]
	TIME [epoch: 12.7 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05757197129447118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05757197129447118 | validation: 0.06562665822288842]
	TIME [epoch: 12.7 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06232008252986718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06232008252986718 | validation: 0.2958316995389316]
	TIME [epoch: 12.7 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33370075613123734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33370075613123734 | validation: 0.27820829945168296]
	TIME [epoch: 12.7 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3135780575604947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3135780575604947 | validation: 0.17682615448706698]
	TIME [epoch: 12.7 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1894314409304696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1894314409304696 | validation: 0.07697239379608234]
	TIME [epoch: 12.7 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07810045913419185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07810045913419185 | validation: 0.2814640506714238]
	TIME [epoch: 12.7 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10801513346624145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10801513346624145 | validation: 0.14449445094963106]
	TIME [epoch: 12.7 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704653011634493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704653011634493 | validation: 0.018157769915833368]
	TIME [epoch: 12.7 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030408740790208857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030408740790208857 | validation: 0.02523368147016826]
	TIME [epoch: 12.7 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0531022011163603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0531022011163603 | validation: 0.020620014315706228]
	TIME [epoch: 12.7 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030006554986027414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030006554986027414 | validation: 0.0762199487355239]
	TIME [epoch: 12.7 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04307810935653482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04307810935653482 | validation: 0.06019012896245096]
	TIME [epoch: 12.7 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04644779111918667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04644779111918667 | validation: 0.04583870872675225]
	TIME [epoch: 12.7 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04808422714667235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04808422714667235 | validation: 0.06493153799659852]
	TIME [epoch: 12.7 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06860846822773585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06860846822773585 | validation: 0.059794593020175035]
	TIME [epoch: 12.7 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06451878009966286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06451878009966286 | validation: 0.053892860983163905]
	TIME [epoch: 12.7 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04421586197956907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04421586197956907 | validation: 0.028565633113967218]
	TIME [epoch: 12.7 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030860653036184692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030860653036184692 | validation: 0.019712228846111204]
	TIME [epoch: 12.7 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018068887321674977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018068887321674977 | validation: 0.015321568488783001]
	TIME [epoch: 12.7 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018933274408718198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018933274408718198 | validation: 0.02563336466726267]
	TIME [epoch: 12.7 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029519022795552925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029519022795552925 | validation: 0.049917575224876894]
	TIME [epoch: 12.7 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05195799383577254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05195799383577254 | validation: 0.08732064185376738]
	TIME [epoch: 12.7 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07337660532882054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07337660532882054 | validation: 0.07568755332059944]
	TIME [epoch: 12.7 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07679794601448563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07679794601448563 | validation: 0.05655183682662649]
	TIME [epoch: 12.7 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543143958608447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04543143958608447 | validation: 0.03993275745166252]
	TIME [epoch: 12.7 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04520776807013638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04520776807013638 | validation: 0.0773279234166217]
	TIME [epoch: 12.7 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0581496624714678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0581496624714678 | validation: 0.0512379746047003]
	TIME [epoch: 12.7 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051433977466268994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051433977466268994 | validation: 0.05134015044653914]
	TIME [epoch: 12.7 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047601701216010026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047601701216010026 | validation: 0.025875490484434816]
	TIME [epoch: 12.7 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028560681673427145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028560681673427145 | validation: 0.03090133566828232]
	TIME [epoch: 12.7 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03245783970123392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03245783970123392 | validation: 0.03260755467406919]
	TIME [epoch: 12.7 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02966448673368367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02966448673368367 | validation: 0.027698374377979274]
	TIME [epoch: 12.7 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02852819180013192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02852819180013192 | validation: 0.033074833147971175]
	TIME [epoch: 12.7 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03271551813901589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03271551813901589 | validation: 0.03858434815779678]
	TIME [epoch: 12.7 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03747886441164096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03747886441164096 | validation: 0.06112648358454196]
	TIME [epoch: 12.7 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053474534449336454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053474534449336454 | validation: 0.05512431259120093]
	TIME [epoch: 12.7 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056956910319064476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056956910319064476 | validation: 0.07808934496994953]
	TIME [epoch: 12.7 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055505868766906094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055505868766906094 | validation: 0.09172255318288702]
	TIME [epoch: 12.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09053116114265912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09053116114265912 | validation: 0.13645694613682055]
	TIME [epoch: 12.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11631648426265366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11631648426265366 | validation: 0.08725109198703838]
	TIME [epoch: 12.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0873384018775246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0873384018775246 | validation: 0.10319647749733868]
	TIME [epoch: 12.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09065677484849353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09065677484849353 | validation: 0.07194052483130874]
	TIME [epoch: 12.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196442063405179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06196442063405179 | validation: 0.055335434696123446]
	TIME [epoch: 12.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000321604200556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05000321604200556 | validation: 0.04755031868906512]
	TIME [epoch: 12.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04120470682413405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04120470682413405 | validation: 0.03079237636692496]
	TIME [epoch: 12.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029409728905434678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029409728905434678 | validation: 0.02127983327841785]
	TIME [epoch: 12.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024909414583742486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024909414583742486 | validation: 0.018609716979506818]
	TIME [epoch: 12.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023853200923970218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023853200923970218 | validation: 0.02875714163180078]
	TIME [epoch: 12.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025125860543768343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025125860543768343 | validation: 0.04359901447916658]
	TIME [epoch: 12.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996647564182266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03996647564182266 | validation: 0.06015088549944886]
	TIME [epoch: 12.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06924299246860201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06924299246860201 | validation: 0.0823563361142593]
	TIME [epoch: 12.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07797676395094884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07797676395094884 | validation: 0.023547863195467256]
	TIME [epoch: 12.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030527079167166325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030527079167166325 | validation: 0.2704268886079834]
	TIME [epoch: 12.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12318868366411626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12318868366411626 | validation: 0.1383345443056382]
	TIME [epoch: 12.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09842901520686709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09842901520686709 | validation: 0.13848560914340322]
	TIME [epoch: 12.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14132161711157537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14132161711157537 | validation: 0.15564507989207668]
	TIME [epoch: 12.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17922276122066177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17922276122066177 | validation: 0.08666251527467163]
	TIME [epoch: 12.7 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07311021264149935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07311021264149935 | validation: 0.09369640524560924]
	TIME [epoch: 12.7 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743906645731473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04743906645731473 | validation: 0.11100733140326502]
	TIME [epoch: 12.7 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05049264526706093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05049264526706093 | validation: 0.09661437496207237]
	TIME [epoch: 196 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04534489154390058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04534489154390058 | validation: 0.04741846175879855]
	TIME [epoch: 26 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036614995965369934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036614995965369934 | validation: 0.04253111648336777]
	TIME [epoch: 26 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04293452169717743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04293452169717743 | validation: 0.05725838155894919]
	TIME [epoch: 25.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06591576203816307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06591576203816307 | validation: 0.075679607075959]
	TIME [epoch: 26 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07678382457230849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07678382457230849 | validation: 0.04883397780273308]
	TIME [epoch: 26 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04124334859364773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04124334859364773 | validation: 0.01822670837129662]
	TIME [epoch: 26 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02184564250530628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02184564250530628 | validation: 0.012483181227964157]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_1008.pth
	Model improved!!!
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016565947866886227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016565947866886227 | validation: 0.015133417284449259]
	TIME [epoch: 26 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016979654950504616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016979654950504616 | validation: 0.04765207602760568]
	TIME [epoch: 25.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035013357183301154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035013357183301154 | validation: 0.08284135215334917]
	TIME [epoch: 26 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10010322249177979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10010322249177979 | validation: 0.23235115216394037]
	TIME [epoch: 25.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16168539148329114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16168539148329114 | validation: 0.10249353889118984]
	TIME [epoch: 26 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308924919239722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09308924919239722 | validation: 0.02818661170669169]
	TIME [epoch: 26 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026600114240688352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026600114240688352 | validation: 0.030962217517160575]
	TIME [epoch: 26 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03925643868617347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03925643868617347 | validation: 0.021505270550973333]
	TIME [epoch: 26 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02755428794587543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02755428794587543 | validation: 0.03760813249226681]
	TIME [epoch: 26 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038569139216049854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038569139216049854 | validation: 0.042058936837929856]
	TIME [epoch: 26 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030048535211627456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030048535211627456 | validation: 0.08797107777586596]
	TIME [epoch: 26 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08872511582959536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08872511582959536 | validation: 0.11282384824498715]
	TIME [epoch: 26 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11122059052622339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11122059052622339 | validation: 0.165299901101369]
	TIME [epoch: 26 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15516284481348522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15516284481348522 | validation: 0.507161889925902]
	TIME [epoch: 26 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3582309273630898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3582309273630898 | validation: 0.5422856008935669]
	TIME [epoch: 26 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4350826320138884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4350826320138884 | validation: 0.3865447587732566]
	TIME [epoch: 26 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2683157605388156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2683157605388156 | validation: 0.33985617570721827]
	TIME [epoch: 26 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1790487528455236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1790487528455236 | validation: 0.2485712435381316]
	TIME [epoch: 26 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1265774244561657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1265774244561657 | validation: 0.16595788753454316]
	TIME [epoch: 26 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08920073879964868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08920073879964868 | validation: 0.08605188259357284]
	TIME [epoch: 26 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704948220754111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704948220754111 | validation: 0.08642227926628304]
	TIME [epoch: 26 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059224699408014024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059224699408014024 | validation: 0.05744957765945984]
	TIME [epoch: 26 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05035483268246229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05035483268246229 | validation: 0.04430894334121477]
	TIME [epoch: 26 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037673704873399036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037673704873399036 | validation: 0.04694540200976787]
	TIME [epoch: 26 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593840536232334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03593840536232334 | validation: 0.03439061216013608]
	TIME [epoch: 26 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03112452496523453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03112452496523453 | validation: 0.05262166998802621]
	TIME [epoch: 26 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0299352542544078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0299352542544078 | validation: 0.05205016897383639]
	TIME [epoch: 26 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942322558238985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03942322558238985 | validation: 0.08186483336432558]
	TIME [epoch: 26 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07038189143864917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07038189143864917 | validation: 0.07545518320600869]
	TIME [epoch: 26 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07699023062496771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07699023062496771 | validation: 0.04223823344417437]
	TIME [epoch: 26 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04654506292592108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04654506292592108 | validation: 0.031758363033235905]
	TIME [epoch: 26 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02222153360612849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02222153360612849 | validation: 0.026953341803027067]
	TIME [epoch: 26 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01948048235148388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01948048235148388 | validation: 0.02416936466327213]
	TIME [epoch: 26 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020324134165305593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020324134165305593 | validation: 0.03175130192429559]
	TIME [epoch: 25.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616074306093334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03616074306093334 | validation: 0.03299778573826056]
	TIME [epoch: 26 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035424453227006256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035424453227006256 | validation: 0.04866838581849656]
	TIME [epoch: 26 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543945570774247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04543945570774247 | validation: 0.11229045960590026]
	TIME [epoch: 26 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10124748016744387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10124748016744387 | validation: 0.07949513900012198]
	TIME [epoch: 26 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06753820440974954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06753820440974954 | validation: 0.04873246165052106]
	TIME [epoch: 26 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386594833129204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0386594833129204 | validation: 0.039204524043161194]
	TIME [epoch: 26 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03678766004600786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03678766004600786 | validation: 0.021109093720866556]
	TIME [epoch: 26 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025056689146794184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025056689146794184 | validation: 0.016212006844475203]
	TIME [epoch: 26 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02039743808842041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02039743808842041 | validation: 0.04233974916238065]
	TIME [epoch: 26 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03640865950996909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03640865950996909 | validation: 0.5283570539957955]
	TIME [epoch: 26 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49221276714004947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49221276714004947 | validation: 0.35587210635957184]
	TIME [epoch: 26 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24174978254244017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24174978254244017 | validation: 0.11248985004374178]
	TIME [epoch: 26 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12178400658464002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12178400658464002 | validation: 0.10338534379609308]
	TIME [epoch: 26 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11765294093421037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11765294093421037 | validation: 0.06321501909679464]
	TIME [epoch: 26 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07617115445978964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07617115445978964 | validation: 0.06329333256952721]
	TIME [epoch: 26 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05432665828770394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05432665828770394 | validation: 0.05181936466774593]
	TIME [epoch: 26 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042836318959506306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042836318959506306 | validation: 0.06463349750610052]
	TIME [epoch: 26 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05751208120573502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05751208120573502 | validation: 0.09630446416543317]
	TIME [epoch: 26 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05746917061386842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05746917061386842 | validation: 0.07949526706196816]
	TIME [epoch: 25.9 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06523857204620968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06523857204620968 | validation: 0.07428278620476082]
	TIME [epoch: 26 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07246904894882296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07246904894882296 | validation: 0.08605296639001896]
	TIME [epoch: 26 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07091711468371693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07091711468371693 | validation: 0.0577500145471849]
	TIME [epoch: 26 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057790940612345854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057790940612345854 | validation: 0.06220004830962827]
	TIME [epoch: 26 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05601956169153788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05601956169153788 | validation: 0.04925546780584882]
	TIME [epoch: 26 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0397944546947116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0397944546947116 | validation: 0.06794784632632422]
	TIME [epoch: 26 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044729238215644646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044729238215644646 | validation: 0.08523834557189153]
	TIME [epoch: 26 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08723237431791629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08723237431791629 | validation: 0.07365186618015787]
	TIME [epoch: 26 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06663012622734826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06663012622734826 | validation: 0.04114138097364678]
	TIME [epoch: 26 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791620436385198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03791620436385198 | validation: 0.03370466152882784]
	TIME [epoch: 26 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023683921695188667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023683921695188667 | validation: 0.030494529212045297]
	TIME [epoch: 26 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02627355957432017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02627355957432017 | validation: 0.034930014196432345]
	TIME [epoch: 26 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031045781154085798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031045781154085798 | validation: 0.035621992937158696]
	TIME [epoch: 26 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03413272473914534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03413272473914534 | validation: 0.033090800063664785]
	TIME [epoch: 26 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02945929524546168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02945929524546168 | validation: 0.0305379966055944]
	TIME [epoch: 26 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02514738802695326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02514738802695326 | validation: 0.03129009206672524]
	TIME [epoch: 26 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022446333409256808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022446333409256808 | validation: 0.021316212272212854]
	TIME [epoch: 26 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02154344004160657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02154344004160657 | validation: 0.032455421335456223]
	TIME [epoch: 26 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020714545351060352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020714545351060352 | validation: 0.021083986223093334]
	TIME [epoch: 26 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017460870078241077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017460870078241077 | validation: 0.05709080884901087]
	TIME [epoch: 26 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05841049868190485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05841049868190485 | validation: 0.10668494795795175]
	TIME [epoch: 26 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13782624393362575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13782624393362575 | validation: 0.13519631091848824]
	TIME [epoch: 26 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09882948616904226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09882948616904226 | validation: 0.07574947752197418]
	TIME [epoch: 26 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07855702238960817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07855702238960817 | validation: 0.05162187459203243]
	TIME [epoch: 26 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039899367099070576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039899367099070576 | validation: 0.05597782771005194]
	TIME [epoch: 26 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058078674002399355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058078674002399355 | validation: 0.06442808815219903]
	TIME [epoch: 26 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049905489861823894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049905489861823894 | validation: 0.044159275358360284]
	TIME [epoch: 26 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04307546918849034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04307546918849034 | validation: 0.016544484538502546]
	TIME [epoch: 26 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021528805473827402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021528805473827402 | validation: 0.026283811016785022]
	TIME [epoch: 26 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02071897997664494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02071897997664494 | validation: 0.033439351202461214]
	TIME [epoch: 26 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025353123032046686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025353123032046686 | validation: 0.05241660999968398]
	TIME [epoch: 26 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04933014488693617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04933014488693617 | validation: 0.03060835721483969]
	TIME [epoch: 26 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032067527355073955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032067527355073955 | validation: 0.09695755126157411]
	TIME [epoch: 26 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061308318814163466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061308318814163466 | validation: 0.07691024974470043]
	TIME [epoch: 26 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611302516536462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05611302516536462 | validation: 0.05035669128917415]
	TIME [epoch: 26 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042746997112737314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042746997112737314 | validation: 0.030347893348300473]
	TIME [epoch: 26 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033536814736822115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033536814736822115 | validation: 0.03305280968485113]
	TIME [epoch: 26 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022088526596254373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022088526596254373 | validation: 0.024924477926524427]
	TIME [epoch: 26 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023422778927770786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023422778927770786 | validation: 0.03345929082276442]
	TIME [epoch: 26 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02803678334221005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02803678334221005 | validation: 0.05282118381476536]
	TIME [epoch: 26 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04990600964918611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04990600964918611 | validation: 0.05707359000004352]
	TIME [epoch: 26 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045322542043573256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045322542043573256 | validation: 0.0447204962680986]
	TIME [epoch: 26 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0372993054600258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0372993054600258 | validation: 0.030094040549409874]
	TIME [epoch: 26 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03092161341177763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03092161341177763 | validation: 0.03479668427206718]
	TIME [epoch: 26 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030303315724831883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030303315724831883 | validation: 0.08765128386396387]
	TIME [epoch: 26 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05843933573439562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05843933573439562 | validation: 0.06878912013356032]
	TIME [epoch: 26 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064215522018151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064215522018151 | validation: 0.06415793477447884]
	TIME [epoch: 26 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059866115267977325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059866115267977325 | validation: 0.03017830715499428]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_113803/states/model_phi1_4c_v_mmd1_1109.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 12279.855 seconds.
