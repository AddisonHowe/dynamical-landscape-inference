Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 996375320

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.200094994198868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.200094994198868 | validation: 4.186506560281046]
	TIME [epoch: 173 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.269657137014767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.269657137014767 | validation: 4.1632891973585044]
	TIME [epoch: 2.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.222987273262061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.222987273262061 | validation: 4.6054550045217]
	TIME [epoch: 2.9 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5308031812807235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5308031812807235 | validation: 3.790746867895767]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.818191002896226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.818191002896226 | validation: 3.7178294242763084]
	TIME [epoch: 2.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.760736586737506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.760736586737506 | validation: 3.5583022792771164]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.743719636640825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.743719636640825 | validation: 3.1925931336692503]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5145591273709584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5145591273709584 | validation: 2.9122701204514336]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.397407199506288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.397407199506288 | validation: 2.6759657075908865]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.221566118314299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.221566118314299 | validation: 2.5641887054249235]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0140083301353604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0140083301353604 | validation: 2.0091310731765324]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.613706701155919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.613706701155919 | validation: 1.7638928740333957]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.146792851844753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.146792851844753 | validation: 4.076792954871251]
	TIME [epoch: 2.91 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.357674733587152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.357674733587152 | validation: 1.5423366686210287]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.146005149989355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.146005149989355 | validation: 3.7947026326543467]
	TIME [epoch: 2.92 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9647810073139262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9647810073139262 | validation: 2.121467460874866]
	TIME [epoch: 2.91 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4485366968280426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4485366968280426 | validation: 1.6141478126235862]
	TIME [epoch: 2.92 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0288525340703796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0288525340703796 | validation: 1.4606517014560054]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.964100073902825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.964100073902825 | validation: 1.3197322482494585]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8098295903351382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8098295903351382 | validation: 1.1829718744227262]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5389321418454693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5389321418454693 | validation: 1.0824374258086753]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4037219115199335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4037219115199335 | validation: 1.0833779452782348]
	TIME [epoch: 2.9 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26988102989107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.26988102989107 | validation: 1.2647016233413837]
	TIME [epoch: 2.91 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4586135711352695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4586135711352695 | validation: 1.1443721910121984]
	TIME [epoch: 2.9 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2902254154758783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2902254154758783 | validation: 1.0614521554450862]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2664782239198495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2664782239198495 | validation: 0.9945559319778999]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1623574791407334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1623574791407334 | validation: 0.9539358718358637]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1152543120453902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1152543120453902 | validation: 1.2490021083367855]
	TIME [epoch: 2.9 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6306457887331558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6306457887331558 | validation: 1.0392083575798716]
	TIME [epoch: 2.9 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2590031187548205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2590031187548205 | validation: 1.2092234037013767]
	TIME [epoch: 2.9 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.453812506196913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.453812506196913 | validation: 1.1208198986916131]
	TIME [epoch: 2.9 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5218117561025168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5218117561025168 | validation: 1.0525166955586334]
	TIME [epoch: 2.9 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3110175091012568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3110175091012568 | validation: 0.8672454044737427]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1006440970957567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1006440970957567 | validation: 0.896625660976985]
	TIME [epoch: 2.92 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0749659739465733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0749659739465733 | validation: 0.9277917519061986]
	TIME [epoch: 2.91 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0518330013669492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0518330013669492 | validation: 0.9098494434061426]
	TIME [epoch: 2.91 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0123545240117735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0123545240117735 | validation: 0.9077075943256997]
	TIME [epoch: 2.92 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0210430042160943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0210430042160943 | validation: 0.8445677058644329]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9924077778289477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9924077778289477 | validation: 0.8566973673396163]
	TIME [epoch: 2.92 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9795294447874701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9795294447874701 | validation: 0.8561832315453963]
	TIME [epoch: 2.92 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0081877828355128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0081877828355128 | validation: 0.9360273049573128]
	TIME [epoch: 2.92 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20792544782143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.20792544782143 | validation: 0.935017571734486]
	TIME [epoch: 2.92 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0933052797172647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0933052797172647 | validation: 0.8216753168384703]
	TIME [epoch: 2.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9870311760373752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9870311760373752 | validation: 0.8401593671589341]
	TIME [epoch: 2.91 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9823601191926784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9823601191926784 | validation: 0.8355127228878946]
	TIME [epoch: 2.9 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9642063151794787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9642063151794787 | validation: 0.9312225295837586]
	TIME [epoch: 2.9 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1585133952233977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1585133952233977 | validation: 1.0570659359884609]
	TIME [epoch: 2.91 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4538737168358506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4538737168358506 | validation: 0.8524247484633009]
	TIME [epoch: 2.9 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0588168721011786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0588168721011786 | validation: 1.1143160753035286]
	TIME [epoch: 2.9 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.349857357972624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.349857357972624 | validation: 0.7920942762782274]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0029523933198012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0029523933198012 | validation: 0.8005152805938288]
	TIME [epoch: 2.92 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9801778588234084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9801778588234084 | validation: 0.8136037756905726]
	TIME [epoch: 2.92 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420731627778431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9420731627778431 | validation: 0.8286613434245904]
	TIME [epoch: 2.92 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617682385873697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9617682385873697 | validation: 0.8567500024859304]
	TIME [epoch: 2.92 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987106002486222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9987106002486222 | validation: 0.7879776970077466]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.922731728514148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.922731728514148 | validation: 0.7681073432929024]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9322705724382123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9322705724382123 | validation: 0.7806848245480869]
	TIME [epoch: 2.93 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9270529110011247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9270529110011247 | validation: 0.7797397589762289]
	TIME [epoch: 2.92 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132620732940419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9132620732940419 | validation: 0.8014273654427115]
	TIME [epoch: 2.92 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9137821185222322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9137821185222322 | validation: 0.7850078918310944]
	TIME [epoch: 2.92 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001750738897879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9001750738897879 | validation: 0.8202305487006237]
	TIME [epoch: 2.93 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0176843126595514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0176843126595514 | validation: 1.2687764795298435]
	TIME [epoch: 2.92 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.772439534676205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.772439534676205 | validation: 1.0264734724836997]
	TIME [epoch: 2.92 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.252496820753704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.252496820753704 | validation: 0.8069011739330794]
	TIME [epoch: 2.93 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976633742815358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9976633742815358 | validation: 0.9395300769715658]
	TIME [epoch: 2.92 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0920550683619694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0920550683619694 | validation: 0.8400752171118079]
	TIME [epoch: 2.92 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.014975126739664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.014975126739664 | validation: 0.8257496956945443]
	TIME [epoch: 2.92 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.998896440721283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.998896440721283 | validation: 0.7803923669973373]
	TIME [epoch: 2.92 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049796444058319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049796444058319 | validation: 0.8027197163868259]
	TIME [epoch: 2.92 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9374821589592162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9374821589592162 | validation: 0.7838956146615207]
	TIME [epoch: 2.92 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9098869788797315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9098869788797315 | validation: 0.7827351805198721]
	TIME [epoch: 2.92 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090099232447472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9090099232447472 | validation: 0.7762288337504515]
	TIME [epoch: 2.92 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8967712565000759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8967712565000759 | validation: 0.7555024600881168]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992626903974026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8992626903974026 | validation: 0.7435679232511737]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8743929888703889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8743929888703889 | validation: 0.7380277917423919]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729264777521728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729264777521728 | validation: 0.7635323411787555]
	TIME [epoch: 2.92 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869952922576108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869952922576108 | validation: 0.7955282627825705]
	TIME [epoch: 2.92 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9187500282634994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9187500282634994 | validation: 0.7580926140351125]
	TIME [epoch: 2.92 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9457922215623197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9457922215623197 | validation: 0.9230890227300041]
	TIME [epoch: 2.92 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0712707482315456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0712707482315456 | validation: 0.7150212567626696]
	TIME [epoch: 2.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868273565544064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868273565544064 | validation: 0.752700336847135]
	TIME [epoch: 2.92 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9460754392197813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9460754392197813 | validation: 0.9354099985229252]
	TIME [epoch: 2.92 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2132736762395413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2132736762395413 | validation: 0.853677600230742]
	TIME [epoch: 2.92 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0249308730731541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0249308730731541 | validation: 0.8736284997682005]
	TIME [epoch: 2.92 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0490505001317179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0490505001317179 | validation: 0.82918803808927]
	TIME [epoch: 2.92 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0143138549919377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0143138549919377 | validation: 0.897434650624469]
	TIME [epoch: 2.91 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.033082791489376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.033082791489376 | validation: 0.7434346450305299]
	TIME [epoch: 2.92 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8972710702457736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8972710702457736 | validation: 0.8274509854129772]
	TIME [epoch: 2.92 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9471751950234228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9471751950234228 | validation: 0.7650023957059686]
	TIME [epoch: 2.92 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911290507953316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911290507953316 | validation: 0.7597286781121922]
	TIME [epoch: 2.92 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905127669968321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8905127669968321 | validation: 0.7722817328106895]
	TIME [epoch: 2.92 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973607329673992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8973607329673992 | validation: 0.7339562621668789]
	TIME [epoch: 2.92 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833359104053057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8833359104053057 | validation: 0.8302492945021076]
	TIME [epoch: 2.92 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0344028234341471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0344028234341471 | validation: 0.870806482439459]
	TIME [epoch: 2.92 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.098861888408377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.098861888408377 | validation: 0.7705327714826337]
	TIME [epoch: 2.92 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343793045529988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9343793045529988 | validation: 0.8413061698681106]
	TIME [epoch: 2.92 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.02459563912274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.02459563912274 | validation: 0.7825566809436797]
	TIME [epoch: 2.92 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9397344608859552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9397344608859552 | validation: 0.7625745949440518]
	TIME [epoch: 2.92 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8929155103207669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8929155103207669 | validation: 0.74807297275833]
	TIME [epoch: 2.92 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9151849321105578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9151849321105578 | validation: 0.8144214550646942]
	TIME [epoch: 2.91 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9766313462264858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9766313462264858 | validation: 0.829353658242416]
	TIME [epoch: 2.92 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.981268933162285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.981268933162285 | validation: 0.7298358905057776]
	TIME [epoch: 2.91 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8709676385994258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8709676385994258 | validation: 0.714985585543302]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680155176080215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8680155176080215 | validation: 0.7441949456459682]
	TIME [epoch: 2.92 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8731694143405868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8731694143405868 | validation: 0.7360022403808835]
	TIME [epoch: 2.92 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789038998216989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789038998216989 | validation: 0.7485823165963474]
	TIME [epoch: 2.92 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8905356983885632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8905356983885632 | validation: 0.7698247601196769]
	TIME [epoch: 2.91 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920395700223367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920395700223367 | validation: 0.7402137413343056]
	TIME [epoch: 2.91 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830866058278665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8830866058278665 | validation: 0.7380748565001224]
	TIME [epoch: 2.91 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712167690737573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712167690737573 | validation: 0.7537572019299866]
	TIME [epoch: 2.91 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8949468121783336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8949468121783336 | validation: 0.7791793332638138]
	TIME [epoch: 2.91 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223041613989285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9223041613989285 | validation: 0.7743445526590818]
	TIME [epoch: 2.91 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9400615244473035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9400615244473035 | validation: 0.8359650421531053]
	TIME [epoch: 2.91 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9964859156420895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9964859156420895 | validation: 0.7309757209148695]
	TIME [epoch: 2.91 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854683205181629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854683205181629 | validation: 0.7426648806417075]
	TIME [epoch: 2.91 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8831580126425773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8831580126425773 | validation: 0.7578829269850955]
	TIME [epoch: 2.91 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8959161909007659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8959161909007659 | validation: 0.7691183592498145]
	TIME [epoch: 2.91 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046653268821805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9046653268821805 | validation: 0.783301302126504]
	TIME [epoch: 2.92 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.926099521519395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.926099521519395 | validation: 0.8496642205003155]
	TIME [epoch: 2.91 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0089209871041496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0089209871041496 | validation: 0.8350160414464413]
	TIME [epoch: 2.91 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0077296255679662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0077296255679662 | validation: 0.7296174696369949]
	TIME [epoch: 2.91 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8727552884881058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8727552884881058 | validation: 0.8231723260113255]
	TIME [epoch: 2.91 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9592022459456648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9592022459456648 | validation: 0.8017186050012164]
	TIME [epoch: 2.91 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956414952822978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.956414952822978 | validation: 0.7301899024907131]
	TIME [epoch: 2.91 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8709882842035052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8709882842035052 | validation: 0.7945644445293147]
	TIME [epoch: 2.91 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9522096729782668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9522096729782668 | validation: 0.7962822129985562]
	TIME [epoch: 2.92 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.964257359877924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.964257359877924 | validation: 0.7450267914775655]
	TIME [epoch: 2.91 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.880099341245344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.880099341245344 | validation: 0.8063919025604845]
	TIME [epoch: 2.91 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9563861930356283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9563861930356283 | validation: 0.7270258361311573]
	TIME [epoch: 2.91 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837497841882319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837497841882319 | validation: 0.7276563778757504]
	TIME [epoch: 2.91 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8758741585828048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8758741585828048 | validation: 0.7345541085254451]
	TIME [epoch: 2.91 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686764376375615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8686764376375615 | validation: 0.7479381164001965]
	TIME [epoch: 2.91 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791078514863897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8791078514863897 | validation: 0.7593043623549319]
	TIME [epoch: 2.91 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8844877115033666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8844877115033666 | validation: 0.7730773752658298]
	TIME [epoch: 2.91 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760794995610809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760794995610809 | validation: 0.760914112173022]
	TIME [epoch: 2.91 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896212607305589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896212607305589 | validation: 0.7551562268832909]
	TIME [epoch: 2.92 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8678680558850619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8678680558850619 | validation: 0.7181187616667754]
	TIME [epoch: 2.91 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617601516354314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8617601516354314 | validation: 0.729333946492511]
	TIME [epoch: 2.91 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467383767133471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467383767133471 | validation: 0.731689175418048]
	TIME [epoch: 2.91 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8738114885100609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8738114885100609 | validation: 0.7507619022366792]
	TIME [epoch: 2.91 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027750952558411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9027750952558411 | validation: 0.8242690983807273]
	TIME [epoch: 2.91 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9899829205415382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9899829205415382 | validation: 0.8137603799989149]
	TIME [epoch: 2.91 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9840821001408234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9840821001408234 | validation: 0.7901364060397257]
	TIME [epoch: 2.91 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9226843893182795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9226843893182795 | validation: 0.764088249672065]
	TIME [epoch: 2.91 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8963893381743038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8963893381743038 | validation: 0.7437593027995176]
	TIME [epoch: 2.91 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8498216418835918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498216418835918 | validation: 0.7348600372984196]
	TIME [epoch: 2.91 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673097050513705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673097050513705 | validation: 0.7169991053368152]
	TIME [epoch: 2.91 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8518644130964301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8518644130964301 | validation: 0.7689065840203161]
	TIME [epoch: 2.91 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.935456315594223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.935456315594223 | validation: 0.7567914365131734]
	TIME [epoch: 2.92 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9122276227897896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9122276227897896 | validation: 0.7344971906438069]
	TIME [epoch: 2.91 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8572724448914167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8572724448914167 | validation: 0.7186658934411947]
	TIME [epoch: 2.91 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8406890916561094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8406890916561094 | validation: 0.7063833576986753]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_152.pth
	Model improved!!!
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8488070930590209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8488070930590209 | validation: 0.757472030134898]
	TIME [epoch: 2.92 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001668638328684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9001668638328684 | validation: 0.7846441848624641]
	TIME [epoch: 2.91 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9301622944337194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9301622944337194 | validation: 0.7695472009178481]
	TIME [epoch: 2.91 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8823638111885839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8823638111885839 | validation: 0.805651756652133]
	TIME [epoch: 2.91 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9081348939273667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9081348939273667 | validation: 0.7534972825490224]
	TIME [epoch: 2.91 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8603934140309213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8603934140309213 | validation: 0.7287749807789338]
	TIME [epoch: 2.91 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427098755208234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427098755208234 | validation: 0.7305202684940355]
	TIME [epoch: 2.91 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841937056985866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.841937056985866 | validation: 0.7034821451579727]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_160.pth
	Model improved!!!
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8415577948934868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8415577948934868 | validation: 0.7219355024730856]
	TIME [epoch: 2.91 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252727807878284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8252727807878284 | validation: 0.7383600482863664]
	TIME [epoch: 2.91 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592485924841411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592485924841411 | validation: 0.8778298992170073]
	TIME [epoch: 2.91 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.038586692156418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.038586692156418 | validation: 0.7580277898485938]
	TIME [epoch: 2.91 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8593387457931744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8593387457931744 | validation: 0.7675758935994059]
	TIME [epoch: 2.91 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8726110689344097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8726110689344097 | validation: 0.769188337256272]
	TIME [epoch: 2.91 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437550179347116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8437550179347116 | validation: 0.7424791140909224]
	TIME [epoch: 2.91 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394976576922443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394976576922443 | validation: 0.7171041254588276]
	TIME [epoch: 2.91 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339852007084982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8339852007084982 | validation: 0.8506505509745106]
	TIME [epoch: 2.91 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938586346291218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.938586346291218 | validation: 0.7551695213732743]
	TIME [epoch: 2.9 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.911763014004068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.911763014004068 | validation: 0.7165399604405264]
	TIME [epoch: 2.9 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8079877491902562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8079877491902562 | validation: 0.7208400761166551]
	TIME [epoch: 2.91 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783415771449654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783415771449654 | validation: 0.7416156761847132]
	TIME [epoch: 2.91 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146380087081181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8146380087081181 | validation: 1.2019064009025897]
	TIME [epoch: 2.91 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2199692763118417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2199692763118417 | validation: 1.029972970649195]
	TIME [epoch: 2.9 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1202181565708826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1202181565708826 | validation: 0.66499911116035]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888683346634053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7888683346634053 | validation: 0.7379748536005364]
	TIME [epoch: 2.91 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098568473607665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098568473607665 | validation: 0.7034414469981314]
	TIME [epoch: 2.91 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7949004713289936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7949004713289936 | validation: 0.6688521551468732]
	TIME [epoch: 2.9 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760271102561803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760271102561803 | validation: 0.682470765692214]
	TIME [epoch: 2.9 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.742224093438198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.742224093438198 | validation: 0.6665093446778774]
	TIME [epoch: 2.91 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734421343447695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.734421343447695 | validation: 0.6698062627231773]
	TIME [epoch: 2.91 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7197049985133813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7197049985133813 | validation: 0.6504162305436525]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117689216216962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117689216216962 | validation: 0.8023383421915455]
	TIME [epoch: 2.91 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850605293600326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7850605293600326 | validation: 1.0094175426238725]
	TIME [epoch: 2.9 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136874983115338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.136874983115338 | validation: 0.8047846060018359]
	TIME [epoch: 2.9 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8039353004930005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8039353004930005 | validation: 0.6614404256876375]
	TIME [epoch: 2.91 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378202149293439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378202149293439 | validation: 1.0853477100440647]
	TIME [epoch: 2.91 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.179345761873318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.179345761873318 | validation: 0.8220074708545356]
	TIME [epoch: 2.91 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9433458662705545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9433458662705545 | validation: 0.656084185249229]
	TIME [epoch: 2.9 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7217751925972777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7217751925972777 | validation: 0.6322467381542105]
	TIME [epoch: 2.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7108543432500386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7108543432500386 | validation: 0.643889954026903]
	TIME [epoch: 2.91 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6935449211081897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6935449211081897 | validation: 0.6597986451370921]
	TIME [epoch: 2.9 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678609561493475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6678609561493475 | validation: 0.6655667824610978]
	TIME [epoch: 2.91 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7289059204359544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7289059204359544 | validation: 0.8547578170387319]
	TIME [epoch: 2.91 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912538553882695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8912538553882695 | validation: 0.7590262372946909]
	TIME [epoch: 2.91 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093901176202598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093901176202598 | validation: 0.7336655489127463]
	TIME [epoch: 2.91 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085736027105666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8085736027105666 | validation: 0.6977720412303128]
	TIME [epoch: 2.91 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292836673058701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292836673058701 | validation: 0.6199521560417165]
	TIME [epoch: 2.91 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169126794693665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169126794693665 | validation: 0.648602437972573]
	TIME [epoch: 2.91 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529098470983198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6529098470983198 | validation: 0.6115286135920693]
	TIME [epoch: 179 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631037532458538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6631037532458538 | validation: 0.7494002766107868]
	TIME [epoch: 6.28 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476768732876741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476768732876741 | validation: 0.7823354264073766]
	TIME [epoch: 6.27 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163312301288497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163312301288497 | validation: 0.8358832198616327]
	TIME [epoch: 6.27 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038693700136006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038693700136006 | validation: 0.6465635885515186]
	TIME [epoch: 6.28 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7058064797159164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7058064797159164 | validation: 0.6011308707617737]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6833516370469238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6833516370469238 | validation: 0.6304597324956691]
	TIME [epoch: 6.26 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581072822607109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581072822607109 | validation: 0.7092219899718286]
	TIME [epoch: 6.27 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6819391344721015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6819391344721015 | validation: 0.670593874731382]
	TIME [epoch: 6.28 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040078818882591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7040078818882591 | validation: 0.7932797738635688]
	TIME [epoch: 6.27 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439443967427848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439443967427848 | validation: 0.5561390611018732]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254949928998119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6254949928998119 | validation: 0.6251611474322959]
	TIME [epoch: 6.28 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271662688415137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6271662688415137 | validation: 0.7871892366848127]
	TIME [epoch: 6.27 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.938152198470541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.938152198470541 | validation: 0.7985984548755752]
	TIME [epoch: 6.27 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589635312365437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589635312365437 | validation: 0.5700088226684201]
	TIME [epoch: 6.27 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660432990475101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.660432990475101 | validation: 0.5714438193976321]
	TIME [epoch: 6.28 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6210183128943202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6210183128943202 | validation: 0.5873640140932386]
	TIME [epoch: 6.28 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043020095058456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043020095058456 | validation: 0.611494516604904]
	TIME [epoch: 6.28 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6549511978947854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6549511978947854 | validation: 0.7308410829502702]
	TIME [epoch: 6.28 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184588495030516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7184588495030516 | validation: 0.7023762219182874]
	TIME [epoch: 6.28 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066179312405845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066179312405845 | validation: 0.6301999823790992]
	TIME [epoch: 6.27 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810129844973624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810129844973624 | validation: 0.6394651017485153]
	TIME [epoch: 6.27 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6183032463389476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6183032463389476 | validation: 0.5874399064132786]
	TIME [epoch: 6.27 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.694677922069952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.694677922069952 | validation: 0.7724096386360825]
	TIME [epoch: 6.29 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857562826571075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857562826571075 | validation: 0.5623646065565174]
	TIME [epoch: 6.27 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704969669737058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6704969669737058 | validation: 0.558476932242863]
	TIME [epoch: 6.27 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5627540426368168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5627540426368168 | validation: 0.5208196536144375]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5431056743248999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5431056743248999 | validation: 0.5232625339059691]
	TIME [epoch: 6.27 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5581436373060197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5581436373060197 | validation: 0.6722139625524637]
	TIME [epoch: 6.28 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303584698253606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6303584698253606 | validation: 0.7342409652758849]
	TIME [epoch: 6.26 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163516916408498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163516916408498 | validation: 0.5703562970886388]
	TIME [epoch: 6.28 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5803208777982194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5803208777982194 | validation: 0.525111059315351]
	TIME [epoch: 6.26 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5320672006455055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5320672006455055 | validation: 0.6176852041680356]
	TIME [epoch: 6.28 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478670988495328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478670988495328 | validation: 1.1351188709147082]
	TIME [epoch: 6.27 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2683262256904118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2683262256904118 | validation: 0.5699222965778925]
	TIME [epoch: 6.28 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6248885912538914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6248885912538914 | validation: 0.7332534986640374]
	TIME [epoch: 6.27 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9300327679477531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9300327679477531 | validation: 0.5391445399031713]
	TIME [epoch: 6.27 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5456964058746324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5456964058746324 | validation: 0.5847242476205242]
	TIME [epoch: 6.28 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5895622590251479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5895622590251479 | validation: 0.5692890957475158]
	TIME [epoch: 6.27 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6309238153769209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6309238153769209 | validation: 0.6178030224488416]
	TIME [epoch: 6.28 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6623168753700905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6623168753700905 | validation: 0.5536169539522436]
	TIME [epoch: 6.29 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5675638946743631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5675638946743631 | validation: 0.47672872036612407]
	TIME [epoch: 6.26 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5139684292540072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5139684292540072 | validation: 0.5071325169374473]
	TIME [epoch: 6.27 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5016837901546879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5016837901546879 | validation: 0.4941431604678753]
	TIME [epoch: 6.26 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5283875206847314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5283875206847314 | validation: 0.5715250604887467]
	TIME [epoch: 6.26 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5939233786951995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5939233786951995 | validation: 0.5432501208391454]
	TIME [epoch: 6.26 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6485284745767058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6485284745767058 | validation: 0.5318324368943544]
	TIME [epoch: 6.27 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5648707429406196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5648707429406196 | validation: 0.5125484872019533]
	TIME [epoch: 6.27 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.528410183550174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.528410183550174 | validation: 0.6357142495888786]
	TIME [epoch: 6.28 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7352062760300541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7352062760300541 | validation: 0.6718347373503888]
	TIME [epoch: 6.29 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6384727724639054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6384727724639054 | validation: 0.476651836610241]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5340671818908791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5340671818908791 | validation: 0.5352549211063978]
	TIME [epoch: 6.23 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5191522514074228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5191522514074228 | validation: 0.6594604551668101]
	TIME [epoch: 6.24 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485053997018841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485053997018841 | validation: 0.6739664364665064]
	TIME [epoch: 6.23 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570200477180018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570200477180018 | validation: 0.4626652052422367]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274717061534007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5274717061534007 | validation: 0.5262025307620272]
	TIME [epoch: 6.23 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5290808310837383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5290808310837383 | validation: 0.6016445661913631]
	TIME [epoch: 6.24 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6591905795757756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6591905795757756 | validation: 1.0471778593445629]
	TIME [epoch: 6.24 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8881242082044637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8881242082044637 | validation: 0.45174741553593933]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_259.pth
	Model improved!!!
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48467676987630925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48467676987630925 | validation: 0.46138231265405977]
	TIME [epoch: 6.26 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5671854333179026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5671854333179026 | validation: 0.6431188760565688]
	TIME [epoch: 6.27 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201230430389866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6201230430389866 | validation: 0.4932581969326744]
	TIME [epoch: 6.28 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5651185395752804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5651185395752804 | validation: 0.5818627077158584]
	TIME [epoch: 6.26 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.567677953533676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.567677953533676 | validation: 0.447969345971538]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280972149222023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5280972149222023 | validation: 0.4817728768774071]
	TIME [epoch: 6.23 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4688360783190531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4688360783190531 | validation: 0.4084228125874808]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.455676827932393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.455676827932393 | validation: 0.47223882400649375]
	TIME [epoch: 6.28 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4611886530442355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4611886530442355 | validation: 0.4234174433497252]
	TIME [epoch: 6.29 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4896399174764824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4896399174764824 | validation: 1.365113279967037]
	TIME [epoch: 6.27 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1299336712518946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1299336712518946 | validation: 0.47160228548449645]
	TIME [epoch: 6.27 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46202300811961833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46202300811961833 | validation: 0.6206612405459011]
	TIME [epoch: 6.23 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266768055291012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266768055291012 | validation: 0.42628980805533134]
	TIME [epoch: 6.22 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4355642890019854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4355642890019854 | validation: 0.5307648316866397]
	TIME [epoch: 6.23 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4932078198751392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4932078198751392 | validation: 0.45178203995497007]
	TIME [epoch: 6.24 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43602396071783284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43602396071783284 | validation: 0.44761404011037964]
	TIME [epoch: 6.25 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5193790932934408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5193790932934408 | validation: 0.7817222373774962]
	TIME [epoch: 6.24 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9085173726377829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9085173726377829 | validation: 0.4468628924640181]
	TIME [epoch: 6.24 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5571939653619531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5571939653619531 | validation: 0.39756161461167605]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4174745957937674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4174745957937674 | validation: 0.5117491814139864]
	TIME [epoch: 6.27 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48448267088059765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48448267088059765 | validation: 0.4888131126052859]
	TIME [epoch: 6.26 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5357451633012622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5357451633012622 | validation: 0.5293618368775774]
	TIME [epoch: 6.27 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5693463711511375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5693463711511375 | validation: 0.4195945474726968]
	TIME [epoch: 6.27 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5114146229518899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5114146229518899 | validation: 0.5859335310415522]
	TIME [epoch: 6.26 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5234089060224026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5234089060224026 | validation: 0.5588511422890072]
	TIME [epoch: 6.24 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5935123008156024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5935123008156024 | validation: 0.4481854668901125]
	TIME [epoch: 6.24 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4523795438579188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4523795438579188 | validation: 0.40165734255978675]
	TIME [epoch: 6.23 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4061406061804614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4061406061804614 | validation: 0.36941226456284293]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40300119777249144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40300119777249144 | validation: 0.5887042698267048]
	TIME [epoch: 6.27 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5141051019824301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5141051019824301 | validation: 0.4007995552783523]
	TIME [epoch: 6.26 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44976854219015605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44976854219015605 | validation: 0.45090331542670015]
	TIME [epoch: 6.26 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4643060741409043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4643060741409043 | validation: 0.48799617444964005]
	TIME [epoch: 6.27 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5953460731455646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5953460731455646 | validation: 0.4590107638689677]
	TIME [epoch: 6.26 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4694404764325209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4694404764325209 | validation: 0.3629634998936144]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3812030818173902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3812030818173902 | validation: 0.34248386931971453]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3905761325299744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3905761325299744 | validation: 0.4128730465279796]
	TIME [epoch: 6.23 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3986063450148404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3986063450148404 | validation: 0.365702261092666]
	TIME [epoch: 6.23 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46556837797654627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46556837797654627 | validation: 0.6921204333708796]
	TIME [epoch: 6.25 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6041436742986678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6041436742986678 | validation: 0.40500316381845214]
	TIME [epoch: 6.23 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5072643568007817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5072643568007817 | validation: 0.5086237703053512]
	TIME [epoch: 6.25 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226797990581206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5226797990581206 | validation: 0.376609287704061]
	TIME [epoch: 6.23 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4411262919264922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4411262919264922 | validation: 0.3843578654290525]
	TIME [epoch: 6.23 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39921711766390866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39921711766390866 | validation: 0.481750935209967]
	TIME [epoch: 6.23 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42491839365691775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42491839365691775 | validation: 0.41097833757904856]
	TIME [epoch: 6.23 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5146689682876269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5146689682876269 | validation: 0.3554096210451584]
	TIME [epoch: 6.23 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41295353087773473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41295353087773473 | validation: 0.3340356064365716]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3399483257288614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3399483257288614 | validation: 0.3184822013355292]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_306.pth
	Model improved!!!
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3336756440726243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3336756440726243 | validation: 0.34972991227434574]
	TIME [epoch: 6.23 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34242825060651805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34242825060651805 | validation: 0.32957246395231743]
	TIME [epoch: 6.23 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4200665853961775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4200665853961775 | validation: 0.4739403566188942]
	TIME [epoch: 6.23 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41943854416907017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41943854416907017 | validation: 0.622749170541391]
	TIME [epoch: 6.23 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563334516691711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7563334516691711 | validation: 0.593070620732618]
	TIME [epoch: 6.24 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6826337104232583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6826337104232583 | validation: 0.3607275784705124]
	TIME [epoch: 6.24 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39456664220819443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39456664220819443 | validation: 0.3175596761609412]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_313.pth
	Model improved!!!
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3847228742395555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3847228742395555 | validation: 1.1418612739309129]
	TIME [epoch: 6.23 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889168226715982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889168226715982 | validation: 0.5557891013493675]
	TIME [epoch: 6.22 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297256313899775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6297256313899775 | validation: 0.3503197826447047]
	TIME [epoch: 6.25 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4259152091550682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4259152091550682 | validation: 0.4361562897411701]
	TIME [epoch: 6.24 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4163913787259946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4163913787259946 | validation: 0.3819417281388682]
	TIME [epoch: 6.24 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38827373586716063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38827373586716063 | validation: 0.4123354338005422]
	TIME [epoch: 6.24 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4384827347766283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4384827347766283 | validation: 0.4594416627019511]
	TIME [epoch: 6.23 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5197048415468561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5197048415468561 | validation: 0.3123703837666627]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_321.pth
	Model improved!!!
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3726940434998271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3726940434998271 | validation: 0.26896069170282444]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3132885613952984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3132885613952984 | validation: 0.3852068723517297]
	TIME [epoch: 6.23 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37057978073242587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37057978073242587 | validation: 0.3044288868554259]
	TIME [epoch: 6.23 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36568656514092057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36568656514092057 | validation: 0.3751162002554725]
	TIME [epoch: 6.23 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3666797855785859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3666797855785859 | validation: 0.30260617291642156]
	TIME [epoch: 6.24 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3891090486949598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3891090486949598 | validation: 0.42313092530177787]
	TIME [epoch: 6.23 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3868340128311034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3868340128311034 | validation: 0.30960664948752614]
	TIME [epoch: 6.24 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4129043537543491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4129043537543491 | validation: 0.4269845289188619]
	TIME [epoch: 6.24 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3503498531670802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3503498531670802 | validation: 0.29083346518835773]
	TIME [epoch: 6.24 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3087270048195998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3087270048195998 | validation: 0.34527690680845996]
	TIME [epoch: 6.24 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3935901996870922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3935901996870922 | validation: 0.4582449854112306]
	TIME [epoch: 6.24 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421752647869252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5421752647869252 | validation: 0.4673015468182291]
	TIME [epoch: 6.24 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39161933709500657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39161933709500657 | validation: 0.35894007097505837]
	TIME [epoch: 6.23 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4634182482885971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4634182482885971 | validation: 0.36999819580040394]
	TIME [epoch: 6.24 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3311358785945423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3311358785945423 | validation: 0.2545006717685579]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3157832939205468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3157832939205468 | validation: 0.3334242667228043]
	TIME [epoch: 6.23 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3402766758758484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3402766758758484 | validation: 0.40262293869747867]
	TIME [epoch: 6.24 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4151421411342584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4151421411342584 | validation: 0.37438193776406226]
	TIME [epoch: 6.23 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43040192244365194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43040192244365194 | validation: 1.123774984926976]
	TIME [epoch: 6.23 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0537302741319756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0537302741319756 | validation: 0.6394725020609832]
	TIME [epoch: 6.23 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5390687656233433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5390687656233433 | validation: 0.5096030596309936]
	TIME [epoch: 6.24 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615090840655396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.615090840655396 | validation: 0.2757941174248775]
	TIME [epoch: 6.23 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2980454555341175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2980454555341175 | validation: 0.4183149296315589]
	TIME [epoch: 6.24 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3954191601458881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3954191601458881 | validation: 0.2926257788570264]
	TIME [epoch: 6.24 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3223715150908462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3223715150908462 | validation: 0.37521528827660694]
	TIME [epoch: 6.23 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40866554898855406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40866554898855406 | validation: 0.24226758617749233]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26365921834751244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26365921834751244 | validation: 0.26346441945709]
	TIME [epoch: 6.26 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2491185862906211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2491185862906211 | validation: 0.22364040778108293]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2730858761966018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2730858761966018 | validation: 0.5364257208408032]
	TIME [epoch: 6.26 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4696573794027621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4696573794027621 | validation: 0.3549399753314761]
	TIME [epoch: 6.27 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40483045355859143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40483045355859143 | validation: 0.3687458586666446]
	TIME [epoch: 6.27 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41151735964753317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41151735964753317 | validation: 0.2467896722507378]
	TIME [epoch: 6.27 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26137087662859754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26137087662859754 | validation: 0.2050679688648751]
	TIME [epoch: 6.27 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_354.pth
	Model improved!!!
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23295606479979752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23295606479979752 | validation: 0.6256432786413496]
	TIME [epoch: 6.26 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47246591226927037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47246591226927037 | validation: 0.34145429101517544]
	TIME [epoch: 6.25 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42556965496644095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42556965496644095 | validation: 0.30088871705286613]
	TIME [epoch: 6.27 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2562712981245452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2562712981245452 | validation: 0.22260227047854883]
	TIME [epoch: 6.26 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2441511603633894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2441511603633894 | validation: 0.21502725883463386]
	TIME [epoch: 6.28 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21021133748205337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21021133748205337 | validation: 0.22381834919592555]
	TIME [epoch: 6.27 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20755486461292513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20755486461292513 | validation: 0.5498284118524038]
	TIME [epoch: 6.28 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570533219048036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.570533219048036 | validation: 0.7461952869183648]
	TIME [epoch: 6.27 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504270679376851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8504270679376851 | validation: 0.28905301901174074]
	TIME [epoch: 6.26 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3684371635314565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3684371635314565 | validation: 0.2796885294376387]
	TIME [epoch: 6.27 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062142173650528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3062142173650528 | validation: 0.24023297793405762]
	TIME [epoch: 6.26 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2934697490644882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2934697490644882 | validation: 0.6535595761438981]
	TIME [epoch: 6.26 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496565273677758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5496565273677758 | validation: 0.204881885476766]
	TIME [epoch: 6.28 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29551568878836315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29551568878836315 | validation: 0.2068936548239027]
	TIME [epoch: 6.24 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22798819178254726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22798819178254726 | validation: 0.18310503125135513]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_369.pth
	Model improved!!!
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23232264614385792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23232264614385792 | validation: 0.22755241883959712]
	TIME [epoch: 6.24 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24126008648717157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24126008648717157 | validation: 0.33425736432679276]
	TIME [epoch: 6.23 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.307964560440783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.307964560440783 | validation: 0.3616934632885776]
	TIME [epoch: 6.24 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40535757470945016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40535757470945016 | validation: 0.21466522415845013]
	TIME [epoch: 6.23 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20563560825858035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20563560825858035 | validation: 0.1756563702335124]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2228055740005945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2228055740005945 | validation: 0.5266085091790896]
	TIME [epoch: 6.26 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4081751708078567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4081751708078567 | validation: 0.6528579367295988]
	TIME [epoch: 6.26 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196973653123106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8196973653123106 | validation: 0.20256650954938174]
	TIME [epoch: 6.26 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2137350405565724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2137350405565724 | validation: 0.21611513638411398]
	TIME [epoch: 6.26 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2054836746464099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2054836746464099 | validation: 0.24269540314588794]
	TIME [epoch: 6.26 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28488071856655445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28488071856655445 | validation: 0.48594271782104353]
	TIME [epoch: 6.23 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42577983638083666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42577983638083666 | validation: 0.22045638074144636]
	TIME [epoch: 6.23 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2599586365759857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2599586365759857 | validation: 0.20535913741331618]
	TIME [epoch: 6.24 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2288753565379916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2288753565379916 | validation: 0.31725794475645774]
	TIME [epoch: 6.24 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24190752495636986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24190752495636986 | validation: 0.21504620428024765]
	TIME [epoch: 6.23 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2456661081482305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2456661081482305 | validation: 0.3818541717584232]
	TIME [epoch: 6.23 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28108787160464677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28108787160464677 | validation: 0.28404446164847047]
	TIME [epoch: 6.23 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.320727565494483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.320727565494483 | validation: 0.2512941818345681]
	TIME [epoch: 6.23 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19758595559153525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19758595559153525 | validation: 0.2084246946439468]
	TIME [epoch: 6.23 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21748960269589454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21748960269589454 | validation: 0.5014873426643]
	TIME [epoch: 6.24 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4510817603436945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4510817603436945 | validation: 0.26054998470226975]
	TIME [epoch: 6.22 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3296308939209878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3296308939209878 | validation: 0.4379229862852625]
	TIME [epoch: 6.24 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31975841302369185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31975841302369185 | validation: 0.17464019977407014]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_392.pth
	Model improved!!!
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21209218247592676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21209218247592676 | validation: 0.18321203785568466]
	TIME [epoch: 6.23 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19671157379306692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19671157379306692 | validation: 0.25624193120921956]
	TIME [epoch: 6.22 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22764303425858645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22764303425858645 | validation: 0.23689617097798676]
	TIME [epoch: 6.23 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2914671121733657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2914671121733657 | validation: 0.3592216235336319]
	TIME [epoch: 6.22 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3037268025014146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3037268025014146 | validation: 0.15908774169452522]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1958972502077544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1958972502077544 | validation: 0.30306140733508075]
	TIME [epoch: 6.22 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22254991603214375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22254991603214375 | validation: 0.33698814372280494]
	TIME [epoch: 6.22 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39118369214635246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39118369214635246 | validation: 0.2697035575792188]
	TIME [epoch: 6.22 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2225187270361566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2225187270361566 | validation: 0.16890922265832486]
	TIME [epoch: 6.23 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14904161484289113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14904161484289113 | validation: 0.25649912062178126]
	TIME [epoch: 6.22 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2042766437620995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2042766437620995 | validation: 0.4177317044372475]
	TIME [epoch: 6.22 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4623340534504683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4623340534504683 | validation: 0.23682931617015163]
	TIME [epoch: 6.22 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30888140208127957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30888140208127957 | validation: 0.48147672610832565]
	TIME [epoch: 6.22 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3459089920238709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3459089920238709 | validation: 0.22534885053417264]
	TIME [epoch: 6.22 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28629809270419265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28629809270419265 | validation: 0.18027098639348116]
	TIME [epoch: 6.22 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17992839669830205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17992839669830205 | validation: 0.15172449798438659]
	TIME [epoch: 6.22 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476139482769873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1476139482769873 | validation: 0.1984494047459595]
	TIME [epoch: 6.22 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21111392721144498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21111392721144498 | validation: 0.6206271004336504]
	TIME [epoch: 6.22 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4986132590351698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4986132590351698 | validation: 0.4604017643139853]
	TIME [epoch: 6.24 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5398061214551422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5398061214551422 | validation: 0.4968998831071453]
	TIME [epoch: 6.22 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32862527725050383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32862527725050383 | validation: 0.21282752485733702]
	TIME [epoch: 6.23 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20372907322853795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20372907322853795 | validation: 0.20623926885291655]
	TIME [epoch: 6.22 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532700389330738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532700389330738 | validation: 0.265289591314083]
	TIME [epoch: 6.25 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.225453941899463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.225453941899463 | validation: 0.14547111568917073]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16418293367687153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16418293367687153 | validation: 0.1808111336477377]
	TIME [epoch: 6.24 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15406062386234776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15406062386234776 | validation: 0.1950839235952662]
	TIME [epoch: 6.24 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18638802391802714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18638802391802714 | validation: 0.3115611015804951]
	TIME [epoch: 6.24 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24412384502480058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24412384502480058 | validation: 0.3140160395740203]
	TIME [epoch: 6.23 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32458973705492905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32458973705492905 | validation: 0.17776220534833923]
	TIME [epoch: 6.23 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2280074700694231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2280074700694231 | validation: 0.5115493940516445]
	TIME [epoch: 6.23 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37911119887115463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37911119887115463 | validation: 0.23986787833548662]
	TIME [epoch: 6.23 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24754937997399637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24754937997399637 | validation: 0.36060677458118184]
	TIME [epoch: 6.23 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681222903548591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2681222903548591 | validation: 0.27916098041168763]
	TIME [epoch: 6.24 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28630650973031907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28630650973031907 | validation: 0.2686639368029395]
	TIME [epoch: 6.24 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2571254172199175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2571254172199175 | validation: 0.1122044176404148]
	TIME [epoch: 6.24 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14819627797864418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14819627797864418 | validation: 0.10845326514284931]
	TIME [epoch: 6.23 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14149596394229547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14149596394229547 | validation: 0.2078884119282619]
	TIME [epoch: 6.26 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615536361378252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615536361378252 | validation: 0.4904728231107558]
	TIME [epoch: 6.26 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5767347518453556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5767347518453556 | validation: 0.6776383450567217]
	TIME [epoch: 6.27 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5068982869433092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5068982869433092 | validation: 0.1766723724507753]
	TIME [epoch: 6.26 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15879646872358205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15879646872358205 | validation: 0.34442810930007006]
	TIME [epoch: 6.26 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4291445749964213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4291445749964213 | validation: 0.31529845230021525]
	TIME [epoch: 6.26 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23839517326053794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23839517326053794 | validation: 0.16146991301632527]
	TIME [epoch: 6.26 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1651108605383071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1651108605383071 | validation: 0.1933910905445544]
	TIME [epoch: 6.26 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24045056883918184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24045056883918184 | validation: 0.27863501628554305]
	TIME [epoch: 6.25 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22513447034384335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22513447034384335 | validation: 0.23648472665822615]
	TIME [epoch: 6.26 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21916812415726689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21916812415726689 | validation: 0.1796460584671524]
	TIME [epoch: 6.25 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1686273499474484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1686273499474484 | validation: 0.1662784361110753]
	TIME [epoch: 6.27 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14461157503190292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14461157503190292 | validation: 0.1523634186084203]
	TIME [epoch: 6.25 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17262106875619462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17262106875619462 | validation: 0.3226443802794887]
	TIME [epoch: 6.26 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181672357733513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181672357733513 | validation: 0.33665993345705386]
	TIME [epoch: 6.25 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36334142722445123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36334142722445123 | validation: 0.362314550341786]
	TIME [epoch: 6.27 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2608692062795585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2608692062795585 | validation: 0.1375152094272908]
	TIME [epoch: 6.25 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17101371641745744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17101371641745744 | validation: 0.6838250587946131]
	TIME [epoch: 6.26 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5634260018019444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5634260018019444 | validation: 0.17193979983477972]
	TIME [epoch: 6.25 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21029086621298085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21029086621298085 | validation: 0.17038116957176427]
	TIME [epoch: 6.26 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22381911515325467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22381911515325467 | validation: 0.20338363706014634]
	TIME [epoch: 6.26 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2890128929344636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2890128929344636 | validation: 0.2870486972839665]
	TIME [epoch: 6.26 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26900109929187704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26900109929187704 | validation: 0.11322907640531957]
	TIME [epoch: 6.25 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14645051999339262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14645051999339262 | validation: 0.13342627713262176]
	TIME [epoch: 6.27 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15368668233243435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15368668233243435 | validation: 0.24850523868247235]
	TIME [epoch: 6.25 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17393831406112498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17393831406112498 | validation: 0.14907540030093633]
	TIME [epoch: 6.25 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19060531257007043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19060531257007043 | validation: 0.4851261576147399]
	TIME [epoch: 6.25 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3070653376387722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3070653376387722 | validation: 0.27719347917257703]
	TIME [epoch: 6.25 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533150590165371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2533150590165371 | validation: 0.3159976413227473]
	TIME [epoch: 6.25 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3308240097956079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3308240097956079 | validation: 0.36632292781070896]
	TIME [epoch: 6.25 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30937294692363654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30937294692363654 | validation: 0.16987004928558508]
	TIME [epoch: 6.26 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20340363973468656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20340363973468656 | validation: 0.165891843202209]
	TIME [epoch: 6.25 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13569233357347638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13569233357347638 | validation: 0.11586405335638648]
	TIME [epoch: 6.25 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336697310599949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336697310599949 | validation: 0.18214458285459473]
	TIME [epoch: 6.25 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14403859570873656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14403859570873656 | validation: 0.2065016535727378]
	TIME [epoch: 6.24 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2039801507123651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2039801507123651 | validation: 0.24674544087748568]
	TIME [epoch: 6.25 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18857245556395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18857245556395 | validation: 0.23824346432900795]
	TIME [epoch: 6.24 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22685540406761057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22685540406761057 | validation: 0.28363052971215186]
	TIME [epoch: 6.26 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3265228627821891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3265228627821891 | validation: 0.44958642579787683]
	TIME [epoch: 6.26 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32309116878108896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32309116878108896 | validation: 0.12988250475468885]
	TIME [epoch: 6.26 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1601989982838734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1601989982838734 | validation: 0.7281663797022753]
	TIME [epoch: 6.25 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.508966147509738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.508966147509738 | validation: 0.2144638980363321]
	TIME [epoch: 6.25 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15339777238030577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15339777238030577 | validation: 0.21461716185437935]
	TIME [epoch: 6.25 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854678443673656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2854678443673656 | validation: 0.24584034919578934]
	TIME [epoch: 6.25 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16226710666888733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16226710666888733 | validation: 0.11890042394128762]
	TIME [epoch: 6.26 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11044384554339465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11044384554339465 | validation: 0.13052601454129067]
	TIME [epoch: 6.25 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12854778786712065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12854778786712065 | validation: 0.4023065210262342]
	TIME [epoch: 6.26 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29913375760001365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29913375760001365 | validation: 0.3210059509560882]
	TIME [epoch: 6.26 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519885485609352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3519885485609352 | validation: 0.14910101636662287]
	TIME [epoch: 6.27 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1678467564432318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1678467564432318 | validation: 0.25103431833230544]
	TIME [epoch: 6.25 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20446753684578153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20446753684578153 | validation: 0.17738137601341833]
	TIME [epoch: 6.26 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18966248140723374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18966248140723374 | validation: 0.18990425337543748]
	TIME [epoch: 6.25 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12923985386539125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12923985386539125 | validation: 0.10560549428470459]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11300845482350584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11300845482350584 | validation: 0.29898425171160253]
	TIME [epoch: 6.26 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17628414574953438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17628414574953438 | validation: 0.12505507289358372]
	TIME [epoch: 6.27 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600461470402518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1600461470402518 | validation: 0.2603424568910924]
	TIME [epoch: 6.27 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14991702125565262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14991702125565262 | validation: 0.14205887275559126]
	TIME [epoch: 6.25 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15789589504539214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15789589504539214 | validation: 0.44668517217939363]
	TIME [epoch: 6.25 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36163302946988746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36163302946988746 | validation: 0.18236607608343436]
	TIME [epoch: 6.27 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16834554641431987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16834554641431987 | validation: 0.12324502810422014]
	TIME [epoch: 6.26 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15771098609566844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15771098609566844 | validation: 0.12031224323823529]
	TIME [epoch: 6.25 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12463021170829862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12463021170829862 | validation: 0.07882037706538883]
	TIME [epoch: 6.25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10119002885857206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10119002885857206 | validation: 0.1604192887967828]
	TIME [epoch: 6.25 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10012277160310387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10012277160310387 | validation: 0.11674990063147184]
	TIME [epoch: 6.25 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14759003501546572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14759003501546572 | validation: 0.6807392585877717]
	TIME [epoch: 6.26 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4839112084271332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4839112084271332 | validation: 0.10111076286640718]
	TIME [epoch: 6.26 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0985667031828968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0985667031828968 | validation: 0.11530955175652649]
	TIME [epoch: 6.26 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17164764886978154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17164764886978154 | validation: 0.3349693066562883]
	TIME [epoch: 6.25 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26984400773787615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26984400773787615 | validation: 0.5115917521136482]
	TIME [epoch: 6.25 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4842970804484741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4842970804484741 | validation: 0.1651433369101754]
	TIME [epoch: 6.25 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14585773016109663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14585773016109663 | validation: 0.42309383966350356]
	TIME [epoch: 6.26 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45823858486850216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45823858486850216 | validation: 0.2610438449009098]
	TIME [epoch: 6.26 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.226152452125068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.226152452125068 | validation: 0.2558238900645934]
	TIME [epoch: 185 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3599340370483507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3599340370483507 | validation: 0.21032491939737485]
	TIME [epoch: 13.3 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1847047454965385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1847047454965385 | validation: 0.22188354807156518]
	TIME [epoch: 13.2 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1623139116469106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1623139116469106 | validation: 0.18926768743974023]
	TIME [epoch: 13.2 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23237591367554294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23237591367554294 | validation: 1.0734345436216128]
	TIME [epoch: 13.2 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537480910845505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8537480910845505 | validation: 0.894063904625604]
	TIME [epoch: 13.2 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428824429020747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7428824429020747 | validation: 0.5702694881119852]
	TIME [epoch: 13.3 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4344726500551718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4344726500551718 | validation: 0.3149592974562764]
	TIME [epoch: 13.2 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2781382056313296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2781382056313296 | validation: 0.1622686758235658]
	TIME [epoch: 13.2 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15849515159213368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15849515159213368 | validation: 0.11569885879669083]
	TIME [epoch: 13.2 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15760857200158992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15760857200158992 | validation: 0.14958829945638508]
	TIME [epoch: 13.3 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15401641513320008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15401641513320008 | validation: 0.13646612476568928]
	TIME [epoch: 13.2 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284509604818997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284509604818997 | validation: 0.140900582377861]
	TIME [epoch: 13.2 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11767060112586623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11767060112586623 | validation: 0.14169866287704633]
	TIME [epoch: 13.2 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10694819564445002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10694819564445002 | validation: 0.11269926726253876]
	TIME [epoch: 13.3 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10786620886828222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10786620886828222 | validation: 0.18920485482058508]
	TIME [epoch: 13.2 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16071072215175675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16071072215175675 | validation: 0.26600080065941983]
	TIME [epoch: 13.2 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21098904222655715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21098904222655715 | validation: 0.18056435584456215]
	TIME [epoch: 13.2 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2158909861028899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2158909861028899 | validation: 0.260620647829611]
	TIME [epoch: 13.2 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16587283742029296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16587283742029296 | validation: 0.12862879530294455]
	TIME [epoch: 13.2 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15999668258713345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15999668258713345 | validation: 0.3930557247521229]
	TIME [epoch: 13.2 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27846449706080617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27846449706080617 | validation: 0.15837155345586096]
	TIME [epoch: 13.2 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14285466531391058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14285466531391058 | validation: 0.09633962636172272]
	TIME [epoch: 13.2 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11924212787434854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11924212787434854 | validation: 0.18593312397939382]
	TIME [epoch: 13.2 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15632739396844927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15632739396844927 | validation: 0.10613568693831904]
	TIME [epoch: 13.2 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305589060206315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305589060206315 | validation: 0.3135618177540244]
	TIME [epoch: 13.2 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22745428088249398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22745428088249398 | validation: 0.14886038967979426]
	TIME [epoch: 13.2 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17351077644412627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17351077644412627 | validation: 0.09938242312654938]
	TIME [epoch: 13.2 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1134145293899742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1134145293899742 | validation: 0.1395296772902015]
	TIME [epoch: 13.2 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1046660813040616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1046660813040616 | validation: 0.11315056302253175]
	TIME [epoch: 13.2 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14494018061961722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14494018061961722 | validation: 0.383328195905902]
	TIME [epoch: 13.3 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21701351576008762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21701351576008762 | validation: 0.19308583152234582]
	TIME [epoch: 13.2 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2278281988078395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2278281988078395 | validation: 0.1274196724983513]
	TIME [epoch: 13.2 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10408915892140161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10408915892140161 | validation: 0.09157881224451508]
	TIME [epoch: 13.2 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08304573136084414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08304573136084414 | validation: 0.08819319467483157]
	TIME [epoch: 13.2 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953863371490534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07953863371490534 | validation: 0.23145321415475428]
	TIME [epoch: 13.2 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14208487634467765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14208487634467765 | validation: 0.6197137709345987]
	TIME [epoch: 13.2 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793538418159539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6793538418159539 | validation: 0.14487324718499248]
	TIME [epoch: 13.2 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11566019320062866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11566019320062866 | validation: 0.1876144238719064]
	TIME [epoch: 13.2 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14443665376846712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14443665376846712 | validation: 0.12884956313627696]
	TIME [epoch: 13.2 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1398491742883382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1398491742883382 | validation: 0.16383011693967775]
	TIME [epoch: 13.2 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1535970751021878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1535970751021878 | validation: 0.21754408133952344]
	TIME [epoch: 13.3 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16555651491150428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16555651491150428 | validation: 0.14545833355827012]
	TIME [epoch: 13.3 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14772773817738255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14772773817738255 | validation: 0.20129442784109033]
	TIME [epoch: 13.2 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.117993422354801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.117993422354801 | validation: 0.09645586551909785]
	TIME [epoch: 13.2 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12022273327416809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12022273327416809 | validation: 0.21126470944291525]
	TIME [epoch: 13.2 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12119845221750304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12119845221750304 | validation: 0.11852140670545411]
	TIME [epoch: 13.2 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14046527869250436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14046527869250436 | validation: 0.24619095965798776]
	TIME [epoch: 13.2 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15514847007172008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15514847007172008 | validation: 0.19208934668194352]
	TIME [epoch: 13.2 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1992143502662723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1992143502662723 | validation: 0.12254334189806465]
	TIME [epoch: 13.2 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11762028568178046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11762028568178046 | validation: 0.14555479476554553]
	TIME [epoch: 13.3 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11727925769740655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11727925769740655 | validation: 0.1655591965796691]
	TIME [epoch: 13.2 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16613462511111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16613462511111 | validation: 0.3134922886345355]
	TIME [epoch: 13.2 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2152953972477062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2152953972477062 | validation: 0.22500353819486657]
	TIME [epoch: 13.2 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2618714571118548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2618714571118548 | validation: 0.32241075836789473]
	TIME [epoch: 13.3 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768717594527625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1768717594527625 | validation: 0.10338704272495602]
	TIME [epoch: 13.2 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11797228527820446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11797228527820446 | validation: 0.1814904573174792]
	TIME [epoch: 13.2 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10046142129526871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10046142129526871 | validation: 0.0787825510446788]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0860427293718725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0860427293718725 | validation: 0.15157810177325795]
	TIME [epoch: 13.2 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637618924206361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08637618924206361 | validation: 0.07619686763777994]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_560.pth
	Model improved!!!
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10134485935060228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10134485935060228 | validation: 0.18856691477699183]
	TIME [epoch: 13.2 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11608938725128236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11608938725128236 | validation: 0.17347624769346773]
	TIME [epoch: 13.2 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16715799348505564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16715799348505564 | validation: 0.2999775509553644]
	TIME [epoch: 13.3 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29704413094031173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29704413094031173 | validation: 0.2998859414712793]
	TIME [epoch: 13.2 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2281736605509343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2281736605509343 | validation: 0.14159210559111854]
	TIME [epoch: 13.2 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16293827785174816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16293827785174816 | validation: 0.2626288916055877]
	TIME [epoch: 13.2 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21033397022373324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21033397022373324 | validation: 0.13245805721296727]
	TIME [epoch: 13.3 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15441312339523636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15441312339523636 | validation: 0.13057399267727834]
	TIME [epoch: 13.2 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1244332137439882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1244332137439882 | validation: 0.1408386677290119]
	TIME [epoch: 13.2 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10181797998368457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10181797998368457 | validation: 0.1801003993789514]
	TIME [epoch: 13.2 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18367686731141972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18367686731141972 | validation: 0.3095727646654819]
	TIME [epoch: 13.3 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22210160545781762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22210160545781762 | validation: 0.1778776421690593]
	TIME [epoch: 13.2 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19228643499975617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19228643499975617 | validation: 0.4741950743974808]
	TIME [epoch: 13.3 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28252707749015205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28252707749015205 | validation: 0.1039197748430031]
	TIME [epoch: 13.2 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09944906228657775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09944906228657775 | validation: 0.0990612635831259]
	TIME [epoch: 13.3 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11880187224141775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11880187224141775 | validation: 0.14284991655402315]
	TIME [epoch: 13.2 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11555853234711534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11555853234711534 | validation: 0.10113219559227789]
	TIME [epoch: 13.3 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12245704327267638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12245704327267638 | validation: 0.1730683203326489]
	TIME [epoch: 13.2 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09427142802621012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09427142802621012 | validation: 0.09924725750214589]
	TIME [epoch: 13.3 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12299613484234577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12299613484234577 | validation: 0.4804430623754874]
	TIME [epoch: 13.2 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24647142533742888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24647142533742888 | validation: 0.15729355141041737]
	TIME [epoch: 13.3 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13748504398041136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13748504398041136 | validation: 0.6275715531592079]
	TIME [epoch: 13.2 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099498897926381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099498897926381 | validation: 0.18041913568299]
	TIME [epoch: 13.3 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23329433701724844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23329433701724844 | validation: 0.22358316706873238]
	TIME [epoch: 13.2 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21032596931198747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21032596931198747 | validation: 0.12717265045450363]
	TIME [epoch: 13.2 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13130391991618093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13130391991618093 | validation: 0.20002646523791037]
	TIME [epoch: 13.2 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15968668020829918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15968668020829918 | validation: 0.134908955894908]
	TIME [epoch: 13.2 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11501579009220589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11501579009220589 | validation: 0.07619093873445436]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_588.pth
	Model improved!!!
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601480266469725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09601480266469725 | validation: 1.1064169948100429]
	TIME [epoch: 13.2 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9398421626677874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9398421626677874 | validation: 0.7011863400831462]
	TIME [epoch: 13.2 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.553447786898672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.553447786898672 | validation: 0.31742973027188937]
	TIME [epoch: 13.2 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3716657665688496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3716657665688496 | validation: 0.17882839563285904]
	TIME [epoch: 13.2 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24660467962224578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24660467962224578 | validation: 0.18949384097637859]
	TIME [epoch: 13.2 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533875728327147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2533875728327147 | validation: 0.18911349270018488]
	TIME [epoch: 13.2 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1937234139805988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1937234139805988 | validation: 0.11771730753984332]
	TIME [epoch: 13.2 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1483839595461229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1483839595461229 | validation: 0.08383247563881914]
	TIME [epoch: 13.2 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10284045758427496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10284045758427496 | validation: 0.1451672107197772]
	TIME [epoch: 13.2 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10138395236909457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10138395236909457 | validation: 0.07186139752634875]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849294010300562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08849294010300562 | validation: 0.1033175079984272]
	TIME [epoch: 13.3 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07872436085661112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07872436085661112 | validation: 0.08731191870246259]
	TIME [epoch: 13.3 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783644622453927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0783644622453927 | validation: 0.13237339802847276]
	TIME [epoch: 13.3 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11619054572709406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11619054572709406 | validation: 0.3260297064495583]
	TIME [epoch: 13.3 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3434344928655922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3434344928655922 | validation: 0.21210178600447693]
	TIME [epoch: 13.3 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19469834542131637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19469834542131637 | validation: 0.5116132068609561]
	TIME [epoch: 13.3 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5904589040873474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5904589040873474 | validation: 0.40804445870086026]
	TIME [epoch: 13.3 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48053745450701135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48053745450701135 | validation: 0.22952204319974925]
	TIME [epoch: 13.3 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27297555541777085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27297555541777085 | validation: 0.25632669813101855]
	TIME [epoch: 13.3 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2874145775808479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2874145775808479 | validation: 0.1670195744924573]
	TIME [epoch: 13.3 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1745226264133258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1745226264133258 | validation: 0.2555655526412784]
	TIME [epoch: 13.3 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16545266636049832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16545266636049832 | validation: 0.14175019512479495]
	TIME [epoch: 13.3 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15322180360420598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15322180360420598 | validation: 0.13804636432643994]
	TIME [epoch: 13.3 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09403489726736686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09403489726736686 | validation: 0.08222122831786279]
	TIME [epoch: 13.3 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511368690387141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09511368690387141 | validation: 0.10092471421684848]
	TIME [epoch: 13.3 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10317118377021438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10317118377021438 | validation: 0.16146489903287686]
	TIME [epoch: 13.3 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11880751066221583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11880751066221583 | validation: 0.10878583636522517]
	TIME [epoch: 13.3 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11229627140856656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11229627140856656 | validation: 0.07868323767998799]
	TIME [epoch: 13.3 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07915950290138055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07915950290138055 | validation: 0.054015605800429856]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06859705013242826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06859705013242826 | validation: 0.08076350952945266]
	TIME [epoch: 13.3 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667824374015861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06667824374015861 | validation: 0.10465551407905918]
	TIME [epoch: 13.3 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11928167967397418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11928167967397418 | validation: 0.39923462795718334]
	TIME [epoch: 13.3 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2256822630872233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2256822630872233 | validation: 0.12759527049171512]
	TIME [epoch: 13.3 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382224175782811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1382224175782811 | validation: 0.1594929921196545]
	TIME [epoch: 13.3 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11985345715003844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11985345715003844 | validation: 0.3280178973011147]
	TIME [epoch: 13.3 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21361151818246066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21361151818246066 | validation: 0.16133690778065923]
	TIME [epoch: 13.3 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19027965192729468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19027965192729468 | validation: 0.19709443216977499]
	TIME [epoch: 13.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11549415498517641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11549415498517641 | validation: 0.06330173506862712]
	TIME [epoch: 13.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09102914990643718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09102914990643718 | validation: 0.17294085857306907]
	TIME [epoch: 13.3 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11393507530082804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11393507530082804 | validation: 0.0764126038345162]
	TIME [epoch: 13.3 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858066780312413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0858066780312413 | validation: 0.07805323316488746]
	TIME [epoch: 13.3 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255445646004703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07255445646004703 | validation: 0.11284787112071126]
	TIME [epoch: 13.3 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10037759104148063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10037759104148063 | validation: 0.7705668885945931]
	TIME [epoch: 13.3 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258629054871409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7258629054871409 | validation: 0.4911468607394036]
	TIME [epoch: 13.3 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40322197616676714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40322197616676714 | validation: 0.32593137139715983]
	TIME [epoch: 13.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2606694649533738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2606694649533738 | validation: 0.16090468992947204]
	TIME [epoch: 13.3 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16817965686572006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16817965686572006 | validation: 0.10986099994433718]
	TIME [epoch: 13.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15314838828141786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15314838828141786 | validation: 0.10466513643328668]
	TIME [epoch: 13.3 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11245886710043665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11245886710043665 | validation: 0.06862857455703339]
	TIME [epoch: 13.3 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067592460042879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.067592460042879 | validation: 0.12756004059532053]
	TIME [epoch: 13.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09560321829115072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09560321829115072 | validation: 0.1765953204646192]
	TIME [epoch: 13.3 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1849002512931095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1849002512931095 | validation: 0.2450654358752714]
	TIME [epoch: 13.3 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1446923792906438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1446923792906438 | validation: 0.14820896228746058]
	TIME [epoch: 13.3 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18646341860573973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18646341860573973 | validation: 0.25752048109546893]
	TIME [epoch: 13.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15723969014314423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15723969014314423 | validation: 0.09407412647850505]
	TIME [epoch: 13.3 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09557628405457574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09557628405457574 | validation: 0.07503908677998167]
	TIME [epoch: 13.3 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07229217531288458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07229217531288458 | validation: 0.07254261095688522]
	TIME [epoch: 13.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06888851347023983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06888851347023983 | validation: 0.08811557405900167]
	TIME [epoch: 13.3 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09626368499418415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09626368499418415 | validation: 0.17654976173888082]
	TIME [epoch: 13.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12377191539926363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12377191539926363 | validation: 0.06696701657756775]
	TIME [epoch: 13.3 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08221533403661888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08221533403661888 | validation: 0.1366107287423521]
	TIME [epoch: 13.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08125558141581429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08125558141581429 | validation: 0.16216419630189916]
	TIME [epoch: 13.3 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17226132057350974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17226132057350974 | validation: 0.29177226892358454]
	TIME [epoch: 13.3 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19862009313305615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19862009313305615 | validation: 0.1660281441491403]
	TIME [epoch: 13.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15758352297445627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15758352297445627 | validation: 0.05277707270400898]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06747720880113045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06747720880113045 | validation: 0.05247346626411992]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_654.pth
	Model improved!!!
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05792206998821886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05792206998821886 | validation: 0.04578008875853038]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_655.pth
	Model improved!!!
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05217185222248981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05217185222248981 | validation: 0.11110663425014647]
	TIME [epoch: 13.3 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906384136614295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06906384136614295 | validation: 0.139457001519741]
	TIME [epoch: 13.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16148455003743295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16148455003743295 | validation: 0.5678065748628278]
	TIME [epoch: 13.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38380639664966554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38380639664966554 | validation: 0.1575677765317748]
	TIME [epoch: 13.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13155221766446445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13155221766446445 | validation: 0.15360785464394755]
	TIME [epoch: 13.3 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1642875951277996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1642875951277996 | validation: 0.14244141113667938]
	TIME [epoch: 13.2 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12978861336465283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12978861336465283 | validation: 0.061896443161042806]
	TIME [epoch: 13.3 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511642674570062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06511642674570062 | validation: 0.06183049359711878]
	TIME [epoch: 13.2 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058988336179989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058988336179989 | validation: 0.10990722525328889]
	TIME [epoch: 13.2 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766159190039617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06766159190039617 | validation: 0.06827231340933756]
	TIME [epoch: 13.2 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07212581635115022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07212581635115022 | validation: 0.09485068855000954]
	TIME [epoch: 13.2 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06435170598845948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06435170598845948 | validation: 0.08944431580508368]
	TIME [epoch: 13.2 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09956259035769122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09956259035769122 | validation: 0.23376836813110416]
	TIME [epoch: 13.3 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14987922979788645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14987922979788645 | validation: 0.2859254130267619]
	TIME [epoch: 13.2 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24746156221242546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24746156221242546 | validation: 0.08409761220674755]
	TIME [epoch: 13.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710107319754037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710107319754037 | validation: 0.06954762259944396]
	TIME [epoch: 13.3 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07106612518878472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07106612518878472 | validation: 0.06848760788385061]
	TIME [epoch: 13.2 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056759063064156595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056759063064156595 | validation: 0.05665358153781023]
	TIME [epoch: 13.2 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047336982553698136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047336982553698136 | validation: 0.049291808349892496]
	TIME [epoch: 13.2 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055992789195653064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055992789195653064 | validation: 0.6751246221550071]
	TIME [epoch: 13.2 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9180902222526015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9180902222526015 | validation: 0.273303246983385]
	TIME [epoch: 13.2 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2811742214187262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2811742214187262 | validation: 0.10386136615173652]
	TIME [epoch: 13.2 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12133409954698973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12133409954698973 | validation: 0.12011112221545504]
	TIME [epoch: 13.2 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299852551632811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1299852551632811 | validation: 0.07991038819911922]
	TIME [epoch: 13.2 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07179605823705469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07179605823705469 | validation: 0.06856697617724866]
	TIME [epoch: 13.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06761005126446104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06761005126446104 | validation: 0.11282862334678678]
	TIME [epoch: 13.2 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09625645367760158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09625645367760158 | validation: 0.05762718715718255]
	TIME [epoch: 13.2 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0664242328865481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0664242328865481 | validation: 0.1099158233062033]
	TIME [epoch: 13.2 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07660604550439577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07660604550439577 | validation: 0.07678817335487052]
	TIME [epoch: 13.2 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08435665087012581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08435665087012581 | validation: 0.13375011216304764]
	TIME [epoch: 13.2 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369442578721731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08369442578721731 | validation: 0.06709101091166231]
	TIME [epoch: 13.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07568016509657154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07568016509657154 | validation: 0.14257921073670343]
	TIME [epoch: 13.3 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09372688241695118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09372688241695118 | validation: 0.07925411000475344]
	TIME [epoch: 13.2 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797296427865071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09797296427865071 | validation: 0.19119062420086363]
	TIME [epoch: 13.2 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1200074878770786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1200074878770786 | validation: 0.0755744562742573]
	TIME [epoch: 13.2 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08431179271030546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08431179271030546 | validation: 0.09856919192675226]
	TIME [epoch: 13.2 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08363316640138459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08363316640138459 | validation: 0.17825524031951923]
	TIME [epoch: 13.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17238392551511403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17238392551511403 | validation: 0.1311845723542102]
	TIME [epoch: 13.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11086073214460189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11086073214460189 | validation: 0.07614680427709321]
	TIME [epoch: 13.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508228857500034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06508228857500034 | validation: 0.8120843021542191]
	TIME [epoch: 13.2 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1108815828686915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1108815828686915 | validation: 0.4898501514748344]
	TIME [epoch: 13.3 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6009072087135843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6009072087135843 | validation: 0.4026646750180506]
	TIME [epoch: 13.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42995504297253057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42995504297253057 | validation: 0.20828799555260186]
	TIME [epoch: 13.2 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24188196463623243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24188196463623243 | validation: 0.2440142209513427]
	TIME [epoch: 13.2 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29254424599970136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29254424599970136 | validation: 0.1665797489884414]
	TIME [epoch: 13.2 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19166514765379356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19166514765379356 | validation: 0.18072158697222954]
	TIME [epoch: 13.2 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14525949067928787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14525949067928787 | validation: 0.4280046353521222]
	TIME [epoch: 13.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24295887816926054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24295887816926054 | validation: 0.15865948413415545]
	TIME [epoch: 13.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1562777082236276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1562777082236276 | validation: 0.09522648882233709]
	TIME [epoch: 13.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08893299798318886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08893299798318886 | validation: 0.10421910712202181]
	TIME [epoch: 13.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0802835763320563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0802835763320563 | validation: 0.0888893475800771]
	TIME [epoch: 13.2 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07288724823939227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07288724823939227 | validation: 0.0772572968413911]
	TIME [epoch: 13.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06394244662886758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06394244662886758 | validation: 0.049136582586724015]
	TIME [epoch: 13.2 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059206892220571065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059206892220571065 | validation: 0.09263172280728987]
	TIME [epoch: 13.2 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06408055067522052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06408055067522052 | validation: 0.06550530326252016]
	TIME [epoch: 13.2 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07785600160031296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07785600160031296 | validation: 0.1659616999623787]
	TIME [epoch: 13.2 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10795332843340102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10795332843340102 | validation: 0.07737639382788236]
	TIME [epoch: 13.2 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10552806333149053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10552806333149053 | validation: 0.17575404928757898]
	TIME [epoch: 13.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10028872096175434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10028872096175434 | validation: 0.056113777254410496]
	TIME [epoch: 13.2 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06693490974752792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06693490974752792 | validation: 0.06123382349783302]
	TIME [epoch: 13.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05420274394025244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05420274394025244 | validation: 0.04248491856634423]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05132428438789127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05132428438789127 | validation: 0.11005627448631446]
	TIME [epoch: 13.2 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06896007567092297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06896007567092297 | validation: 0.07762345854075553]
	TIME [epoch: 13.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09103922834314085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09103922834314085 | validation: 0.39771662356489657]
	TIME [epoch: 13.2 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2127350911425247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2127350911425247 | validation: 0.09125901269188522]
	TIME [epoch: 13.2 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09538219880830809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09538219880830809 | validation: 0.30184433650635684]
	TIME [epoch: 13.2 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22566194289258296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22566194289258296 | validation: 0.08641591337140664]
	TIME [epoch: 13.2 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0731957425492748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0731957425492748 | validation: 0.043198721130295215]
	TIME [epoch: 13.2 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05881180364571585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05881180364571585 | validation: 0.05670567958368682]
	TIME [epoch: 13.2 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05682045010476822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05682045010476822 | validation: 0.0634403801397701]
	TIME [epoch: 13.2 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06286544165559235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06286544165559235 | validation: 0.17620374025599275]
	TIME [epoch: 13.2 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10759275701265127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10759275701265127 | validation: 0.07879806143569819]
	TIME [epoch: 13.2 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10615381988662577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10615381988662577 | validation: 0.14985673213924836]
	TIME [epoch: 13.2 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09328184990083407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09328184990083407 | validation: 0.0532253618628612]
	TIME [epoch: 13.2 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06078130471878932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06078130471878932 | validation: 0.07158625266522123]
	TIME [epoch: 13.3 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048439088120135046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048439088120135046 | validation: 0.06150987829751791]
	TIME [epoch: 13.2 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06211209059954699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06211209059954699 | validation: 0.09449045193759953]
	TIME [epoch: 13.2 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454975786671413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08454975786671413 | validation: 0.06708189729641537]
	TIME [epoch: 13.2 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06994187875568134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06994187875568134 | validation: 0.07006720267137882]
	TIME [epoch: 13.2 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06175338957784478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06175338957784478 | validation: 0.0671474619179757]
	TIME [epoch: 13.2 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065315320933709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07065315320933709 | validation: 0.21904144657216262]
	TIME [epoch: 13.2 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11697782303350117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11697782303350117 | validation: 0.49959377777756764]
	TIME [epoch: 13.2 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117855067140508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117855067140508 | validation: 0.1503696580070809]
	TIME [epoch: 13.2 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10413253593671275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10413253593671275 | validation: 0.1842080236408959]
	TIME [epoch: 13.2 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15780812557745277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15780812557745277 | validation: 0.08374881834210446]
	TIME [epoch: 13.2 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09382518371092465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09382518371092465 | validation: 0.2019136093184971]
	TIME [epoch: 13.2 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18347483188675504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18347483188675504 | validation: 0.25064829993377763]
	TIME [epoch: 13.2 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15315166718033751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15315166718033751 | validation: 0.08995675934474948]
	TIME [epoch: 13.2 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10273496500031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10273496500031 | validation: 0.2654631493344231]
	TIME [epoch: 13.3 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21364361522114472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21364361522114472 | validation: 0.21056630300470724]
	TIME [epoch: 13.2 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11889477891315675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11889477891315675 | validation: 0.1255956674474151]
	TIME [epoch: 13.3 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13956105614250683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13956105614250683 | validation: 0.12114837031273017]
	TIME [epoch: 13.2 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09375625425009602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09375625425009602 | validation: 0.07963717246483533]
	TIME [epoch: 13.3 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07706430233273873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07706430233273873 | validation: 0.055668590302569904]
	TIME [epoch: 13.2 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06873828166584935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06873828166584935 | validation: 0.051742711699900835]
	TIME [epoch: 13.2 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041646872269404785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041646872269404785 | validation: 0.04093804524813238]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_751.pth
	Model improved!!!
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040797038586809206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040797038586809206 | validation: 0.06462468178768001]
	TIME [epoch: 13.3 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615234341867633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04615234341867633 | validation: 0.04745660307700418]
	TIME [epoch: 13.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05827607748864768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05827607748864768 | validation: 0.11429390414183255]
	TIME [epoch: 13.3 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970313114490277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970313114490277 | validation: 0.09690474807630803]
	TIME [epoch: 13.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10516037718135898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10516037718135898 | validation: 0.24182034580366332]
	TIME [epoch: 13.3 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11837254978198064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11837254978198064 | validation: 0.054244979146402084]
	TIME [epoch: 13.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05349185603278871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05349185603278871 | validation: 0.040736618653592085]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_758.pth
	Model improved!!!
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04451844598775111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04451844598775111 | validation: 0.04504696612044467]
	TIME [epoch: 13.3 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047190046365361525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047190046365361525 | validation: 0.09597024620323033]
	TIME [epoch: 13.3 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895759997463523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0895759997463523 | validation: 0.04014392565703703]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_761.pth
	Model improved!!!
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042000548099044355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042000548099044355 | validation: 0.04047729010535575]
	TIME [epoch: 13.2 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047212433644131004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047212433644131004 | validation: 0.0534779679762912]
	TIME [epoch: 13.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051470166925803106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051470166925803106 | validation: 0.06375324625408044]
	TIME [epoch: 13.2 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225099205012965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07225099205012965 | validation: 0.09678278520499875]
	TIME [epoch: 13.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07745357638034653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07745357638034653 | validation: 0.06754418339161909]
	TIME [epoch: 13.3 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846616504051554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06846616504051554 | validation: 0.09050604579480953]
	TIME [epoch: 13.2 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0842040899863526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0842040899863526 | validation: 0.2687539586831264]
	TIME [epoch: 13.2 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17553327648095277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17553327648095277 | validation: 0.19443302958123593]
	TIME [epoch: 13.2 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19783904406742608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19783904406742608 | validation: 0.46547503115314814]
	TIME [epoch: 13.3 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23197267793550339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23197267793550339 | validation: 0.10580661056841965]
	TIME [epoch: 13.2 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06809089976435628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06809089976435628 | validation: 0.08873092449153014]
	TIME [epoch: 13.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1193297289707349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1193297289707349 | validation: 0.21784248896515318]
	TIME [epoch: 13.3 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15524843165122146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15524843165122146 | validation: 0.12754363472448654]
	TIME [epoch: 13.3 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10243190815719011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10243190815719011 | validation: 0.06254854282989498]
	TIME [epoch: 13.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490728124596521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04490728124596521 | validation: 0.04438480405769845]
	TIME [epoch: 13.3 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04405792641558997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04405792641558997 | validation: 0.8399261575672133]
	TIME [epoch: 13.3 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.03824106364154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.03824106364154 | validation: 0.7469510935302879]
	TIME [epoch: 13.2 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8772509823641943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8772509823641943 | validation: 0.2749040467359875]
	TIME [epoch: 13.2 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3091523999239817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3091523999239817 | validation: 0.2962159837722151]
	TIME [epoch: 13.2 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3425112043420424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3425112043420424 | validation: 0.23690206633054364]
	TIME [epoch: 13.2 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24170748534902864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24170748534902864 | validation: 0.2001326645498845]
	TIME [epoch: 13.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22276820058459548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22276820058459548 | validation: 0.18729874246964706]
	TIME [epoch: 13.2 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1878775366938494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1878775366938494 | validation: 0.14636038237214083]
	TIME [epoch: 13.3 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14729140728370277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14729140728370277 | validation: 0.13083305514035057]
	TIME [epoch: 13.3 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12562961381196364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12562961381196364 | validation: 0.08796105468858009]
	TIME [epoch: 13.3 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10990892832791158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10990892832791158 | validation: 0.10973022024371962]
	TIME [epoch: 13.3 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11158875176366134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11158875176366134 | validation: 0.10206834931045022]
	TIME [epoch: 13.3 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11494239666912089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11494239666912089 | validation: 0.09814823011228443]
	TIME [epoch: 13.3 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09893966352965722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09893966352965722 | validation: 0.08601826311178587]
	TIME [epoch: 13.3 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08739024342156956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08739024342156956 | validation: 0.0860123001739113]
	TIME [epoch: 13.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09355659025110902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09355659025110902 | validation: 0.13348212212529176]
	TIME [epoch: 13.2 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07417426962695844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07417426962695844 | validation: 0.05492454292565956]
	TIME [epoch: 13.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181463432664627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07181463432664627 | validation: 0.050616149099496216]
	TIME [epoch: 13.3 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048281436482867195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048281436482867195 | validation: 0.049716697757968414]
	TIME [epoch: 13.3 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05478826948325398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05478826948325398 | validation: 0.10716742363688506]
	TIME [epoch: 13.2 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320095173309066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320095173309066 | validation: 0.1404350155707185]
	TIME [epoch: 13.2 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14283566463564845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14283566463564845 | validation: 0.40577773565611547]
	TIME [epoch: 13.3 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.217946919389267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.217946919389267 | validation: 0.09869548337577547]
	TIME [epoch: 13.3 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0997126961728025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0997126961728025 | validation: 0.147261062327927]
	TIME [epoch: 13.3 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13562733612877506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13562733612877506 | validation: 0.0647032006113353]
	TIME [epoch: 13.3 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06557778601044026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06557778601044026 | validation: 0.39711410516212214]
	TIME [epoch: 13.3 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27580627399069363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27580627399069363 | validation: 0.16540771609627736]
	TIME [epoch: 13.3 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12610627466756488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12610627466756488 | validation: 0.07496168869898498]
	TIME [epoch: 13.3 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07159632573309509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07159632573309509 | validation: 0.08820733489207744]
	TIME [epoch: 13.3 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07321136063347863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07321136063347863 | validation: 0.052098689641532026]
	TIME [epoch: 13.3 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05345954812320192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05345954812320192 | validation: 0.09013582888834694]
	TIME [epoch: 13.3 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426708492618172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426708492618172 | validation: 0.06604303794414518]
	TIME [epoch: 13.3 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701401057079982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06701401057079982 | validation: 0.09973500890575347]
	TIME [epoch: 13.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06369684292274505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06369684292274505 | validation: 0.07394897374870037]
	TIME [epoch: 13.3 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493732537413955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08493732537413955 | validation: 0.1319102737930724]
	TIME [epoch: 13.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08158039152715153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08158039152715153 | validation: 0.15065986671323472]
	TIME [epoch: 13.3 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16433315040893676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16433315040893676 | validation: 0.2274944498106639]
	TIME [epoch: 13.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12128530620548478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12128530620548478 | validation: 0.05444569106080352]
	TIME [epoch: 13.3 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0488222255205644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0488222255205644 | validation: 0.03793336267485149]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_815.pth
	Model improved!!!
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04045938913252527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04045938913252527 | validation: 0.054974655942531675]
	TIME [epoch: 13.3 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04190556220221019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04190556220221019 | validation: 0.0737128012457121]
	TIME [epoch: 13.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557305932516763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557305932516763 | validation: 0.11045466714672314]
	TIME [epoch: 13.3 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11487147432186287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11487147432186287 | validation: 0.06495124943985162]
	TIME [epoch: 13.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653819103452727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06653819103452727 | validation: 0.044290478033706275]
	TIME [epoch: 13.3 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038985363606218515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038985363606218515 | validation: 0.04837982015354467]
	TIME [epoch: 13.3 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055570931176322835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055570931176322835 | validation: 0.22994307694856683]
	TIME [epoch: 13.3 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1242782374171604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1242782374171604 | validation: 0.10860542211121627]
	TIME [epoch: 13.3 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09526238024074679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09526238024074679 | validation: 0.08018501388648652]
	TIME [epoch: 13.3 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061106672552000765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061106672552000765 | validation: 0.060711536970954855]
	TIME [epoch: 13.3 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060365318741006944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060365318741006944 | validation: 0.09485000758724216]
	TIME [epoch: 13.3 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05416021140436189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05416021140436189 | validation: 0.12211539817736901]
	TIME [epoch: 13.3 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13579752161942424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13579752161942424 | validation: 0.31414873671951377]
	TIME [epoch: 13.3 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15826820390682397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15826820390682397 | validation: 0.07343372981470478]
	TIME [epoch: 13.3 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06332267874464102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06332267874464102 | validation: 0.12200545036601125]
	TIME [epoch: 13.3 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1137680973945044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1137680973945044 | validation: 0.1780903089511058]
	TIME [epoch: 13.3 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15587085024418396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15587085024418396 | validation: 0.8392528960557346]
	TIME [epoch: 13.3 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1680697552507762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1680697552507762 | validation: 0.5409372676346303]
	TIME [epoch: 13.3 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5601537860625893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5601537860625893 | validation: 0.3706296648426326]
	TIME [epoch: 13.3 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3143353100356876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3143353100356876 | validation: 0.1361605513469447]
	TIME [epoch: 13.3 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12930539228020715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12930539228020715 | validation: 0.13425248405782703]
	TIME [epoch: 13.3 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15358483345619253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15358483345619253 | validation: 0.10351538999071962]
	TIME [epoch: 13.3 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09457565810774249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09457565810774249 | validation: 0.06845089516282993]
	TIME [epoch: 13.3 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08333386402003344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08333386402003344 | validation: 0.07092029741428578]
	TIME [epoch: 13.3 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06541820578412587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06541820578412587 | validation: 0.05891077576839322]
	TIME [epoch: 13.3 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05735802999145004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05735802999145004 | validation: 0.06984622322471198]
	TIME [epoch: 13.3 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122377765353761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05122377765353761 | validation: 0.03880088961534622]
	TIME [epoch: 13.3 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051024030985219355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051024030985219355 | validation: 0.049448316514107686]
	TIME [epoch: 13.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044648008317167406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044648008317167406 | validation: 0.04396510119331365]
	TIME [epoch: 13.3 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05357493899240955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05357493899240955 | validation: 0.07205996625167455]
	TIME [epoch: 13.3 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06556998903416063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06556998903416063 | validation: 0.06042049405805532]
	TIME [epoch: 13.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057767398258934406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057767398258934406 | validation: 0.10355556612196032]
	TIME [epoch: 13.3 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08017366404247017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08017366404247017 | validation: 0.12365215826541712]
	TIME [epoch: 13.2 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08787883277976612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08787883277976612 | validation: 0.08195226779292174]
	TIME [epoch: 13.3 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10284036619178435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10284036619178435 | validation: 0.11459594602932868]
	TIME [epoch: 13.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323117782639962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08323117782639962 | validation: 0.07106269225089776]
	TIME [epoch: 13.2 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07085459758130362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07085459758130362 | validation: 0.07547991019721095]
	TIME [epoch: 13.2 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08162422763572667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08162422763572667 | validation: 0.04780696461683164]
	TIME [epoch: 13.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044732489223770475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044732489223770475 | validation: 0.06828508244548798]
	TIME [epoch: 13.2 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05513291789930952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05513291789930952 | validation: 0.05928406069498576]
	TIME [epoch: 13.3 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06042209436548753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06042209436548753 | validation: 0.059897087950940835]
	TIME [epoch: 13.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0488972612803761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0488972612803761 | validation: 0.06620951929806561]
	TIME [epoch: 13.3 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787603904115524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787603904115524 | validation: 0.2196018677047302]
	TIME [epoch: 13.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12315281601253547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12315281601253547 | validation: 0.09330195867470385]
	TIME [epoch: 13.3 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0907988444880348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0907988444880348 | validation: 0.12235140455105664]
	TIME [epoch: 13.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242646062961666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08242646062961666 | validation: 0.2638505583021122]
	TIME [epoch: 13.3 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22933702331616915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22933702331616915 | validation: 0.1014882845242213]
	TIME [epoch: 13.3 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08623570444400815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08623570444400815 | validation: 0.25870911199315527]
	TIME [epoch: 13.3 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20661675113595557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20661675113595557 | validation: 0.06606484659291724]
	TIME [epoch: 13.3 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08808024040874286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08808024040874286 | validation: 0.1314575111640196]
	TIME [epoch: 13.3 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13817849287359887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13817849287359887 | validation: 0.09051215304212765]
	TIME [epoch: 13.3 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0932015378935565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0932015378935565 | validation: 0.1066068851587147]
	TIME [epoch: 13.3 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07657321678256054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07657321678256054 | validation: 0.14208660034021212]
	TIME [epoch: 13.3 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1661331445441153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1661331445441153 | validation: 0.3658844066849481]
	TIME [epoch: 13.3 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20027565413794116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20027565413794116 | validation: 0.11245147029706302]
	TIME [epoch: 13.2 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276802206428524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06276802206428524 | validation: 0.07130430242805035]
	TIME [epoch: 13.2 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07094564429612231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07094564429612231 | validation: 0.07724547056509275]
	TIME [epoch: 13.2 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725400066451634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0725400066451634 | validation: 0.05169240328700245]
	TIME [epoch: 13.3 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05087852575576749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05087852575576749 | validation: 0.053223708150179044]
	TIME [epoch: 13.2 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038825697997316395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038825697997316395 | validation: 0.03729248877348729]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_875.pth
	Model improved!!!
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032804831226050865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032804831226050865 | validation: 0.03553435203503487]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_876.pth
	Model improved!!!
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03227787521967856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03227787521967856 | validation: 0.042291189784811646]
	TIME [epoch: 13.3 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03807998938823457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03807998938823457 | validation: 0.0593483498848183]
	TIME [epoch: 13.3 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054632064583291715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054632064583291715 | validation: 0.11539920389913522]
	TIME [epoch: 13.3 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08560940202613558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08560940202613558 | validation: 0.1226913710410478]
	TIME [epoch: 13.3 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11023782586334262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11023782586334262 | validation: 0.26762380321011364]
	TIME [epoch: 13.3 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13138714810505298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13138714810505298 | validation: 0.07841228059516599]
	TIME [epoch: 13.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05589026327961483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05589026327961483 | validation: 0.053163641253947326]
	TIME [epoch: 13.3 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05171024004384255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05171024004384255 | validation: 0.03159230605338259]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_884.pth
	Model improved!!!
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0374847859006079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0374847859006079 | validation: 0.043968122219666055]
	TIME [epoch: 13.3 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03453286659213697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03453286659213697 | validation: 0.041968402700624324]
	TIME [epoch: 13.2 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03896249407495936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03896249407495936 | validation: 0.033176118458578284]
	TIME [epoch: 13.2 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531264567641519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03531264567641519 | validation: 0.060837710218866616]
	TIME [epoch: 13.2 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773591474277688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04773591474277688 | validation: 0.10084138416356846]
	TIME [epoch: 13.3 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09088238829674015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09088238829674015 | validation: 0.15945318490709381]
	TIME [epoch: 13.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982318326913593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09982318326913593 | validation: 0.07944317260029557]
	TIME [epoch: 13.3 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07247942530068896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07247942530068896 | validation: 0.09951697662273092]
	TIME [epoch: 13.3 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09545802957336602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09545802957336602 | validation: 0.055067093880781494]
	TIME [epoch: 13.3 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05545673862133988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05545673862133988 | validation: 0.074645103584227]
	TIME [epoch: 13.3 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07613110833033644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07613110833033644 | validation: 0.12654930722997218]
	TIME [epoch: 13.3 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09981844099077172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09981844099077172 | validation: 0.061502396222020896]
	TIME [epoch: 13.3 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345355537430437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06345355537430437 | validation: 0.10281096763846331]
	TIME [epoch: 13.3 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08092540303308356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08092540303308356 | validation: 0.046652825914311635]
	TIME [epoch: 13.3 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615275096442862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04615275096442862 | validation: 0.03232318478623305]
	TIME [epoch: 13.3 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449955803091851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03449955803091851 | validation: 0.0380914361701022]
	TIME [epoch: 13.3 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03222827607416801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03222827607416801 | validation: 0.034956658798877396]
	TIME [epoch: 13.3 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033252775150509874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033252775150509874 | validation: 0.03895239133851322]
	TIME [epoch: 13.2 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038632005898357046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038632005898357046 | validation: 0.11237517230161223]
	TIME [epoch: 13.3 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06973309889795845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06973309889795845 | validation: 0.17809506329227937]
	TIME [epoch: 13.2 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825334910567716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825334910567716 | validation: 0.3956736556645486]
	TIME [epoch: 13.3 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19731512956074665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19731512956074665 | validation: 0.07585455979850707]
	TIME [epoch: 13.2 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07773836450854525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07773836450854525 | validation: 0.07912940734326325]
	TIME [epoch: 13.3 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08401406422475344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08401406422475344 | validation: 0.0531341680921009]
	TIME [epoch: 13.2 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04458636431855768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04458636431855768 | validation: 0.07506684685963662]
	TIME [epoch: 13.3 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05828689777290747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05828689777290747 | validation: 0.048336655289356836]
	TIME [epoch: 13.3 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06255512667166448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06255512667166448 | validation: 0.05110323941704488]
	TIME [epoch: 13.3 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05777078197105422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05777078197105422 | validation: 0.05560045097932236]
	TIME [epoch: 13.3 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038914890939502754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038914890939502754 | validation: 0.04509951439254446]
	TIME [epoch: 13.2 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03405404884783235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03405404884783235 | validation: 0.03854915776063649]
	TIME [epoch: 13.2 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03248018314309415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03248018314309415 | validation: 0.03991247573820039]
	TIME [epoch: 13.2 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047383592727648347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047383592727648347 | validation: 0.06895083925449773]
	TIME [epoch: 13.2 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06746407218805821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06746407218805821 | validation: 0.09233294500649813]
	TIME [epoch: 13.2 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08094096581251133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08094096581251133 | validation: 0.13623164408885227]
	TIME [epoch: 13.2 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07158700979719075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07158700979719075 | validation: 0.10053665106031257]
	TIME [epoch: 13.2 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09997627643106477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09997627643106477 | validation: 0.1540430993356198]
	TIME [epoch: 13.2 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787019044514633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787019044514633 | validation: 0.07591682491279238]
	TIME [epoch: 13.3 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062343373728661756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062343373728661756 | validation: 0.0646836441916898]
	TIME [epoch: 13.2 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07235463437219433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07235463437219433 | validation: 0.045457306996902695]
	TIME [epoch: 13.2 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0364503178933758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0364503178933758 | validation: 0.03611268679626608]
	TIME [epoch: 13.3 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028328795020732045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028328795020732045 | validation: 0.10435752647980677]
	TIME [epoch: 13.3 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07969779026369159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07969779026369159 | validation: 0.07333376253026543]
	TIME [epoch: 13.3 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04540812504939014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04540812504939014 | validation: 0.07709493452129301]
	TIME [epoch: 13.2 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073319704636738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073319704636738 | validation: 0.12963841564825204]
	TIME [epoch: 13.3 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1294889390126662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1294889390126662 | validation: 0.07827020535544432]
	TIME [epoch: 13.2 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07955499371529476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07955499371529476 | validation: 0.06748960442944933]
	TIME [epoch: 13.3 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06655775670875098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06655775670875098 | validation: 0.07051809895448324]
	TIME [epoch: 13.2 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07275767030815178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07275767030815178 | validation: 0.03790577758970903]
	TIME [epoch: 13.2 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03234807922151059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03234807922151059 | validation: 0.03447012601470099]
	TIME [epoch: 13.3 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03173856161434218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03173856161434218 | validation: 0.06365828146602603]
	TIME [epoch: 13.2 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05208372751946918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05208372751946918 | validation: 0.07526851130206741]
	TIME [epoch: 13.2 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696024349493315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0696024349493315 | validation: 0.4168201479237304]
	TIME [epoch: 13.2 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20288313864102217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20288313864102217 | validation: 0.08819159719301378]
	TIME [epoch: 13.2 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791521640769196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0791521640769196 | validation: 0.07726549139633955]
	TIME [epoch: 13.2 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059165751448646574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059165751448646574 | validation: 0.04366008478426145]
	TIME [epoch: 13.2 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043203599778960065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043203599778960065 | validation: 0.044126812544570106]
	TIME [epoch: 13.2 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029965420877274336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029965420877274336 | validation: 0.04145446031895537]
	TIME [epoch: 13.3 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603537700083096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03603537700083096 | validation: 0.03223096546749327]
	TIME [epoch: 13.2 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02673282676458464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02673282676458464 | validation: 0.04065700144677451]
	TIME [epoch: 13.2 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032578648423389964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032578648423389964 | validation: 0.06849021750901234]
	TIME [epoch: 13.2 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0488835193395181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0488835193395181 | validation: 0.14512944598593389]
	TIME [epoch: 13.2 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15076821604015753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15076821604015753 | validation: 0.2635193599916379]
	TIME [epoch: 13.3 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14463834255671146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14463834255671146 | validation: 0.06432669897069283]
	TIME [epoch: 13.2 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044937838108572964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044937838108572964 | validation: 0.05536831641632929]
	TIME [epoch: 13.2 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04059474562840176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04059474562840176 | validation: 0.03670956150052095]
	TIME [epoch: 13.3 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03762635818058612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03762635818058612 | validation: 0.09019020023022173]
	TIME [epoch: 13.2 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011704449336098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06011704449336098 | validation: 0.07022136230386934]
	TIME [epoch: 13.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06712874450780322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06712874450780322 | validation: 0.08493192637725783]
	TIME [epoch: 13.3 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06315745470650568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06315745470650568 | validation: 0.13467342265553628]
	TIME [epoch: 13.3 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11619975406397134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11619975406397134 | validation: 0.1306815755815968]
	TIME [epoch: 13.3 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07669689953217802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07669689953217802 | validation: 0.0781688563510763]
	TIME [epoch: 13.3 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07154607576412385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07154607576412385 | validation: 0.048251781897952976]
	TIME [epoch: 13.2 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057656380792582416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057656380792582416 | validation: 0.03042800526048103]
	TIME [epoch: 13.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_957.pth
	Model improved!!!
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025371318454547955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025371318454547955 | validation: 0.03152688174253256]
	TIME [epoch: 13.2 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02459609694884744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02459609694884744 | validation: 0.018773002822610785]
	TIME [epoch: 13.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_959.pth
	Model improved!!!
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024403882070285102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024403882070285102 | validation: 0.025697373825194127]
	TIME [epoch: 13.3 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021700136798780894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021700136798780894 | validation: 0.022965841514699537]
	TIME [epoch: 13.3 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02390384420976066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02390384420976066 | validation: 0.04260046884276023]
	TIME [epoch: 13.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03697168662233253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03697168662233253 | validation: 0.34081149581588055]
	TIME [epoch: 13.3 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587207031472983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3587207031472983 | validation: 0.34726063502247495]
	TIME [epoch: 13.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20555025748918312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20555025748918312 | validation: 0.07523052811666653]
	TIME [epoch: 13.3 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07873594751872282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07873594751872282 | validation: 0.14533199028923455]
	TIME [epoch: 13.3 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1270033578987972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1270033578987972 | validation: 0.04897608113135619]
	TIME [epoch: 13.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04653209817383868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04653209817383868 | validation: 0.09494757257066963]
	TIME [epoch: 13.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10094983438237803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10094983438237803 | validation: 0.09679904405076604]
	TIME [epoch: 13.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06455513227084123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06455513227084123 | validation: 0.04560680784371384]
	TIME [epoch: 13.3 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03844847431775029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03844847431775029 | validation: 0.05824738513561284]
	TIME [epoch: 13.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0411730029604037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0411730029604037 | validation: 0.02948402154857224]
	TIME [epoch: 13.3 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024006781405767504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024006781405767504 | validation: 0.02521329259630123]
	TIME [epoch: 13.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026047445875577616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026047445875577616 | validation: 0.03937402582990891]
	TIME [epoch: 13.3 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03076016585692612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03076016585692612 | validation: 0.03539514770864849]
	TIME [epoch: 13.3 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032206948397703175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032206948397703175 | validation: 0.03343041335544255]
	TIME [epoch: 13.3 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031815017542740714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031815017542740714 | validation: 0.05444655516163023]
	TIME [epoch: 13.3 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04755274624750443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04755274624750443 | validation: 0.049673577615021496]
	TIME [epoch: 13.3 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03926088891119263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03926088891119263 | validation: 0.06182617758582956]
	TIME [epoch: 13.3 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056203538178873184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056203538178873184 | validation: 0.05368073279016411]
	TIME [epoch: 13.3 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06634412684570348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06634412684570348 | validation: 0.052261918026925494]
	TIME [epoch: 13.3 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053517666085484195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053517666085484195 | validation: 0.039732301131738826]
	TIME [epoch: 13.3 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359188966773275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03359188966773275 | validation: 0.049777528982313]
	TIME [epoch: 13.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03506261999671743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03506261999671743 | validation: 0.1063567982574285]
	TIME [epoch: 13.3 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111265555199196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.111265555199196 | validation: 0.46163965335596374]
	TIME [epoch: 13.3 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24944820652934843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24944820652934843 | validation: 0.1501842258206988]
	TIME [epoch: 13.3 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07781531345548659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07781531345548659 | validation: 0.0664834663264098]
	TIME [epoch: 13.3 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060047154376576393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060047154376576393 | validation: 0.05348403362241089]
	TIME [epoch: 13.3 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04071985239704908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04071985239704908 | validation: 0.03748424838323094]
	TIME [epoch: 13.3 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0380541739749029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0380541739749029 | validation: 0.04297163617748151]
	TIME [epoch: 13.3 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03410129896340534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03410129896340534 | validation: 0.0396035827678447]
	TIME [epoch: 13.3 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030605357751621536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030605357751621536 | validation: 0.24488485649593617]
	TIME [epoch: 13.3 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2241444313076385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2241444313076385 | validation: 0.14140707286080964]
	TIME [epoch: 13.3 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11578184962123436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11578184962123436 | validation: 0.2203181347199039]
	TIME [epoch: 13.3 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09881206507316197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09881206507316197 | validation: 0.1348442430387102]
	TIME [epoch: 13.3 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039886149646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039886149646 | validation: 0.1102138424015567]
	TIME [epoch: 13.3 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10238044858330447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10238044858330447 | validation: 0.07904864814556022]
	TIME [epoch: 13.3 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648874634876066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0648874634876066 | validation: 0.07089450600917989]
	TIME [epoch: 13.3 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03981571577053364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03981571577053364 | validation: 0.04686707444385588]
	TIME [epoch: 13.3 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034682028350231424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034682028350231424 | validation: 0.050011392226976606]
	TIME [epoch: 13.3 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037089074658403824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037089074658403824 | validation: 0.03131435678862088]
	TIME [epoch: 205 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025489825521575995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025489825521575995 | validation: 0.029058819368797797]
	TIME [epoch: 27.3 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027462073685448365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027462073685448365 | validation: 0.05475538240378495]
	TIME [epoch: 27.2 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04608244910557218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04608244910557218 | validation: 0.2621102658952561]
	TIME [epoch: 27.3 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15498986828439779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15498986828439779 | validation: 0.07044626540531532]
	TIME [epoch: 27.3 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252113172922602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08252113172922602 | validation: 0.059057807168495624]
	TIME [epoch: 27.2 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04096254220397321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04096254220397321 | validation: 0.021214343969583715]
	TIME [epoch: 27.2 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024716928876248576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024716928876248576 | validation: 0.4914399426500894]
	TIME [epoch: 27.2 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.594273102308884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.594273102308884 | validation: 0.29553580831757004]
	TIME [epoch: 27.2 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17324172135805654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17324172135805654 | validation: 0.17701361160352658]
	TIME [epoch: 27.2 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08956155245765354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08956155245765354 | validation: 0.21988325148806231]
	TIME [epoch: 27.2 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24049596273665358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24049596273665358 | validation: 0.07959136898976843]
	TIME [epoch: 27.2 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09608221853993251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09608221853993251 | validation: 0.065075098203838]
	TIME [epoch: 27.3 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054513157077346774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054513157077346774 | validation: 0.0963589648205339]
	TIME [epoch: 27.3 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06802083262127823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06802083262127823 | validation: 0.09989429966062334]
	TIME [epoch: 27.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057710137664716436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057710137664716436 | validation: 0.07480544002245512]
	TIME [epoch: 27.2 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06190373793167299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06190373793167299 | validation: 0.08871199639222592]
	TIME [epoch: 27.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06913393706967873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06913393706967873 | validation: 0.07058456884855835]
	TIME [epoch: 27.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06783915884496054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06783915884496054 | validation: 0.03738609932033587]
	TIME [epoch: 27.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04893345263078106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04893345263078106 | validation: 0.03174268794264904]
	TIME [epoch: 27.2 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02582928832366085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02582928832366085 | validation: 0.027028443873586938]
	TIME [epoch: 27.3 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022657305077653664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022657305077653664 | validation: 0.023966444509238807]
	TIME [epoch: 27.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02115960785447045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02115960785447045 | validation: 0.020964842635716952]
	TIME [epoch: 27.3 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019659568504562228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019659568504562228 | validation: 0.021822277078294428]
	TIME [epoch: 27.3 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02007627960694828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02007627960694828 | validation: 0.028660107501198143]
	TIME [epoch: 27.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024996764092954784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024996764092954784 | validation: 0.15966225368405468]
	TIME [epoch: 27.3 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07768290219308738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07768290219308738 | validation: 0.09640613481152732]
	TIME [epoch: 27.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0927021068346592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0927021068346592 | validation: 0.06723916819120647]
	TIME [epoch: 27.3 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664364704345934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05664364704345934 | validation: 0.02990393358809548]
	TIME [epoch: 27.3 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02684136340588087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02684136340588087 | validation: 0.027769104077329233]
	TIME [epoch: 27.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028255921789167183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028255921789167183 | validation: 0.05310434209641464]
	TIME [epoch: 27.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0548537310748396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0548537310748396 | validation: 0.5935654294724894]
	TIME [epoch: 27.3 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49443886639292856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49443886639292856 | validation: 0.2997342667103901]
	TIME [epoch: 27.3 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24130792056418127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24130792056418127 | validation: 0.20695402210302755]
	TIME [epoch: 27.3 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21277266492652117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21277266492652117 | validation: 0.1868465891841901]
	TIME [epoch: 27.3 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22006453035156603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22006453035156603 | validation: 0.16636633734407053]
	TIME [epoch: 27.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1706729208432203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706729208432203 | validation: 0.13005002574696362]
	TIME [epoch: 27.3 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13290781534539672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13290781534539672 | validation: 0.10862633020518758]
	TIME [epoch: 27.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12230568010735407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12230568010735407 | validation: 0.11708942200540103]
	TIME [epoch: 27.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10008386084001608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10008386084001608 | validation: 0.11552633774312603]
	TIME [epoch: 27.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08587682567066147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08587682567066147 | validation: 0.136671881676639]
	TIME [epoch: 27.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10198549656045995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10198549656045995 | validation: 0.0651636422307938]
	TIME [epoch: 27.3 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06323300953309263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06323300953309263 | validation: 0.06532575702556198]
	TIME [epoch: 27.3 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512356488514466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512356488514466 | validation: 0.08106349832322979]
	TIME [epoch: 27.3 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05499661212969019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05499661212969019 | validation: 0.04532257123613719]
	TIME [epoch: 27.3 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04026101036166219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04026101036166219 | validation: 0.04155156201922035]
	TIME [epoch: 27.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311045323926198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04311045323926198 | validation: 0.07659508630573146]
	TIME [epoch: 27.2 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06432463626800708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06432463626800708 | validation: 0.05689168491612018]
	TIME [epoch: 27.3 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057524358459045084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057524358459045084 | validation: 0.0453755253916074]
	TIME [epoch: 27.3 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615223828344677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04615223828344677 | validation: 0.2674192195370206]
	TIME [epoch: 27.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13598374840051325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13598374840051325 | validation: 0.08043801790464439]
	TIME [epoch: 27.3 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07661009626328415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07661009626328415 | validation: 0.12768286026605577]
	TIME [epoch: 27.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11533750469383741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11533750469383741 | validation: 0.07055264140364002]
	TIME [epoch: 27.2 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07236894216319215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07236894216319215 | validation: 0.05112057195071817]
	TIME [epoch: 27.2 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04005412924674419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04005412924674419 | validation: 0.036396567051543385]
	TIME [epoch: 27.2 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033614810843245295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033614810843245295 | validation: 0.046794903248679035]
	TIME [epoch: 27.2 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040903095174739085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040903095174739085 | validation: 0.10224385122196825]
	TIME [epoch: 27.2 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05503649861272023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05503649861272023 | validation: 0.04872366261100645]
	TIME [epoch: 27.2 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340829141517035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04340829141517035 | validation: 0.03752644219785176]
	TIME [epoch: 27.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03760493517291542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03760493517291542 | validation: 0.03444228904088438]
	TIME [epoch: 27.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_122309/states/model_phi1_4c_v_mmd1_1060.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 11511.800 seconds.
