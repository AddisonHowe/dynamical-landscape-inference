Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3912625597

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.797249768480091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.797249768480091 | validation: 4.810021225685558]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.20224712305759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.20224712305759 | validation: 3.829221484251655]
	TIME [epoch: 1.43 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6869467806955787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6869467806955787 | validation: 3.5673126317338077]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.166647719904854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.166647719904854 | validation: 3.476707743594622]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8908634163475297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8908634163475297 | validation: 2.43530390178689]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.512569198270102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.512569198270102 | validation: 3.361947687658169]
	TIME [epoch: 1.41 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5306731169101675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5306731169101675 | validation: 3.1900483964861337]
	TIME [epoch: 1.41 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0312902762419673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0312902762419673 | validation: 2.13461864330243]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8658698660944877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8658698660944877 | validation: 2.4366179741635805]
	TIME [epoch: 1.41 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8781026194338182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8781026194338182 | validation: 1.9669819416091157]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6646864011303195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6646864011303195 | validation: 1.9755534904787346]
	TIME [epoch: 1.41 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6260874472975198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6260874472975198 | validation: 2.0513972440214796]
	TIME [epoch: 1.4 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5577854678631917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5577854678631917 | validation: 1.7568132208388876]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4967674756302074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4967674756302074 | validation: 1.7183259474208494]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4515472356118742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4515472356118742 | validation: 1.74931748894059]
	TIME [epoch: 1.41 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4057113488424409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4057113488424409 | validation: 1.5974278364707106]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3638480334125718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3638480334125718 | validation: 1.7316399257554096]
	TIME [epoch: 1.41 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3416301905291623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3416301905291623 | validation: 1.4526017936597948]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.341190883480755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.341190883480755 | validation: 2.0072525136932966]
	TIME [epoch: 1.42 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.454266603014367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.454266603014367 | validation: 1.3776343276271268]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.454105416170638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.454105416170638 | validation: 1.7503850698991792]
	TIME [epoch: 1.41 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2907857102633018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2907857102633018 | validation: 1.442654589463647]
	TIME [epoch: 1.4 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1740513158445938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1740513158445938 | validation: 1.3294040583678595]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1595986613451157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1595986613451157 | validation: 1.5102186372933624]
	TIME [epoch: 1.41 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1494911543455084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1494911543455084 | validation: 1.2765928987921056]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1153641918912351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1153641918912351 | validation: 1.4406144433405434]
	TIME [epoch: 1.41 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0904062848732465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0904062848732465 | validation: 1.2042688451231582]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0792290226288819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0792290226288819 | validation: 1.4483870669559573]
	TIME [epoch: 1.41 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0795074361109733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0795074361109733 | validation: 1.1521158031365368]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0718350068779212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0718350068779212 | validation: 1.4600126934255364]
	TIME [epoch: 1.41 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0801818878902514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0801818878902514 | validation: 1.186727902489093]
	TIME [epoch: 1.41 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0475056404741327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0475056404741327 | validation: 1.3374416931590183]
	TIME [epoch: 1.41 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031400092369258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.031400092369258 | validation: 1.201761907444521]
	TIME [epoch: 1.41 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.978482312631294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.978482312631294 | validation: 1.1991161127163994]
	TIME [epoch: 1.41 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9391327129303011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9391327129303011 | validation: 1.127285639118833]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9190445756155546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9190445756155546 | validation: 1.167717162451894]
	TIME [epoch: 1.42 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026570373170794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026570373170794 | validation: 1.13036689690062]
	TIME [epoch: 1.41 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9090534179073879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9090534179073879 | validation: 1.1666558147755228]
	TIME [epoch: 1.41 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9103683118397704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9103683118397704 | validation: 1.1138886243750983]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9275110850554245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9275110850554245 | validation: 1.253676421375538]
	TIME [epoch: 1.41 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9680232525876022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9680232525876022 | validation: 1.0393483581094032]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9341265964473934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9341265964473934 | validation: 1.3186744777710853]
	TIME [epoch: 1.41 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945917070143478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945917070143478 | validation: 1.0049149855965507]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8284508317143804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8284508317143804 | validation: 0.9775988882056146]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820501485861584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820501485861584 | validation: 1.1578081490260375]
	TIME [epoch: 1.41 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529797975545055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8529797975545055 | validation: 0.9598091810222315]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8203170192933803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8203170192933803 | validation: 1.0544251418674768]
	TIME [epoch: 1.4 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155621505333406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8155621505333406 | validation: 0.9522545738019904]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135829255815601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135829255815601 | validation: 1.0610800629824648]
	TIME [epoch: 1.41 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8456120772780557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8456120772780557 | validation: 1.0961271141820121]
	TIME [epoch: 1.4 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8929650095981964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8929650095981964 | validation: 1.1113987671554597]
	TIME [epoch: 1.4 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9179817134066175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9179817134066175 | validation: 1.041660696588307]
	TIME [epoch: 1.41 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823236858493863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823236858493863 | validation: 0.9536571702437823]
	TIME [epoch: 1.4 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787647925981827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787647925981827 | validation: 0.9308923893359552]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7796496903741286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7796496903741286 | validation: 1.1231668709803906]
	TIME [epoch: 1.4 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814390108506088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.814390108506088 | validation: 0.9303190457186279]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8071802985239873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8071802985239873 | validation: 1.0694541990573454]
	TIME [epoch: 1.41 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011827142204068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8011827142204068 | validation: 0.9268568410491098]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788325942731282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788325942731282 | validation: 0.9788195236204591]
	TIME [epoch: 1.41 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985000300106566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7985000300106566 | validation: 0.9846817082865087]
	TIME [epoch: 1.4 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381611042534273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381611042534273 | validation: 1.0854107764918266]
	TIME [epoch: 1.41 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052092890731299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9052092890731299 | validation: 0.9496195968574233]
	TIME [epoch: 1.41 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812502259917066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812502259917066 | validation: 1.049092097478611]
	TIME [epoch: 1.4 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977009500018386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7977009500018386 | validation: 0.8951924059690907]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774489902153934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774489902153934 | validation: 0.9767229928121449]
	TIME [epoch: 1.41 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670838963758755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7670838963758755 | validation: 0.9030968438137039]
	TIME [epoch: 1.41 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762281072792053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762281072792053 | validation: 0.9502566541523566]
	TIME [epoch: 1.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7670118097448306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7670118097448306 | validation: 0.8971394625565878]
	TIME [epoch: 1.41 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641515137611531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641515137611531 | validation: 0.971035162185631]
	TIME [epoch: 1.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634725258788507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7634725258788507 | validation: 0.9128624160658287]
	TIME [epoch: 1.4 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7660804473603438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7660804473603438 | validation: 0.93430715391599]
	TIME [epoch: 1.41 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651697479704959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651697479704959 | validation: 1.0025901587560964]
	TIME [epoch: 1.41 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152715453333578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152715453333578 | validation: 1.0610419694088422]
	TIME [epoch: 1.41 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9383166463798334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9383166463798334 | validation: 1.179195023250049]
	TIME [epoch: 1.41 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9650669291796149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9650669291796149 | validation: 0.9660535920409187]
	TIME [epoch: 1.41 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763843756144451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763843756144451 | validation: 0.9187668717866253]
	TIME [epoch: 1.41 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8148810059598486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8148810059598486 | validation: 0.9744418655560255]
	TIME [epoch: 1.41 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8156118158517862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8156118158517862 | validation: 0.9480419816471359]
	TIME [epoch: 1.41 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7621202521487345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7621202521487345 | validation: 0.8818623505805578]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7798694795457403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7798694795457403 | validation: 1.0955423869211298]
	TIME [epoch: 1.41 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135456300051324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135456300051324 | validation: 0.8813277422165232]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7580130933355145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7580130933355145 | validation: 0.8837410065956157]
	TIME [epoch: 1.41 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677550177536716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677550177536716 | validation: 1.0047562122986098]
	TIME [epoch: 1.41 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786014891595109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786014891595109 | validation: 0.8676489082918565]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767227435734875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7767227435734875 | validation: 0.9660481268781681]
	TIME [epoch: 1.41 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688512313188258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688512313188258 | validation: 0.8674877703980202]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650072616282672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650072616282672 | validation: 0.9392177525885277]
	TIME [epoch: 1.41 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754587138157689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7754587138157689 | validation: 0.9401173473673463]
	TIME [epoch: 1.41 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813465083572603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.813465083572603 | validation: 0.9989668356709691]
	TIME [epoch: 1.41 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164151336714062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8164151336714062 | validation: 0.9710912053405982]
	TIME [epoch: 1.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8270370742060056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8270370742060056 | validation: 0.9473971097449039]
	TIME [epoch: 1.41 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7682303811000354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7682303811000354 | validation: 0.9002344836654429]
	TIME [epoch: 1.4 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502197318920074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502197318920074 | validation: 0.9044707606309702]
	TIME [epoch: 1.41 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505851059246053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7505851059246053 | validation: 0.8705015759469922]
	TIME [epoch: 1.41 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7534980572611895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7534980572611895 | validation: 1.001500849967798]
	TIME [epoch: 1.41 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773787141891593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773787141891593 | validation: 0.866348120573176]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837257115184777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7837257115184777 | validation: 1.1363333445549206]
	TIME [epoch: 1.42 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832110453963197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.832110453963197 | validation: 0.8871026230887105]
	TIME [epoch: 1.41 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7758077526074691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7758077526074691 | validation: 0.9460054899994358]
	TIME [epoch: 1.41 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758032024229216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758032024229216 | validation: 0.9328735477512862]
	TIME [epoch: 1.41 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718672295093404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7718672295093404 | validation: 0.9547316050349661]
	TIME [epoch: 1.41 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286553750293939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286553750293939 | validation: 0.9570033849406571]
	TIME [epoch: 1.41 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8242382901785484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8242382901785484 | validation: 0.9832054592606103]
	TIME [epoch: 1.41 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797136117853195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.797136117853195 | validation: 0.875035689300712]
	TIME [epoch: 1.41 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475954151594975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7475954151594975 | validation: 0.9211687922470152]
	TIME [epoch: 1.41 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485891019216746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485891019216746 | validation: 0.8592667137249749]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598552373601887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7598552373601887 | validation: 0.9919443549751481]
	TIME [epoch: 1.41 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626928054583645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7626928054583645 | validation: 0.8882534080973511]
	TIME [epoch: 1.41 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574259735239626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574259735239626 | validation: 0.9007143715060599]
	TIME [epoch: 1.41 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475556687665001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7475556687665001 | validation: 0.9191093732116666]
	TIME [epoch: 1.41 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7644672291169566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7644672291169566 | validation: 0.9046351226579361]
	TIME [epoch: 1.41 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963416608205495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963416608205495 | validation: 1.058385757907826]
	TIME [epoch: 1.41 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672789287355014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672789287355014 | validation: 0.9227529541556498]
	TIME [epoch: 1.41 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806664651450984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806664651450984 | validation: 0.8776483811433917]
	TIME [epoch: 1.41 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502497895907578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502497895907578 | validation: 0.9519035722040976]
	TIME [epoch: 1.41 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7544590094955769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7544590094955769 | validation: 0.8532795109145428]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507023438240176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507023438240176 | validation: 0.9372107854090507]
	TIME [epoch: 1.41 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7493135207780582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7493135207780582 | validation: 0.8749515425579475]
	TIME [epoch: 1.41 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564479778446395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564479778446395 | validation: 0.9594826522472163]
	TIME [epoch: 1.41 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710953657004234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710953657004234 | validation: 0.8608306035001662]
	TIME [epoch: 1.41 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858684872743364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858684872743364 | validation: 1.0801837794189264]
	TIME [epoch: 1.41 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810984553251553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810984553251553 | validation: 0.8596026492309359]
	TIME [epoch: 1.41 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504514327108754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7504514327108754 | validation: 0.8858087582701755]
	TIME [epoch: 1.41 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423843163456197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7423843163456197 | validation: 0.8678540894905116]
	TIME [epoch: 1.41 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7429138314324726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7429138314324726 | validation: 0.9077032680410796]
	TIME [epoch: 1.41 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690014097484013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690014097484013 | validation: 1.0870292887114565]
	TIME [epoch: 1.41 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8211748080907506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8211748080907506 | validation: 0.941378889860531]
	TIME [epoch: 1.41 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325792239819377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8325792239819377 | validation: 0.9393582258403872]
	TIME [epoch: 1.41 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359720994194237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7359720994194237 | validation: 0.9194744590369419]
	TIME [epoch: 1.41 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7478466060877383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7478466060877383 | validation: 0.8845897207548751]
	TIME [epoch: 1.41 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7664616275318463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7664616275318463 | validation: 0.9794211235164614]
	TIME [epoch: 1.41 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7496779663454103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496779663454103 | validation: 0.8695592226981028]
	TIME [epoch: 1.41 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7353385110465223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7353385110465223 | validation: 0.8825348651876382]
	TIME [epoch: 1.41 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307206796791865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7307206796791865 | validation: 0.9020769575769475]
	TIME [epoch: 1.41 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7262404638205225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7262404638205225 | validation: 0.8895437704741934]
	TIME [epoch: 1.41 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359676732544831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7359676732544831 | validation: 0.9109808415384526]
	TIME [epoch: 1.41 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7336559062865365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7336559062865365 | validation: 0.9129976816667343]
	TIME [epoch: 1.41 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592273197830772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7592273197830772 | validation: 1.052424839730741]
	TIME [epoch: 1.41 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9280436675577866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9280436675577866 | validation: 1.2848541410080083]
	TIME [epoch: 1.41 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9760557475942215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9760557475942215 | validation: 1.0494073482601778]
	TIME [epoch: 1.41 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853110240445936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7853110240445936 | validation: 0.8825833346340952]
	TIME [epoch: 1.41 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8385870789823779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8385870789823779 | validation: 0.8945987663317911]
	TIME [epoch: 1.41 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7398216789882865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7398216789882865 | validation: 0.9352378785556739]
	TIME [epoch: 1.41 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7490015828923923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7490015828923923 | validation: 0.856344935958062]
	TIME [epoch: 1.41 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.732808659664252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.732808659664252 | validation: 0.8315933814746398]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317364711479236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317364711479236 | validation: 0.9018229016637772]
	TIME [epoch: 1.41 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7372023915427565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7372023915427565 | validation: 0.8448222086925701]
	TIME [epoch: 1.41 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7326853880045349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7326853880045349 | validation: 0.9378635189849664]
	TIME [epoch: 1.4 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7418678612937423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7418678612937423 | validation: 0.8751830790636309]
	TIME [epoch: 1.41 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7526418427943665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7526418427943665 | validation: 0.9660170403419639]
	TIME [epoch: 1.42 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7637775168112085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7637775168112085 | validation: 0.8772932664502604]
	TIME [epoch: 1.41 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520465591307707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520465591307707 | validation: 0.8906293113191049]
	TIME [epoch: 1.42 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7285891368581446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7285891368581446 | validation: 0.8512134568775878]
	TIME [epoch: 1.41 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215577919792615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7215577919792615 | validation: 0.8724530990608219]
	TIME [epoch: 1.41 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126986263312398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126986263312398 | validation: 0.8466139181162238]
	TIME [epoch: 1.4 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715376982927432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.715376982927432 | validation: 0.9002024936416639]
	TIME [epoch: 1.4 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169622161021504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7169622161021504 | validation: 0.7860865446956412]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455992768224661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7455992768224661 | validation: 1.0813784690002588]
	TIME [epoch: 1.41 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8554414776121108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8554414776121108 | validation: 0.8699814138539509]
	TIME [epoch: 1.41 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7775448907155853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7775448907155853 | validation: 0.9881028710206456]
	TIME [epoch: 1.4 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556335894937481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556335894937481 | validation: 0.9633812650999506]
	TIME [epoch: 1.41 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449819603848846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449819603848846 | validation: 0.8760458250594092]
	TIME [epoch: 1.4 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129947734227858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7129947734227858 | validation: 0.8206844474790188]
	TIME [epoch: 1.41 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719177048318073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719177048318073 | validation: 0.874171813252869]
	TIME [epoch: 1.41 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993137606314275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993137606314275 | validation: 0.8164865579342966]
	TIME [epoch: 1.41 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6898501509818943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6898501509818943 | validation: 0.7574906578277443]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679405226519145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.679405226519145 | validation: 0.7507488654825999]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6594413986604218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6594413986604218 | validation: 0.6354534243211392]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254022491956723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6254022491956723 | validation: 1.286769384645104]
	TIME [epoch: 1.41 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0305178896097118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0305178896097118 | validation: 1.16660025595636]
	TIME [epoch: 1.41 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9010766103107732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9010766103107732 | validation: 0.911116441751679]
	TIME [epoch: 1.41 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737664835318465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737664835318465 | validation: 0.7794093816530429]
	TIME [epoch: 1.41 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597397130841611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597397130841611 | validation: 0.7042517487063614]
	TIME [epoch: 1.41 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102716188646192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6102716188646192 | validation: 0.7892206756677169]
	TIME [epoch: 1.41 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.831427851150882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.831427851150882 | validation: 1.1849845266544337]
	TIME [epoch: 1.41 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592744092355793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592744092355793 | validation: 1.1155662493596394]
	TIME [epoch: 1.41 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7704060702571005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7704060702571005 | validation: 1.0790816768842073]
	TIME [epoch: 1.42 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7804556746523605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7804556746523605 | validation: 1.0127490168315336]
	TIME [epoch: 1.41 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7392195947296792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7392195947296792 | validation: 1.026987316385669]
	TIME [epoch: 1.41 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246001328559296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246001328559296 | validation: 1.0443550446628471]
	TIME [epoch: 1.41 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119371484895644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119371484895644 | validation: 0.9515234374789128]
	TIME [epoch: 1.41 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7041445930987774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7041445930987774 | validation: 0.8647679346134503]
	TIME [epoch: 1.41 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899901560185887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6899901560185887 | validation: 0.8230267870978502]
	TIME [epoch: 1.41 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6807356949361625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6807356949361625 | validation: 0.7793619237637792]
	TIME [epoch: 1.41 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6688018464221804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6688018464221804 | validation: 0.7421769450504329]
	TIME [epoch: 1.42 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503608215049477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503608215049477 | validation: 0.7009976515028593]
	TIME [epoch: 1.41 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6321230960393013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6321230960393013 | validation: 0.6975963258748765]
	TIME [epoch: 1.41 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.607907123047329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607907123047329 | validation: 0.6288727872501534]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5697648882957336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5697648882957336 | validation: 0.7920038652915]
	TIME [epoch: 1.42 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440149812665059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8440149812665059 | validation: 1.381493729155066]
	TIME [epoch: 1.41 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.136123306647115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.136123306647115 | validation: 1.0557541936956638]
	TIME [epoch: 1.41 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364267609796682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364267609796682 | validation: 1.0191114321086554]
	TIME [epoch: 1.41 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879403916067432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879403916067432 | validation: 0.9918674439260355]
	TIME [epoch: 1.41 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636787669451371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7636787669451371 | validation: 1.032823425215797]
	TIME [epoch: 1.41 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7303296417781515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7303296417781515 | validation: 0.9699467104219466]
	TIME [epoch: 1.41 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993795354643456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993795354643456 | validation: 0.9025626720506174]
	TIME [epoch: 1.41 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6923865806531438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6923865806531438 | validation: 0.782629975405944]
	TIME [epoch: 1.41 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666020448948242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6666020448948242 | validation: 0.750146498983516]
	TIME [epoch: 1.41 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510846958783555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6510846958783555 | validation: 0.7308468852468823]
	TIME [epoch: 1.41 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6393838478718129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6393838478718129 | validation: 0.6926645378538082]
	TIME [epoch: 1.41 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156878741958343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6156878741958343 | validation: 0.6344552085493158]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.586008721611106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.586008721611106 | validation: 0.6011357077780741]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5737602925249589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5737602925249589 | validation: 0.8805665683450284]
	TIME [epoch: 2.78 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9738985784390656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9738985784390656 | validation: 1.2510245665155815]
	TIME [epoch: 2.78 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9600026969008093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9600026969008093 | validation: 1.0414255740914242]
	TIME [epoch: 2.77 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748520801515434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7748520801515434 | validation: 1.0767309366916933]
	TIME [epoch: 2.77 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098865339000078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098865339000078 | validation: 1.0153536008286284]
	TIME [epoch: 2.77 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.747021443442545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.747021443442545 | validation: 1.0266829282228962]
	TIME [epoch: 2.78 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7240477393992625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7240477393992625 | validation: 0.9913185797690249]
	TIME [epoch: 2.77 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944894181300754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944894181300754 | validation: 0.9421349370713051]
	TIME [epoch: 2.77 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6831802740241679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6831802740241679 | validation: 0.8517537498249242]
	TIME [epoch: 2.77 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6562381741905803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6562381741905803 | validation: 0.7948114097550777]
	TIME [epoch: 2.77 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459030204600044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6459030204600044 | validation: 0.7403045390827869]
	TIME [epoch: 2.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.619756026204644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.619756026204644 | validation: 0.6896978907399836]
	TIME [epoch: 2.78 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.595427348708666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.595427348708666 | validation: 0.6668538811882141]
	TIME [epoch: 2.77 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5832220310596433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5832220310596433 | validation: 0.6301965325292156]
	TIME [epoch: 2.78 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5788362461124633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5788362461124633 | validation: 0.8542979360104539]
	TIME [epoch: 2.77 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418928442760962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8418928442760962 | validation: 1.1129562119068985]
	TIME [epoch: 2.78 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8852778924909839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8852778924909839 | validation: 0.9859119131663494]
	TIME [epoch: 2.78 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426516735930193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6426516735930193 | validation: 0.888029721654608]
	TIME [epoch: 2.77 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940267047072487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6940267047072487 | validation: 0.7280718922133145]
	TIME [epoch: 2.78 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898974075584132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898974075584132 | validation: 0.6615236979211878]
	TIME [epoch: 2.78 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5750677306808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5750677306808 | validation: 0.6327209810643724]
	TIME [epoch: 2.77 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5409943044017372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5409943044017372 | validation: 0.6338098289237633]
	TIME [epoch: 2.77 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5273958770282454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5273958770282454 | validation: 0.6029948427095685]
	TIME [epoch: 2.77 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5452742876620903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5452742876620903 | validation: 0.7502197611636691]
	TIME [epoch: 2.78 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5930927798888704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5930927798888704 | validation: 0.6583836333527293]
	TIME [epoch: 2.77 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5174302997693359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5174302997693359 | validation: 0.5599947193570552]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47524264315343523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47524264315343523 | validation: 0.5470287215759649]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4697366512879617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4697366512879617 | validation: 0.543266408958942]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_230.pth
	Model improved!!!
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46211460309453367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46211460309453367 | validation: 0.6681252827367854]
	TIME [epoch: 2.78 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.495214100493353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.495214100493353 | validation: 0.6193802377777614]
	TIME [epoch: 2.77 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5644497785001032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5644497785001032 | validation: 1.0525244169550134]
	TIME [epoch: 2.77 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7106768884512648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7106768884512648 | validation: 0.6706585577069947]
	TIME [epoch: 2.77 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.544228931010928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.544228931010928 | validation: 0.5859554029425164]
	TIME [epoch: 2.77 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4897075202845933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4897075202845933 | validation: 0.58557916087418]
	TIME [epoch: 2.77 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544665404021572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4544665404021572 | validation: 0.5140446925646103]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45514586986209454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45514586986209454 | validation: 0.7503790308083279]
	TIME [epoch: 2.78 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499619227713961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5499619227713961 | validation: 0.7037675711945716]
	TIME [epoch: 2.77 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020479859456393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5020479859456393 | validation: 0.503937024627529]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40328400813407256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40328400813407256 | validation: 0.49870526103710644]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_241.pth
	Model improved!!!
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4285122470283476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4285122470283476 | validation: 0.517796717675412]
	TIME [epoch: 2.78 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3864286111417524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3864286111417524 | validation: 0.5659448711803684]
	TIME [epoch: 2.78 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4113818538228482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4113818538228482 | validation: 0.5298834638253007]
	TIME [epoch: 2.78 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4944442499201116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4944442499201116 | validation: 1.0810278857843134]
	TIME [epoch: 2.79 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361880997256725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7361880997256725 | validation: 0.8197144645519823]
	TIME [epoch: 2.79 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5117598124317486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5117598124317486 | validation: 0.56264220535987]
	TIME [epoch: 2.78 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39593061158588766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39593061158588766 | validation: 0.4752543235211166]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39919430509639386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39919430509639386 | validation: 0.5174721295348258]
	TIME [epoch: 2.78 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3673389963486895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3673389963486895 | validation: 0.5147471221351964]
	TIME [epoch: 2.78 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39190776645706654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39190776645706654 | validation: 0.5970704557587478]
	TIME [epoch: 2.78 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3997318295731012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3997318295731012 | validation: 0.46736366969751847]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3619081301151556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3619081301151556 | validation: 0.5956804003368755]
	TIME [epoch: 2.78 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43528234076102196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43528234076102196 | validation: 0.4525769842155769]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3190701956579607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3190701956579607 | validation: 0.4612326715819881]
	TIME [epoch: 2.78 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3544877511169891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3544877511169891 | validation: 0.7228086291674947]
	TIME [epoch: 2.79 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47941339719271825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47941339719271825 | validation: 0.5118947160422105]
	TIME [epoch: 2.79 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3678789957685546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3678789957685546 | validation: 0.5330257749042341]
	TIME [epoch: 2.79 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4618957322712272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4618957322712272 | validation: 0.9783595650508797]
	TIME [epoch: 2.78 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6168499767613299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6168499767613299 | validation: 0.78020357606849]
	TIME [epoch: 2.79 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4482744325405071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4482744325405071 | validation: 0.5141493870785746]
	TIME [epoch: 2.79 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34763649590907053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34763649590907053 | validation: 0.4283918769452331]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3365802347022845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3365802347022845 | validation: 0.49337521966556913]
	TIME [epoch: 2.79 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3274826340042714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3274826340042714 | validation: 0.4772452873370508]
	TIME [epoch: 2.79 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30625662084345157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30625662084345157 | validation: 0.40404501566288054]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2729354290406824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2729354290406824 | validation: 0.3969790611831302]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26270891981981115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26270891981981115 | validation: 0.45640491530067484]
	TIME [epoch: 2.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2773155830336393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2773155830336393 | validation: 0.4027807689347882]
	TIME [epoch: 2.79 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2807683964146709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2807683964146709 | validation: 0.603065487457323]
	TIME [epoch: 2.79 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4287532602856851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4287532602856851 | validation: 0.4042795002833402]
	TIME [epoch: 2.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23171914700855462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23171914700855462 | validation: 0.38835871884896345]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533433314151099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2533433314151099 | validation: 0.6127917531939993]
	TIME [epoch: 2.79 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38036701175812376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38036701175812376 | validation: 0.43451006623072885]
	TIME [epoch: 2.79 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23713384664975468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23713384664975468 | validation: 0.4946925646081864]
	TIME [epoch: 2.79 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4124976603147523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4124976603147523 | validation: 0.860075864638586]
	TIME [epoch: 2.79 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5443648646509713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5443648646509713 | validation: 0.701797066976761]
	TIME [epoch: 2.79 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43933871921165163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43933871921165163 | validation: 0.4241121369726473]
	TIME [epoch: 2.79 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.238238804695154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.238238804695154 | validation: 0.45161962407473477]
	TIME [epoch: 2.79 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33046837904431753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33046837904431753 | validation: 0.531604845892112]
	TIME [epoch: 2.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28839136440754987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28839136440754987 | validation: 0.46579813555721716]
	TIME [epoch: 2.79 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727686796111604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2727686796111604 | validation: 0.37411885834955955]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_281.pth
	Model improved!!!
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23436578040951656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23436578040951656 | validation: 0.3588766185740219]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22139559881588328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22139559881588328 | validation: 0.4704845920214231]
	TIME [epoch: 2.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2402657373062555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2402657373062555 | validation: 0.3347021526113388]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_284.pth
	Model improved!!!
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2379757584850341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2379757584850341 | validation: 0.41435024575999524]
	TIME [epoch: 2.79 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24325876271035443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24325876271035443 | validation: 0.39061722955206685]
	TIME [epoch: 2.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26625321375715966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26625321375715966 | validation: 0.37425529669224394]
	TIME [epoch: 2.79 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19150515053785996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19150515053785996 | validation: 0.3366878938051957]
	TIME [epoch: 2.79 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.176348963584365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.176348963584365 | validation: 0.323608679431506]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16783089096387865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16783089096387865 | validation: 0.460344978209933]
	TIME [epoch: 2.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18675758363704212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18675758363704212 | validation: 0.3897316896896487]
	TIME [epoch: 2.81 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2698360155004797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2698360155004797 | validation: 0.9291230310338234]
	TIME [epoch: 2.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5364391815603908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5364391815603908 | validation: 1.0636863647974708]
	TIME [epoch: 2.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964267543161834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5964267543161834 | validation: 0.6867071400851267]
	TIME [epoch: 2.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2951417336056186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2951417336056186 | validation: 0.5530298524936629]
	TIME [epoch: 2.81 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4609406650551337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4609406650551337 | validation: 0.537498785024486]
	TIME [epoch: 2.81 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5276049465234334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5276049465234334 | validation: 0.6005920320838622]
	TIME [epoch: 2.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5717701998617577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5717701998617577 | validation: 0.5416113381046037]
	TIME [epoch: 2.79 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2550985271613201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2550985271613201 | validation: 0.5135126625417517]
	TIME [epoch: 2.79 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37395371970120517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37395371970120517 | validation: 0.39983541469464856]
	TIME [epoch: 2.79 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22885440236703225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22885440236703225 | validation: 0.39695807218201934]
	TIME [epoch: 2.79 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.229987048499211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.229987048499211 | validation: 0.36126447682380247]
	TIME [epoch: 2.79 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19538028587691617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19538028587691617 | validation: 0.35930953909770147]
	TIME [epoch: 2.8 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19864380322778516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19864380322778516 | validation: 0.3631681953329984]
	TIME [epoch: 2.82 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1830600015929598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1830600015929598 | validation: 0.3549557264520796]
	TIME [epoch: 2.79 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731801676667548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1731801676667548 | validation: 0.33667373526796995]
	TIME [epoch: 2.79 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16654418366292265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16654418366292265 | validation: 0.38151357267908237]
	TIME [epoch: 2.79 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16282373279352083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16282373279352083 | validation: 0.3379820923192962]
	TIME [epoch: 2.79 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652931459227116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1652931459227116 | validation: 0.39383282381294865]
	TIME [epoch: 2.78 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18525369735288258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18525369735288258 | validation: 0.4355207854629397]
	TIME [epoch: 2.78 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25176772720149926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25176772720149926 | validation: 0.537674622495572]
	TIME [epoch: 2.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33042617526348694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33042617526348694 | validation: 0.3855718815026682]
	TIME [epoch: 2.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16178565170069617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16178565170069617 | validation: 0.3709240854002096]
	TIME [epoch: 2.79 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21371961245002846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21371961245002846 | validation: 0.7542725893865513]
	TIME [epoch: 2.79 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3474918013556082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3474918013556082 | validation: 0.47100168949935617]
	TIME [epoch: 2.79 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1727505936942139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1727505936942139 | validation: 0.4424036512803963]
	TIME [epoch: 2.79 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2989539659156701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2989539659156701 | validation: 0.5349709303359078]
	TIME [epoch: 2.79 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22537732880000433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22537732880000433 | validation: 0.5621502795315535]
	TIME [epoch: 2.79 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19965751101199375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19965751101199375 | validation: 0.35511268201167356]
	TIME [epoch: 2.8 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18340166606649447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18340166606649447 | validation: 0.36416383086611454]
	TIME [epoch: 2.79 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16315463475935216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16315463475935216 | validation: 0.4511930587166754]
	TIME [epoch: 2.79 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16005855067653343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16005855067653343 | validation: 0.32851100583702336]
	TIME [epoch: 2.79 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15566395072777256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15566395072777256 | validation: 0.42408339374897414]
	TIME [epoch: 2.79 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16320365046344637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16320365046344637 | validation: 0.36996578840559247]
	TIME [epoch: 2.8 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1927061561467754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927061561467754 | validation: 0.4347929313796888]
	TIME [epoch: 2.8 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24151150982599015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24151150982599015 | validation: 0.4945214209539313]
	TIME [epoch: 2.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2698759629596771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2698759629596771 | validation: 0.34992055524919885]
	TIME [epoch: 2.8 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402001987063094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1402001987063094 | validation: 0.34634315296158813]
	TIME [epoch: 2.8 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14249952123245535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14249952123245535 | validation: 0.39628442137971553]
	TIME [epoch: 2.8 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14960304559517457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14960304559517457 | validation: 0.34013654024290385]
	TIME [epoch: 2.8 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14570437245647952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14570437245647952 | validation: 0.4497666221393426]
	TIME [epoch: 2.79 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15678145229158902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15678145229158902 | validation: 0.3618912987561821]
	TIME [epoch: 2.79 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16472074057291888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16472074057291888 | validation: 0.7044007703968789]
	TIME [epoch: 2.79 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31342240152262835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31342240152262835 | validation: 0.502532412315315]
	TIME [epoch: 2.79 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15809607881237786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15809607881237786 | validation: 0.3979459918115893]
	TIME [epoch: 2.79 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2760413139995528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2760413139995528 | validation: 0.6111127419699439]
	TIME [epoch: 2.79 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20376763210169702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20376763210169702 | validation: 0.5051570121421268]
	TIME [epoch: 2.79 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17793160672492542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17793160672492542 | validation: 0.3246202935491971]
	TIME [epoch: 2.79 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19617235491854346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19617235491854346 | validation: 0.38512731538645456]
	TIME [epoch: 2.79 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.164836712714473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.164836712714473 | validation: 0.4422550097253972]
	TIME [epoch: 2.79 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15828276278599154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15828276278599154 | validation: 0.34586538991015375]
	TIME [epoch: 2.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14343300407632747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14343300407632747 | validation: 0.3458416346591537]
	TIME [epoch: 2.79 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14837270340096695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14837270340096695 | validation: 0.4007526297698363]
	TIME [epoch: 2.79 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16980822501045637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16980822501045637 | validation: 0.3630494771823912]
	TIME [epoch: 2.79 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1982683221311212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1982683221311212 | validation: 0.40737999407949366]
	TIME [epoch: 2.79 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18104285119840846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18104285119840846 | validation: 0.5074794800972703]
	TIME [epoch: 2.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22177640185243852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22177640185243852 | validation: 0.3607424738081464]
	TIME [epoch: 2.79 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15115296433925468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15115296433925468 | validation: 0.38472295495408715]
	TIME [epoch: 2.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12615701381260558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12615701381260558 | validation: 0.342368147151096]
	TIME [epoch: 2.79 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179356983757589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1179356983757589 | validation: 0.37793385703004384]
	TIME [epoch: 2.79 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12092659871939095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12092659871939095 | validation: 0.33456754618327883]
	TIME [epoch: 2.79 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12146516920920483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12146516920920483 | validation: 0.38183905736985246]
	TIME [epoch: 2.79 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12990333704330365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12990333704330365 | validation: 0.4024777631022774]
	TIME [epoch: 2.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13396118844271784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13396118844271784 | validation: 0.41046571532092646]
	TIME [epoch: 2.79 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18008061091325492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18008061091325492 | validation: 0.7416209266791925]
	TIME [epoch: 2.79 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43010837168142124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43010837168142124 | validation: 0.4316978988609325]
	TIME [epoch: 2.79 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13037917798553608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13037917798553608 | validation: 0.5363703836153567]
	TIME [epoch: 2.79 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40149927465983526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40149927465983526 | validation: 0.6875554222150941]
	TIME [epoch: 2.79 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3661968043016873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3661968043016873 | validation: 0.807578534800256]
	TIME [epoch: 2.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34052334726877337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34052334726877337 | validation: 0.5163868732929698]
	TIME [epoch: 2.79 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.189732035303513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.189732035303513 | validation: 0.3233155774306766]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_361.pth
	Model improved!!!
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1963220398613775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1963220398613775 | validation: 0.36196196574498585]
	TIME [epoch: 2.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13013735501780377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13013735501780377 | validation: 0.44610653380920573]
	TIME [epoch: 2.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13857596934569727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13857596934569727 | validation: 0.3409489011127824]
	TIME [epoch: 2.8 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12040039138313514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12040039138313514 | validation: 0.33969707534234994]
	TIME [epoch: 2.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1320036254158398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1320036254158398 | validation: 0.41293344025656503]
	TIME [epoch: 2.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13237035872918135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13237035872918135 | validation: 0.38775027359594655]
	TIME [epoch: 2.79 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13887089502685793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13887089502685793 | validation: 0.319467854601554]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16322931706363578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16322931706363578 | validation: 0.48126148687844605]
	TIME [epoch: 2.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.195302401306255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.195302401306255 | validation: 0.3951624176599548]
	TIME [epoch: 2.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15808267741163698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15808267741163698 | validation: 0.33573657627645687]
	TIME [epoch: 2.79 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13484731165235642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13484731165235642 | validation: 0.3816869407172665]
	TIME [epoch: 2.79 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11705118472787626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11705118472787626 | validation: 0.32983348167302945]
	TIME [epoch: 2.79 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10657046123800416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10657046123800416 | validation: 0.35447999151082843]
	TIME [epoch: 2.79 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10272982800098984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10272982800098984 | validation: 0.35581144290351074]
	TIME [epoch: 2.79 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067614964080676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10067614964080676 | validation: 0.3054598361820616]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12208362452355408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12208362452355408 | validation: 0.5756624717366927]
	TIME [epoch: 2.79 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17210088287644495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17210088287644495 | validation: 0.40248849719633695]
	TIME [epoch: 2.79 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24770952174348732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24770952174348732 | validation: 0.5422675295647527]
	TIME [epoch: 2.79 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15870875328897194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15870875328897194 | validation: 0.5004640496713849]
	TIME [epoch: 2.8 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24752152292231094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24752152292231094 | validation: 0.3986308564337153]
	TIME [epoch: 2.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2212386906804551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2212386906804551 | validation: 0.3341985287556535]
	TIME [epoch: 2.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1188324120421157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1188324120421157 | validation: 0.3549663045921938]
	TIME [epoch: 2.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13044832523464567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13044832523464567 | validation: 0.4387901922791474]
	TIME [epoch: 2.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14198148597220736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14198148597220736 | validation: 0.3487340006121194]
	TIME [epoch: 2.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13039683509493408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13039683509493408 | validation: 0.35473226404554653]
	TIME [epoch: 2.8 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12300297893860354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12300297893860354 | validation: 0.33968176603321065]
	TIME [epoch: 2.79 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11300943746922251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11300943746922251 | validation: 0.38087257770859684]
	TIME [epoch: 2.8 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10839916918188643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10839916918188643 | validation: 0.3592230230187335]
	TIME [epoch: 2.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12131425193056092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12131425193056092 | validation: 0.45163215318587907]
	TIME [epoch: 2.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14802392310989237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14802392310989237 | validation: 0.41270106722479555]
	TIME [epoch: 2.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1789447161008921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1789447161008921 | validation: 0.5316418332424606]
	TIME [epoch: 2.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116717543370235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3116717543370235 | validation: 0.3987180547773525]
	TIME [epoch: 2.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13428985596580348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13428985596580348 | validation: 0.5292937065647628]
	TIME [epoch: 2.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30998511982062626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30998511982062626 | validation: 0.7149125299322412]
	TIME [epoch: 2.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2746142387899563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2746142387899563 | validation: 0.3093312778765382]
	TIME [epoch: 2.8 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15233313892567277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15233313892567277 | validation: 0.4319858686853697]
	TIME [epoch: 2.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2584076526521249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2584076526521249 | validation: 0.5901008369906527]
	TIME [epoch: 2.8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20730869574903643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20730869574903643 | validation: 0.4367516182912337]
	TIME [epoch: 2.81 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16147407282029114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16147407282029114 | validation: 0.30300631211839224]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_400.pth
	Model improved!!!
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13766230202394578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13766230202394578 | validation: 0.3805862286479684]
	TIME [epoch: 2.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11893405590484614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11893405590484614 | validation: 0.36978987087607584]
	TIME [epoch: 2.8 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149208530812572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149208530812572 | validation: 0.3365055699747378]
	TIME [epoch: 2.8 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1130123326263612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1130123326263612 | validation: 0.34468919142901194]
	TIME [epoch: 2.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10137628665489694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10137628665489694 | validation: 0.36035120345335997]
	TIME [epoch: 2.79 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10299875006273425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10299875006273425 | validation: 0.3263819258359155]
	TIME [epoch: 2.8 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13261516582482355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13261516582482355 | validation: 0.46634617158093206]
	TIME [epoch: 2.8 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1344593399495483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1344593399495483 | validation: 0.35970179608097785]
	TIME [epoch: 2.8 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12108887985116543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12108887985116543 | validation: 0.4123373955931229]
	TIME [epoch: 2.8 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19915713523649992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19915713523649992 | validation: 0.5172552028188463]
	TIME [epoch: 2.79 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22295428964342637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22295428964342637 | validation: 0.33559315259474554]
	TIME [epoch: 2.79 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10611226577077133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10611226577077133 | validation: 0.3620615346400567]
	TIME [epoch: 2.79 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13911644500857695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13911644500857695 | validation: 0.3840880565302319]
	TIME [epoch: 2.8 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12933625574261837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12933625574261837 | validation: 0.34843045607883943]
	TIME [epoch: 2.8 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09801294230070376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09801294230070376 | validation: 0.3193069062635931]
	TIME [epoch: 2.79 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09234869581857628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09234869581857628 | validation: 0.4044123819966787]
	TIME [epoch: 2.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0983365344224675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0983365344224675 | validation: 0.33621630687586745]
	TIME [epoch: 2.8 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09048304709638526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09048304709638526 | validation: 0.41066665752130826]
	TIME [epoch: 2.8 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09598754361508506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09598754361508506 | validation: 0.3082357651301492]
	TIME [epoch: 2.8 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11837025846487055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11837025846487055 | validation: 0.7928733008073796]
	TIME [epoch: 2.8 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33319055186493557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33319055186493557 | validation: 0.30872770466222116]
	TIME [epoch: 2.8 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1323263637723273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1323263637723273 | validation: 0.28892280607561077]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09772007892784085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09772007892784085 | validation: 0.3744598725846586]
	TIME [epoch: 2.8 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11826662935341549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11826662935341549 | validation: 0.2923420528566338]
	TIME [epoch: 2.8 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1097125637626126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1097125637626126 | validation: 0.35248814223277036]
	TIME [epoch: 2.8 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1611847386317455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1611847386317455 | validation: 0.509228170261989]
	TIME [epoch: 2.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20512066517351701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20512066517351701 | validation: 0.38877435643472863]
	TIME [epoch: 2.8 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11311649918000445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11311649918000445 | validation: 0.29522223014912513]
	TIME [epoch: 2.81 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09960622856727752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09960622856727752 | validation: 0.3636186080542203]
	TIME [epoch: 2.8 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11120472702326079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11120472702326079 | validation: 0.35213469742058423]
	TIME [epoch: 2.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11418156347852194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11418156347852194 | validation: 0.34874686474666067]
	TIME [epoch: 2.79 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14254910002648827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14254910002648827 | validation: 0.32425425933144114]
	TIME [epoch: 2.79 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15300626153203237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15300626153203237 | validation: 0.5155997235885227]
	TIME [epoch: 2.79 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1406223630982205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1406223630982205 | validation: 0.3062092625605281]
	TIME [epoch: 2.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11439322768828852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11439322768828852 | validation: 0.3420684356099389]
	TIME [epoch: 2.79 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10063665900579263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10063665900579263 | validation: 0.3376065704418955]
	TIME [epoch: 2.79 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09369935909967053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09369935909967053 | validation: 0.2829484845893934]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551082562917348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10551082562917348 | validation: 0.46897703964010234]
	TIME [epoch: 2.8 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12986091064648048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12986091064648048 | validation: 0.2937999830539514]
	TIME [epoch: 2.8 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0953426068177911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0953426068177911 | validation: 0.3116425382196386]
	TIME [epoch: 2.8 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08750659286265554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08750659286265554 | validation: 0.32339336555236564]
	TIME [epoch: 2.8 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08477754539450061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08477754539450061 | validation: 0.31766839463131674]
	TIME [epoch: 2.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511306390802154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09511306390802154 | validation: 0.32274007288013007]
	TIME [epoch: 2.8 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12111745423487497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12111745423487497 | validation: 0.41516093062012943]
	TIME [epoch: 2.8 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12687946931948269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12687946931948269 | validation: 0.29754135972095613]
	TIME [epoch: 2.79 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14039122934437656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14039122934437656 | validation: 0.4220207920154893]
	TIME [epoch: 2.8 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20786424393046068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20786424393046068 | validation: 0.5505056454595127]
	TIME [epoch: 2.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1795356466667613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1795356466667613 | validation: 0.34209797933302144]
	TIME [epoch: 2.8 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13332932992715704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13332932992715704 | validation: 0.40393940817678065]
	TIME [epoch: 2.79 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10850530314196256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10850530314196256 | validation: 0.29402996280912813]
	TIME [epoch: 2.8 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09581429663339502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09581429663339502 | validation: 0.45661777424668915]
	TIME [epoch: 2.79 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10784544527029179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10784544527029179 | validation: 0.320769024503566]
	TIME [epoch: 2.8 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12285327055088732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12285327055088732 | validation: 0.5233540697126222]
	TIME [epoch: 2.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12841504310458707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12841504310458707 | validation: 0.3151374169312975]
	TIME [epoch: 2.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08521459359435696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08521459359435696 | validation: 0.29099278341502965]
	TIME [epoch: 2.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09153765041415177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09153765041415177 | validation: 0.4385008771054646]
	TIME [epoch: 2.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10494959486317575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10494959486317575 | validation: 0.2763609136302234]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12730460926619558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12730460926619558 | validation: 0.39129931941395596]
	TIME [epoch: 2.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18108208297799727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18108208297799727 | validation: 0.5628601766705205]
	TIME [epoch: 2.81 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.204324478033246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.204324478033246 | validation: 0.357448192982903]
	TIME [epoch: 2.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11630476380668492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11630476380668492 | validation: 0.33935349494919503]
	TIME [epoch: 2.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12975665005509168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12975665005509168 | validation: 0.5550281733271349]
	TIME [epoch: 2.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558594835450877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558594835450877 | validation: 0.28234366163529245]
	TIME [epoch: 2.79 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867121423247007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08867121423247007 | validation: 0.43918117250840255]
	TIME [epoch: 2.79 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10574122265252507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10574122265252507 | validation: 0.28842761477205525]
	TIME [epoch: 2.8 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10489235897328879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10489235897328879 | validation: 0.33343606011742355]
	TIME [epoch: 2.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09560129718010835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09560129718010835 | validation: 0.27869182123942915]
	TIME [epoch: 2.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08703418993254587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08703418993254587 | validation: 0.2766869762361459]
	TIME [epoch: 2.8 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08945778702377297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08945778702377297 | validation: 0.3523776607223233]
	TIME [epoch: 2.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15668256480360718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15668256480360718 | validation: 0.3835850604181418]
	TIME [epoch: 2.79 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28408041859706046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28408041859706046 | validation: 0.30552924472779863]
	TIME [epoch: 2.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1137258288953906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1137258288953906 | validation: 0.31784429825071814]
	TIME [epoch: 2.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796227531749924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09796227531749924 | validation: 0.32548649490577014]
	TIME [epoch: 2.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09338108395198394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09338108395198394 | validation: 0.301616170792229]
	TIME [epoch: 2.79 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07961029085974337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07961029085974337 | validation: 0.3023443558935617]
	TIME [epoch: 2.79 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07885555835772223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07885555835772223 | validation: 0.26983556190299895]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_476.pth
	Model improved!!!
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07193712688124487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07193712688124487 | validation: 0.3313859852602365]
	TIME [epoch: 2.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848460985280394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07848460985280394 | validation: 0.3047454457286812]
	TIME [epoch: 2.79 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910389868923481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10910389868923481 | validation: 0.6650852772373566]
	TIME [epoch: 2.79 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20011036263495421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20011036263495421 | validation: 0.299470224773323]
	TIME [epoch: 2.79 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11772453983415516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11772453983415516 | validation: 0.2953196386792945]
	TIME [epoch: 2.79 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14517535439214696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14517535439214696 | validation: 0.38558504752390577]
	TIME [epoch: 2.79 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12830406093165053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12830406093165053 | validation: 0.27625628146598413]
	TIME [epoch: 2.79 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1045832483963946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1045832483963946 | validation: 0.30601741255510884]
	TIME [epoch: 2.79 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11895376802394605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11895376802394605 | validation: 0.46131455841783603]
	TIME [epoch: 2.79 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12147690142740344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12147690142740344 | validation: 0.3140750278274352]
	TIME [epoch: 2.78 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10368638904976997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10368638904976997 | validation: 0.2780222813408318]
	TIME [epoch: 2.79 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09107585744853866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09107585744853866 | validation: 0.4101296531927087]
	TIME [epoch: 2.78 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08807438934417784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08807438934417784 | validation: 0.2965393192879583]
	TIME [epoch: 2.79 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09764957653586027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09764957653586027 | validation: 0.507632267450423]
	TIME [epoch: 2.79 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13875803477346463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13875803477346463 | validation: 0.33891182157935024]
	TIME [epoch: 2.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11192325466525617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11192325466525617 | validation: 0.26339127996003125]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09395288769958667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09395288769958667 | validation: 0.48266321440285864]
	TIME [epoch: 2.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10536158026132342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10536158026132342 | validation: 0.27103151157150995]
	TIME [epoch: 2.79 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0748123762273233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0748123762273233 | validation: 0.2630708134996309]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_495.pth
	Model improved!!!
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09113438927218677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09113438927218677 | validation: 0.3886090923568447]
	TIME [epoch: 2.79 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10335959098797075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10335959098797075 | validation: 0.27495675453021756]
	TIME [epoch: 2.79 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09506954181209726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09506954181209726 | validation: 0.2707487932921806]
	TIME [epoch: 2.79 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792421756910069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07792421756910069 | validation: 0.312141356346934]
	TIME [epoch: 2.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0708248559285256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0708248559285256 | validation: 0.2511888344384614]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_500.pth
	Model improved!!!
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07700530701488316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07700530701488316 | validation: 0.3861234909172638]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19534142551945952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19534142551945952 | validation: 0.47929449429735]
	TIME [epoch: 5.98 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32166608035604677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32166608035604677 | validation: 0.2873773230241981]
	TIME [epoch: 5.97 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1910595857359801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1910595857359801 | validation: 0.5240132869336444]
	TIME [epoch: 5.97 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14039260258525169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14039260258525169 | validation: 0.31707203747408785]
	TIME [epoch: 5.97 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874778361045738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07874778361045738 | validation: 0.27053013879707927]
	TIME [epoch: 5.96 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08039202151714626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08039202151714626 | validation: 0.46139835715658606]
	TIME [epoch: 5.98 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09628321348847484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09628321348847484 | validation: 0.3588198415245384]
	TIME [epoch: 5.97 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1721929944496827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1721929944496827 | validation: 0.574007294088793]
	TIME [epoch: 5.98 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18775182517938405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18775182517938405 | validation: 0.503354106731596]
	TIME [epoch: 5.97 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1277092483919577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1277092483919577 | validation: 0.30931633903612965]
	TIME [epoch: 5.97 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10407691611477145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10407691611477145 | validation: 0.2876937124677016]
	TIME [epoch: 5.97 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09151618528417199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09151618528417199 | validation: 0.35387102600406056]
	TIME [epoch: 5.98 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09019907488956345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09019907488956345 | validation: 0.32390766675594257]
	TIME [epoch: 5.97 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801752254003762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0801752254003762 | validation: 0.34680170766234214]
	TIME [epoch: 5.97 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11410586355059538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11410586355059538 | validation: 0.3269966203783072]
	TIME [epoch: 5.97 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07535587298825855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07535587298825855 | validation: 0.2778388621813002]
	TIME [epoch: 5.97 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08835917089386085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08835917089386085 | validation: 0.34994134378017683]
	TIME [epoch: 5.97 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08231478508373481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08231478508373481 | validation: 0.2739613849993337]
	TIME [epoch: 5.97 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07303441071026939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07303441071026939 | validation: 0.3495497914129427]
	TIME [epoch: 5.97 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07611621999121923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07611621999121923 | validation: 0.26886923855324246]
	TIME [epoch: 5.98 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07078108891779884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07078108891779884 | validation: 0.301237881891544]
	TIME [epoch: 5.98 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339299601816382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07339299601816382 | validation: 0.27634234472057656]
	TIME [epoch: 5.99 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229599142958347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09229599142958347 | validation: 0.23298067012697105]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_524.pth
	Model improved!!!
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1561464426429668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1561464426429668 | validation: 0.3968950797922066]
	TIME [epoch: 5.99 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23390737306186038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23390737306186038 | validation: 0.39549734409071335]
	TIME [epoch: 5.99 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08974785331367827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08974785331367827 | validation: 0.2464350848689305]
	TIME [epoch: 5.99 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08164428395626752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08164428395626752 | validation: 0.25240905567598587]
	TIME [epoch: 5.98 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242565728398144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07242565728398144 | validation: 0.2583283975497753]
	TIME [epoch: 5.98 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07344818094074639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07344818094074639 | validation: 0.3539337322638861]
	TIME [epoch: 6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07922431206153988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07922431206153988 | validation: 0.2565673977152416]
	TIME [epoch: 5.98 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0747929036805137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0747929036805137 | validation: 0.4869705452463233]
	TIME [epoch: 6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11122501691888716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11122501691888716 | validation: 0.31379823011595487]
	TIME [epoch: 5.98 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13317094484571057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13317094484571057 | validation: 0.5888078449807354]
	TIME [epoch: 5.99 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563446544460131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1563446544460131 | validation: 0.2523228125371455]
	TIME [epoch: 5.98 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0901690842879268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0901690842879268 | validation: 0.2574528945866404]
	TIME [epoch: 5.99 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08809458510609393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08809458510609393 | validation: 0.3299927710453944]
	TIME [epoch: 5.98 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08411133816995589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08411133816995589 | validation: 0.2873869129927675]
	TIME [epoch: 5.99 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704242616086349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704242616086349 | validation: 0.24715831325575885]
	TIME [epoch: 5.97 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08423591783075121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08423591783075121 | validation: 0.3614478122390882]
	TIME [epoch: 5.99 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08785260873584576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08785260873584576 | validation: 0.2262047356832207]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07260626580180847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07260626580180847 | validation: 0.26424633131425285]
	TIME [epoch: 5.99 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07822799894239549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07822799894239549 | validation: 0.3666342329727113]
	TIME [epoch: 5.99 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822728922809539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0822728922809539 | validation: 0.2418266652781127]
	TIME [epoch: 5.99 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299709194578866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08299709194578866 | validation: 0.3099745425257796]
	TIME [epoch: 5.99 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13009199136378896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13009199136378896 | validation: 0.3330741091208707]
	TIME [epoch: 5.99 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15367951581042233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15367951581042233 | validation: 0.27843168741663143]
	TIME [epoch: 5.99 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08173542205458713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08173542205458713 | validation: 0.24235581680182838]
	TIME [epoch: 5.99 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10383713279881064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10383713279881064 | validation: 0.3126499560908307]
	TIME [epoch: 5.99 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11909828546679066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11909828546679066 | validation: 0.2878245924248454]
	TIME [epoch: 5.99 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09909347075560966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09909347075560966 | validation: 0.6361351209025075]
	TIME [epoch: 5.99 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17346873402481605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17346873402481605 | validation: 0.2767853060471956]
	TIME [epoch: 5.99 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1034016330511387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1034016330511387 | validation: 0.23912313179812356]
	TIME [epoch: 5.99 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06576750739372665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06576750739372665 | validation: 0.37167103250661926]
	TIME [epoch: 5.99 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299574493039049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07299574493039049 | validation: 0.3157277932895342]
	TIME [epoch: 5.98 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07551032052387915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07551032052387915 | validation: 0.2738387299505974]
	TIME [epoch: 5.99 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08323022913741294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08323022913741294 | validation: 0.41528879313267525]
	TIME [epoch: 5.98 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09769041145456239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09769041145456239 | validation: 0.2681553950294668]
	TIME [epoch: 5.99 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07432712590885124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07432712590885124 | validation: 0.20829744137845055]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447886460476656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07447886460476656 | validation: 0.2798215980394215]
	TIME [epoch: 5.98 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1127361459276537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1127361459276537 | validation: 0.32459618910633203]
	TIME [epoch: 5.99 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08152041850702707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08152041850702707 | validation: 0.23936516441984215]
	TIME [epoch: 5.99 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524497615610944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524497615610944 | validation: 0.23441758458260833]
	TIME [epoch: 5.98 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446446271024297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05446446271024297 | validation: 0.2560034206088259]
	TIME [epoch: 5.98 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05325743733175333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05325743733175333 | validation: 0.22074109332555844]
	TIME [epoch: 5.98 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05267280783765224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05267280783765224 | validation: 0.5140058430702713]
	TIME [epoch: 5.99 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09622446840886667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09622446840886667 | validation: 0.30216618057824696]
	TIME [epoch: 5.99 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12329049041700632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12329049041700632 | validation: 0.39124822832572437]
	TIME [epoch: 5.99 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11219933397847802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11219933397847802 | validation: 0.3376012246551636]
	TIME [epoch: 5.98 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295510515789861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1295510515789861 | validation: 0.2245948332386959]
	TIME [epoch: 5.99 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07034704720837887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07034704720837887 | validation: 0.2443883754164909]
	TIME [epoch: 5.98 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06141123352616933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06141123352616933 | validation: 0.2384586197140303]
	TIME [epoch: 5.99 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08593587696503682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08593587696503682 | validation: 0.28860042783387646]
	TIME [epoch: 5.98 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09098464602177572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09098464602177572 | validation: 0.23129669792217006]
	TIME [epoch: 5.99 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09195552666084526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09195552666084526 | validation: 0.22918119460086547]
	TIME [epoch: 5.99 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0698100793897934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0698100793897934 | validation: 0.24301817971127815]
	TIME [epoch: 6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09982853070268143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09982853070268143 | validation: 0.20586334317972185]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05076293481492556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05076293481492556 | validation: 0.19202908766829904]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_578.pth
	Model improved!!!
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05824458467190946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05824458467190946 | validation: 0.4199492428615411]
	TIME [epoch: 5.99 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337022507462228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09337022507462228 | validation: 0.22160383941301853]
	TIME [epoch: 5.99 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07412233384032922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07412233384032922 | validation: 0.2932559397822829]
	TIME [epoch: 6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08598015440627055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08598015440627055 | validation: 0.22715067351100152]
	TIME [epoch: 5.99 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09291962558050047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09291962558050047 | validation: 0.47640829431361437]
	TIME [epoch: 6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11607346575614141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11607346575614141 | validation: 0.22407644130824905]
	TIME [epoch: 5.99 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08164397299926687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08164397299926687 | validation: 0.22344750140104608]
	TIME [epoch: 6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055483338218648995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055483338218648995 | validation: 0.1988886663373128]
	TIME [epoch: 5.99 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0626689298933052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0626689298933052 | validation: 0.3372787252132132]
	TIME [epoch: 6.01 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09120878732427254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09120878732427254 | validation: 0.24553273191981867]
	TIME [epoch: 6.01 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09872929545270492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09872929545270492 | validation: 0.23317873002775094]
	TIME [epoch: 6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061555047299337855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061555047299337855 | validation: 0.2035335989984783]
	TIME [epoch: 6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525675161323932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06525675161323932 | validation: 0.4048801264576429]
	TIME [epoch: 6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07577659043454527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07577659043454527 | validation: 0.20647579203756417]
	TIME [epoch: 5.99 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07430018305466349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07430018305466349 | validation: 0.3276883632221145]
	TIME [epoch: 6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015041499400178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07015041499400178 | validation: 0.1945835497705194]
	TIME [epoch: 5.99 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06057264169888273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06057264169888273 | validation: 0.21616379329673682]
	TIME [epoch: 5.99 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07251555737335524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07251555737335524 | validation: 0.2702339498013573]
	TIME [epoch: 5.98 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09805699664272352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09805699664272352 | validation: 0.28074594606086867]
	TIME [epoch: 5.99 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15531440066119814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15531440066119814 | validation: 0.22096067648229265]
	TIME [epoch: 5.98 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07148226489672888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07148226489672888 | validation: 0.2561342675155272]
	TIME [epoch: 5.99 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06501514723658015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06501514723658015 | validation: 0.1995852724975034]
	TIME [epoch: 5.97 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06188901555916641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06188901555916641 | validation: 0.2597329476722929]
	TIME [epoch: 5.97 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059082401230429474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059082401230429474 | validation: 0.1841263483708659]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051067020915826856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051067020915826856 | validation: 0.20019231957800765]
	TIME [epoch: 5.98 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03996329008821421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03996329008821421 | validation: 0.18570814965757088]
	TIME [epoch: 5.97 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038833347702866096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038833347702866096 | validation: 0.22155534926382528]
	TIME [epoch: 5.98 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04162381180305063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04162381180305063 | validation: 0.14340973071914473]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05801614351050489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05801614351050489 | validation: 0.34398328511382315]
	TIME [epoch: 5.97 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1502217813314969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1502217813314969 | validation: 0.3522232315184541]
	TIME [epoch: 5.97 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13269984764730702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13269984764730702 | validation: 0.2854916559639867]
	TIME [epoch: 5.97 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12637557080444675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12637557080444675 | validation: 0.34772589925337494]
	TIME [epoch: 5.97 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674463224738444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0674463224738444 | validation: 0.22261176176469122]
	TIME [epoch: 5.98 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06422747390130307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06422747390130307 | validation: 0.1992683463517031]
	TIME [epoch: 5.97 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885175913918994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05885175913918994 | validation: 0.17500341701363398]
	TIME [epoch: 5.98 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053461856947310205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053461856947310205 | validation: 0.32320628546727304]
	TIME [epoch: 5.98 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08712216259284077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08712216259284077 | validation: 0.22637726474177144]
	TIME [epoch: 5.98 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10137279338759166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10137279338759166 | validation: 0.3533027933851613]
	TIME [epoch: 5.98 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11809761596902174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11809761596902174 | validation: 0.2590493234165783]
	TIME [epoch: 5.98 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1168598668335792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1168598668335792 | validation: 0.22269414666012013]
	TIME [epoch: 5.99 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043435402484759016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043435402484759016 | validation: 0.1478131430915211]
	TIME [epoch: 5.99 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054449493136023826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054449493136023826 | validation: 0.49733026980590156]
	TIME [epoch: 5.98 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18570680330984782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18570680330984782 | validation: 0.2735882442144818]
	TIME [epoch: 5.99 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06958811320901642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06958811320901642 | validation: 0.16176436739318975]
	TIME [epoch: 5.98 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07105283934201098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07105283934201098 | validation: 0.2296986919582727]
	TIME [epoch: 5.99 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08980932180172385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08980932180172385 | validation: 0.2842311769791623]
	TIME [epoch: 5.98 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06288525072446911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06288525072446911 | validation: 0.14267921030240013]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_625.pth
	Model improved!!!
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044376167410593075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044376167410593075 | validation: 0.17130625918207842]
	TIME [epoch: 5.98 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04736587523687553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04736587523687553 | validation: 0.18527114733562478]
	TIME [epoch: 5.99 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03931684021550538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03931684021550538 | validation: 0.15613910462782812]
	TIME [epoch: 5.99 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03744556342361496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03744556342361496 | validation: 0.17141802816749588]
	TIME [epoch: 5.98 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396041155080856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04396041155080856 | validation: 0.17175473445851613]
	TIME [epoch: 5.97 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986072153078867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05986072153078867 | validation: 0.3722574831820733]
	TIME [epoch: 5.98 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15096150794464364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15096150794464364 | validation: 0.36635371126309335]
	TIME [epoch: 5.98 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2719063466588852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2719063466588852 | validation: 0.16796887294042193]
	TIME [epoch: 5.98 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04787142002298582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04787142002298582 | validation: 0.280272307700959]
	TIME [epoch: 5.97 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08662108243257656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08662108243257656 | validation: 0.19282447092050248]
	TIME [epoch: 5.98 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06072029157025602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06072029157025602 | validation: 0.1710828759404771]
	TIME [epoch: 5.99 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054042527211228375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054042527211228375 | validation: 0.14979240254389767]
	TIME [epoch: 5.99 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865488120088769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03865488120088769 | validation: 0.1447930967748031]
	TIME [epoch: 5.99 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03914048547839902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03914048547839902 | validation: 0.21793069809551335]
	TIME [epoch: 5.98 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08243037166290057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08243037166290057 | validation: 0.5497182441267242]
	TIME [epoch: 5.98 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13084666396874625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13084666396874625 | validation: 0.2735838447323989]
	TIME [epoch: 5.99 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11975756163795243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11975756163795243 | validation: 0.12731275043940068]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_642.pth
	Model improved!!!
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06105828958508054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06105828958508054 | validation: 0.21549604506250164]
	TIME [epoch: 6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08521216634575113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08521216634575113 | validation: 0.20124274282744087]
	TIME [epoch: 5.99 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04400300741376095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04400300741376095 | validation: 0.11252897187061764]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_645.pth
	Model improved!!!
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045548868469013605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045548868469013605 | validation: 0.16251905968104252]
	TIME [epoch: 5.98 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316218173889586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04316218173889586 | validation: 0.19052108526931583]
	TIME [epoch: 5.98 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04185374135442828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04185374135442828 | validation: 0.13799396749240925]
	TIME [epoch: 5.98 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037163158029638946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037163158029638946 | validation: 0.12343089324249346]
	TIME [epoch: 5.97 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395831257090496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0395831257090496 | validation: 0.13243132753699607]
	TIME [epoch: 5.97 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05232557432882693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05232557432882693 | validation: 0.15309463241840193]
	TIME [epoch: 5.97 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08047322954181234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08047322954181234 | validation: 0.19547866986203616]
	TIME [epoch: 5.97 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719886263161315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09719886263161315 | validation: 0.25790240518434104]
	TIME [epoch: 5.97 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14244677616878093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14244677616878093 | validation: 0.18411645236907595]
	TIME [epoch: 5.98 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05575395409776926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05575395409776926 | validation: 0.1303418665090079]
	TIME [epoch: 5.97 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04990655828888545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04990655828888545 | validation: 0.15293778204827665]
	TIME [epoch: 5.98 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04972391259108416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04972391259108416 | validation: 0.12414605784444821]
	TIME [epoch: 5.98 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626781256005012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04626781256005012 | validation: 0.15129492855845555]
	TIME [epoch: 5.98 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662333612748148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06662333612748148 | validation: 0.16544971014317755]
	TIME [epoch: 5.97 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04886182512144867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04886182512144867 | validation: 0.15815826240654435]
	TIME [epoch: 5.97 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05196413377885954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05196413377885954 | validation: 0.3418294324579821]
	TIME [epoch: 5.98 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524285268757656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524285268757656 | validation: 0.12361142253902054]
	TIME [epoch: 5.98 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04308869217212088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04308869217212088 | validation: 0.13816290422210217]
	TIME [epoch: 5.99 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04443034349374742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04443034349374742 | validation: 0.2345306113541125]
	TIME [epoch: 5.98 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07091414444181263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07091414444181263 | validation: 0.1496605157004318]
	TIME [epoch: 5.99 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08521563474091416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08521563474091416 | validation: 0.11840545466412421]
	TIME [epoch: 5.99 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04646307200766214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04646307200766214 | validation: 0.2047990899447569]
	TIME [epoch: 5.99 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036721441206544074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036721441206544074 | validation: 0.1268938610787694]
	TIME [epoch: 5.99 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033961353331706835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033961353331706835 | validation: 0.1627850949594798]
	TIME [epoch: 5.98 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06612353043488856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06612353043488856 | validation: 0.20385469784428656]
	TIME [epoch: 5.98 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14939582101907456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14939582101907456 | validation: 0.18580355724077843]
	TIME [epoch: 6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05044299621467163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05044299621467163 | validation: 0.10039427745328139]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_672.pth
	Model improved!!!
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04589433620465884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04589433620465884 | validation: 0.15338440079974977]
	TIME [epoch: 5.99 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06033502690157337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06033502690157337 | validation: 0.1377711474625121]
	TIME [epoch: 5.99 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848237170022306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07848237170022306 | validation: 0.16076574051331277]
	TIME [epoch: 6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08473463780570527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08473463780570527 | validation: 0.13912379933303942]
	TIME [epoch: 5.99 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04719804913198157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04719804913198157 | validation: 0.10739593783966708]
	TIME [epoch: 6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03141228202402496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03141228202402496 | validation: 0.12153085730088836]
	TIME [epoch: 5.98 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03325218982222837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03325218982222837 | validation: 0.20785808896113456]
	TIME [epoch: 5.98 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07015696385055412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07015696385055412 | validation: 0.5431132250758957]
	TIME [epoch: 5.98 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12996366443468277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12996366443468277 | validation: 0.14109247560491403]
	TIME [epoch: 5.98 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07401132215805482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07401132215805482 | validation: 0.12501133943670453]
	TIME [epoch: 5.98 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042548068494071174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042548068494071174 | validation: 0.1392600240458779]
	TIME [epoch: 5.98 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04722652948053976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04722652948053976 | validation: 0.10703979285840584]
	TIME [epoch: 5.97 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05939833660996655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05939833660996655 | validation: 0.13321348472411884]
	TIME [epoch: 5.97 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07280178394281733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07280178394281733 | validation: 0.11944572165411238]
	TIME [epoch: 5.98 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04651180322112781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04651180322112781 | validation: 0.1015437809869622]
	TIME [epoch: 5.99 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362253176995046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0362253176995046 | validation: 0.10667821490558711]
	TIME [epoch: 5.98 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025905397881288105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025905397881288105 | validation: 0.0884514271157334]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_689.pth
	Model improved!!!
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02371227870100145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02371227870100145 | validation: 0.08410976715501339]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_690.pth
	Model improved!!!
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021631958046330637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021631958046330637 | validation: 0.08483311255164011]
	TIME [epoch: 5.97 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022755678413593123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022755678413593123 | validation: 0.09523317758659337]
	TIME [epoch: 5.98 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028508973254622063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028508973254622063 | validation: 0.09870700428024977]
	TIME [epoch: 5.98 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047372383594402295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047372383594402295 | validation: 0.12371702121127469]
	TIME [epoch: 5.98 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735960839041233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735960839041233 | validation: 0.14094883760821966]
	TIME [epoch: 6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09851922737286893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09851922737286893 | validation: 0.10282434844905447]
	TIME [epoch: 6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10579445896000744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10579445896000744 | validation: 0.19689453030917398]
	TIME [epoch: 6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20638349499446967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20638349499446967 | validation: 0.3916227154137856]
	TIME [epoch: 5.99 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09597628500976903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09597628500976903 | validation: 0.15871129258521985]
	TIME [epoch: 5.99 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662919009717712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06662919009717712 | validation: 0.09787000687025378]
	TIME [epoch: 5.98 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039662149075463025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039662149075463025 | validation: 0.14374571335917957]
	TIME [epoch: 5.97 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03079735147446876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03079735147446876 | validation: 0.08742968537528428]
	TIME [epoch: 5.97 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031758423000725795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031758423000725795 | validation: 0.11073073904584146]
	TIME [epoch: 5.98 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029517540013870615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029517540013870615 | validation: 0.17557419026967241]
	TIME [epoch: 5.97 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530441321288173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04530441321288173 | validation: 0.2858343857504038]
	TIME [epoch: 5.97 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512797137443839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512797137443839 | validation: 0.17253199554817172]
	TIME [epoch: 5.97 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06949810939972585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06949810939972585 | validation: 0.09133445337760461]
	TIME [epoch: 5.97 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033512682925498666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033512682925498666 | validation: 0.13807772403495513]
	TIME [epoch: 5.98 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026214559736371445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026214559736371445 | validation: 0.08448523741791553]
	TIME [epoch: 5.98 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02325505048750638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02325505048750638 | validation: 0.08915377764471365]
	TIME [epoch: 5.99 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02790602445220448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02790602445220448 | validation: 0.1031117381396478]
	TIME [epoch: 5.98 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04496950453089309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04496950453089309 | validation: 0.21528963130860626]
	TIME [epoch: 5.98 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653616937630862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0653616937630862 | validation: 0.1805297971167907]
	TIME [epoch: 5.97 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08395009476207232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08395009476207232 | validation: 0.1666261466481705]
	TIME [epoch: 5.98 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05284649797337021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05284649797337021 | validation: 0.08610919473383177]
	TIME [epoch: 5.98 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026665168545007348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026665168545007348 | validation: 0.07444846934449482]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_716.pth
	Model improved!!!
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027938533488674047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027938533488674047 | validation: 0.15624336977327336]
	TIME [epoch: 6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04704262095733617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04704262095733617 | validation: 0.15686485464851305]
	TIME [epoch: 6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09973368869329989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09973368869329989 | validation: 0.18801490176695168]
	TIME [epoch: 5.99 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10356021278390928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10356021278390928 | validation: 0.14691665613577717]
	TIME [epoch: 6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13976716367750108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13976716367750108 | validation: 0.09000461574281336]
	TIME [epoch: 6 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0346087193716658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0346087193716658 | validation: 0.08223613311528831]
	TIME [epoch: 5.97 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05721109307758614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05721109307758614 | validation: 0.11723145693421624]
	TIME [epoch: 5.97 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05900792069618901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05900792069618901 | validation: 0.08889081643368452]
	TIME [epoch: 5.97 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03428329318189343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03428329318189343 | validation: 0.09564623573664377]
	TIME [epoch: 5.97 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03051684484122384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03051684484122384 | validation: 0.09047760901813055]
	TIME [epoch: 5.96 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029779483756890608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029779483756890608 | validation: 0.08562843377506857]
	TIME [epoch: 5.97 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02237714605096371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02237714605096371 | validation: 0.06988086325962577]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01888272668622745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01888272668622745 | validation: 0.0704175705111961]
	TIME [epoch: 6 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022876729308620503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022876729308620503 | validation: 0.18849968738882217]
	TIME [epoch: 6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046023451867239544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046023451867239544 | validation: 0.15798637201350368]
	TIME [epoch: 6.02 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08945429308304824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08945429308304824 | validation: 0.24195145275880822]
	TIME [epoch: 6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10264014104284383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10264014104284383 | validation: 0.15409105168059622]
	TIME [epoch: 6.01 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08071325611150501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08071325611150501 | validation: 0.1608603243456626]
	TIME [epoch: 6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03986598176902553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03986598176902553 | validation: 0.09849001846316595]
	TIME [epoch: 6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052899676566358134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052899676566358134 | validation: 0.09452498951906187]
	TIME [epoch: 6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03496500055425602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03496500055425602 | validation: 0.08183911400416828]
	TIME [epoch: 6 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02113064837928792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02113064837928792 | validation: 0.078802160470262]
	TIME [epoch: 6 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026630238047686855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026630238047686855 | validation: 0.0984158173485149]
	TIME [epoch: 6.01 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04607953082183265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04607953082183265 | validation: 0.314798373001524]
	TIME [epoch: 6 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10744461552301346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10744461552301346 | validation: 0.15758203563637677]
	TIME [epoch: 6.01 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07334999233687962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07334999233687962 | validation: 0.18217142086943136]
	TIME [epoch: 6.01 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06230452736155371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06230452736155371 | validation: 0.19813089571311737]
	TIME [epoch: 6.01 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710698030131246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04710698030131246 | validation: 0.10206844375209381]
	TIME [epoch: 6.02 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04444899460353415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04444899460353415 | validation: 0.09188847577961021]
	TIME [epoch: 6.01 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028954510166679385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028954510166679385 | validation: 0.0900018771502984]
	TIME [epoch: 6.03 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027467988601958333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027467988601958333 | validation: 0.06927882563599101]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_747.pth
	Model improved!!!
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02438723673735959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02438723673735959 | validation: 0.09149348327918871]
	TIME [epoch: 5.98 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020288027267040338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020288027267040338 | validation: 0.0767352083653594]
	TIME [epoch: 5.97 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022784234496116582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022784234496116582 | validation: 0.07265263968456262]
	TIME [epoch: 5.99 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03039097594271818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03039097594271818 | validation: 0.09339260838835195]
	TIME [epoch: 6.01 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05430774443408823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05430774443408823 | validation: 0.19193271055398936]
	TIME [epoch: 6.01 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12193131067602055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12193131067602055 | validation: 0.27095552368448333]
	TIME [epoch: 6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09872311866570072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09872311866570072 | validation: 0.1029575476804189]
	TIME [epoch: 6.01 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719710956646289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05719710956646289 | validation: 0.12611687793317947]
	TIME [epoch: 6.01 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05240132899409515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05240132899409515 | validation: 0.159180721435334]
	TIME [epoch: 6.01 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033012771773685876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033012771773685876 | validation: 0.10212639717587667]
	TIME [epoch: 6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033736239307358806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033736239307358806 | validation: 0.09755130762828655]
	TIME [epoch: 6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02862106307285294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02862106307285294 | validation: 0.05905755071041771]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020565862538090764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020565862538090764 | validation: 0.05379218959551803]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021180955206830613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021180955206830613 | validation: 0.08128540471386224]
	TIME [epoch: 5.99 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04398345388350153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04398345388350153 | validation: 0.09972932220133919]
	TIME [epoch: 5.99 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06536876991198909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06536876991198909 | validation: 0.11369582774100012]
	TIME [epoch: 6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09325420905154332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09325420905154332 | validation: 0.0719214721899115]
	TIME [epoch: 6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03033096804945788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03033096804945788 | validation: 0.06961378583706544]
	TIME [epoch: 5.99 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05137992828123399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05137992828123399 | validation: 0.11459192551772793]
	TIME [epoch: 5.97 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227450910254785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07227450910254785 | validation: 0.11000383005446052]
	TIME [epoch: 5.96 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038942838790445224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038942838790445224 | validation: 0.07746674866445417]
	TIME [epoch: 5.96 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052469632460094444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052469632460094444 | validation: 0.16535529885195013]
	TIME [epoch: 5.96 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07635874683154685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07635874683154685 | validation: 0.08186691877948052]
	TIME [epoch: 5.96 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052458486762171845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052458486762171845 | validation: 0.16441755741832043]
	TIME [epoch: 5.97 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05651727332675965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05651727332675965 | validation: 0.17812128037021135]
	TIME [epoch: 5.96 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362715259484961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08362715259484961 | validation: 0.1534469693196634]
	TIME [epoch: 5.96 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03387932237375785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03387932237375785 | validation: 0.08171519681810208]
	TIME [epoch: 5.97 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034453804532914016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034453804532914016 | validation: 0.08577787476526577]
	TIME [epoch: 5.96 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707700967171646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03707700967171646 | validation: 0.06473718697823225]
	TIME [epoch: 5.96 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021490288120219966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021490288120219966 | validation: 0.06224126024446639]
	TIME [epoch: 5.96 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020707471021188115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020707471021188115 | validation: 0.0797060213385079]
	TIME [epoch: 5.96 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030373144208011463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030373144208011463 | validation: 0.08151962791966502]
	TIME [epoch: 5.96 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032486895330726545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032486895330726545 | validation: 0.1025273470014188]
	TIME [epoch: 5.96 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053605982780164194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053605982780164194 | validation: 0.22402477955357114]
	TIME [epoch: 5.96 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08661824927359583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08661824927359583 | validation: 0.08741944960237458]
	TIME [epoch: 5.96 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06524248592783247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06524248592783247 | validation: 0.06744778130588568]
	TIME [epoch: 5.96 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024306326600422024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024306326600422024 | validation: 0.060425005668628795]
	TIME [epoch: 5.96 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020905602744139266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020905602744139266 | validation: 0.06305248636373441]
	TIME [epoch: 5.98 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02528601313758716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02528601313758716 | validation: 0.09976211472502469]
	TIME [epoch: 5.98 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03724924173972784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03724924173972784 | validation: 0.314871005640889]
	TIME [epoch: 5.98 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08366060540051538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08366060540051538 | validation: 0.17419788459276075]
	TIME [epoch: 5.97 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08723217286474515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08723217286474515 | validation: 0.09570139839494729]
	TIME [epoch: 5.96 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061496252209297476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061496252209297476 | validation: 0.10795012456615066]
	TIME [epoch: 5.97 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06621573503736615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06621573503736615 | validation: 0.11280977778319819]
	TIME [epoch: 5.96 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05330208615439702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05330208615439702 | validation: 0.08594261431234884]
	TIME [epoch: 5.96 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034207348813016916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034207348813016916 | validation: 0.08107541461161222]
	TIME [epoch: 5.96 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030196741168923433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030196741168923433 | validation: 0.07779587252860715]
	TIME [epoch: 5.96 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02861765873334812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02861765873334812 | validation: 0.13963615561838538]
	TIME [epoch: 5.96 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03407842933485863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03407842933485863 | validation: 0.1746774581487654]
	TIME [epoch: 5.97 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03262694862425995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03262694862425995 | validation: 0.06437138314114187]
	TIME [epoch: 5.96 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023657188364082635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023657188364082635 | validation: 0.06535545781041027]
	TIME [epoch: 5.99 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03196649622822094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03196649622822094 | validation: 0.10261305442920399]
	TIME [epoch: 6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04921456974927997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04921456974927997 | validation: 0.10993418910849054]
	TIME [epoch: 6.01 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0388420846951072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0388420846951072 | validation: 0.0661040847211805]
	TIME [epoch: 5.96 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025329653114718548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025329653114718548 | validation: 0.0760724558278933]
	TIME [epoch: 5.97 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01887589974708281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01887589974708281 | validation: 0.06237154082313944]
	TIME [epoch: 5.97 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020972214778219662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020972214778219662 | validation: 0.06657564545756989]
	TIME [epoch: 5.97 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034691489943896904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034691489943896904 | validation: 0.09219526384801838]
	TIME [epoch: 5.97 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814072166052512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06814072166052512 | validation: 0.11824762911133219]
	TIME [epoch: 5.98 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06989716737854346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06989716737854346 | validation: 0.10529815659757338]
	TIME [epoch: 5.97 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0421374696770545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0421374696770545 | validation: 0.12861392330245547]
	TIME [epoch: 5.98 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054885572536026654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054885572536026654 | validation: 0.08101148609833503]
	TIME [epoch: 5.98 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047629698592143194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047629698592143194 | validation: 0.14602585118959874]
	TIME [epoch: 6.02 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07757489970441649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07757489970441649 | validation: 0.20044220732247583]
	TIME [epoch: 6.02 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046330414328013214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046330414328013214 | validation: 0.0763175555533453]
	TIME [epoch: 6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03263914970104275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03263914970104275 | validation: 0.09634286463118241]
	TIME [epoch: 6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02675183064242862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02675183064242862 | validation: 0.06464565438218474]
	TIME [epoch: 6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018888292987077403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018888292987077403 | validation: 0.06317371513988478]
	TIME [epoch: 5.99 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02060205849063296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02060205849063296 | validation: 0.07689037152684053]
	TIME [epoch: 5.99 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201603415030847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03201603415030847 | validation: 0.06252932427164204]
	TIME [epoch: 6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03962228761838648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03962228761838648 | validation: 0.08869413471503819]
	TIME [epoch: 5.99 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06944610214646872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06944610214646872 | validation: 0.05864329731847926]
	TIME [epoch: 5.99 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034412262043839854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034412262043839854 | validation: 0.05611843624895152]
	TIME [epoch: 5.97 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02367230542204336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02367230542204336 | validation: 0.07097946385952417]
	TIME [epoch: 5.97 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042428456382188354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042428456382188354 | validation: 0.0935294294827122]
	TIME [epoch: 5.96 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06820237286264172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06820237286264172 | validation: 0.11301579270644374]
	TIME [epoch: 5.97 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06856137006107238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06856137006107238 | validation: 0.17163038215861245]
	TIME [epoch: 5.96 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038637740812844455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038637740812844455 | validation: 0.14742383024036015]
	TIME [epoch: 5.95 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048271870784380735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048271870784380735 | validation: 0.12817176354112503]
	TIME [epoch: 5.96 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03301195379091409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03301195379091409 | validation: 0.058616002569922866]
	TIME [epoch: 5.96 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029518751888596623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029518751888596623 | validation: 0.0842918625580818]
	TIME [epoch: 5.97 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05260512025463954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05260512025463954 | validation: 0.09655207568095871]
	TIME [epoch: 5.96 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07553306683005616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07553306683005616 | validation: 0.09002055187297686]
	TIME [epoch: 5.97 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0737019397078289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0737019397078289 | validation: 0.06396384565911088]
	TIME [epoch: 5.96 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025487979770839324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025487979770839324 | validation: 0.07142763053618094]
	TIME [epoch: 5.97 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027008092204052928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027008092204052928 | validation: 0.10303561288720334]
	TIME [epoch: 5.98 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804124228405468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03804124228405468 | validation: 0.10588712730576455]
	TIME [epoch: 5.98 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0260546100175476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0260546100175476 | validation: 0.09569079542252593]
	TIME [epoch: 6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025523644280655722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025523644280655722 | validation: 0.10401755438687084]
	TIME [epoch: 6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04474211049561177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04474211049561177 | validation: 0.11082761239583148]
	TIME [epoch: 5.98 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052932056977970036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052932056977970036 | validation: 0.10181751576465814]
	TIME [epoch: 5.99 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03639686891138352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03639686891138352 | validation: 0.09494315632932171]
	TIME [epoch: 5.97 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03027983173592614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03027983173592614 | validation: 0.10078835602027983]
	TIME [epoch: 5.96 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02395034866770266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02395034866770266 | validation: 0.05865129890783737]
	TIME [epoch: 5.96 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017547766652822847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017547766652822847 | validation: 0.05410656595495217]
	TIME [epoch: 5.95 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013693863207649924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013693863207649924 | validation: 0.18497734027861634]
	TIME [epoch: 5.96 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026840306492618222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026840306492618222 | validation: 0.09055384344668277]
	TIME [epoch: 5.96 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029774149640572664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029774149640572664 | validation: 0.13064192581996997]
	TIME [epoch: 5.96 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08267926701706006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08267926701706006 | validation: 0.16412065730218584]
	TIME [epoch: 5.96 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1655880482836171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1655880482836171 | validation: 0.08610778330451244]
	TIME [epoch: 5.96 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052480817604196164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052480817604196164 | validation: 0.09902034920678165]
	TIME [epoch: 5.95 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584877412496555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09584877412496555 | validation: 0.13194843633958817]
	TIME [epoch: 5.95 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10235612050753816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10235612050753816 | validation: 0.05247783560587419]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_850.pth
	Model improved!!!
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021587921256161026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021587921256161026 | validation: 0.07000772458125766]
	TIME [epoch: 5.99 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027712883349626447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027712883349626447 | validation: 0.05938719624931832]
	TIME [epoch: 5.98 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02769857335793472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02769857335793472 | validation: 0.06340747316732606]
	TIME [epoch: 6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017056209177933948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017056209177933948 | validation: 0.06539952944527634]
	TIME [epoch: 5.99 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015251900689210068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015251900689210068 | validation: 0.05158745077740004]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014680060360310884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014680060360310884 | validation: 0.046027483598541544]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_856.pth
	Model improved!!!
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017696789921322578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017696789921322578 | validation: 0.05355545885598986]
	TIME [epoch: 5.99 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019163134305788008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019163134305788008 | validation: 0.09444204043186406]
	TIME [epoch: 5.99 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024828057372247053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024828057372247053 | validation: 0.18388152815426217]
	TIME [epoch: 6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06452336240127945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06452336240127945 | validation: 0.307406144628248]
	TIME [epoch: 5.99 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895341229027484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0895341229027484 | validation: 0.1092662336162424]
	TIME [epoch: 6.01 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06841073066777037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06841073066777037 | validation: 0.09857299319733645]
	TIME [epoch: 6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04272730397183292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04272730397183292 | validation: 0.13358781201013256]
	TIME [epoch: 6.02 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03697205411965008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03697205411965008 | validation: 0.059239709579983535]
	TIME [epoch: 6.01 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020436777806751988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020436777806751988 | validation: 0.06133347275513693]
	TIME [epoch: 6.02 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014360436994292252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014360436994292252 | validation: 0.06444938109940855]
	TIME [epoch: 6.01 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014872072209727806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014872072209727806 | validation: 0.0637019631583479]
	TIME [epoch: 6.02 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019003950086904062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019003950086904062 | validation: 0.08270463527147351]
	TIME [epoch: 6.02 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02540422646108276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02540422646108276 | validation: 0.0754062849742208]
	TIME [epoch: 6.02 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04656952707721789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04656952707721789 | validation: 0.09638187277633603]
	TIME [epoch: 6.02 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0886619036498983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0886619036498983 | validation: 0.09286977612402002]
	TIME [epoch: 6.01 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896875468849706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0896875468849706 | validation: 0.05733673505019848]
	TIME [epoch: 6.01 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022088590446212742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022088590446212742 | validation: 0.0521596802950995]
	TIME [epoch: 6.01 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998771221281416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04998771221281416 | validation: 0.0868757514171544]
	TIME [epoch: 6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041426981246625305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041426981246625305 | validation: 0.05518180873160618]
	TIME [epoch: 6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020841099160813652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020841099160813652 | validation: 0.06944597197982243]
	TIME [epoch: 5.98 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025421943264890237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025421943264890237 | validation: 0.07206128615751699]
	TIME [epoch: 5.99 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02502327347032801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02502327347032801 | validation: 0.08608511596021166]
	TIME [epoch: 5.98 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021969743171979236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021969743171979236 | validation: 0.05460589908893834]
	TIME [epoch: 5.99 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021337313676423292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021337313676423292 | validation: 0.09829653552354652]
	TIME [epoch: 5.99 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768152645026462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768152645026462 | validation: 0.1264575147474501]
	TIME [epoch: 6.03 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898154873712606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0898154873712606 | validation: 0.11077222665194837]
	TIME [epoch: 5.99 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08076296665587827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08076296665587827 | validation: 0.16642642584425027]
	TIME [epoch: 5.98 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042142709748038264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042142709748038264 | validation: 0.2305692295134609]
	TIME [epoch: 5.98 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142926421387218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07142926421387218 | validation: 0.13425068698748413]
	TIME [epoch: 5.99 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03318492033874743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03318492033874743 | validation: 0.1095089999888196]
	TIME [epoch: 5.98 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022248889692778686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022248889692778686 | validation: 0.0600054551798562]
	TIME [epoch: 5.99 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01478400563037726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01478400563037726 | validation: 0.0693349444439265]
	TIME [epoch: 6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016310834180923522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016310834180923522 | validation: 0.058480375633148285]
	TIME [epoch: 5.99 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011246142541947926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011246142541947926 | validation: 0.05544887259282402]
	TIME [epoch: 5.99 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010356053421744288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010356053421744288 | validation: 0.06629900843948912]
	TIME [epoch: 6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010866111566881912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010866111566881912 | validation: 0.04616172968655551]
	TIME [epoch: 6.01 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010100878096322381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010100878096322381 | validation: 0.046854516378113525]
	TIME [epoch: 6.02 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010527435445583806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010527435445583806 | validation: 0.06572832828534668]
	TIME [epoch: 6.01 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029854402893833454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029854402893833454 | validation: 0.12680138648298103]
	TIME [epoch: 6.02 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14661156945648132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14661156945648132 | validation: 0.15833258890886723]
	TIME [epoch: 5.98 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17699875892638786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17699875892638786 | validation: 0.08342840240377246]
	TIME [epoch: 5.99 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032116234603160855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032116234603160855 | validation: 0.14415457362992729]
	TIME [epoch: 5.97 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09069287196148122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09069287196148122 | validation: 0.18561250323187323]
	TIME [epoch: 5.99 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0759478121696527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0759478121696527 | validation: 0.07045225820914207]
	TIME [epoch: 6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029864859260248046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029864859260248046 | validation: 0.07286983563349281]
	TIME [epoch: 5.99 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02867965746644039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02867965746644039 | validation: 0.054379868246076636]
	TIME [epoch: 5.99 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016065558282597273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016065558282597273 | validation: 0.30511212229559814]
	TIME [epoch: 6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21143619047189835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21143619047189835 | validation: 0.1331715083488055]
	TIME [epoch: 5.99 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06481430514915411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06481430514915411 | validation: 0.401843667212358]
	TIME [epoch: 6.01 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522226724142059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07522226724142059 | validation: 0.16791881115935434]
	TIME [epoch: 6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393451233670147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03393451233670147 | validation: 0.5398223310306601]
	TIME [epoch: 6.01 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5322851232369894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5322851232369894 | validation: 0.39986995434163375]
	TIME [epoch: 6.01 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39017938390998147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39017938390998147 | validation: 0.2742065467395698]
	TIME [epoch: 6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11684859317463467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11684859317463467 | validation: 0.15123269299991585]
	TIME [epoch: 6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03283164347980717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03283164347980717 | validation: 0.2767230688698243]
	TIME [epoch: 5.99 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051069946690836004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051069946690836004 | validation: 0.23113858660225]
	TIME [epoch: 6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029637902737384817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029637902737384817 | validation: 0.16429063877211805]
	TIME [epoch: 5.99 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0243766367943888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0243766367943888 | validation: 0.11358928419375225]
	TIME [epoch: 6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023548660289794042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023548660289794042 | validation: 0.11516906147229718]
	TIME [epoch: 5.99 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01951704016624503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01951704016624503 | validation: 0.10029430040793424]
	TIME [epoch: 5.99 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018955821017403965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018955821017403965 | validation: 0.09237326656042427]
	TIME [epoch: 5.99 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019320361909406124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019320361909406124 | validation: 0.09555630774802516]
	TIME [epoch: 6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01804591107121042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01804591107121042 | validation: 0.07905161205488453]
	TIME [epoch: 6.01 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015565583929957954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015565583929957954 | validation: 0.08086474565235552]
	TIME [epoch: 6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016111344354023463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016111344354023463 | validation: 0.07099694394073579]
	TIME [epoch: 6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016436117951259828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016436117951259828 | validation: 0.08894055520899447]
	TIME [epoch: 5.99 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029683868665125568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029683868665125568 | validation: 0.0913592938144908]
	TIME [epoch: 5.99 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400180261299716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0400180261299716 | validation: 0.08790306998464892]
	TIME [epoch: 6.01 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06146729773782725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06146729773782725 | validation: 0.08481977484973219]
	TIME [epoch: 6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036721576273702164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036721576273702164 | validation: 0.06368088333094499]
	TIME [epoch: 6.01 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039040296413919026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039040296413919026 | validation: 0.12305572879645338]
	TIME [epoch: 5.99 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05278063927235122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05278063927235122 | validation: 0.07605409685427757]
	TIME [epoch: 6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03316174057667487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03316174057667487 | validation: 0.061654580717178824]
	TIME [epoch: 6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021746795332791745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021746795332791745 | validation: 0.058999534723966865]
	TIME [epoch: 6.01 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018030639596564473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018030639596564473 | validation: 0.07358243858253243]
	TIME [epoch: 6.01 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01944690174225291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01944690174225291 | validation: 0.05666908456010383]
	TIME [epoch: 6.01 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015586832512036056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015586832512036056 | validation: 0.04542292276576926]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_933.pth
	Model improved!!!
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015500136264196026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015500136264196026 | validation: 0.05961013307437]
	TIME [epoch: 6.01 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018467664431612973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018467664431612973 | validation: 0.06134102359373651]
	TIME [epoch: 6.03 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02498513600696907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02498513600696907 | validation: 0.10544496078862431]
	TIME [epoch: 6.02 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051638813904816096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051638813904816096 | validation: 0.15528707013774218]
	TIME [epoch: 6.01 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09381674582630613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09381674582630613 | validation: 0.13528539040403467]
	TIME [epoch: 6.01 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05433979035355163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05433979035355163 | validation: 0.0731073091542403]
	TIME [epoch: 6.01 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026238707638415862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026238707638415862 | validation: 0.07688038197199927]
	TIME [epoch: 6.01 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025866009476839695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025866009476839695 | validation: 0.18765779597078602]
	TIME [epoch: 6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047564579953272616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047564579953272616 | validation: 0.07011439923203604]
	TIME [epoch: 6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03523537557828485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03523537557828485 | validation: 0.06564713924739125]
	TIME [epoch: 5.98 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030024385646505714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030024385646505714 | validation: 0.05030044745080045]
	TIME [epoch: 5.99 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019836887139534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02019836887139534 | validation: 0.0594564175648495]
	TIME [epoch: 5.98 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033036832311694554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033036832311694554 | validation: 0.0532907013437671]
	TIME [epoch: 5.98 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03049365620795153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03049365620795153 | validation: 0.07147415604844912]
	TIME [epoch: 5.97 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039802187782336275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039802187782336275 | validation: 0.07338753579408629]
	TIME [epoch: 5.99 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687968549726419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03687968549726419 | validation: 0.060936696394072155]
	TIME [epoch: 5.98 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033249500163871105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033249500163871105 | validation: 0.053540148478726374]
	TIME [epoch: 6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02076937467848343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02076937467848343 | validation: 0.04688136559365102]
	TIME [epoch: 5.99 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012918915446968396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012918915446968396 | validation: 0.039096659160671426]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011971001330576048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011971001330576048 | validation: 0.043302958338521574]
	TIME [epoch: 5.99 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015576749224597959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015576749224597959 | validation: 0.04288344852045981]
	TIME [epoch: 6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03421989375015892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03421989375015892 | validation: 0.09148717492803174]
	TIME [epoch: 5.99 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0899811018226951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0899811018226951 | validation: 0.11302302895695876]
	TIME [epoch: 5.99 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061410533335025906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061410533335025906 | validation: 0.0696277326402316]
	TIME [epoch: 5.98 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027674090451527064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027674090451527064 | validation: 0.0681446806692038]
	TIME [epoch: 5.98 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857512961571167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03857512961571167 | validation: 0.1039989391546077]
	TIME [epoch: 5.98 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06103400571703344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06103400571703344 | validation: 0.14401576925819395]
	TIME [epoch: 5.99 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07692127165239858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07692127165239858 | validation: 0.08795682725501444]
	TIME [epoch: 5.97 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06955231357931015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06955231357931015 | validation: 0.06282853431700235]
	TIME [epoch: 6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021201878175419964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021201878175419964 | validation: 0.04397266923182637]
	TIME [epoch: 5.99 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030695541533633155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030695541533633155 | validation: 0.0965635023777826]
	TIME [epoch: 6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027397960736316583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027397960736316583 | validation: 0.07015212174285426]
	TIME [epoch: 5.99 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019629444294000344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019629444294000344 | validation: 0.0423438905871735]
	TIME [epoch: 6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011771151700332633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011771151700332633 | validation: 0.04105071501536237]
	TIME [epoch: 5.99 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013398784454946645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013398784454946645 | validation: 0.05667722554629639]
	TIME [epoch: 5.99 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013841001137455138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013841001137455138 | validation: 0.06596093088618829]
	TIME [epoch: 6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018905798076152093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018905798076152093 | validation: 0.05812792699167443]
	TIME [epoch: 6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027426681436592312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027426681436592312 | validation: 0.06165432647181843]
	TIME [epoch: 5.99 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04047012006923392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04047012006923392 | validation: 0.0639539180019442]
	TIME [epoch: 6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0415102650130803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0415102650130803 | validation: 0.062083578644046226]
	TIME [epoch: 6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026774888867100746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026774888867100746 | validation: 0.04401142515381654]
	TIME [epoch: 6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01459303215717723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01459303215717723 | validation: 0.057005830600080847]
	TIME [epoch: 6.01 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016299118379396533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016299118379396533 | validation: 0.06411261239186493]
	TIME [epoch: 6.01 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03810352314883196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03810352314883196 | validation: 0.21612769318238978]
	TIME [epoch: 6.01 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965990812442818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07965990812442818 | validation: 0.09470922324399544]
	TIME [epoch: 6.01 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06430587152918159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06430587152918159 | validation: 0.053177293057508415]
	TIME [epoch: 6.02 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017521560134693404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017521560134693404 | validation: 0.06725073909257659]
	TIME [epoch: 6.01 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021840557903869833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021840557903869833 | validation: 0.06825081734135612]
	TIME [epoch: 6.01 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04480092317224045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04480092317224045 | validation: 0.04910307807681914]
	TIME [epoch: 6.01 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0574025142542523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0574025142542523 | validation: 0.07105704635908314]
	TIME [epoch: 6.01 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056031079229150134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056031079229150134 | validation: 0.07186484694247207]
	TIME [epoch: 6.02 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858114303446648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03858114303446648 | validation: 0.0405747503541309]
	TIME [epoch: 6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018748347727003997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018748347727003997 | validation: 0.06511037907903001]
	TIME [epoch: 6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01822112757043454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01822112757043454 | validation: 0.05131667711086967]
	TIME [epoch: 5.98 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021398940175171435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021398940175171435 | validation: 0.06092912888485075]
	TIME [epoch: 5.99 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04255985744486671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04255985744486671 | validation: 0.09401262906286612]
	TIME [epoch: 5.99 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04455459670011399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04455459670011399 | validation: 0.06332199195709362]
	TIME [epoch: 5.98 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035851333702747346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035851333702747346 | validation: 0.04496223091771842]
	TIME [epoch: 5.98 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017603101670050164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017603101670050164 | validation: 0.05496818227229688]
	TIME [epoch: 5.98 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012216461880106623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012216461880106623 | validation: 0.08863842866900468]
	TIME [epoch: 5.98 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018273080055340746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018273080055340746 | validation: 0.060944809617652265]
	TIME [epoch: 5.98 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026292221442347632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026292221442347632 | validation: 0.12673307615149995]
	TIME [epoch: 5.99 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152008465289945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04152008465289945 | validation: 0.11483593058839724]
	TIME [epoch: 6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813243590200125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03813243590200125 | validation: 0.26603134155599245]
	TIME [epoch: 6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042893266048289046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042893266048289046 | validation: 0.047252718520006426]
	TIME [epoch: 6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016656226944168625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016656226944168625 | validation: 0.09542560502859217]
	TIME [epoch: 6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024732253574925336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024732253574925336 | validation: 0.05918887094117107]
	TIME [epoch: 6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03412546658376066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03412546658376066 | validation: 0.05586795146251931]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0323111639754917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0323111639754917 | validation: 0.05223055852064889]
	TIME [epoch: 12.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030590975129385904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030590975129385904 | validation: 0.0568379385217842]
	TIME [epoch: 12.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392803738720358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0392803738720358 | validation: 0.06613680994152075]
	TIME [epoch: 12.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000147894710285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05000147894710285 | validation: 0.06877226393092493]
	TIME [epoch: 12.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02951723831416161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02951723831416161 | validation: 0.06881623384862394]
	TIME [epoch: 12.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262381670255639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04262381670255639 | validation: 0.09180930780829938]
	TIME [epoch: 12.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07342766924433036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07342766924433036 | validation: 0.07972483956007256]
	TIME [epoch: 12.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055973968819966045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055973968819966045 | validation: 0.1042764675073986]
	TIME [epoch: 12.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021868360030431883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021868360030431883 | validation: 0.034487488946800475]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_1010.pth
	Model improved!!!
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011302421884222405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011302421884222405 | validation: 0.04091403367529725]
	TIME [epoch: 12.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01422675080445186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01422675080445186 | validation: 0.047109697619132265]
	TIME [epoch: 12.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015260765632576015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015260765632576015 | validation: 0.041633584986218344]
	TIME [epoch: 12.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019349383356942355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019349383356942355 | validation: 0.04960845186519197]
	TIME [epoch: 12.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026469700107293864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026469700107293864 | validation: 0.056129513303665314]
	TIME [epoch: 12.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03346823132426718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03346823132426718 | validation: 0.055393017918333336]
	TIME [epoch: 12.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603166193301667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03603166193301667 | validation: 0.06404868977471077]
	TIME [epoch: 12.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03064455082282972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03064455082282972 | validation: 0.09254030514333711]
	TIME [epoch: 12.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023041459935208043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023041459935208043 | validation: 0.05573986553815733]
	TIME [epoch: 12.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013686503692024605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013686503692024605 | validation: 0.04413938593160948]
	TIME [epoch: 12.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018741954809582368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018741954809582368 | validation: 0.08332853919169651]
	TIME [epoch: 12.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03538552130521735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03538552130521735 | validation: 0.11475960243433825]
	TIME [epoch: 12.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06516237601761411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06516237601761411 | validation: 0.09594426459020299]
	TIME [epoch: 12.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672093446035245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04672093446035245 | validation: 0.03978157287300045]
	TIME [epoch: 12.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014232825967237146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014232825967237146 | validation: 0.03915031549941406]
	TIME [epoch: 12.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00833088963532334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00833088963532334 | validation: 0.048250781709709804]
	TIME [epoch: 12.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012189964131060882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012189964131060882 | validation: 0.3160042355003526]
	TIME [epoch: 12.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25651837947877576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25651837947877576 | validation: 0.14785638081992586]
	TIME [epoch: 12.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09183605850068333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09183605850068333 | validation: 0.39086964023223186]
	TIME [epoch: 12.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721593965890719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721593965890719 | validation: 0.29404075108729844]
	TIME [epoch: 12.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09005243137557646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09005243137557646 | validation: 0.1535229938306963]
	TIME [epoch: 12.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1101809139713118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1101809139713118 | validation: 0.1117545615350188]
	TIME [epoch: 12.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028446093994355666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028446093994355666 | validation: 0.0899208155446314]
	TIME [epoch: 12.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02176940144043912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02176940144043912 | validation: 0.07162772136385746]
	TIME [epoch: 12.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024038902890996822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024038902890996822 | validation: 0.07516148136453875]
	TIME [epoch: 12.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020473279087645544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020473279087645544 | validation: 0.07380530341405643]
	TIME [epoch: 12.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016479479703340915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016479479703340915 | validation: 0.062285411874775966]
	TIME [epoch: 12.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01988174104800882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01988174104800882 | validation: 0.07228794788327139]
	TIME [epoch: 12.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02380108007076907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02380108007076907 | validation: 0.064633079872338]
	TIME [epoch: 12.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023073887867571818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023073887867571818 | validation: 0.057587333920390615]
	TIME [epoch: 12.8 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025623360176057854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025623360176057854 | validation: 0.06279432668933964]
	TIME [epoch: 12.8 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027300575856474555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027300575856474555 | validation: 0.06373683401211049]
	TIME [epoch: 12.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025508046616468764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025508046616468764 | validation: 0.059265655477462764]
	TIME [epoch: 12.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021365779593994157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021365779593994157 | validation: 0.04779744514771535]
	TIME [epoch: 12.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019282070932532275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019282070932532275 | validation: 0.049295309507287915]
	TIME [epoch: 12.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014364142529405842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014364142529405842 | validation: 0.04864959465433445]
	TIME [epoch: 12.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013371720212857936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013371720212857936 | validation: 0.04321141815481871]
	TIME [epoch: 12.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01430507787142002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01430507787142002 | validation: 0.044626747388006]
	TIME [epoch: 12.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01795773305774203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01795773305774203 | validation: 0.06294577832689344]
	TIME [epoch: 12.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03186537967011548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03186537967011548 | validation: 0.08053367517086046]
	TIME [epoch: 12.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054874634868656014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054874634868656014 | validation: 0.07166935097171724]
	TIME [epoch: 12.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05034712156997685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05034712156997685 | validation: 0.055463924137773214]
	TIME [epoch: 12.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024138818915239876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024138818915239876 | validation: 0.07933113838914509]
	TIME [epoch: 12.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027663731069250297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027663731069250297 | validation: 0.10158297245472128]
	TIME [epoch: 12.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049160110796894314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049160110796894314 | validation: 0.07050679982562578]
	TIME [epoch: 12.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05756092876039996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05756092876039996 | validation: 0.05577320666657906]
	TIME [epoch: 12.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025999619182419626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025999619182419626 | validation: 0.056970007614886346]
	TIME [epoch: 12.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01811890775634504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01811890775634504 | validation: 0.06285189147352231]
	TIME [epoch: 12.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016022994651644383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016022994651644383 | validation: 0.07525294983054066]
	TIME [epoch: 12.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01600014772050917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01600014772050917 | validation: 0.0509311152834855]
	TIME [epoch: 12.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014029888529105965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014029888529105965 | validation: 0.04074518625796386]
	TIME [epoch: 12.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015918487706712024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015918487706712024 | validation: 0.042252142323573966]
	TIME [epoch: 12.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025396885344340127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025396885344340127 | validation: 0.047755233456640345]
	TIME [epoch: 12.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027774275165354592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027774275165354592 | validation: 0.05625626682573823]
	TIME [epoch: 12.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04170644469731367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04170644469731367 | validation: 0.055445482408784]
	TIME [epoch: 12.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02785241758221303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02785241758221303 | validation: 0.05183138628075604]
	TIME [epoch: 12.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024261504908708376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024261504908708376 | validation: 0.07184178750324353]
	TIME [epoch: 12.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03840058260852281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03840058260852281 | validation: 0.07563976749741819]
	TIME [epoch: 12.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049923157910898706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049923157910898706 | validation: 0.08019134847398271]
	TIME [epoch: 12.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034164678708593556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034164678708593556 | validation: 0.13177683983710803]
	TIME [epoch: 12.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03071923745848918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03071923745848918 | validation: 0.12474157571133247]
	TIME [epoch: 12.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020672783143900746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020672783143900746 | validation: 0.0558129951464157]
	TIME [epoch: 12.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022280841005691956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022280841005691956 | validation: 0.06467172078345138]
	TIME [epoch: 12.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02095016387690723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02095016387690723 | validation: 0.05801198316381361]
	TIME [epoch: 12.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578098619010904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03578098619010904 | validation: 0.07665925936891434]
	TIME [epoch: 12.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298362937726143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06298362937726143 | validation: 0.09586757859146444]
	TIME [epoch: 12.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04328441644664332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04328441644664332 | validation: 0.04766447319368144]
	TIME [epoch: 12.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04160983095211412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04160983095211412 | validation: 0.0937941917625012]
	TIME [epoch: 12.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051271305260310374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051271305260310374 | validation: 0.0810129282411989]
	TIME [epoch: 12.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02680330622635677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02680330622635677 | validation: 0.049071033725507265]
	TIME [epoch: 12.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014344495917189186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014344495917189186 | validation: 0.042175846365151995]
	TIME [epoch: 12.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014468494756128275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014468494756128275 | validation: 0.03600701355389333]
	TIME [epoch: 12.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012625506738648658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012625506738648658 | validation: 0.05302871904517514]
	TIME [epoch: 12.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01488834916997093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01488834916997093 | validation: 0.037743129543927056]
	TIME [epoch: 12.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017380641268436182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017380641268436182 | validation: 0.05050331873786728]
	TIME [epoch: 12.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03008178474602186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03008178474602186 | validation: 0.11413567563690477]
	TIME [epoch: 12.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035734143539771925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035734143539771925 | validation: 0.06283205495052967]
	TIME [epoch: 12.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04249405598155559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04249405598155559 | validation: 0.04957184778800312]
	TIME [epoch: 12.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02694478744105706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02694478744105706 | validation: 0.0638229375438238]
	TIME [epoch: 12.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017612979490096336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017612979490096336 | validation: 0.05115358331812945]
	TIME [epoch: 12.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024174733167871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024174733167871 | validation: 0.08042747906957139]
	TIME [epoch: 12.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05110054440909351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05110054440909351 | validation: 0.09209184959446154]
	TIME [epoch: 12.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07463162236769239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07463162236769239 | validation: 0.05728923356507529]
	TIME [epoch: 12.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02202419795484115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02202419795484115 | validation: 0.052115260033636625]
	TIME [epoch: 12.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018758181757871237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018758181757871237 | validation: 0.04607343272010089]
	TIME [epoch: 12.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02584250364097562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02584250364097562 | validation: 0.0709553364493748]
	TIME [epoch: 12.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027307469018307575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027307469018307575 | validation: 0.07734632384684248]
	TIME [epoch: 12.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022527236666409874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022527236666409874 | validation: 0.05375590776152525]
	TIME [epoch: 12.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015505721588429385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015505721588429385 | validation: 0.04017666629206776]
	TIME [epoch: 12.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014711469465871437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014711469465871437 | validation: 0.0472959420354818]
	TIME [epoch: 12.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01180282877718693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01180282877718693 | validation: 0.06086095445664596]
	TIME [epoch: 12.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01835509792874363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01835509792874363 | validation: 0.03920213524277021]
	TIME [epoch: 12.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017947217215135418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017947217215135418 | validation: 0.03906668197962603]
	TIME [epoch: 12.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016673639716707652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016673639716707652 | validation: 0.05956060345099817]
	TIME [epoch: 12.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032976015532638694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032976015532638694 | validation: 0.05198206797728458]
	TIME [epoch: 12.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03394277693693777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03394277693693777 | validation: 0.15001450060378294]
	TIME [epoch: 12.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058613730038735996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058613730038735996 | validation: 0.1675020176681717]
	TIME [epoch: 12.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08378945967337584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08378945967337584 | validation: 0.0839426455203297]
	TIME [epoch: 12.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07386685540922898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07386685540922898 | validation: 0.13107929938300536]
	TIME [epoch: 12.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06197487554938702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06197487554938702 | validation: 0.06074161459927845]
	TIME [epoch: 12.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03516389503598777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03516389503598777 | validation: 0.06253438752813899]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_113803/states/model_phi1_4b_v_mmd1_1111.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6297.090 seconds.
