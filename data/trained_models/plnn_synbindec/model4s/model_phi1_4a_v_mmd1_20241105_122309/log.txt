Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2873846660

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.962170137886449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.962170137886449 | validation: 3.7178257667549897]
	TIME [epoch: 167 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.550410595604099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.550410595604099 | validation: 2.9768314389379706]
	TIME [epoch: 0.835 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.551880666347904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.551880666347904 | validation: 3.610824234853993]
	TIME [epoch: 0.715 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.957869804801762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.957869804801762 | validation: 3.7137673683387717]
	TIME [epoch: 0.716 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.396974079188358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.396974079188358 | validation: 3.3725244473214517]
	TIME [epoch: 0.72 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1687262805190146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1687262805190146 | validation: 1.7525676535810968]
	TIME [epoch: 0.716 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8355560594763745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8355560594763745 | validation: 4.332485662842482]
	TIME [epoch: 0.719 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.670366624376321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.670366624376321 | validation: 2.792447991966799]
	TIME [epoch: 0.712 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.771320179158296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.771320179158296 | validation: 2.0226300747009787]
	TIME [epoch: 0.715 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.497371319776129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.497371319776129 | validation: 1.7799492684846918]
	TIME [epoch: 0.712 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.208720106151855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.208720106151855 | validation: 1.986788852396352]
	TIME [epoch: 0.71 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.793325588873521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.793325588873521 | validation: 1.8947075037186019]
	TIME [epoch: 0.709 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6235506869062044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6235506869062044 | validation: 1.6808898456295147]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6421829379213733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6421829379213733 | validation: 1.4072041110022717]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5355187976479874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5355187976479874 | validation: 1.3838731603518768]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4533682650292814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4533682650292814 | validation: 1.4055296766120524]
	TIME [epoch: 0.713 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4004328422731913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4004328422731913 | validation: 1.2535461592236539]
	TIME [epoch: 0.717 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3582269524377073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3582269524377073 | validation: 1.3023927489332043]
	TIME [epoch: 0.713 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.311039728964036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.311039728964036 | validation: 1.1126114979796904]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2816905882295664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2816905882295664 | validation: 1.3389864436398378]
	TIME [epoch: 0.715 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2730014413350534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2730014413350534 | validation: 0.8495028472179769]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.374715838775406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.374715838775406 | validation: 1.335087005815662]
	TIME [epoch: 0.713 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2306068953194547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2306068953194547 | validation: 0.7827251490708633]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2492188979082872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2492188979082872 | validation: 1.1301530343072455]
	TIME [epoch: 0.71 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.121544855661747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.121544855661747 | validation: 1.07395387632514]
	TIME [epoch: 0.707 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0915212030364614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0915212030364614 | validation: 0.9055911084660251]
	TIME [epoch: 0.707 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0960845684576683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0960845684576683 | validation: 1.6482930516488996]
	TIME [epoch: 0.708 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3909075585996926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3909075585996926 | validation: 0.9736817815819472]
	TIME [epoch: 0.708 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5780219849184345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5780219849184345 | validation: 1.030418003881316]
	TIME [epoch: 0.708 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047707473288655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.047707473288655 | validation: 1.5254586981106368]
	TIME [epoch: 0.706 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3216532067071927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3216532067071927 | validation: 0.6623015050791691]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2509487059368671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2509487059368671 | validation: 0.6459124418103488]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1201819588183974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1201819588183974 | validation: 1.0712644879538151]
	TIME [epoch: 0.712 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0015899551961607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0015899551961607 | validation: 1.0359113455871325]
	TIME [epoch: 0.708 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0043793182475875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0043793182475875 | validation: 0.7527302531181823]
	TIME [epoch: 0.709 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.073803384103437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.073803384103437 | validation: 0.9451581302153775]
	TIME [epoch: 0.708 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0126342840041138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0126342840041138 | validation: 1.2153620164717207]
	TIME [epoch: 0.707 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1048874332979548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1048874332979548 | validation: 0.636882452486642]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.092309452590037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.092309452590037 | validation: 0.7358661443808528]
	TIME [epoch: 0.711 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9637377226037671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9637377226037671 | validation: 1.2450702060075145]
	TIME [epoch: 0.708 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0939643969156958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0939643969156958 | validation: 0.5375350719037594]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0884857536200718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0884857536200718 | validation: 0.5733787634465589]
	TIME [epoch: 0.712 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9660760353591685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9660760353591685 | validation: 1.1280297636691474]
	TIME [epoch: 0.71 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.987302559518156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.987302559518156 | validation: 0.5485840072613523]
	TIME [epoch: 0.709 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9572287477760216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9572287477760216 | validation: 0.63663460888992]
	TIME [epoch: 0.707 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.912123312342332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.912123312342332 | validation: 1.0297531369456197]
	TIME [epoch: 0.705 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9395004835444833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9395004835444833 | validation: 0.6513140440344891]
	TIME [epoch: 0.706 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9621836682974686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9621836682974686 | validation: 0.8877085624412948]
	TIME [epoch: 0.71 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0199729368466295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0199729368466295 | validation: 0.9565604449374014]
	TIME [epoch: 0.708 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9714983298963469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9714983298963469 | validation: 0.642724147933875]
	TIME [epoch: 0.707 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9347723135115393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9347723135115393 | validation: 0.7798875163420746]
	TIME [epoch: 0.706 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8611119004981146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8611119004981146 | validation: 0.8355852420963545]
	TIME [epoch: 0.709 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596498366625243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8596498366625243 | validation: 0.6886190678191011]
	TIME [epoch: 0.71 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8498513966524458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498513966524458 | validation: 0.8743511195189249]
	TIME [epoch: 0.71 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549267432933049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8549267432933049 | validation: 0.5621487601488523]
	TIME [epoch: 0.709 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901168300074883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.901168300074883 | validation: 1.0316451663874144]
	TIME [epoch: 0.705 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923414042272459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.923414042272459 | validation: 0.6157665842322688]
	TIME [epoch: 0.706 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0034992512287872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0034992512287872 | validation: 0.9252241874053944]
	TIME [epoch: 0.707 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220984192689881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9220984192689881 | validation: 0.9008019852854601]
	TIME [epoch: 0.705 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9818689152321213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9818689152321213 | validation: 0.6867575763793579]
	TIME [epoch: 0.705 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9987001697576184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9987001697576184 | validation: 0.6619943300090763]
	TIME [epoch: 0.707 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454618782246643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8454618782246643 | validation: 0.8359809817500099]
	TIME [epoch: 0.709 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370941750319335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8370941750319335 | validation: 0.6233984323342429]
	TIME [epoch: 0.707 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8543531014414965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8543531014414965 | validation: 0.7660917952749987]
	TIME [epoch: 0.708 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340488186148403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8340488186148403 | validation: 0.7616519177004833]
	TIME [epoch: 0.718 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347241725923814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8347241725923814 | validation: 0.6998010473321155]
	TIME [epoch: 0.706 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958856091492762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8958856091492762 | validation: 1.1467329723999158]
	TIME [epoch: 0.708 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0084925938998253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0084925938998253 | validation: 0.6104894126433351]
	TIME [epoch: 0.708 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8591451320489205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8591451320489205 | validation: 0.7019744410060748]
	TIME [epoch: 0.706 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8045507329188354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8045507329188354 | validation: 0.6283527160226401]
	TIME [epoch: 0.707 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967300474139698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967300474139698 | validation: 0.6726653490212904]
	TIME [epoch: 0.708 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940185650461442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7940185650461442 | validation: 0.6229641071065093]
	TIME [epoch: 0.705 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980995767206163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980995767206163 | validation: 0.81859164064263]
	TIME [epoch: 0.706 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765320854740932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8765320854740932 | validation: 0.7095328020219109]
	TIME [epoch: 0.708 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2473624052724728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2473624052724728 | validation: 0.728765234676252]
	TIME [epoch: 0.706 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8178657900469867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8178657900469867 | validation: 1.0706153687548465]
	TIME [epoch: 0.706 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507418087845021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507418087845021 | validation: 0.5623361125715663]
	TIME [epoch: 0.706 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.992061086066234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.992061086066234 | validation: 0.7625172558288594]
	TIME [epoch: 0.708 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8220200768955838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8220200768955838 | validation: 0.7809231935740206]
	TIME [epoch: 0.713 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547449019911459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547449019911459 | validation: 0.6894019806532617]
	TIME [epoch: 0.711 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8894755639951282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8894755639951282 | validation: 0.7062496536568756]
	TIME [epoch: 0.708 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8387695233452527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8387695233452527 | validation: 0.9239215423001309]
	TIME [epoch: 0.706 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8699845758074028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8699845758074028 | validation: 0.6355999769365153]
	TIME [epoch: 0.706 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286249644580835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286249644580835 | validation: 0.7692967461462072]
	TIME [epoch: 0.707 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8251061433998426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8251061433998426 | validation: 0.7416418930975248]
	TIME [epoch: 0.71 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505913090841938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8505913090841938 | validation: 0.7957841651382312]
	TIME [epoch: 0.706 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9305753958600776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9305753958600776 | validation: 0.7778875322998436]
	TIME [epoch: 0.706 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8664882362758556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8664882362758556 | validation: 0.7425019924296059]
	TIME [epoch: 0.705 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465419550451339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8465419550451339 | validation: 0.6407441660875399]
	TIME [epoch: 0.712 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110167784831563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110167784831563 | validation: 0.8004613329809228]
	TIME [epoch: 0.709 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8022013983736775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8022013983736775 | validation: 0.6088121548670007]
	TIME [epoch: 0.706 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7993452259376614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7993452259376614 | validation: 0.8307703009280601]
	TIME [epoch: 0.705 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8185155765095945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8185155765095945 | validation: 0.617188705030776]
	TIME [epoch: 0.707 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8231599081670027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8231599081670027 | validation: 0.869514240871779]
	TIME [epoch: 0.71 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575113959011483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8575113959011483 | validation: 0.7095935693394961]
	TIME [epoch: 0.71 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8711205777181883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8711205777181883 | validation: 0.8128463945847262]
	TIME [epoch: 0.707 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0891097077874667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0891097077874667 | validation: 0.7884619163432914]
	TIME [epoch: 0.705 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8389278931021639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8389278931021639 | validation: 0.6473503792247272]
	TIME [epoch: 0.706 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819675394547417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819675394547417 | validation: 0.653705482534795]
	TIME [epoch: 0.706 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839332729521803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7839332729521803 | validation: 0.7012167768092201]
	TIME [epoch: 0.705 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857843361075842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857843361075842 | validation: 0.6288851589098181]
	TIME [epoch: 0.709 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026350944924164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026350944924164 | validation: 0.7673594432619857]
	TIME [epoch: 0.707 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167553077211668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167553077211668 | validation: 0.6633205992709573]
	TIME [epoch: 0.706 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673421437368002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673421437368002 | validation: 0.7701164752969307]
	TIME [epoch: 0.706 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8768083203505702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8768083203505702 | validation: 0.920731382652612]
	TIME [epoch: 0.705 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291832396500805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9291832396500805 | validation: 0.612907473215744]
	TIME [epoch: 0.706 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289568399865996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8289568399865996 | validation: 0.8110128190901129]
	TIME [epoch: 0.706 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119786874163798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119786874163798 | validation: 0.578987766564487]
	TIME [epoch: 0.705 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823811615980465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7823811615980465 | validation: 0.7057481030429964]
	TIME [epoch: 0.705 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77727584984847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77727584984847 | validation: 0.5990088747949533]
	TIME [epoch: 0.709 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7659740374339805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7659740374339805 | validation: 0.7098405939820048]
	TIME [epoch: 0.709 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765442060457324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765442060457324 | validation: 0.5969814701287812]
	TIME [epoch: 0.708 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8863039139197975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8863039139197975 | validation: 0.9864462201006221]
	TIME [epoch: 0.706 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0716092695202115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0716092695202115 | validation: 0.8548780013585834]
	TIME [epoch: 0.705 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9252268955630543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9252268955630543 | validation: 0.5592010033158055]
	TIME [epoch: 0.708 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021126687690929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021126687690929 | validation: 0.781937228010141]
	TIME [epoch: 0.711 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8071966812697917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8071966812697917 | validation: 0.6080559898018054]
	TIME [epoch: 0.707 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745678025297718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745678025297718 | validation: 0.6416455173966988]
	TIME [epoch: 0.705 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592150348636623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7592150348636623 | validation: 0.7351858032250647]
	TIME [epoch: 0.708 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748362758260559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7748362758260559 | validation: 0.6734960962696663]
	TIME [epoch: 0.713 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8114536365546045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8114536365546045 | validation: 1.037880246430337]
	TIME [epoch: 0.711 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9477399640254888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9477399640254888 | validation: 0.7620121943913286]
	TIME [epoch: 0.711 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883851630852169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8883851630852169 | validation: 0.6376695877646471]
	TIME [epoch: 0.709 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408513555735943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408513555735943 | validation: 0.646536332770807]
	TIME [epoch: 0.708 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759326623478479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759326623478479 | validation: 0.6371307137364832]
	TIME [epoch: 0.71 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641072495851967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641072495851967 | validation: 0.668216486436981]
	TIME [epoch: 0.714 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771273954162942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771273954162942 | validation: 0.6813753163255779]
	TIME [epoch: 0.708 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959962665087281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959962665087281 | validation: 0.7603291803449146]
	TIME [epoch: 0.71 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8646985320264889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8646985320264889 | validation: 0.7595147234989805]
	TIME [epoch: 0.71 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8850065398784096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8850065398784096 | validation: 0.7851500371808152]
	TIME [epoch: 0.71 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8301853473470459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8301853473470459 | validation: 0.5912544775190255]
	TIME [epoch: 0.71 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581736928441274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581736928441274 | validation: 0.6818656726676178]
	TIME [epoch: 0.71 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.748943259970379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.748943259970379 | validation: 0.5824243757902698]
	TIME [epoch: 0.709 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656727965118074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656727965118074 | validation: 0.6986371066551944]
	TIME [epoch: 0.707 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941875159510734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941875159510734 | validation: 0.6992954785465793]
	TIME [epoch: 0.707 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734029685636098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8734029685636098 | validation: 0.7800510129238947]
	TIME [epoch: 0.707 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8705171873778429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8705171873778429 | validation: 0.8487614788651661]
	TIME [epoch: 0.709 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499599766688806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499599766688806 | validation: 0.5354670361869026]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.768346241692346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.768346241692346 | validation: 0.6749431574422853]
	TIME [epoch: 0.712 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499398038193589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499398038193589 | validation: 0.6150546330591585]
	TIME [epoch: 0.711 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7318985778187745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7318985778187745 | validation: 0.6449364443969836]
	TIME [epoch: 0.709 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7370079619746832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7370079619746832 | validation: 0.6878107095196664]
	TIME [epoch: 0.708 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966003481143068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966003481143068 | validation: 0.8073735714781791]
	TIME [epoch: 0.709 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9266495618598151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9266495618598151 | validation: 0.8895711236502184]
	TIME [epoch: 0.711 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9125907137742718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9125907137742718 | validation: 0.556277882004589]
	TIME [epoch: 0.711 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448176724503922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7448176724503922 | validation: 0.6413237170084332]
	TIME [epoch: 0.715 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618973452738947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618973452738947 | validation: 0.7241599839401487]
	TIME [epoch: 0.711 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261738836565092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8261738836565092 | validation: 0.7122657826630276]
	TIME [epoch: 0.708 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008735331286089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8008735331286089 | validation: 0.6254334290761198]
	TIME [epoch: 0.71 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7646168371607691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7646168371607691 | validation: 0.5876337427158156]
	TIME [epoch: 0.711 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290665076485697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7290665076485697 | validation: 0.6492128043795856]
	TIME [epoch: 0.708 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7204199988707543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7204199988707543 | validation: 0.5651599930699701]
	TIME [epoch: 0.708 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7240390268349411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7240390268349411 | validation: 0.705114468635564]
	TIME [epoch: 0.708 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421554640534026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421554640534026 | validation: 0.646857073073654]
	TIME [epoch: 0.707 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856189736321977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7856189736321977 | validation: 0.7994757801760985]
	TIME [epoch: 0.707 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8439091458412469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8439091458412469 | validation: 0.7539221417885924]
	TIME [epoch: 0.708 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437358030541438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8437358030541438 | validation: 0.519223148412745]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7925650929283248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7925650929283248 | validation: 0.5953102787476956]
	TIME [epoch: 0.711 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7120631397924643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7120631397924643 | validation: 0.6090446926388404]
	TIME [epoch: 0.707 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7052699273984143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7052699273984143 | validation: 0.6210791393945142]
	TIME [epoch: 0.712 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711520990660151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711520990660151 | validation: 0.6688107033336181]
	TIME [epoch: 0.709 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7708127924042506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7708127924042506 | validation: 0.647273945671188]
	TIME [epoch: 0.708 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863364840937854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7863364840937854 | validation: 0.654764597086787]
	TIME [epoch: 0.708 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7529651116625964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7529651116625964 | validation: 0.7299693622379424]
	TIME [epoch: 0.708 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630160910112781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630160910112781 | validation: 0.6310745044052815]
	TIME [epoch: 0.711 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631844382567354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7631844382567354 | validation: 0.6512371733319431]
	TIME [epoch: 0.711 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741467495041249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.741467495041249 | validation: 0.5916729522484303]
	TIME [epoch: 0.709 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7066061173216025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7066061173216025 | validation: 0.6009285553065269]
	TIME [epoch: 0.704 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013340450544938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013340450544938 | validation: 0.6035630404123958]
	TIME [epoch: 0.707 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258980598390775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7258980598390775 | validation: 0.5931252756173067]
	TIME [epoch: 0.705 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7306425453283177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7306425453283177 | validation: 0.6397806757894156]
	TIME [epoch: 0.709 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595138321706608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7595138321706608 | validation: 0.6666977711733515]
	TIME [epoch: 0.71 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686703707448489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7686703707448489 | validation: 0.61543448398534]
	TIME [epoch: 0.709 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7631862146900434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7631862146900434 | validation: 0.5680450263151386]
	TIME [epoch: 0.709 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6932130651982648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6932130651982648 | validation: 0.663551323294716]
	TIME [epoch: 0.709 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274166454026171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7274166454026171 | validation: 0.604119217424772]
	TIME [epoch: 0.708 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327478834014841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7327478834014841 | validation: 0.5925953002437162]
	TIME [epoch: 0.711 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227254619413347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227254619413347 | validation: 0.5522975427863366]
	TIME [epoch: 0.71 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6840443469496235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6840443469496235 | validation: 0.6949159906080546]
	TIME [epoch: 0.708 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184612095698311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7184612095698311 | validation: 0.6833337106148362]
	TIME [epoch: 0.708 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969127079222387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7969127079222387 | validation: 0.6264962216519616]
	TIME [epoch: 0.708 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7635748402323276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7635748402323276 | validation: 0.5576640903081023]
	TIME [epoch: 0.709 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6803159783249743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6803159783249743 | validation: 0.6421035839721636]
	TIME [epoch: 0.708 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6986520664409008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6986520664409008 | validation: 0.5830255263145429]
	TIME [epoch: 0.709 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060066746842583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060066746842583 | validation: 0.587771351263214]
	TIME [epoch: 0.707 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7125056919681229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7125056919681229 | validation: 0.5851496368524413]
	TIME [epoch: 0.71 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228712583489074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7228712583489074 | validation: 0.6617733753840311]
	TIME [epoch: 0.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373691797311044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7373691797311044 | validation: 0.5859170183484518]
	TIME [epoch: 0.708 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.719745732594592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.719745732594592 | validation: 0.5467548879268069]
	TIME [epoch: 0.706 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813272691043275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6813272691043275 | validation: 0.548113735054363]
	TIME [epoch: 0.705 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.66176331049301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.66176331049301 | validation: 0.5520633822722062]
	TIME [epoch: 0.709 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690794277811599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690794277811599 | validation: 0.5260767489318882]
	TIME [epoch: 0.707 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6630561372663536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6630561372663536 | validation: 0.5422873109982591]
	TIME [epoch: 0.705 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6679943985121721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6679943985121721 | validation: 0.584160438615091]
	TIME [epoch: 0.707 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827970028187526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827970028187526 | validation: 0.5782267932981863]
	TIME [epoch: 0.706 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7293113576932688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7293113576932688 | validation: 0.5233948759268685]
	TIME [epoch: 0.705 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6621077675197995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6621077675197995 | validation: 0.6897421044771165]
	TIME [epoch: 0.707 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7530472290457777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7530472290457777 | validation: 0.6866972404976143]
	TIME [epoch: 0.706 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8216131082902745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8216131082902745 | validation: 0.6705533618864573]
	TIME [epoch: 0.705 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630928426648609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630928426648609 | validation: 0.4836799768075184]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618585597716721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618585597716721 | validation: 0.6217852142047272]
	TIME [epoch: 168 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842276922320233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842276922320233 | validation: 0.5726139022197879]
	TIME [epoch: 1.41 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6791304936595931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6791304936595931 | validation: 0.5059643244780355]
	TIME [epoch: 1.39 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6686996851240773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6686996851240773 | validation: 0.6172235741052562]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6921786449399936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6921786449399936 | validation: 0.6024883530829318]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439471620610645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439471620610645 | validation: 0.5996354010762563]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6944979866551162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6944979866551162 | validation: 0.5142529662080799]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569782680694862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569782680694862 | validation: 0.5303489422961233]
	TIME [epoch: 1.39 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6413021802282214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6413021802282214 | validation: 0.524704588667915]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6396194074935451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6396194074935451 | validation: 0.49884014736308346]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6332949121840724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6332949121840724 | validation: 0.5156114681009093]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334241427387138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6334241427387138 | validation: 0.5204975200822276]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6289214260799642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6289214260799642 | validation: 0.5048137047066272]
	TIME [epoch: 1.39 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491287800835233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6491287800835233 | validation: 0.5353728965389645]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6726538051065409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6726538051065409 | validation: 0.5967671529001529]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6882733177692215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6882733177692215 | validation: 0.5193682652699465]
	TIME [epoch: 1.39 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631244483477997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6631244483477997 | validation: 0.5240054892419163]
	TIME [epoch: 1.39 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6340015892020797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6340015892020797 | validation: 0.7422342960764515]
	TIME [epoch: 1.39 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341011673807643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341011673807643 | validation: 0.5147062902578748]
	TIME [epoch: 1.39 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871170881469008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6871170881469008 | validation: 0.5099997816521895]
	TIME [epoch: 1.39 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6239145438850743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6239145438850743 | validation: 0.6094079615210239]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6569809184029074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6569809184029074 | validation: 0.47984993232415996]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_222.pth
	Model improved!!!
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156989936424723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6156989936424723 | validation: 0.5196672293220173]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5982540805323693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5982540805323693 | validation: 0.5212068991817018]
	TIME [epoch: 1.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6112098744728748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6112098744728748 | validation: 0.5134772728120259]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6114644955395947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6114644955395947 | validation: 0.4903659838658195]
	TIME [epoch: 1.39 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6458640993942205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6458640993942205 | validation: 0.7634481046040152]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7045869174152791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7045869174152791 | validation: 0.5279041547443699]
	TIME [epoch: 1.39 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471871075708979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6471871075708979 | validation: 0.46432285048981803]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825722812910626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5825722812910626 | validation: 0.5761501583225481]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5917870357714341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5917870357714341 | validation: 0.489931384578557]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018581741225307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6018581741225307 | validation: 0.5030077214731365]
	TIME [epoch: 1.39 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5615031394173701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5615031394173701 | validation: 0.4673354913151029]
	TIME [epoch: 1.39 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508027984704474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508027984704474 | validation: 0.47013800951337353]
	TIME [epoch: 1.39 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5331396008443582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5331396008443582 | validation: 0.4839829847707301]
	TIME [epoch: 1.39 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518649335841785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.518649335841785 | validation: 0.43401676606388534]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5248679592081708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5248679592081708 | validation: 1.0453998787663163]
	TIME [epoch: 1.39 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9827090853691816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9827090853691816 | validation: 0.4671479583966074]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.669511689786876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.669511689786876 | validation: 0.4453318585384313]
	TIME [epoch: 1.39 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694159488631655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5694159488631655 | validation: 0.6418426321262516]
	TIME [epoch: 1.39 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6024679473123697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6024679473123697 | validation: 0.48210124345780453]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5464334836150018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5464334836150018 | validation: 0.46023921499567266]
	TIME [epoch: 1.38 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000652203484323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000652203484323 | validation: 0.4711261719392356]
	TIME [epoch: 1.39 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49334604707030366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49334604707030366 | validation: 0.4275602047617362]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49381160295709847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49381160295709847 | validation: 0.5460977791039847]
	TIME [epoch: 1.39 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5042198588945925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5042198588945925 | validation: 0.4249340456808044]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5466960808938777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5466960808938777 | validation: 0.5581762381056874]
	TIME [epoch: 1.39 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5111086924328984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5111086924328984 | validation: 0.41743815167747667]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4844196450548556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4844196450548556 | validation: 0.5164714485782893]
	TIME [epoch: 1.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4672496925788821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4672496925788821 | validation: 0.4240320594119572]
	TIME [epoch: 1.39 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5034880888909808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5034880888909808 | validation: 0.546494791169302]
	TIME [epoch: 1.39 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.491382646475892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.491382646475892 | validation: 0.4388328470833278]
	TIME [epoch: 1.39 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5284425085971285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5284425085971285 | validation: 0.47442662272460956]
	TIME [epoch: 1.39 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42152165282716825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42152165282716825 | validation: 0.4172626526045169]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3962198193127718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3962198193127718 | validation: 0.4399035193028068]
	TIME [epoch: 1.4 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38214337570200485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38214337570200485 | validation: 0.4265602599402971]
	TIME [epoch: 1.39 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5478091615927214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5478091615927214 | validation: 0.9131826713189721]
	TIME [epoch: 1.39 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8007521958165401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8007521958165401 | validation: 0.39274179483354027]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_258.pth
	Model improved!!!
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41922007229283603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41922007229283603 | validation: 0.414715451612081]
	TIME [epoch: 1.39 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38944491229326944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38944491229326944 | validation: 0.47202096381040165]
	TIME [epoch: 1.39 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4168589491185499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4168589491185499 | validation: 0.3981366396371142]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37300666402703464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37300666402703464 | validation: 0.4151484790682233]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488698767029805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3488698767029805 | validation: 0.3906095844574491]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32028689911442315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32028689911442315 | validation: 0.3223933065093859]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_264.pth
	Model improved!!!
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31015994168315386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31015994168315386 | validation: 0.8039159442765963]
	TIME [epoch: 1.4 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284643498462967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284643498462967 | validation: 0.47092414608887023]
	TIME [epoch: 1.39 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7687582036213434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7687582036213434 | validation: 0.28852146099894166]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5148788271671002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5148788271671002 | validation: 0.7241228515765656]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264418987809691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6264418987809691 | validation: 0.4755586150789792]
	TIME [epoch: 1.38 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3765189105660538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3765189105660538 | validation: 0.3773991968653892]
	TIME [epoch: 1.39 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4403355457796316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4403355457796316 | validation: 0.3959119371089605]
	TIME [epoch: 1.38 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3144168605982491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3144168605982491 | validation: 0.45836401188071046]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32087234171850826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32087234171850826 | validation: 0.3433437040635111]
	TIME [epoch: 1.39 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3107433252870812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3107433252870812 | validation: 0.4082557271647327]
	TIME [epoch: 1.39 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29532943251662924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29532943251662924 | validation: 0.35297951633196517]
	TIME [epoch: 1.39 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33816119682792706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33816119682792706 | validation: 0.5300811929890307]
	TIME [epoch: 1.39 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4086533733709361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4086533733709361 | validation: 0.35142308396339655]
	TIME [epoch: 1.39 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3750532900357536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3750532900357536 | validation: 0.3655498130741661]
	TIME [epoch: 1.39 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24973614042354902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24973614042354902 | validation: 0.3626315232678008]
	TIME [epoch: 1.39 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23682311850594218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23682311850594218 | validation: 0.31290442381899464]
	TIME [epoch: 1.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27680471594572886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27680471594572886 | validation: 0.5021439704257077]
	TIME [epoch: 1.39 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35985331726672604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35985331726672604 | validation: 0.33060059017704313]
	TIME [epoch: 1.4 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3983271501268028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3983271501268028 | validation: 0.3377227157857041]
	TIME [epoch: 1.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23965218288601264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23965218288601264 | validation: 0.35566907923807767]
	TIME [epoch: 1.39 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23900707375878674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23900707375878674 | validation: 0.3261179423369425]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2613876576607842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2613876576607842 | validation: 0.32953547404847483]
	TIME [epoch: 1.39 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26517939699618837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26517939699618837 | validation: 0.44222733908901724]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31927909199297616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31927909199297616 | validation: 0.366614904154654]
	TIME [epoch: 1.39 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5369443510924483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5369443510924483 | validation: 0.3137181880371886]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21226206918197052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21226206918197052 | validation: 0.476659903987345]
	TIME [epoch: 1.39 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3589100475753638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3589100475753638 | validation: 0.31113223254682354]
	TIME [epoch: 1.39 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28745373569016935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28745373569016935 | validation: 0.3422342347929408]
	TIME [epoch: 1.39 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24295506734484654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24295506734484654 | validation: 0.30948243918746526]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1903297452062825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1903297452062825 | validation: 0.2672972218546376]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1797509371443434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1797509371443434 | validation: 0.2939851316507083]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1928612398825606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1928612398825606 | validation: 0.2861012683157474]
	TIME [epoch: 1.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25427957096741227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25427957096741227 | validation: 0.49132193811092506]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39255339701682246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39255339701682246 | validation: 0.2851422772168345]
	TIME [epoch: 1.39 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2804305666586217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2804305666586217 | validation: 0.3959043234186932]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25306962625062807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25306962625062807 | validation: 0.3193755583036035]
	TIME [epoch: 1.39 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27128612596464907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27128612596464907 | validation: 0.28872246963275167]
	TIME [epoch: 1.4 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17436580203406613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17436580203406613 | validation: 0.28114476969705954]
	TIME [epoch: 1.4 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1659524094439981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1659524094439981 | validation: 0.190126355783474]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15072871231600624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15072871231600624 | validation: 0.22575381120091695]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13397321851688787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13397321851688787 | validation: 0.2438986273197303]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14541049615299392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14541049615299392 | validation: 0.3143500831670991]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.425516871721767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.425516871721767 | validation: 0.4900125172066642]
	TIME [epoch: 1.39 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39400093907705613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39400093907705613 | validation: 0.3338867191693848]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28355005752867846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28355005752867846 | validation: 0.34457668479288245]
	TIME [epoch: 1.38 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062974632026176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2062974632026176 | validation: 0.2999347572600059]
	TIME [epoch: 1.39 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18073534675991437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18073534675991437 | validation: 0.2465364421129207]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15831395096643827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15831395096643827 | validation: 0.23094136699857334]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13606224802155722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13606224802155722 | validation: 0.1921431240686693]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1166544162895322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1166544162895322 | validation: 0.19033163938136793]
	TIME [epoch: 1.38 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12103753039697428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12103753039697428 | validation: 0.2929311749934181]
	TIME [epoch: 1.38 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2122836246291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2122836246291 | validation: 0.40768333043010574]
	TIME [epoch: 1.38 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6222075323231444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6222075323231444 | validation: 0.3872256039064004]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2668235167036026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2668235167036026 | validation: 0.30201799714135213]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19502935844838676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19502935844838676 | validation: 0.28000280221629253]
	TIME [epoch: 1.39 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1650116664068921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1650116664068921 | validation: 0.29041533345744436]
	TIME [epoch: 1.39 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599974674852847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1599974674852847 | validation: 0.2133869452371877]
	TIME [epoch: 1.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12910590373362502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12910590373362502 | validation: 0.19560142962780488]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1071226152193729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1071226152193729 | validation: 0.20266501256237773]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10068339317601989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10068339317601989 | validation: 0.15833588068189589]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1072508101792732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1072508101792732 | validation: 0.22800901798969686]
	TIME [epoch: 1.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1347233999132079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1347233999132079 | validation: 0.2357669954668336]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29603993823135005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29603993823135005 | validation: 0.42942222086706494]
	TIME [epoch: 1.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3364131250027248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3364131250027248 | validation: 0.25519792536609714]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16740735409569016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16740735409569016 | validation: 0.2790018529028198]
	TIME [epoch: 1.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.164077778936146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.164077778936146 | validation: 0.2604579171251577]
	TIME [epoch: 1.39 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12337099115080687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12337099115080687 | validation: 0.20088390868242254]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10791993762718617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10791993762718617 | validation: 0.21454873082048184]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11516091598211846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11516091598211846 | validation: 0.23403603631602193]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18728108383331638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18728108383331638 | validation: 0.46578693743524724]
	TIME [epoch: 1.39 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38204783614718524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38204783614718524 | validation: 0.3087709414948079]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3604163784295912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3604163784295912 | validation: 0.26138703668026364]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1413798245173232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1413798245173232 | validation: 0.2935358133569995]
	TIME [epoch: 1.38 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576961130314623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1576961130314623 | validation: 0.2684232430125103]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1414413231549211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1414413231549211 | validation: 0.20514386741418178]
	TIME [epoch: 1.38 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10884254686622796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10884254686622796 | validation: 0.2036404027601443]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09288122464127892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09288122464127892 | validation: 0.16073793658828534]
	TIME [epoch: 1.38 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07776714100004663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07776714100004663 | validation: 0.15618959058970525]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07627617116647371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07627617116647371 | validation: 0.17165275609198247]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10944729962807882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10944729962807882 | validation: 0.26809201779840836]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2935021190618128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2935021190618128 | validation: 0.5781168001151213]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42377942682666786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42377942682666786 | validation: 0.24598197902715946]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2121202219962703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2121202219962703 | validation: 0.2989768301050128]
	TIME [epoch: 1.38 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522106645606959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522106645606959 | validation: 0.27599394743913147]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16636905232997087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16636905232997087 | validation: 0.2376207784489508]
	TIME [epoch: 1.38 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15144911045333348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15144911045333348 | validation: 0.2194776580022742]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11834611611061767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11834611611061767 | validation: 0.1915382894278113]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10466614169101862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10466614169101862 | validation: 0.14594512077455662]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09575148603308276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09575148603308276 | validation: 0.17466046220394454]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10193047464816508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10193047464816508 | validation: 0.1677109481025683]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14288147759000672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14288147759000672 | validation: 0.36642004107446846]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2730570658692559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2730570658692559 | validation: 0.2769122476012473]
	TIME [epoch: 1.39 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2768163376333631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2768163376333631 | validation: 0.261564402598402]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1460362507941908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1460362507941908 | validation: 0.20521258674683432]
	TIME [epoch: 1.38 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09599595237268035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09599595237268035 | validation: 0.18227653563537663]
	TIME [epoch: 1.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874632786064091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874632786064091 | validation: 0.18396847885216272]
	TIME [epoch: 1.38 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09097381401385128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09097381401385128 | validation: 0.15586852885100377]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10288554202907443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10288554202907443 | validation: 0.17482662654957934]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10072844079167158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10072844079167158 | validation: 0.22557096253604458]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1449592091715908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1449592091715908 | validation: 0.29219872350617104]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2247877485303537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2247877485303537 | validation: 0.28092423856639676]
	TIME [epoch: 1.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19204928509512237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19204928509512237 | validation: 0.17145120233107491]
	TIME [epoch: 1.39 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1401586434236993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1401586434236993 | validation: 0.2194357850457882]
	TIME [epoch: 1.41 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1155552188158897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1155552188158897 | validation: 0.1985149809154579]
	TIME [epoch: 1.39 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13422014424146667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13422014424146667 | validation: 0.24908638766000013]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13789286117221206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13789286117221206 | validation: 0.20168490775726022]
	TIME [epoch: 1.39 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11839093600526181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11839093600526181 | validation: 0.19932900255953015]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12333419851980086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12333419851980086 | validation: 0.18482557842461353]
	TIME [epoch: 1.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14098396544555733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14098396544555733 | validation: 0.38280651981990343]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2697359327804926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2697359327804926 | validation: 0.21752341229107774]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16392502884879054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16392502884879054 | validation: 0.21533910728115427]
	TIME [epoch: 1.38 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10473906256732228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10473906256732228 | validation: 0.17242015436255223]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07491342661337179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07491342661337179 | validation: 0.15976945714924817]
	TIME [epoch: 1.38 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0753942753581487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0753942753581487 | validation: 0.15852258653742057]
	TIME [epoch: 1.38 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07147849490027829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07147849490027829 | validation: 0.21184818390369695]
	TIME [epoch: 1.39 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16911979303262228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16911979303262228 | validation: 0.38338789008233026]
	TIME [epoch: 1.38 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27138038984003593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27138038984003593 | validation: 0.20616525798383512]
	TIME [epoch: 1.38 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17890265034935973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17890265034935973 | validation: 0.18154371947276396]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10538187807405254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10538187807405254 | validation: 0.1332540203528276]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_383.pth
	Model improved!!!
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06795632753779997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06795632753779997 | validation: 0.13078298441887246]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07716633255455105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07716633255455105 | validation: 0.17455862163221825]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176845572120989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176845572120989 | validation: 0.23483829582901905]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2037383023092743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2037383023092743 | validation: 0.3116998272393954]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2396744090146264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2396744090146264 | validation: 0.19756197393582942]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15119358152462462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15119358152462462 | validation: 0.14038211877888407]
	TIME [epoch: 1.39 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07066935193583715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07066935193583715 | validation: 0.1315486750113362]
	TIME [epoch: 1.38 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06704764196085987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06704764196085987 | validation: 0.123504056363904]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647429587236327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0647429587236327 | validation: 0.127622520775357]
	TIME [epoch: 1.39 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644664647922922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0644664647922922 | validation: 0.12972541490782252]
	TIME [epoch: 1.39 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07775272119672068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07775272119672068 | validation: 0.1985205157999324]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15363420244412807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15363420244412807 | validation: 0.4948470767345657]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555743446273245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3555743446273245 | validation: 0.2046825684711241]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1561561076024136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1561561076024136 | validation: 0.21512103006307104]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12660059250928737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12660059250928737 | validation: 0.1851879246690708]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12282985572486838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12282985572486838 | validation: 0.19394855121246227]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12058516963469319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12058516963469319 | validation: 0.1710016755885757]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08957837449381462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08957837449381462 | validation: 0.15535976392363915]
	TIME [epoch: 1.4 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07249239425374447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07249239425374447 | validation: 0.13554097532270226]
	TIME [epoch: 1.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07145889959864732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07145889959864732 | validation: 0.15487574136277257]
	TIME [epoch: 1.42 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09117338879462943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09117338879462943 | validation: 0.20763871265252334]
	TIME [epoch: 1.4 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18608831288387015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18608831288387015 | validation: 0.39536196537864754]
	TIME [epoch: 1.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3027860466185279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3027860466185279 | validation: 0.19844891094030137]
	TIME [epoch: 1.39 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13301090017324402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13301090017324402 | validation: 0.1842179780993335]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09597230719998194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09597230719998194 | validation: 0.17111154292950215]
	TIME [epoch: 1.39 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07672432465107425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07672432465107425 | validation: 0.14772421578202374]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0900387519932508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0900387519932508 | validation: 0.14816939454070943]
	TIME [epoch: 1.39 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07746298898262712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07746298898262712 | validation: 0.1285522621436146]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07800196244544914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07800196244544914 | validation: 0.15166139494957204]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10553279278118076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10553279278118076 | validation: 0.1682971279655702]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12622387557477885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12622387557477885 | validation: 0.16128040302356686]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13661392074955864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13661392074955864 | validation: 0.1849896498055814]
	TIME [epoch: 1.39 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12995223137037368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12995223137037368 | validation: 0.2181406452259103]
	TIME [epoch: 1.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21166212006075377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21166212006075377 | validation: 0.3248045230476495]
	TIME [epoch: 1.39 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23775696288147974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23775696288147974 | validation: 0.18795178842062163]
	TIME [epoch: 1.4 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1112677963132806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1112677963132806 | validation: 0.1691686941563853]
	TIME [epoch: 1.39 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06648544232327555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06648544232327555 | validation: 0.16408755825811552]
	TIME [epoch: 1.39 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086690253648437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086690253648437 | validation: 0.13667526438956995]
	TIME [epoch: 1.39 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804310284227617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06804310284227617 | validation: 0.1240377499228951]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431133481212943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07431133481212943 | validation: 0.14961540188436515]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10358519466628958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10358519466628958 | validation: 0.2093244617527626]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14806968435494222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14806968435494222 | validation: 0.21167730260150086]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16450303255497478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16450303255497478 | validation: 0.213397845877445]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14073147302668468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14073147302668468 | validation: 0.14451152480931373]
	TIME [epoch: 1.39 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0818564397233591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0818564397233591 | validation: 0.1174537771548636]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_428.pth
	Model improved!!!
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06631039874627703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06631039874627703 | validation: 0.11724717255445444]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_429.pth
	Model improved!!!
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07725271297578505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07725271297578505 | validation: 0.14151201768577623]
	TIME [epoch: 1.39 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10718838935735167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10718838935735167 | validation: 0.20501149289624898]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451993007397482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1451993007397482 | validation: 0.1990298570651282]
	TIME [epoch: 1.39 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.133461683035449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.133461683035449 | validation: 0.20036624500903388]
	TIME [epoch: 1.39 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12366261862061492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12366261862061492 | validation: 0.142245513629339]
	TIME [epoch: 1.39 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0776128634246598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0776128634246598 | validation: 0.1461747009667704]
	TIME [epoch: 1.39 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0680085570333896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0680085570333896 | validation: 0.14378919094896453]
	TIME [epoch: 1.39 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09321021055211827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09321021055211827 | validation: 0.1870230986699714]
	TIME [epoch: 1.39 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11740750911347388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11740750911347388 | validation: 0.19241959678048481]
	TIME [epoch: 1.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13607368188253458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13607368188253458 | validation: 0.19995059626846723]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1508991404940507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1508991404940507 | validation: 0.20155200703800671]
	TIME [epoch: 1.4 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17504974726875422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17504974726875422 | validation: 0.25638552631377765]
	TIME [epoch: 1.4 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15886220072927734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15886220072927734 | validation: 0.1452911842775256]
	TIME [epoch: 1.4 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10448766253185615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10448766253185615 | validation: 0.13106195554041827]
	TIME [epoch: 1.4 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05721844422297896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05721844422297896 | validation: 0.14759937319294544]
	TIME [epoch: 1.4 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05929518822437455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05929518822437455 | validation: 0.10779755297371914]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_445.pth
	Model improved!!!
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05778107202975392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05778107202975392 | validation: 0.10776645593376291]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_446.pth
	Model improved!!!
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06002314108282514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06002314108282514 | validation: 0.1116603170583342]
	TIME [epoch: 1.4 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07072777195601773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07072777195601773 | validation: 0.20362521615164308]
	TIME [epoch: 1.39 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14376271082433065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14376271082433065 | validation: 0.26126999309431675]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2990387834831272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2990387834831272 | validation: 0.26950733069611016]
	TIME [epoch: 1.39 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1917294605145399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1917294605145399 | validation: 0.1675543852726611]
	TIME [epoch: 1.4 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0996948442612578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0996948442612578 | validation: 0.1801608078136273]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08985237495682831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08985237495682831 | validation: 0.1951249573361027]
	TIME [epoch: 1.39 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973467915209283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0973467915209283 | validation: 0.17017331950917583]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08301532667413407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08301532667413407 | validation: 0.1301532934127397]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07564039002119713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07564039002119713 | validation: 0.12963006893160525]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08219819347986329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08219819347986329 | validation: 0.12077595719156309]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08488070101401815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08488070101401815 | validation: 0.11889625917452136]
	TIME [epoch: 1.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841956246119403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0841956246119403 | validation: 0.14621663065994145]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11474929144724673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11474929144724673 | validation: 0.23397391326621758]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1678013146926739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1678013146926739 | validation: 0.21041503236818132]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17636447974250602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17636447974250602 | validation: 0.2396619523244882]
	TIME [epoch: 1.39 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13862782184467298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13862782184467298 | validation: 0.1523943999485322]
	TIME [epoch: 1.39 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295049578042406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08295049578042406 | validation: 0.12776831171971362]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973525391577336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05973525391577336 | validation: 0.12445173114922886]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057580346501294066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057580346501294066 | validation: 0.11932765348873212]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057103172660080234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057103172660080234 | validation: 0.1199496321577426]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07017481746579984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07017481746579984 | validation: 0.12926174706938362]
	TIME [epoch: 1.39 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09610888721842382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09610888721842382 | validation: 0.1656750293326819]
	TIME [epoch: 1.4 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12433364951638792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12433364951638792 | validation: 0.1903500829236755]
	TIME [epoch: 1.4 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14625544294118173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14625544294118173 | validation: 0.25974035180670996]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17952230766865193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17952230766865193 | validation: 0.18376123673251996]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13672960438188617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13672960438188617 | validation: 0.27650569105885897]
	TIME [epoch: 1.4 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17417337527229976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17417337527229976 | validation: 0.15302822206038247]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08894921775608261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08894921775608261 | validation: 0.12081601032138178]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06325875311795479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06325875311795479 | validation: 0.14175466452519175]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062371017234225404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062371017234225404 | validation: 0.13404614217773322]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804667046286454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06804667046286454 | validation: 0.12179654724897744]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0715557612234636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0715557612234636 | validation: 0.13638005779473525]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08756579214516468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08756579214516468 | validation: 0.1547306857745281]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11139957060515605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11139957060515605 | validation: 0.17403607900458642]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12770664724287656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12770664724287656 | validation: 0.16886963567957528]
	TIME [epoch: 1.39 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11225578155838267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11225578155838267 | validation: 0.13255768612886384]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08075430304476297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08075430304476297 | validation: 0.12783214951253954]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07084663929024407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07084663929024407 | validation: 0.13422010047414012]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07209894596257402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07209894596257402 | validation: 0.14127100801840667]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08730655875240133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08730655875240133 | validation: 0.159788447030751]
	TIME [epoch: 1.45 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13750648405693297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13750648405693297 | validation: 0.17824377611809927]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17198149354066047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17198149354066047 | validation: 0.2351362361728432]
	TIME [epoch: 1.4 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1502991558219947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1502991558219947 | validation: 0.12834032994178105]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641092178715174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641092178715174 | validation: 0.15365262737154878]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07950110785768157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07950110785768157 | validation: 0.14431916811550433]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298215828222236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298215828222236 | validation: 0.11178842616657084]
	TIME [epoch: 1.4 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06264260896987763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06264260896987763 | validation: 0.10977669087459546]
	TIME [epoch: 1.4 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05805528761793083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05805528761793083 | validation: 0.1079965890771054]
	TIME [epoch: 1.4 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07378760190359643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07378760190359643 | validation: 0.17912430759695286]
	TIME [epoch: 1.4 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14243458200880074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14243458200880074 | validation: 0.4805355478500508]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36838641869651567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36838641869651567 | validation: 0.1458746756585852]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0877217219177732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0877217219177732 | validation: 0.14957177818968317]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08747665487884888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08747665487884888 | validation: 0.16859968590574695]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07519229419891142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07519229419891142 | validation: 0.14091553123657372]
	TIME [epoch: 172 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710326504622093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07710326504622093 | validation: 0.13074459201401925]
	TIME [epoch: 2.78 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05516424469178883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05516424469178883 | validation: 0.10289999500802094]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_503.pth
	Model improved!!!
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05253244493600264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05253244493600264 | validation: 0.11033877075536842]
	TIME [epoch: 2.76 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06695956049705215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06695956049705215 | validation: 0.13074659425073995]
	TIME [epoch: 2.75 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1011087666306696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1011087666306696 | validation: 0.16052033906562263]
	TIME [epoch: 2.76 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12714476526470797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12714476526470797 | validation: 0.13657370660412735]
	TIME [epoch: 2.76 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10006784805425505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10006784805425505 | validation: 0.11022008248573215]
	TIME [epoch: 2.76 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08065781451906091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08065781451906091 | validation: 0.1945864396324228]
	TIME [epoch: 2.76 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15121373906354346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15121373906354346 | validation: 0.20082950130427576]
	TIME [epoch: 2.76 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18446209991417603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18446209991417603 | validation: 0.2389524797591674]
	TIME [epoch: 2.76 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12865078615474765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12865078615474765 | validation: 0.12442392595586949]
	TIME [epoch: 2.76 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05718337958385619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05718337958385619 | validation: 0.10228932081370484]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_513.pth
	Model improved!!!
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04840627395094067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04840627395094067 | validation: 0.12468172465144672]
	TIME [epoch: 2.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0564990995073031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0564990995073031 | validation: 0.10189323314852938]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050514564579044095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050514564579044095 | validation: 0.08367403438878603]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_516.pth
	Model improved!!!
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311907609332263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04311907609332263 | validation: 0.08483329044076206]
	TIME [epoch: 2.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047433733851991886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047433733851991886 | validation: 0.11227797935417368]
	TIME [epoch: 2.75 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08859673421787849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08859673421787849 | validation: 0.23032219860143008]
	TIME [epoch: 2.74 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1952490128770809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1952490128770809 | validation: 0.2667972993923124]
	TIME [epoch: 2.75 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21375864391437424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21375864391437424 | validation: 0.13646803718340447]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07578092811857252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07578092811857252 | validation: 0.09724495714767871]
	TIME [epoch: 2.74 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03810658541802111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03810658541802111 | validation: 0.09888831804332991]
	TIME [epoch: 2.74 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044162204448023046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044162204448023046 | validation: 0.10507189869222594]
	TIME [epoch: 2.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051890499788443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051890499788443 | validation: 0.10115171424595795]
	TIME [epoch: 2.74 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07212021522892553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07212021522892553 | validation: 0.15442093316449565]
	TIME [epoch: 2.74 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373547936091061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373547936091061 | validation: 0.19582271467794782]
	TIME [epoch: 2.75 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14622476231170603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14622476231170603 | validation: 0.16334458967185145]
	TIME [epoch: 2.74 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1234027536431229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1234027536431229 | validation: 0.12473612632898666]
	TIME [epoch: 2.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09153286296659202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09153286296659202 | validation: 0.11423370742542632]
	TIME [epoch: 2.74 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06827686297539963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06827686297539963 | validation: 0.14542161545410656]
	TIME [epoch: 2.74 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895679743905064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0895679743905064 | validation: 0.15094347306538594]
	TIME [epoch: 2.74 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10260683591493487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10260683591493487 | validation: 0.12980294422509064]
	TIME [epoch: 2.75 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07978639805636842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07978639805636842 | validation: 0.10486731283035754]
	TIME [epoch: 2.75 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0733627276271041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0733627276271041 | validation: 0.14657607315770452]
	TIME [epoch: 2.75 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913894873584038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11913894873584038 | validation: 0.17034119275985207]
	TIME [epoch: 2.75 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10440051218875061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10440051218875061 | validation: 0.13307021783672374]
	TIME [epoch: 2.74 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07933715391401361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07933715391401361 | validation: 0.130294711373062]
	TIME [epoch: 2.75 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05402198014284476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05402198014284476 | validation: 0.09719790820178453]
	TIME [epoch: 2.74 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03720338552150565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03720338552150565 | validation: 0.07503756555183447]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03643767734981469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03643767734981469 | validation: 0.10107162082005093]
	TIME [epoch: 2.77 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045801159631786915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045801159631786915 | validation: 0.1074211614766327]
	TIME [epoch: 2.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08098896633453496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08098896633453496 | validation: 0.20164720061353802]
	TIME [epoch: 2.75 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15346810612056522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15346810612056522 | validation: 0.2601517836033926]
	TIME [epoch: 2.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001774372856928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2001774372856928 | validation: 0.10918931569588489]
	TIME [epoch: 2.76 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07451875560052311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07451875560052311 | validation: 0.1384928543778999]
	TIME [epoch: 2.76 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05989870478623894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05989870478623894 | validation: 0.1590867921836415]
	TIME [epoch: 2.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08880296802153499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08880296802153499 | validation: 0.15509000882619645]
	TIME [epoch: 2.76 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11777043184186231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11777043184186231 | validation: 0.15139272427235173]
	TIME [epoch: 2.76 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11496423665250845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11496423665250845 | validation: 0.14932529221100274]
	TIME [epoch: 2.76 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09016172163628328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09016172163628328 | validation: 0.09275291394091413]
	TIME [epoch: 2.76 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060853747435372925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060853747435372925 | validation: 0.1156416644332786]
	TIME [epoch: 2.76 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05371545357065195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05371545357065195 | validation: 0.08689624490241371]
	TIME [epoch: 2.76 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05795343526317681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05795343526317681 | validation: 0.12275364457920833]
	TIME [epoch: 2.76 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07008551784132552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07008551784132552 | validation: 0.13433158547660837]
	TIME [epoch: 2.75 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08246457513115438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08246457513115438 | validation: 0.1221934026370269]
	TIME [epoch: 2.76 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08414641836317356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08414641836317356 | validation: 0.12602859444010134]
	TIME [epoch: 2.76 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09156148591638122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09156148591638122 | validation: 0.14147160328679795]
	TIME [epoch: 2.76 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08105377839995764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08105377839995764 | validation: 0.1284228376925876]
	TIME [epoch: 2.75 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08930921250501253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08930921250501253 | validation: 0.15634224350118453]
	TIME [epoch: 2.76 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09781298982737305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09781298982737305 | validation: 0.1350861300944685]
	TIME [epoch: 2.76 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10022383254100664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10022383254100664 | validation: 0.10218361708936673]
	TIME [epoch: 2.77 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0701277891161817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0701277891161817 | validation: 0.0637370615132667]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_563.pth
	Model improved!!!
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04338850599475661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04338850599475661 | validation: 0.06281054235425587]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_564.pth
	Model improved!!!
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03744449497710688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03744449497710688 | validation: 0.06776972235247514]
	TIME [epoch: 2.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04923443778847483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04923443778847483 | validation: 0.28548681995228653]
	TIME [epoch: 2.74 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21202598198901598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21202598198901598 | validation: 0.2224017248418143]
	TIME [epoch: 2.75 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2208950979230252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2208950979230252 | validation: 0.2168101148442564]
	TIME [epoch: 2.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11401094617334082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11401094617334082 | validation: 0.1240179088590965]
	TIME [epoch: 2.76 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07018729442726881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07018729442726881 | validation: 0.11570998272495432]
	TIME [epoch: 2.76 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0542105477725473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0542105477725473 | validation: 0.1307480935319851]
	TIME [epoch: 2.76 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05080772448410837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05080772448410837 | validation: 0.12025173023008782]
	TIME [epoch: 2.75 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049899393661934355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049899393661934355 | validation: 0.11618569553137009]
	TIME [epoch: 2.76 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608152168132428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05608152168132428 | validation: 0.12806538025136638]
	TIME [epoch: 2.75 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850111896156497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0850111896156497 | validation: 0.15405478178225995]
	TIME [epoch: 2.76 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12841678807873635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12841678807873635 | validation: 0.11794785568845334]
	TIME [epoch: 2.75 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10423428224141429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10423428224141429 | validation: 0.12263957536798492]
	TIME [epoch: 2.76 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07990847321966148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07990847321966148 | validation: 0.21367095994978724]
	TIME [epoch: 2.74 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15191827226166962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15191827226166962 | validation: 0.1571471907817379]
	TIME [epoch: 2.76 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11347742310436829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11347742310436829 | validation: 0.15706212183560866]
	TIME [epoch: 2.74 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255405416773174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07255405416773174 | validation: 0.09383779559543529]
	TIME [epoch: 2.76 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047757155456342994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047757155456342994 | validation: 0.09466990550281028]
	TIME [epoch: 2.75 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03468619622084316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03468619622084316 | validation: 0.10715288781263327]
	TIME [epoch: 2.76 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0354222562486604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0354222562486604 | validation: 0.07487502754574314]
	TIME [epoch: 2.75 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03367892412525041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03367892412525041 | validation: 0.06805137077746454]
	TIME [epoch: 2.76 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03180818047092294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03180818047092294 | validation: 0.06971090201463856]
	TIME [epoch: 2.75 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03892226517518681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03892226517518681 | validation: 0.09026629732674772]
	TIME [epoch: 2.77 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07485235946525011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07485235946525011 | validation: 0.2123959766632611]
	TIME [epoch: 2.76 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17450187102240403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17450187102240403 | validation: 0.3092477381130941]
	TIME [epoch: 2.76 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22407771449148844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22407771449148844 | validation: 0.12912334623557414]
	TIME [epoch: 2.75 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461156048854875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05461156048854875 | validation: 0.16913603345565784]
	TIME [epoch: 2.77 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07632591064038602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07632591064038602 | validation: 0.14876644217293086]
	TIME [epoch: 2.76 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616835039089802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09616835039089802 | validation: 0.13080723000963893]
	TIME [epoch: 2.77 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08151585705611691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08151585705611691 | validation: 0.13502490129498299]
	TIME [epoch: 2.75 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07116211597044116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07116211597044116 | validation: 0.1175288981338164]
	TIME [epoch: 2.75 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07526083040154825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07526083040154825 | validation: 0.12283004614700684]
	TIME [epoch: 2.75 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0778585205685828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0778585205685828 | validation: 0.1155678999390951]
	TIME [epoch: 2.75 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0879535377141453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0879535377141453 | validation: 0.12934691800488837]
	TIME [epoch: 2.76 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0861756467446288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0861756467446288 | validation: 0.10888394615684988]
	TIME [epoch: 2.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08452093002040247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08452093002040247 | validation: 0.1163424257563439]
	TIME [epoch: 2.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548890068532705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06548890068532705 | validation: 0.11134207135654883]
	TIME [epoch: 2.76 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05707272408958091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05707272408958091 | validation: 0.0858604292843078]
	TIME [epoch: 2.76 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04825112957151469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04825112957151469 | validation: 0.07525138815624009]
	TIME [epoch: 2.75 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04650426206057965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04650426206057965 | validation: 0.08625153996569136]
	TIME [epoch: 2.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017067894086312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05017067894086312 | validation: 0.07040982826099107]
	TIME [epoch: 2.75 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439986076014818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06439986076014818 | validation: 0.1629125221207682]
	TIME [epoch: 2.75 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11803172604999042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11803172604999042 | validation: 0.18193384132276252]
	TIME [epoch: 2.75 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13314230906176114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13314230906176114 | validation: 0.1861914793449104]
	TIME [epoch: 2.75 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14152805279257233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14152805279257233 | validation: 0.16736581749840168]
	TIME [epoch: 2.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09031421374138805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09031421374138805 | validation: 0.10784797323367741]
	TIME [epoch: 2.75 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04876946705749537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04876946705749537 | validation: 0.09553042251361046]
	TIME [epoch: 2.75 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040332449961066184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040332449961066184 | validation: 0.08949746751833614]
	TIME [epoch: 2.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039307261875175505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039307261875175505 | validation: 0.07321605847352224]
	TIME [epoch: 2.75 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04646955683138581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04646955683138581 | validation: 0.09268085332377163]
	TIME [epoch: 2.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05435193451303397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05435193451303397 | validation: 0.08828652779146795]
	TIME [epoch: 2.76 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07247350784121612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07247350784121612 | validation: 0.1196922161544634]
	TIME [epoch: 2.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08891665676085037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08891665676085037 | validation: 0.2722972711582178]
	TIME [epoch: 2.75 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20746389029948703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20746389029948703 | validation: 0.18282650385790344]
	TIME [epoch: 2.76 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15841449983786837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15841449983786837 | validation: 0.1900328225213438]
	TIME [epoch: 2.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10457973786955506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10457973786955506 | validation: 0.13279641915200271]
	TIME [epoch: 2.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07711307809841776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07711307809841776 | validation: 0.11673676504578996]
	TIME [epoch: 2.75 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046355066984876195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046355066984876195 | validation: 0.11741316396643822]
	TIME [epoch: 2.76 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04295692344550988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04295692344550988 | validation: 0.08505726582775244]
	TIME [epoch: 2.75 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034034157961694386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034034157961694386 | validation: 0.0790344874773379]
	TIME [epoch: 2.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03479630471107859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03479630471107859 | validation: 0.08456948135364824]
	TIME [epoch: 2.75 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03821086627403742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03821086627403742 | validation: 0.07981715219482133]
	TIME [epoch: 2.76 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057943094023220135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057943094023220135 | validation: 0.14680533773744436]
	TIME [epoch: 2.76 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09315764783033574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09315764783033574 | validation: 0.17189283217950102]
	TIME [epoch: 2.76 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1378361431069344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1378361431069344 | validation: 0.16157818727885945]
	TIME [epoch: 2.76 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10538833565444126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10538833565444126 | validation: 0.16766185140258927]
	TIME [epoch: 2.75 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11506028938620252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11506028938620252 | validation: 0.1500359486782168]
	TIME [epoch: 2.76 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10617281450988404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10617281450988404 | validation: 0.09163404569347532]
	TIME [epoch: 2.76 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06153373573941254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06153373573941254 | validation: 0.07381518591952983]
	TIME [epoch: 2.76 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031188718850147266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031188718850147266 | validation: 0.054356631346388844]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02527181474926904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02527181474926904 | validation: 0.047089262158376145]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_635.pth
	Model improved!!!
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025552234595999677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025552234595999677 | validation: 0.04892172915645957]
	TIME [epoch: 2.75 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029299664710400616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029299664710400616 | validation: 0.06484688184608142]
	TIME [epoch: 2.76 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056712629090680644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056712629090680644 | validation: 0.1993284514793278]
	TIME [epoch: 2.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20145072241552966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20145072241552966 | validation: 0.2825465239247402]
	TIME [epoch: 2.74 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21430462856951793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21430462856951793 | validation: 0.15131056865843454]
	TIME [epoch: 2.74 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10499370202314282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10499370202314282 | validation: 0.1403830222159456]
	TIME [epoch: 2.74 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04681246944255963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04681246944255963 | validation: 0.11777699045198271]
	TIME [epoch: 2.75 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043439556895765286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043439556895765286 | validation: 0.0941088218028252]
	TIME [epoch: 2.74 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0451963217238915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0451963217238915 | validation: 0.10617762606538067]
	TIME [epoch: 2.74 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806993121970911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03806993121970911 | validation: 0.09075762414446668]
	TIME [epoch: 2.74 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04113410284207228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04113410284207228 | validation: 0.06707246278859127]
	TIME [epoch: 2.74 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507369332799929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0507369332799929 | validation: 0.1424283342027811]
	TIME [epoch: 2.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08653161914932853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08653161914932853 | validation: 0.1777576749346723]
	TIME [epoch: 2.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1468638183819167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1468638183819167 | validation: 0.168039330303173]
	TIME [epoch: 2.75 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1144877028438352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1144877028438352 | validation: 0.10743900422977033]
	TIME [epoch: 2.74 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060551588381533426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060551588381533426 | validation: 0.08657032991762595]
	TIME [epoch: 2.75 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04570715966361218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04570715966361218 | validation: 0.12877925150127348]
	TIME [epoch: 2.74 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07317161590086661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07317161590086661 | validation: 0.16552207975571084]
	TIME [epoch: 2.75 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11089495192960643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11089495192960643 | validation: 0.09795592955798466]
	TIME [epoch: 2.75 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205136402782395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08205136402782395 | validation: 0.05747787687365385]
	TIME [epoch: 2.74 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03617055480776375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03617055480776375 | validation: 0.08010267932641503]
	TIME [epoch: 2.75 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029737923671525737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029737923671525737 | validation: 0.07566759223141709]
	TIME [epoch: 2.74 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04966486776762279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04966486776762279 | validation: 0.1434513843716124]
	TIME [epoch: 2.76 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12066380395108248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12066380395108248 | validation: 0.17118804777655386]
	TIME [epoch: 2.76 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12218440527688254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12218440527688254 | validation: 0.13669363952804056]
	TIME [epoch: 2.76 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10004524281072814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10004524281072814 | validation: 0.1396945025164154]
	TIME [epoch: 2.76 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058766578578988184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058766578578988184 | validation: 0.11131363854448624]
	TIME [epoch: 2.76 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05160834632297026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05160834632297026 | validation: 0.10615708827825768]
	TIME [epoch: 2.76 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06903458232874059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06903458232874059 | validation: 0.12052099840323854]
	TIME [epoch: 2.76 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06227663215584436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06227663215584436 | validation: 0.08567321436514498]
	TIME [epoch: 2.76 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05507019943789384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05507019943789384 | validation: 0.05592978937150705]
	TIME [epoch: 2.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044166226189491696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044166226189491696 | validation: 0.06962616924230934]
	TIME [epoch: 2.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03470328122149379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03470328122149379 | validation: 0.04946616917042736]
	TIME [epoch: 2.76 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029615555378608303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029615555378608303 | validation: 0.05195119568178598]
	TIME [epoch: 2.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02973321927105807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02973321927105807 | validation: 0.06829972479296868]
	TIME [epoch: 2.75 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043089038702682124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043089038702682124 | validation: 0.20094418867327515]
	TIME [epoch: 2.75 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15689624047924486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15689624047924486 | validation: 0.3022340907051934]
	TIME [epoch: 2.75 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34462313132146705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34462313132146705 | validation: 0.23625498506783058]
	TIME [epoch: 2.75 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13880369340216725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13880369340216725 | validation: 0.10588088545987764]
	TIME [epoch: 2.75 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06835621349857438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06835621349857438 | validation: 0.11917066052277439]
	TIME [epoch: 2.74 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06593244950686183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06593244950686183 | validation: 0.14217596419170325]
	TIME [epoch: 2.74 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229675496257208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06229675496257208 | validation: 0.1043036353034427]
	TIME [epoch: 2.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042461926557509154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042461926557509154 | validation: 0.07475338132292801]
	TIME [epoch: 2.75 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038645490899045835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038645490899045835 | validation: 0.07826356274355954]
	TIME [epoch: 2.75 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031130725559389553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031130725559389553 | validation: 0.06514817796577659]
	TIME [epoch: 2.75 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035905614325147155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035905614325147155 | validation: 0.06718931451652152]
	TIME [epoch: 2.75 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055395781655634586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055395781655634586 | validation: 0.13558994656863793]
	TIME [epoch: 2.75 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09595568368700663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09595568368700663 | validation: 0.19760098721918193]
	TIME [epoch: 2.75 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15951072035220235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15951072035220235 | validation: 0.14956680726620086]
	TIME [epoch: 2.75 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09545931416306619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09545931416306619 | validation: 0.09186429942243836]
	TIME [epoch: 2.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513802495674824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04513802495674824 | validation: 0.09061591066278951]
	TIME [epoch: 2.74 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03938473989891776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03938473989891776 | validation: 0.10151798200239066]
	TIME [epoch: 2.75 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046260770368002586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046260770368002586 | validation: 0.10884912485279348]
	TIME [epoch: 2.74 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05962848593116414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05962848593116414 | validation: 0.09647034649976179]
	TIME [epoch: 2.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076552620866926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.076552620866926 | validation: 0.13488049656621878]
	TIME [epoch: 2.75 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09026766850556626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09026766850556626 | validation: 0.1258246187597277]
	TIME [epoch: 2.75 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10936683327268563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10936683327268563 | validation: 0.15085711066665047]
	TIME [epoch: 2.76 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08569404078218704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08569404078218704 | validation: 0.0973307255855422]
	TIME [epoch: 2.75 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053207159238276125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053207159238276125 | validation: 0.06319952787451547]
	TIME [epoch: 2.74 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644543141338213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03644543141338213 | validation: 0.07692936908732931]
	TIME [epoch: 2.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02875430999023136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02875430999023136 | validation: 0.05160438052505417]
	TIME [epoch: 2.75 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027027108310031096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027027108310031096 | validation: 0.049121423888720595]
	TIME [epoch: 2.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030439356909835555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030439356909835555 | validation: 0.07535330655577625]
	TIME [epoch: 2.75 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049536970016896796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049536970016896796 | validation: 0.11102433383076855]
	TIME [epoch: 2.75 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09771566778797106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09771566778797106 | validation: 0.19073826663477728]
	TIME [epoch: 2.75 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17109962407305226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17109962407305226 | validation: 0.2123840971360933]
	TIME [epoch: 2.76 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13083625877927965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13083625877927965 | validation: 0.09220473960432159]
	TIME [epoch: 2.76 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044679040947335213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044679040947335213 | validation: 0.1365446130470287]
	TIME [epoch: 2.76 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04862663607013144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04862663607013144 | validation: 0.11167584959574633]
	TIME [epoch: 2.75 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04615693578252039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04615693578252039 | validation: 0.07164155187712262]
	TIME [epoch: 2.75 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053468768024078656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053468768024078656 | validation: 0.11364361554599355]
	TIME [epoch: 2.75 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07142506701203559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07142506701203559 | validation: 0.13448974074648498]
	TIME [epoch: 2.76 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11020902359373114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11020902359373114 | validation: 0.16487261722945856]
	TIME [epoch: 2.75 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12257968983695776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12257968983695776 | validation: 0.11143259565480708]
	TIME [epoch: 2.75 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08197905322386713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08197905322386713 | validation: 0.09621985978002223]
	TIME [epoch: 2.75 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048288074074967796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048288074074967796 | validation: 0.05502854305423369]
	TIME [epoch: 2.76 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027922744436343096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027922744436343096 | validation: 0.05803530665595874]
	TIME [epoch: 2.76 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02445996098638817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02445996098638817 | validation: 0.0548022352855879]
	TIME [epoch: 2.76 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023154563371541863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023154563371541863 | validation: 0.04064235961630285]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_714.pth
	Model improved!!!
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030418020990093626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030418020990093626 | validation: 0.0862684680422141]
	TIME [epoch: 2.75 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05153350071557891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05153350071557891 | validation: 0.12080680958415352]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10843586651955871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10843586651955871 | validation: 0.2039315597791113]
	TIME [epoch: 2.75 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17117221504822847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17117221504822847 | validation: 0.18261704415707525]
	TIME [epoch: 2.75 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12880236868728034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12880236868728034 | validation: 0.10326759804010616]
	TIME [epoch: 2.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08153291578072301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08153291578072301 | validation: 0.10005021249777715]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942709293223337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03942709293223337 | validation: 0.08567319811314922]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032844375660344184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032844375660344184 | validation: 0.07188762620015907]
	TIME [epoch: 2.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03489229780864771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03489229780864771 | validation: 0.06733428847792314]
	TIME [epoch: 2.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034483302522586905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034483302522586905 | validation: 0.07390188703481053]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651648588071962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03651648588071962 | validation: 0.0693882122244761]
	TIME [epoch: 2.75 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04679306773756382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04679306773756382 | validation: 0.123267160478188]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600788480627714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600788480627714 | validation: 0.16165347513482678]
	TIME [epoch: 2.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14055809432039076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14055809432039076 | validation: 0.19885676977708094]
	TIME [epoch: 2.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14559125477574014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14559125477574014 | validation: 0.13538171051561637]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07198275776620011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07198275776620011 | validation: 0.09223712825270874]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05640229939187644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05640229939187644 | validation: 0.1081214667664217]
	TIME [epoch: 2.75 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0469812229611353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0469812229611353 | validation: 0.12726073057985868]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373951033977307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06373951033977307 | validation: 0.08748679475536052]
	TIME [epoch: 2.74 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06916433524671727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06916433524671727 | validation: 0.08004911475955448]
	TIME [epoch: 2.75 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04775036562219975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04775036562219975 | validation: 0.0835550301988973]
	TIME [epoch: 2.74 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036990029047390315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036990029047390315 | validation: 0.05087867236537741]
	TIME [epoch: 2.74 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03335886265983898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03335886265983898 | validation: 0.06494629555432695]
	TIME [epoch: 2.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033336722149573984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033336722149573984 | validation: 0.08493011895386275]
	TIME [epoch: 2.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05497568454597495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05497568454597495 | validation: 0.24334936494399015]
	TIME [epoch: 2.75 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18707854505869054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18707854505869054 | validation: 0.15929969240707603]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10192763098947488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10192763098947488 | validation: 0.13762179236507463]
	TIME [epoch: 2.75 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057606403830163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057606403830163 | validation: 0.10123843453678308]
	TIME [epoch: 2.75 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06039650645594576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06039650645594576 | validation: 0.10864099592426367]
	TIME [epoch: 2.75 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05475835965262591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05475835965262591 | validation: 0.13360813237778799]
	TIME [epoch: 2.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723704932153112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0723704932153112 | validation: 0.10977125371467394]
	TIME [epoch: 2.75 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07287988380588584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07287988380588584 | validation: 0.08159353411341189]
	TIME [epoch: 2.75 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0534887405440972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0534887405440972 | validation: 0.06257884518404168]
	TIME [epoch: 2.75 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031653518211289026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031653518211289026 | validation: 0.04787954644027362]
	TIME [epoch: 2.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02157978079976162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02157978079976162 | validation: 0.03807110463600943]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_749.pth
	Model improved!!!
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022348153932436058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022348153932436058 | validation: 0.07900708827913452]
	TIME [epoch: 2.76 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04097305111510107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04097305111510107 | validation: 0.1973836382496543]
	TIME [epoch: 2.75 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15137100286163654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15137100286163654 | validation: 0.21644094679396453]
	TIME [epoch: 2.75 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16683032921338828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16683032921338828 | validation: 0.15015682931517604]
	TIME [epoch: 2.75 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510843303037436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09510843303037436 | validation: 0.0829855673402101]
	TIME [epoch: 2.75 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04900032999905623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04900032999905623 | validation: 0.10386046956248603]
	TIME [epoch: 2.76 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03544419844750781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03544419844750781 | validation: 0.09522753271990135]
	TIME [epoch: 2.76 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043025631603228885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043025631603228885 | validation: 0.06686262377234313]
	TIME [epoch: 2.77 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04773330100851995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04773330100851995 | validation: 0.08589575388992836]
	TIME [epoch: 2.75 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05133946488445938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05133946488445938 | validation: 0.08572873028193909]
	TIME [epoch: 2.76 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06758345463597952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06758345463597952 | validation: 0.09168280139794177]
	TIME [epoch: 2.75 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06961345428954369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06961345428954369 | validation: 0.06966580510105895]
	TIME [epoch: 2.76 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05411975157420035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05411975157420035 | validation: 0.08546032172035244]
	TIME [epoch: 2.76 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05161407148083261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05161407148083261 | validation: 0.17813064777400617]
	TIME [epoch: 2.76 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12620214954837952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12620214954837952 | validation: 0.15634557461660126]
	TIME [epoch: 2.76 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11522308485069502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11522308485069502 | validation: 0.12709111886590893]
	TIME [epoch: 2.75 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06658648685629051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06658648685629051 | validation: 0.07943955575884618]
	TIME [epoch: 2.75 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05950913076100714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05950913076100714 | validation: 0.08795464764553891]
	TIME [epoch: 2.75 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02777211878021074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02777211878021074 | validation: 0.09599438447811591]
	TIME [epoch: 2.75 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030430329096681048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030430329096681048 | validation: 0.062360395580588625]
	TIME [epoch: 2.75 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02785881163436044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02785881163436044 | validation: 0.05751130293303504]
	TIME [epoch: 2.76 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026084729240748273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026084729240748273 | validation: 0.07815976945658587]
	TIME [epoch: 2.75 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04482667333351767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04482667333351767 | validation: 0.11155662811402721]
	TIME [epoch: 2.76 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09883888772608955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09883888772608955 | validation: 0.18949281196036707]
	TIME [epoch: 2.75 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14173097776507063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14173097776507063 | validation: 0.13098254325193692]
	TIME [epoch: 2.76 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10721746853024458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10721746853024458 | validation: 0.11274996424022639]
	TIME [epoch: 2.75 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06364232977355169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06364232977355169 | validation: 0.12823650358758815]
	TIME [epoch: 2.75 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06994290942285536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06994290942285536 | validation: 0.0875797511014463]
	TIME [epoch: 2.77 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058364524968981084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058364524968981084 | validation: 0.06037539984411971]
	TIME [epoch: 2.76 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349540966133147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03349540966133147 | validation: 0.07046756863347985]
	TIME [epoch: 2.76 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026147117527337213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026147117527337213 | validation: 0.037958599575882025]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_780.pth
	Model improved!!!
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026916184335712304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026916184335712304 | validation: 0.07023423776419258]
	TIME [epoch: 2.75 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02943159106340909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02943159106340909 | validation: 0.07479289864447301]
	TIME [epoch: 2.75 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05701325226893619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05701325226893619 | validation: 0.1393530403031946]
	TIME [epoch: 2.75 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248156037299794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10248156037299794 | validation: 0.16181394489673007]
	TIME [epoch: 2.75 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12016045881871641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12016045881871641 | validation: 0.09756702187608468]
	TIME [epoch: 2.75 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06302194811710207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06302194811710207 | validation: 0.10566393440300825]
	TIME [epoch: 2.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852022221731513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03852022221731513 | validation: 0.0717844197422559]
	TIME [epoch: 2.75 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038865668599681776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038865668599681776 | validation: 0.08967901719929236]
	TIME [epoch: 2.75 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06469708098724279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06469708098724279 | validation: 0.12809302940652928]
	TIME [epoch: 2.75 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711082988403655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08711082988403655 | validation: 0.09115367807577678]
	TIME [epoch: 2.75 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08131578045001807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08131578045001807 | validation: 0.11566349948725488]
	TIME [epoch: 2.76 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07061503571965597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07061503571965597 | validation: 0.12614399955802802]
	TIME [epoch: 2.75 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10154226554888215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10154226554888215 | validation: 0.13081929972403072]
	TIME [epoch: 2.75 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08432465321858552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08432465321858552 | validation: 0.08726191502000769]
	TIME [epoch: 2.74 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608488623409829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03608488623409829 | validation: 0.06442071801476577]
	TIME [epoch: 2.75 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03220489546772752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03220489546772752 | validation: 0.07099680193475764]
	TIME [epoch: 2.75 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022890986226873286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022890986226873286 | validation: 0.06266721203156879]
	TIME [epoch: 2.85 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019595639811513462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019595639811513462 | validation: 0.04757936118065747]
	TIME [epoch: 2.75 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02352720066442329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02352720066442329 | validation: 0.06084690599256696]
	TIME [epoch: 2.75 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029579255497489302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029579255497489302 | validation: 0.059683474952407535]
	TIME [epoch: 2.75 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878606864480766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04878606864480766 | validation: 0.09721968416733227]
	TIME [epoch: 2.75 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10102300705670776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10102300705670776 | validation: 0.22282124828406435]
	TIME [epoch: 2.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17788287153599006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17788287153599006 | validation: 0.23292135616579446]
	TIME [epoch: 2.74 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16951776285918294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16951776285918294 | validation: 0.09639001010285347]
	TIME [epoch: 2.75 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05840925799162026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05840925799162026 | validation: 0.11227588921051446]
	TIME [epoch: 2.74 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041548052547522286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041548052547522286 | validation: 0.08360240217086301]
	TIME [epoch: 2.75 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039702163471277044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039702163471277044 | validation: 0.08985902942112234]
	TIME [epoch: 2.74 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03850686388864109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03850686388864109 | validation: 0.08510689684390188]
	TIME [epoch: 2.76 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0361191535989278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0361191535989278 | validation: 0.0666900578886882]
	TIME [epoch: 2.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201803202029162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03201803202029162 | validation: 0.05962502992516133]
	TIME [epoch: 2.76 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02944368370399051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02944368370399051 | validation: 0.06866252537089455]
	TIME [epoch: 2.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02978281595895395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02978281595895395 | validation: 0.04518845691590481]
	TIME [epoch: 2.76 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03385446591808499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03385446591808499 | validation: 0.08677682350955584]
	TIME [epoch: 2.75 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04791796647206411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04791796647206411 | validation: 0.08014297325555313]
	TIME [epoch: 2.76 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320295953591906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320295953591906 | validation: 0.1912558741442928]
	TIME [epoch: 2.76 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13789988275956552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13789988275956552 | validation: 0.20751419705332302]
	TIME [epoch: 2.76 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15680754230221497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15680754230221497 | validation: 0.09652084156838373]
	TIME [epoch: 2.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07730929720697632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07730929720697632 | validation: 0.08560819340056007]
	TIME [epoch: 2.75 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027887342127783567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027887342127783567 | validation: 0.08166931126921423]
	TIME [epoch: 2.75 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031734419162428584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031734419162428584 | validation: 0.06483665027795477]
	TIME [epoch: 2.76 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0359151073216781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0359151073216781 | validation: 0.06136101896036013]
	TIME [epoch: 2.75 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0297462067948036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0297462067948036 | validation: 0.06263931803917477]
	TIME [epoch: 2.75 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027311421485528533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027311421485528533 | validation: 0.04648883241786564]
	TIME [epoch: 2.74 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375931829079022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0375931829079022 | validation: 0.13933694465919938]
	TIME [epoch: 2.74 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08565070331246798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08565070331246798 | validation: 0.1989078253556499]
	TIME [epoch: 2.74 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15628658312679122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15628658312679122 | validation: 0.16151893149214788]
	TIME [epoch: 2.74 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09785657627207794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09785657627207794 | validation: 0.10570931008313889]
	TIME [epoch: 2.75 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046174381517617075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046174381517617075 | validation: 0.08105489941211731]
	TIME [epoch: 2.74 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04470672085936404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04470672085936404 | validation: 0.09378687233809947]
	TIME [epoch: 2.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03751995399438863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03751995399438863 | validation: 0.1226101287458064]
	TIME [epoch: 2.75 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06145955363485151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06145955363485151 | validation: 0.12203558977977154]
	TIME [epoch: 2.75 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08794697639127044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08794697639127044 | validation: 0.11386759959445239]
	TIME [epoch: 2.75 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06894577452842728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06894577452842728 | validation: 0.06387835272168944]
	TIME [epoch: 2.74 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03518080401849822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03518080401849822 | validation: 0.0511382735293994]
	TIME [epoch: 2.75 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022408887046845197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022408887046845197 | validation: 0.04045067312636254]
	TIME [epoch: 2.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023405859868687547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023405859868687547 | validation: 0.04001321184422177]
	TIME [epoch: 2.76 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027611618332072985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027611618332072985 | validation: 0.06071877972052456]
	TIME [epoch: 2.75 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037484838538257334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037484838538257334 | validation: 0.05973801854049887]
	TIME [epoch: 2.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04645783910278478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04645783910278478 | validation: 0.09522147173728077]
	TIME [epoch: 2.75 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08091290772038721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08091290772038721 | validation: 0.21764583511018679]
	TIME [epoch: 2.75 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22380352288739225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22380352288739225 | validation: 0.14846040564734583]
	TIME [epoch: 2.75 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09371122478627858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09371122478627858 | validation: 0.0606328776121997]
	TIME [epoch: 2.75 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045434533944688694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045434533944688694 | validation: 0.10416879914608357]
	TIME [epoch: 2.75 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707058682217595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03707058682217595 | validation: 0.08670262545346305]
	TIME [epoch: 2.75 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034285676531879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034285676531879 | validation: 0.05276633647230651]
	TIME [epoch: 2.75 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037141522561005946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037141522561005946 | validation: 0.08374905715766709]
	TIME [epoch: 2.75 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032937529684597434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032937529684597434 | validation: 0.08022496587228771]
	TIME [epoch: 2.75 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04094127117710455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04094127117710455 | validation: 0.08473817067030501]
	TIME [epoch: 2.75 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388360356891778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07388360356891778 | validation: 0.13482200483837734]
	TIME [epoch: 2.75 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08698312307277871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08698312307277871 | validation: 0.05488172967665114]
	TIME [epoch: 2.75 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05664516139301716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05664516139301716 | validation: 0.1039693120891176]
	TIME [epoch: 2.74 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05000874476736443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05000874476736443 | validation: 0.09362125138550303]
	TIME [epoch: 2.75 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07181456468040218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07181456468040218 | validation: 0.14148107413744607]
	TIME [epoch: 2.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09461871477745781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09461871477745781 | validation: 0.09658076294030088]
	TIME [epoch: 2.75 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05867336388006268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05867336388006268 | validation: 0.05124230255953734]
	TIME [epoch: 2.75 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022770659875175733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022770659875175733 | validation: 0.05708092011857277]
	TIME [epoch: 2.74 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01828320583217817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01828320583217817 | validation: 0.047720872167126706]
	TIME [epoch: 2.74 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02284778311678185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02284778311678185 | validation: 0.06168380252124958]
	TIME [epoch: 2.74 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03258583011698274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03258583011698274 | validation: 0.07159348602434266]
	TIME [epoch: 2.74 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060733868601668205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060733868601668205 | validation: 0.14971191000776876]
	TIME [epoch: 2.74 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11071806013904016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11071806013904016 | validation: 0.15608810490486183]
	TIME [epoch: 2.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10648974382642971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10648974382642971 | validation: 0.08352699801071534]
	TIME [epoch: 2.75 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03251835924805475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03251835924805475 | validation: 0.08574556395184336]
	TIME [epoch: 2.75 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029191730068561093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029191730068561093 | validation: 0.07292149146289556]
	TIME [epoch: 2.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034038112857012276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034038112857012276 | validation: 0.06711329239201445]
	TIME [epoch: 2.74 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035492919642121715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035492919642121715 | validation: 0.08955059267694232]
	TIME [epoch: 2.75 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055318010619667186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055318010619667186 | validation: 0.11199473226626887]
	TIME [epoch: 2.74 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09892395812953102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09892395812953102 | validation: 0.1965049368677307]
	TIME [epoch: 2.74 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14189743487535325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14189743487535325 | validation: 0.12626955774490067]
	TIME [epoch: 2.74 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08882597970444983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08882597970444983 | validation: 0.07091772497958612]
	TIME [epoch: 2.74 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02885868649322874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02885868649322874 | validation: 0.07723212266135081]
	TIME [epoch: 2.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028130512744309166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028130512744309166 | validation: 0.054465886430516953]
	TIME [epoch: 2.74 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02348347057615655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02348347057615655 | validation: 0.055546958875900346]
	TIME [epoch: 2.75 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023987490676518747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023987490676518747 | validation: 0.05026932162720813]
	TIME [epoch: 2.74 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029708517965316244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029708517965316244 | validation: 0.06887517669403428]
	TIME [epoch: 2.74 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04326006940900128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04326006940900128 | validation: 0.0662371139993089]
	TIME [epoch: 2.74 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677067192447824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07677067192447824 | validation: 0.134367102488818]
	TIME [epoch: 2.75 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08658960602490096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08658960602490096 | validation: 0.07272853345307731]
	TIME [epoch: 2.75 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823735680380322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05823735680380322 | validation: 0.0946945637472978]
	TIME [epoch: 2.75 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04452505560812158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04452505560812158 | validation: 0.08216514704187151]
	TIME [epoch: 2.75 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462929328318422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06462929328318422 | validation: 0.08696557332235885]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_122309/states/model_phi1_4a_v_mmd1_881.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2165.977 seconds.
