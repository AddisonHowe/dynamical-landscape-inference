Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1647313701

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.390889955172092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.390889955172092 | validation: 3.9533781759518023]
	TIME [epoch: 163 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.397378939395143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.397378939395143 | validation: 5.1376626373753655]
	TIME [epoch: 2.86 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.427312604426678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.427312604426678 | validation: 3.87274587218693]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.249928459920195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.249928459920195 | validation: 4.558309646230758]
	TIME [epoch: 2.83 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.894241808222956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.894241808222956 | validation: 4.114129624697148]
	TIME [epoch: 2.83 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.547946243198969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.547946243198969 | validation: 3.6425900061845575]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.085222720652517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085222720652517 | validation: 3.449294835516959]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.963154058781671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.963154058781671 | validation: 3.316447773570029]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.816998871626651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.816998871626651 | validation: 3.052438225528891]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.571073828793174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.571073828793174 | validation: 2.850599807015306]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.354924134931756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.354924134931756 | validation: 2.6026063807009248]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.085015570934554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.085015570934554 | validation: 2.5773805204446756]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.202695916435658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.202695916435658 | validation: 2.051462359021363]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5022712602753567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5022712602753567 | validation: 2.0565573782245306]
	TIME [epoch: 2.84 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9368189355913588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9368189355913588 | validation: 1.9464913509544257]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.858138584247437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.858138584247437 | validation: 1.4731499344797063]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9604720018666808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9604720018666808 | validation: 1.635149117520843]
	TIME [epoch: 2.83 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.043385462310652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.043385462310652 | validation: 1.7863552122223738]
	TIME [epoch: 2.83 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4265562132215184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4265562132215184 | validation: 1.6428304937023661]
	TIME [epoch: 2.83 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.037901374593815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.037901374593815 | validation: 1.187607545861226]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5789417038712419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5789417038712419 | validation: 1.2223664737255477]
	TIME [epoch: 2.84 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5528629852342886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5528629852342886 | validation: 1.2007401177973105]
	TIME [epoch: 2.83 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5119987310955751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5119987310955751 | validation: 1.0923975880550505]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4120037528929306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4120037528929306 | validation: 1.0808376156531272]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5118281734757961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5118281734757961 | validation: 1.0020254320321642]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2435602894062268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2435602894062268 | validation: 1.0367042164281794]
	TIME [epoch: 2.84 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3351009453304818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3351009453304818 | validation: 0.9124123822545066]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1828887387924727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1828887387924727 | validation: 0.9271877129876833]
	TIME [epoch: 2.84 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1013276560593548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1013276560593548 | validation: 0.8914871613688865]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0794479822590544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0794479822590544 | validation: 0.8509328310403195]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0883212098049166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0883212098049166 | validation: 0.8573821353155172]
	TIME [epoch: 2.84 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0390087373243575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0390087373243575 | validation: 0.940340876723235]
	TIME [epoch: 2.84 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0547488940401788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0547488940401788 | validation: 0.8869796081496784]
	TIME [epoch: 2.83 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0566659520412975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0566659520412975 | validation: 0.9052572457401891]
	TIME [epoch: 2.83 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0170271228606333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0170271228606333 | validation: 0.8599728051219953]
	TIME [epoch: 2.83 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.026535474431809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.026535474431809 | validation: 0.8833156383555336]
	TIME [epoch: 2.83 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0579797279927456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0579797279927456 | validation: 0.8155135784992642]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0555762331450564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0555762331450564 | validation: 0.8766140861102394]
	TIME [epoch: 2.83 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0994815066763364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0994815066763364 | validation: 0.8605093972432819]
	TIME [epoch: 2.83 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0288820724021537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0288820724021537 | validation: 0.915714719945055]
	TIME [epoch: 2.83 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0299106923328136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0299106923328136 | validation: 0.8260192202934834]
	TIME [epoch: 2.83 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9849502956307457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9849502956307457 | validation: 0.8097500487830396]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9565202116521772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9565202116521772 | validation: 0.8278221947250289]
	TIME [epoch: 2.83 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.956777978588836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.956777978588836 | validation: 0.7738742471334303]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9877982465214743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9877982465214743 | validation: 0.8416359987471648]
	TIME [epoch: 2.85 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796664650912783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9796664650912783 | validation: 0.8330849670899682]
	TIME [epoch: 2.85 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0738436108913103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0738436108913103 | validation: 0.8229757542335201]
	TIME [epoch: 2.84 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0558628543579016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0558628543579016 | validation: 0.8006835391559683]
	TIME [epoch: 2.83 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.955779052671915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.955779052671915 | validation: 0.8310798710513305]
	TIME [epoch: 2.83 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9838000874131265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9838000874131265 | validation: 0.8401032096799336]
	TIME [epoch: 2.83 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9657746832114603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9657746832114603 | validation: 0.7966350324852075]
	TIME [epoch: 2.83 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9256113477209507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9256113477209507 | validation: 0.8180022301982529]
	TIME [epoch: 2.83 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447571467538138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9447571467538138 | validation: 0.828101051079236]
	TIME [epoch: 2.83 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.089987778180298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.089987778180298 | validation: 0.9048746606220139]
	TIME [epoch: 2.83 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0484255031324907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0484255031324907 | validation: 0.7627674834778486]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9263565197328211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9263565197328211 | validation: 0.7430103622779629]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343655418492014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9343655418492014 | validation: 0.7662815079867525]
	TIME [epoch: 2.84 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9161133024207393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9161133024207393 | validation: 0.7162931059996728]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9933968622041158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9933968622041158 | validation: 0.782963326622423]
	TIME [epoch: 2.85 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9037168240980334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9037168240980334 | validation: 0.7963490111758852]
	TIME [epoch: 2.84 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.08331675972282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.08331675972282 | validation: 0.822063814209919]
	TIME [epoch: 2.84 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9558446832078105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9558446832078105 | validation: 0.7700371192279332]
	TIME [epoch: 2.84 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8997429134677429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8997429134677429 | validation: 0.7018728785483587]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9398864719004203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9398864719004203 | validation: 0.7212141701481866]
	TIME [epoch: 2.85 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049553178126368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049553178126368 | validation: 0.8306566419561844]
	TIME [epoch: 2.84 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9481156694482304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9481156694482304 | validation: 0.7915333085942274]
	TIME [epoch: 2.84 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010419269200094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010419269200094 | validation: 0.7067585232258441]
	TIME [epoch: 2.85 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8926196263657619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8926196263657619 | validation: 0.8277994678943629]
	TIME [epoch: 2.85 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9618505599885644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9618505599885644 | validation: 0.7871164212571367]
	TIME [epoch: 2.84 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9771402456252872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9771402456252872 | validation: 0.7070318893490175]
	TIME [epoch: 2.84 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9039163187994715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9039163187994715 | validation: 0.8197328537288711]
	TIME [epoch: 2.84 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017828164506508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017828164506508 | validation: 0.7234510185216356]
	TIME [epoch: 2.84 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880575743658313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8880575743658313 | validation: 0.745297577497127]
	TIME [epoch: 2.84 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9555318567384613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9555318567384613 | validation: 0.8825690472102995]
	TIME [epoch: 2.84 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079361488706945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.079361488706945 | validation: 0.7947134456067346]
	TIME [epoch: 2.84 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9703050343847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9703050343847 | validation: 0.7763537684967965]
	TIME [epoch: 2.84 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9639047221924778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9639047221924778 | validation: 0.7279591271887286]
	TIME [epoch: 2.84 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9176379190427351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9176379190427351 | validation: 0.7076918844593029]
	TIME [epoch: 2.84 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8830488459435355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8830488459435355 | validation: 0.7830228100565599]
	TIME [epoch: 2.85 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9362508219705161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9362508219705161 | validation: 0.7161100994986214]
	TIME [epoch: 2.84 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9364630558342651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9364630558342651 | validation: 0.7560122339015376]
	TIME [epoch: 2.85 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238640838440236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238640838440236 | validation: 0.773361551948501]
	TIME [epoch: 2.84 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9127177274726205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9127177274726205 | validation: 0.7191893740578897]
	TIME [epoch: 2.84 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8734846265321116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8734846265321116 | validation: 0.7271974896855117]
	TIME [epoch: 2.84 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814047017880767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814047017880767 | validation: 0.7013967736033454]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074435915575018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074435915575018 | validation: 0.7497457513902251]
	TIME [epoch: 2.85 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9390032512672485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9390032512672485 | validation: 0.7402790026529429]
	TIME [epoch: 2.84 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8958110961163425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8958110961163425 | validation: 0.7080803631068413]
	TIME [epoch: 2.84 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8987712585173121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8987712585173121 | validation: 0.7397374458814587]
	TIME [epoch: 2.84 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983227181160345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983227181160345 | validation: 0.7936264739242458]
	TIME [epoch: 2.85 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9441258729435299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9441258729435299 | validation: 0.7532599403619805]
	TIME [epoch: 2.84 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9249258547492323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9249258547492323 | validation: 0.7210125617482371]
	TIME [epoch: 2.84 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8774258670431647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8774258670431647 | validation: 0.7986367278004645]
	TIME [epoch: 2.84 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9908243006142772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9908243006142772 | validation: 0.7012410365972335]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851470405517375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851470405517375 | validation: 0.7185013963323552]
	TIME [epoch: 2.84 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9251584275748723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9251584275748723 | validation: 0.7896215526811038]
	TIME [epoch: 2.84 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9617178230820657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9617178230820657 | validation: 0.7089736232955698]
	TIME [epoch: 2.84 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659726545784011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659726545784011 | validation: 0.710010715100186]
	TIME [epoch: 2.84 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661647571444289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661647571444289 | validation: 0.7037901418128643]
	TIME [epoch: 2.85 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8622233237889927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8622233237889927 | validation: 0.7593828634343028]
	TIME [epoch: 2.85 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9025313082250588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9025313082250588 | validation: 0.7598177284097081]
	TIME [epoch: 2.84 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0001107517087984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0001107517087984 | validation: 0.7599403358102427]
	TIME [epoch: 2.84 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268386335346478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9268386335346478 | validation: 0.8322696805358629]
	TIME [epoch: 2.85 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360171766238263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9360171766238263 | validation: 0.7731050893407315]
	TIME [epoch: 2.85 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9622254369979504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9622254369979504 | validation: 0.6991623026800614]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_105.pth
	Model improved!!!
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8983191136659323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8983191136659323 | validation: 0.7834279623591464]
	TIME [epoch: 2.85 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.90676619501156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.90676619501156 | validation: 0.7099522820968712]
	TIME [epoch: 2.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728629136989335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728629136989335 | validation: 0.7087080845855408]
	TIME [epoch: 2.84 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814899688794392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814899688794392 | validation: 0.709426381129167]
	TIME [epoch: 2.84 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673871689835153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673871689835153 | validation: 0.7727663484065743]
	TIME [epoch: 2.84 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9288966409554437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9288966409554437 | validation: 0.7065893241024231]
	TIME [epoch: 2.84 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8896943161300352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8896943161300352 | validation: 0.7653233619164297]
	TIME [epoch: 2.84 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9177942740010357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9177942740010357 | validation: 0.7811378229920657]
	TIME [epoch: 2.84 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052568893169405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9052568893169405 | validation: 0.7573826910765592]
	TIME [epoch: 2.84 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9352773073427989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9352773073427989 | validation: 0.7547724225882497]
	TIME [epoch: 2.84 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862850267745769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862850267745769 | validation: 0.7656446446929943]
	TIME [epoch: 2.84 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9167440192013437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9167440192013437 | validation: 0.7820860707339081]
	TIME [epoch: 2.84 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9434425670677452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9434425670677452 | validation: 0.7264810785504573]
	TIME [epoch: 2.84 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143734625246946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9143734625246946 | validation: 0.7735393248160185]
	TIME [epoch: 2.85 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9174557604810474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9174557604810474 | validation: 0.7259092877095954]
	TIME [epoch: 2.85 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343229929814655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9343229929814655 | validation: 0.7345939891276045]
	TIME [epoch: 2.85 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798023558839247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8798023558839247 | validation: 0.7504826537718694]
	TIME [epoch: 2.85 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814959725414977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814959725414977 | validation: 0.8175981326299842]
	TIME [epoch: 2.85 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0077854956680394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0077854956680394 | validation: 0.7297166438740281]
	TIME [epoch: 2.85 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818733426712018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818733426712018 | validation: 0.7731658891091828]
	TIME [epoch: 2.85 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9844396216020749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9844396216020749 | validation: 0.8025382981215036]
	TIME [epoch: 2.85 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9495331948657909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9495331948657909 | validation: 0.7683333759431474]
	TIME [epoch: 2.85 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951362436366553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8951362436366553 | validation: 0.69495359452254]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86239565902171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.86239565902171 | validation: 0.7180559185336509]
	TIME [epoch: 2.85 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652979569202919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8652979569202919 | validation: 0.7762420664376031]
	TIME [epoch: 2.84 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224564021097088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224564021097088 | validation: 0.7577679647683224]
	TIME [epoch: 2.85 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9410993273591777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9410993273591777 | validation: 0.7555179404055521]
	TIME [epoch: 2.85 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9264856217555365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9264856217555365 | validation: 0.7593968153042916]
	TIME [epoch: 2.85 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9146307962492385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9146307962492385 | validation: 0.8978672400807004]
	TIME [epoch: 2.84 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1273061131221123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1273061131221123 | validation: 0.8461494910329755]
	TIME [epoch: 2.85 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0072644726440951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0072644726440951 | validation: 0.7278461571539121]
	TIME [epoch: 2.85 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890952844631659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8890952844631659 | validation: 0.7340866178430187]
	TIME [epoch: 2.85 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005976789874822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005976789874822 | validation: 0.7387737658800129]
	TIME [epoch: 2.85 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014177369218095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014177369218095 | validation: 0.7097590385685831]
	TIME [epoch: 2.85 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8947508324107639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8947508324107639 | validation: 0.7229279427282252]
	TIME [epoch: 2.85 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8856081350980983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8856081350980983 | validation: 0.7229199501172417]
	TIME [epoch: 2.85 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8745376750689223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8745376750689223 | validation: 0.7110660167183274]
	TIME [epoch: 2.85 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8763034533121765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8763034533121765 | validation: 0.7031326974938072]
	TIME [epoch: 2.85 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052112461828068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9052112461828068 | validation: 0.6986370380276012]
	TIME [epoch: 2.85 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841066538063942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841066538063942 | validation: 0.7592754071752751]
	TIME [epoch: 2.85 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8914740230185788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8914740230185788 | validation: 0.7094451657193824]
	TIME [epoch: 2.84 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9001727402307165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9001727402307165 | validation: 0.7837705196658709]
	TIME [epoch: 2.85 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284596393501577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9284596393501577 | validation: 0.7906192769848112]
	TIME [epoch: 2.85 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9409884973389344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9409884973389344 | validation: 0.7590172057688641]
	TIME [epoch: 2.85 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9142602554938923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9142602554938923 | validation: 0.7057063827970561]
	TIME [epoch: 2.85 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8664992808936965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8664992808936965 | validation: 0.7684515080201727]
	TIME [epoch: 2.85 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964194548751795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964194548751795 | validation: 0.8101058382829592]
	TIME [epoch: 2.85 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9890818136612824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9890818136612824 | validation: 0.7088962248168095]
	TIME [epoch: 2.85 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8936932237776756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8936932237776756 | validation: 0.7510888702380824]
	TIME [epoch: 2.85 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8894359375985486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8894359375985486 | validation: 0.7003005670971509]
	TIME [epoch: 2.85 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706829126732601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706829126732601 | validation: 0.7051527018809377]
	TIME [epoch: 2.85 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925702449519582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8925702449519582 | validation: 0.7022835336295152]
	TIME [epoch: 2.85 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634455776012212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634455776012212 | validation: 0.7345454311863815]
	TIME [epoch: 2.85 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827192956751002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827192956751002 | validation: 0.742464377875635]
	TIME [epoch: 2.85 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.933889821966892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.933889821966892 | validation: 0.8028299825848952]
	TIME [epoch: 2.85 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9704659718157322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9704659718157322 | validation: 0.8361696737692462]
	TIME [epoch: 2.84 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9611974427490014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9611974427490014 | validation: 0.7363796118035775]
	TIME [epoch: 2.84 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9086439080632599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9086439080632599 | validation: 0.7133421104515517]
	TIME [epoch: 2.85 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8577193657639507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8577193657639507 | validation: 0.7509888700828722]
	TIME [epoch: 2.85 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937859712134062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937859712134062 | validation: 0.7461585877709214]
	TIME [epoch: 2.85 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202535419885877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202535419885877 | validation: 0.7070623702424714]
	TIME [epoch: 2.85 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8798603169619651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8798603169619651 | validation: 0.7535146207592657]
	TIME [epoch: 2.85 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9091723415556004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9091723415556004 | validation: 0.735550500897098]
	TIME [epoch: 2.85 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721110802612709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721110802612709 | validation: 0.7725922429910181]
	TIME [epoch: 2.84 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9302512857954076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9302512857954076 | validation: 0.7248122008605451]
	TIME [epoch: 2.84 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8786864074309066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8786864074309066 | validation: 0.7008532109664869]
	TIME [epoch: 2.84 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642400474994925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642400474994925 | validation: 0.7134715271405647]
	TIME [epoch: 2.84 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8530584006646771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8530584006646771 | validation: 0.7016910405120131]
	TIME [epoch: 2.85 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8540179872364283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8540179872364283 | validation: 0.7089205435939546]
	TIME [epoch: 2.85 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8522742583088815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8522742583088815 | validation: 0.7733122613925365]
	TIME [epoch: 2.85 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9717423238354246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9717423238354246 | validation: 0.7959610375671393]
	TIME [epoch: 2.85 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.960645413388438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.960645413388438 | validation: 0.8506682459607678]
	TIME [epoch: 2.85 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9923708921887968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9923708921887968 | validation: 0.7497565514765713]
	TIME [epoch: 2.85 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9170098899049695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9170098899049695 | validation: 0.7074446614255631]
	TIME [epoch: 2.85 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626746898142788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8626746898142788 | validation: 0.7614028754225739]
	TIME [epoch: 2.85 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8793398733203367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8793398733203367 | validation: 0.767707955266447]
	TIME [epoch: 2.85 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9359667976920892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9359667976920892 | validation: 0.720274422855759]
	TIME [epoch: 2.85 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854915056682663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854915056682663 | validation: 0.6974661098060432]
	TIME [epoch: 2.84 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8534341585825576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8534341585825576 | validation: 0.7166829829602487]
	TIME [epoch: 2.84 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8458794471086819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8458794471086819 | validation: 0.7196116915610631]
	TIME [epoch: 2.84 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476590220971295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8476590220971295 | validation: 0.7805445653748229]
	TIME [epoch: 2.84 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0004178242859312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0004178242859312 | validation: 0.8917316219495188]
	TIME [epoch: 2.84 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0765301549515953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0765301549515953 | validation: 0.7633163192531005]
	TIME [epoch: 2.84 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915442238092799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8915442238092799 | validation: 0.7635449115666585]
	TIME [epoch: 2.84 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327925422597596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9327925422597596 | validation: 0.7582850490477352]
	TIME [epoch: 2.84 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9135241664084081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9135241664084081 | validation: 0.7350685472149228]
	TIME [epoch: 2.84 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721938337733262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721938337733262 | validation: 0.7006269642501803]
	TIME [epoch: 2.84 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820349489785442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820349489785442 | validation: 0.694562852475144]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959224836239743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959224836239743 | validation: 0.6662690246821102]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464907085470432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7464907085470432 | validation: 0.8833512238305294]
	TIME [epoch: 2.84 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0129187579464667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0129187579464667 | validation: 1.681830843690856]
	TIME [epoch: 2.84 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.725985312401067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.725985312401067 | validation: 0.7249447283101912]
	TIME [epoch: 2.84 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269861229917612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9269861229917612 | validation: 0.7436462345294923]
	TIME [epoch: 2.84 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8819199284123039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8819199284123039 | validation: 0.7302231695390147]
	TIME [epoch: 2.84 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8932023116618416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8932023116618416 | validation: 0.6994458941811713]
	TIME [epoch: 2.84 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732451855688834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8732451855688834 | validation: 0.710764426290547]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8580755407683376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8580755407683376 | validation: 0.7216283848321077]
	TIME [epoch: 6.11 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8539052365091689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8539052365091689 | validation: 0.6989774798171459]
	TIME [epoch: 6.09 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576853634507814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8576853634507814 | validation: 0.7083274346893188]
	TIME [epoch: 6.1 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512641177419116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8512641177419116 | validation: 0.7117971185104742]
	TIME [epoch: 6.1 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8478707250608843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8478707250608843 | validation: 0.6851234996327107]
	TIME [epoch: 6.11 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560874673574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8560874673574 | validation: 0.6923011583687679]
	TIME [epoch: 6.1 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8484525888862267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8484525888862267 | validation: 0.6915274800393594]
	TIME [epoch: 6.1 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447026927664476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8447026927664476 | validation: 0.6754303242695432]
	TIME [epoch: 6.1 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371783428051335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8371783428051335 | validation: 0.673252098446339]
	TIME [epoch: 6.1 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839313981947463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839313981947463 | validation: 0.6826836427297571]
	TIME [epoch: 6.09 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8494576736009212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8494576736009212 | validation: 0.6771617650971421]
	TIME [epoch: 6.09 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8373124335256477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8373124335256477 | validation: 0.6782519667040005]
	TIME [epoch: 6.1 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8246779705887795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8246779705887795 | validation: 0.7273335177509304]
	TIME [epoch: 6.1 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650047187837921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8650047187837921 | validation: 0.7039227561697972]
	TIME [epoch: 6.1 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8562953643238425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8562953643238425 | validation: 0.7380945440220588]
	TIME [epoch: 6.1 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282654192660127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9282654192660127 | validation: 0.7576783540165796]
	TIME [epoch: 6.1 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779361401631869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8779361401631869 | validation: 0.7549528217033681]
	TIME [epoch: 6.1 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681730189467365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681730189467365 | validation: 0.6632592119028925]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819068152256297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819068152256297 | validation: 0.6600479464995145]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_220.pth
	Model improved!!!
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824713252454295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824713252454295 | validation: 0.7619141434425579]
	TIME [epoch: 6.09 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951021932645191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7951021932645191 | validation: 1.334480406115967]
	TIME [epoch: 6.09 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5399377857827952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5399377857827952 | validation: 0.6522255408328992]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748334970761783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7748334970761783 | validation: 0.9168754657863348]
	TIME [epoch: 6.1 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.031425439987783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.031425439987783 | validation: 0.6784238064810785]
	TIME [epoch: 6.1 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162926242256672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8162926242256672 | validation: 0.6575882505091425]
	TIME [epoch: 6.1 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.807494115818204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.807494115818204 | validation: 0.6844737427968326]
	TIME [epoch: 6.1 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.843608895982191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.843608895982191 | validation: 0.6392443314709381]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486072716442996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486072716442996 | validation: 0.6245249351669204]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7224804352917493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7224804352917493 | validation: 1.0665923663092725]
	TIME [epoch: 6.09 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0415185903305824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0415185903305824 | validation: 1.4380831677574137]
	TIME [epoch: 6.09 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6489601212423242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6489601212423242 | validation: 0.8957915758191158]
	TIME [epoch: 6.1 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1075201948037665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1075201948037665 | validation: 0.7908312699927209]
	TIME [epoch: 6.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9486321663934115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9486321663934115 | validation: 0.7957287567180562]
	TIME [epoch: 6.1 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9532384127099428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9532384127099428 | validation: 0.6946698006281986]
	TIME [epoch: 6.09 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374416225069043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8374416225069043 | validation: 0.7337793017582789]
	TIME [epoch: 6.09 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782761253188008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8782761253188008 | validation: 0.7092671752132661]
	TIME [epoch: 6.09 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364394152181361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364394152181361 | validation: 0.6805729304921142]
	TIME [epoch: 6.1 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8362509953027472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8362509953027472 | validation: 0.6964013368814475]
	TIME [epoch: 6.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278618803946717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8278618803946717 | validation: 0.6853579333341692]
	TIME [epoch: 6.11 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222406264071972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222406264071972 | validation: 0.7023767292107165]
	TIME [epoch: 6.1 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314516245958217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8314516245958217 | validation: 0.6518580327544428]
	TIME [epoch: 6.1 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814988952965293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.814988952965293 | validation: 0.6765722053298702]
	TIME [epoch: 6.1 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102868433952203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8102868433952203 | validation: 0.673867630159692]
	TIME [epoch: 6.09 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739971404068438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739971404068438 | validation: 0.6692353228089076]
	TIME [epoch: 6.09 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759317554851406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759317554851406 | validation: 0.6085790903988209]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696755227137462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696755227137462 | validation: 0.6130355188480896]
	TIME [epoch: 6.09 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618444683014822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6618444683014822 | validation: 0.6362755440372221]
	TIME [epoch: 6.11 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6806050096700159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6806050096700159 | validation: 1.1282531543530687]
	TIME [epoch: 6.1 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1701902260981247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1701902260981247 | validation: 1.3987300014371324]
	TIME [epoch: 6.1 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6740324732973897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6740324732973897 | validation: 0.8383782066669672]
	TIME [epoch: 6.1 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0934358893749092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0934358893749092 | validation: 0.8805330920589299]
	TIME [epoch: 6.1 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0194897265661367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0194897265661367 | validation: 0.7973926570703859]
	TIME [epoch: 6.1 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9419032482093431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9419032482093431 | validation: 0.7564075474706089]
	TIME [epoch: 6.1 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.875349570036259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.875349570036259 | validation: 0.7348645537402749]
	TIME [epoch: 6.1 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8480915971386535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8480915971386535 | validation: 0.7045928056309464]
	TIME [epoch: 6.1 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469536959326317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8469536959326317 | validation: 0.6861141721882849]
	TIME [epoch: 6.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8310825072325079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8310825072325079 | validation: 0.6586771379897838]
	TIME [epoch: 6.1 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959227082040767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959227082040767 | validation: 0.6543235827257444]
	TIME [epoch: 6.1 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933414782894229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933414782894229 | validation: 0.6512621344829181]
	TIME [epoch: 6.1 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688342123003608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688342123003608 | validation: 0.6585797694467891]
	TIME [epoch: 6.09 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485977585919602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485977585919602 | validation: 0.6477602794718361]
	TIME [epoch: 6.1 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7199722601823864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7199722601823864 | validation: 0.6448540333583148]
	TIME [epoch: 6.1 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266484151426811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266484151426811 | validation: 1.11401358184552]
	TIME [epoch: 6.1 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0810807885427811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0810807885427811 | validation: 1.2012743029774056]
	TIME [epoch: 6.1 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4043524849529785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4043524849529785 | validation: 0.6891901669935102]
	TIME [epoch: 6.1 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975527879334188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975527879334188 | validation: 0.906820345287429]
	TIME [epoch: 6.11 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9712641528480631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9712641528480631 | validation: 0.6770777859518022]
	TIME [epoch: 6.1 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653946083974682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653946083974682 | validation: 0.6934691976392419]
	TIME [epoch: 6.1 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8278853151154624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8278853151154624 | validation: 0.6174946529381304]
	TIME [epoch: 6.1 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117984426464085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117984426464085 | validation: 0.6221631569518672]
	TIME [epoch: 6.09 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6859858274535171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6859858274535171 | validation: 0.6408064466006507]
	TIME [epoch: 6.09 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6869497402099409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6869497402099409 | validation: 0.9191823552434081]
	TIME [epoch: 6.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8890861213692968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8890861213692968 | validation: 1.0465317228836954]
	TIME [epoch: 6.09 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1816426888494098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1816426888494098 | validation: 0.7053488731183473]
	TIME [epoch: 6.1 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875846384583771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7875846384583771 | validation: 0.7666267467211685]
	TIME [epoch: 6.09 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.878910946673176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.878910946673176 | validation: 0.6338188499431494]
	TIME [epoch: 6.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480921760052263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7480921760052263 | validation: 0.688991370544985]
	TIME [epoch: 6.09 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779872888685999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.779872888685999 | validation: 0.6094931546740142]
	TIME [epoch: 6.1 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6785834626811612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6785834626811612 | validation: 0.6145093766431853]
	TIME [epoch: 6.11 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702342540751021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6702342540751021 | validation: 0.6778011900128103]
	TIME [epoch: 6.09 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.741099239012651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.741099239012651 | validation: 0.9839865276213164]
	TIME [epoch: 6.1 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9687875569962919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9687875569962919 | validation: 0.9787671243813261]
	TIME [epoch: 6.1 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0588082063538036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0588082063538036 | validation: 0.6874911698742857]
	TIME [epoch: 6.1 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7701192608130148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7701192608130148 | validation: 0.7173807444428851]
	TIME [epoch: 6.1 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8388654693141129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8388654693141129 | validation: 0.651727148645763]
	TIME [epoch: 6.1 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253899911436894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253899911436894 | validation: 0.6278525597408127]
	TIME [epoch: 6.1 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292387129519261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292387129519261 | validation: 0.568691358948743]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.658656017988506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.658656017988506 | validation: 0.5498078709538019]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_289.pth
	Model improved!!!
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6053990146513055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6053990146513055 | validation: 0.5913023575407543]
	TIME [epoch: 6.1 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6182408971745964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6182408971745964 | validation: 0.7031583216871983]
	TIME [epoch: 6.1 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7070687687382698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7070687687382698 | validation: 1.0138480889588155]
	TIME [epoch: 6.1 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.159031341053603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.159031341053603 | validation: 0.6095343422445239]
	TIME [epoch: 6.1 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7164194257449802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7164194257449802 | validation: 0.784657938926264]
	TIME [epoch: 6.1 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.92057662675293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.92057662675293 | validation: 0.5974449220905099]
	TIME [epoch: 6.11 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900696766487161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6900696766487161 | validation: 0.6037787561366065]
	TIME [epoch: 6.09 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6872582534159872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6872582534159872 | validation: 0.8014330764107035]
	TIME [epoch: 6.11 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163423983788322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163423983788322 | validation: 0.8745578631616241]
	TIME [epoch: 6.09 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.00222122956152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.00222122956152 | validation: 0.5375397504862331]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6019934077734286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6019934077734286 | validation: 0.652718496919701]
	TIME [epoch: 6.09 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871764317863682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6871764317863682 | validation: 0.6841526898809687]
	TIME [epoch: 6.1 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576712507204133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576712507204133 | validation: 0.5388716704050726]
	TIME [epoch: 6.1 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6087894241980973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6087894241980973 | validation: 0.562875659266646]
	TIME [epoch: 6.1 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5846052568173564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5846052568173564 | validation: 0.593207357453642]
	TIME [epoch: 6.1 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6078988728012559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6078988728012559 | validation: 0.6996560533889462]
	TIME [epoch: 6.09 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794219905764311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794219905764311 | validation: 0.5404861335460357]
	TIME [epoch: 6.1 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.610032327566232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.610032327566232 | validation: 0.5167014855079385]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.561511015402957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.561511015402957 | validation: 0.5537850053087111]
	TIME [epoch: 6.1 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5435878184228049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5435878184228049 | validation: 0.6556582133668631]
	TIME [epoch: 6.09 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7248073580250302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7248073580250302 | validation: 0.7835657186649342]
	TIME [epoch: 6.09 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8286291856922173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8286291856922173 | validation: 0.7624452428911389]
	TIME [epoch: 6.09 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609725088366276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609725088366276 | validation: 0.6418209489768801]
	TIME [epoch: 6.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361212305482421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7361212305482421 | validation: 0.6586543393069862]
	TIME [epoch: 6.1 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638970323096563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638970323096563 | validation: 0.539337086121468]
	TIME [epoch: 6.09 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6255723671698488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6255723671698488 | validation: 0.4962888015491017]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_315.pth
	Model improved!!!
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522187194973606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.522187194973606 | validation: 0.5328107337015279]
	TIME [epoch: 6.1 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5318976233837158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5318976233837158 | validation: 0.6672889131137354]
	TIME [epoch: 6.1 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105999129424087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7105999129424087 | validation: 0.7480331419754696]
	TIME [epoch: 6.11 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649908657339893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649908657339893 | validation: 0.6919446326627103]
	TIME [epoch: 6.1 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7818993680305362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7818993680305362 | validation: 0.5179420602732655]
	TIME [epoch: 6.1 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6144078295572432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6144078295572432 | validation: 0.5217440822770028]
	TIME [epoch: 6.1 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5688474132680142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5688474132680142 | validation: 0.5150467808282982]
	TIME [epoch: 6.09 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5281287478782675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5281287478782675 | validation: 0.8744064918067193]
	TIME [epoch: 6.1 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902072796876395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8902072796876395 | validation: 0.9979647291384364]
	TIME [epoch: 6.09 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.108809180449505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.108809180449505 | validation: 0.646833389551586]
	TIME [epoch: 6.1 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7109769688605806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7109769688605806 | validation: 0.6800162778973509]
	TIME [epoch: 6.1 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793135904516056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.793135904516056 | validation: 0.5351553578012261]
	TIME [epoch: 6.1 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6146718866802797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6146718866802797 | validation: 0.4967353867081201]
	TIME [epoch: 6.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5502062306128405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5502062306128405 | validation: 0.6533856121970338]
	TIME [epoch: 6.1 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6334046742407808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6334046742407808 | validation: 0.7174065910171239]
	TIME [epoch: 6.1 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767580365367295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7767580365367295 | validation: 0.533200216168963]
	TIME [epoch: 6.1 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530754386033123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5530754386033123 | validation: 0.4903907560550873]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_332.pth
	Model improved!!!
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.514925064965125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.514925064965125 | validation: 0.46811442164832995]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_333.pth
	Model improved!!!
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5056005896970264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5056005896970264 | validation: 0.49230387082838994]
	TIME [epoch: 6.1 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316940936608786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5316940936608786 | validation: 0.6447273395826835]
	TIME [epoch: 6.1 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6164097903914297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6164097903914297 | validation: 0.7019945215973337]
	TIME [epoch: 6.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791378151860646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.791378151860646 | validation: 0.4690325662596486]
	TIME [epoch: 6.1 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5508014523711775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5508014523711775 | validation: 0.46411726032432254]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656039488241281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4656039488241281 | validation: 0.5311998439623274]
	TIME [epoch: 6.09 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5167825249643135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5167825249643135 | validation: 0.7552780353256049]
	TIME [epoch: 6.09 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7251282005420366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7251282005420366 | validation: 0.8087402628141434]
	TIME [epoch: 6.09 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9177368179780132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9177368179780132 | validation: 0.5583669763378332]
	TIME [epoch: 6.08 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6392232231503562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6392232231503562 | validation: 0.5511521458717814]
	TIME [epoch: 6.09 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6298166590444814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6298166590444814 | validation: 0.4941956069370216]
	TIME [epoch: 6.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5215634541713575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5215634541713575 | validation: 0.4836945399365945]
	TIME [epoch: 6.1 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5300403560768784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5300403560768784 | validation: 0.5778958364435157]
	TIME [epoch: 6.09 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5947301958026706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5947301958026706 | validation: 0.6545450546399225]
	TIME [epoch: 6.09 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5825194883955963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5825194883955963 | validation: 0.5636357037412765]
	TIME [epoch: 6.09 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5848958568700989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5848958568700989 | validation: 0.4913269541465489]
	TIME [epoch: 6.09 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4793594891213911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4793594891213911 | validation: 0.3965983382933713]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41194399772533047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41194399772533047 | validation: 0.39306260224751655]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4014717971949777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4014717971949777 | validation: 0.45229009149467514]
	TIME [epoch: 6.11 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44001563573643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44001563573643 | validation: 0.634641953428242]
	TIME [epoch: 6.1 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6297092686336874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6297092686336874 | validation: 0.7467243020482712]
	TIME [epoch: 6.11 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8679216359539379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8679216359539379 | validation: 0.47517582674951786]
	TIME [epoch: 6.1 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5528261545857716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5528261545857716 | validation: 0.4886798077780423]
	TIME [epoch: 6.1 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5201473720387295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5201473720387295 | validation: 0.4109751695718729]
	TIME [epoch: 6.1 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40981621391616485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40981621391616485 | validation: 0.6316686808984641]
	TIME [epoch: 6.1 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725766406555202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5725766406555202 | validation: 0.8474783791595935]
	TIME [epoch: 6.06 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9569777274354608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9569777274354608 | validation: 0.434586043524926]
	TIME [epoch: 6.07 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5302635852322479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5302635852322479 | validation: 0.5417971306129049]
	TIME [epoch: 6.06 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.484882291698352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.484882291698352 | validation: 0.5516580261445672]
	TIME [epoch: 6.06 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5888324127827208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5888324127827208 | validation: 0.598545235274077]
	TIME [epoch: 6.07 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097578519889482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097578519889482 | validation: 0.5494767534281033]
	TIME [epoch: 6.06 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.546592515559631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.546592515559631 | validation: 0.38704416873502134]
	TIME [epoch: 6.06 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4313372175221663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4313372175221663 | validation: 0.37335995273577555]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36145407622845677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36145407622845677 | validation: 0.40312532512229976]
	TIME [epoch: 6.09 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3724730943930041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3724730943930041 | validation: 0.5163543577273298]
	TIME [epoch: 6.09 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4542681876668354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4542681876668354 | validation: 0.705822444122913]
	TIME [epoch: 6.09 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8083749339248704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8083749339248704 | validation: 0.4624982371058479]
	TIME [epoch: 6.11 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4445990697321238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4445990697321238 | validation: 0.3465044167490328]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38813497505104183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38813497505104183 | validation: 0.6405752850225179]
	TIME [epoch: 6.09 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5806974225659169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5806974225659169 | validation: 0.7719003092928283]
	TIME [epoch: 6.09 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801617189000182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7801617189000182 | validation: 0.6257288434792398]
	TIME [epoch: 6.1 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7229632924138325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7229632924138325 | validation: 0.6130866917649302]
	TIME [epoch: 6.09 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817968259774483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5817968259774483 | validation: 0.5336944108642222]
	TIME [epoch: 6.09 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5893666225514613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5893666225514613 | validation: 0.4229184041033632]
	TIME [epoch: 6.1 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46340165101869396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46340165101869396 | validation: 0.46482209947403397]
	TIME [epoch: 6.1 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37824222429590193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37824222429590193 | validation: 0.315558711870289]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355734726350043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.355734726350043 | validation: 0.5046389184961166]
	TIME [epoch: 6.1 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42849930188447866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42849930188447866 | validation: 0.601256680635925]
	TIME [epoch: 6.09 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6541316357150713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6541316357150713 | validation: 0.33208547728434246]
	TIME [epoch: 6.09 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39094452096993904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39094452096993904 | validation: 0.4864400251189853]
	TIME [epoch: 6.09 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3723066833952449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3723066833952449 | validation: 0.3638725039479084]
	TIME [epoch: 6.09 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3989400858529866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3989400858529866 | validation: 0.6317589059111928]
	TIME [epoch: 6.09 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5453278358989634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5453278358989634 | validation: 0.8518318496428027]
	TIME [epoch: 6.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0484756487291693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0484756487291693 | validation: 0.6305135431634086]
	TIME [epoch: 6.1 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8372990525361536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8372990525361536 | validation: 0.47665281360093825]
	TIME [epoch: 6.1 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.56340090304977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.56340090304977 | validation: 0.5690463714126444]
	TIME [epoch: 6.1 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5349584576545388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5349584576545388 | validation: 0.5615040503681314]
	TIME [epoch: 6.1 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926328899720726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5926328899720726 | validation: 0.562523320956802]
	TIME [epoch: 6.09 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5932129642983641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5932129642983641 | validation: 0.3872218335775221]
	TIME [epoch: 6.1 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34743986180354386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34743986180354386 | validation: 0.3697969922345464]
	TIME [epoch: 6.09 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3216745916727085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3216745916727085 | validation: 0.33215060360718374]
	TIME [epoch: 6.1 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3504088844188189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3504088844188189 | validation: 0.4548666501529649]
	TIME [epoch: 6.1 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4434245705513734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4434245705513734 | validation: 0.5935048105816794]
	TIME [epoch: 6.1 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5674497507381965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5674497507381965 | validation: 0.5272958958927626]
	TIME [epoch: 6.1 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5833727041423389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5833727041423389 | validation: 0.4455666457585359]
	TIME [epoch: 6.1 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41953609574531286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41953609574531286 | validation: 0.3862248938429005]
	TIME [epoch: 6.09 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4155825246228706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4155825246228706 | validation: 0.4724151925218816]
	TIME [epoch: 6.09 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4709980565304842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4709980565304842 | validation: 0.5542299160686616]
	TIME [epoch: 6.09 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43820163538174234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43820163538174234 | validation: 0.5336511474642766]
	TIME [epoch: 6.09 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069503969923101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6069503969923101 | validation: 0.33288037022328143]
	TIME [epoch: 6.1 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3680643191848674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3680643191848674 | validation: 0.41156229777519054]
	TIME [epoch: 6.09 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3306209082258144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3306209082258144 | validation: 0.36540372063448573]
	TIME [epoch: 6.1 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39733917152532355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39733917152532355 | validation: 0.6338546140002426]
	TIME [epoch: 6.09 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5116661593530969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5116661593530969 | validation: 0.732458345315669]
	TIME [epoch: 6.09 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8317734027078787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8317734027078787 | validation: 0.530569913590531]
	TIME [epoch: 6.09 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474177579668912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7474177579668912 | validation: 0.4030049887348763]
	TIME [epoch: 6.09 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4256312814558173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4256312814558173 | validation: 0.5745015199820858]
	TIME [epoch: 6.09 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5023997423567221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5023997423567221 | validation: 0.5556617283194768]
	TIME [epoch: 6.09 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.608420453152723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.608420453152723 | validation: 0.39737917633420405]
	TIME [epoch: 6.1 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41584728868407234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41584728868407234 | validation: 0.45267880394628685]
	TIME [epoch: 6.09 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3576068110030135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3576068110030135 | validation: 0.2907107660960409]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3128686315357511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3128686315357511 | validation: 0.32918515786741065]
	TIME [epoch: 6.07 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3237359446996233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3237359446996233 | validation: 0.5147809338657993]
	TIME [epoch: 6.07 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48938921703424265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48938921703424265 | validation: 0.46556951177995903]
	TIME [epoch: 6.07 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44359921937715896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44359921937715896 | validation: 0.4104855914822052]
	TIME [epoch: 6.07 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4337255054902854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4337255054902854 | validation: 0.3824271023467769]
	TIME [epoch: 6.07 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3307775378031282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3307775378031282 | validation: 0.2746141767542017]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30671182083919524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30671182083919524 | validation: 0.5077635055937274]
	TIME [epoch: 6.08 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39153283023582547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39153283023582547 | validation: 0.5440685995674303]
	TIME [epoch: 6.08 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6278884570514579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6278884570514579 | validation: 0.35781262577905376]
	TIME [epoch: 6.07 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4369950641524518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4369950641524518 | validation: 0.5735676837604545]
	TIME [epoch: 6.07 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4476175838978411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4476175838978411 | validation: 0.2709987485978997]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.278673840408397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.278673840408397 | validation: 0.2741830364684432]
	TIME [epoch: 6.1 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27310855255571675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27310855255571675 | validation: 0.5620236030079676]
	TIME [epoch: 6.1 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47626096535900997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47626096535900997 | validation: 0.6436923744644653]
	TIME [epoch: 6.11 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7512203876623599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7512203876623599 | validation: 0.45882794771292296]
	TIME [epoch: 6.11 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5802465772284301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5802465772284301 | validation: 0.4300019262358782]
	TIME [epoch: 6.11 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4356824141362314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4356824141362314 | validation: 0.29509858649063503]
	TIME [epoch: 6.08 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26409368278418727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26409368278418727 | validation: 0.30036496253094724]
	TIME [epoch: 6.08 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32334841320852226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32334841320852226 | validation: 0.4728479670370764]
	TIME [epoch: 6.07 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3782623191158177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3782623191158177 | validation: 0.6593522807459824]
	TIME [epoch: 6.07 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737858962392784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6737858962392784 | validation: 0.46387074070661627]
	TIME [epoch: 6.07 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.541522329357555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.541522329357555 | validation: 0.4332613835489772]
	TIME [epoch: 6.06 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4263543522287039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4263543522287039 | validation: 0.5024418063956632]
	TIME [epoch: 6.07 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5118778088539232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5118778088539232 | validation: 0.4412095524594996]
	TIME [epoch: 6.08 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49327936365973507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49327936365973507 | validation: 0.32679728548709724]
	TIME [epoch: 6.08 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2594677962247981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2594677962247981 | validation: 0.22517609058766888]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_440.pth
	Model improved!!!
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24191927757637424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24191927757637424 | validation: 0.3130288789445624]
	TIME [epoch: 6.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24500174972140146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24500174972140146 | validation: 0.3611770800668966]
	TIME [epoch: 6.11 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3922148301026809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3922148301026809 | validation: 0.6018170877502191]
	TIME [epoch: 6.07 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6903582854791582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6903582854791582 | validation: 0.25580793033262106]
	TIME [epoch: 6.07 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2968321397627449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2968321397627449 | validation: 0.3613996379266198]
	TIME [epoch: 6.07 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2806517383069661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2806517383069661 | validation: 0.6692213182338134]
	TIME [epoch: 6.08 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7500052416215378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7500052416215378 | validation: 0.4252941285805079]
	TIME [epoch: 6.07 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.492685678574603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.492685678574603 | validation: 0.49817063968663844]
	TIME [epoch: 6.07 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.379101491214099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.379101491214099 | validation: 0.250917829981705]
	TIME [epoch: 6.08 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33012460663850773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33012460663850773 | validation: 0.4232129278172077]
	TIME [epoch: 6.07 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31401191886299035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31401191886299035 | validation: 0.4064431968679067]
	TIME [epoch: 6.08 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44963329840730437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44963329840730437 | validation: 0.4131533540810344]
	TIME [epoch: 6.07 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42355816943068464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42355816943068464 | validation: 0.3619118202949001]
	TIME [epoch: 6.08 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3517765978726537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3517765978726537 | validation: 0.3114501365438782]
	TIME [epoch: 6.07 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30668996615069866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30668996615069866 | validation: 0.3762764814648033]
	TIME [epoch: 6.08 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3676629732089364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3676629732089364 | validation: 0.43544270037481053]
	TIME [epoch: 6.07 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3575018910635139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3575018910635139 | validation: 0.4364348082024363]
	TIME [epoch: 6.08 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.480427196378962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.480427196378962 | validation: 0.3325062553830104]
	TIME [epoch: 6.07 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2886301279876596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2886301279876596 | validation: 0.21602075249382927]
	TIME [epoch: 6.08 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2367751715540382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2367751715540382 | validation: 0.5010323820649962]
	TIME [epoch: 6.1 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36054298124203327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36054298124203327 | validation: 0.6551681058032695]
	TIME [epoch: 6.1 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7130577082070005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7130577082070005 | validation: 0.24417130518576774]
	TIME [epoch: 6.09 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2908031353354907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2908031353354907 | validation: 0.6257813912819442]
	TIME [epoch: 6.1 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4856168427415651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4856168427415651 | validation: 0.36209085274998776]
	TIME [epoch: 6.11 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36492966704872415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36492966704872415 | validation: 0.3976521707374698]
	TIME [epoch: 6.1 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4124126401878988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4124126401878988 | validation: 0.5806309211952186]
	TIME [epoch: 6.1 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.522067375444375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.522067375444375 | validation: 0.3768207504961153]
	TIME [epoch: 6.08 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39803273362022235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39803273362022235 | validation: 0.31180239242078167]
	TIME [epoch: 6.07 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2797007954371927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2797007954371927 | validation: 0.3914661395565871]
	TIME [epoch: 6.07 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31944106320574867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31944106320574867 | validation: 0.45528244137857465]
	TIME [epoch: 6.06 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47515719884525226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47515719884525226 | validation: 0.26654517273701134]
	TIME [epoch: 6.07 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2567844414124106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2567844414124106 | validation: 0.23594350449922677]
	TIME [epoch: 6.06 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2188477495494866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2188477495494866 | validation: 0.24247587856339048]
	TIME [epoch: 6.07 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23311150909837164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23311150909837164 | validation: 0.4376280420956965]
	TIME [epoch: 6.06 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3450682274441376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3450682274441376 | validation: 0.5924664220919879]
	TIME [epoch: 6.07 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6842852394230863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6842852394230863 | validation: 0.36914987854426473]
	TIME [epoch: 6.07 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4699116670260506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4699116670260506 | validation: 0.6854839673286984]
	TIME [epoch: 6.07 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5544440604399374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5544440604399374 | validation: 0.3590555094150717]
	TIME [epoch: 6.06 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29754787366967833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29754787366967833 | validation: 0.723815245340818]
	TIME [epoch: 6.07 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321992801722594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8321992801722594 | validation: 0.2616016424182158]
	TIME [epoch: 6.06 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27621853284647446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27621853284647446 | validation: 0.45470467542004783]
	TIME [epoch: 6.08 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36101270747717723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36101270747717723 | validation: 0.3035792263947543]
	TIME [epoch: 6.07 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336819144319619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.336819144319619 | validation: 0.3126718396248067]
	TIME [epoch: 6.07 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3581881408714546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3581881408714546 | validation: 0.4563670178365129]
	TIME [epoch: 6.07 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4301395773520686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4301395773520686 | validation: 0.42617557247993476]
	TIME [epoch: 6.07 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3826079974212715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3826079974212715 | validation: 0.3805641460982205]
	TIME [epoch: 6.07 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4251600253742942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4251600253742942 | validation: 0.2772824082451581]
	TIME [epoch: 6.07 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24384036853338395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24384036853338395 | validation: 0.22422754463041264]
	TIME [epoch: 6.07 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21135157462404458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21135157462404458 | validation: 0.23932674448272886]
	TIME [epoch: 6.08 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2257059259564957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2257059259564957 | validation: 0.4363457263114987]
	TIME [epoch: 6.08 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40621200770861554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40621200770861554 | validation: 0.5790111488482452]
	TIME [epoch: 6.07 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6579033395331665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6579033395331665 | validation: 0.5192274835711114]
	TIME [epoch: 6.08 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6739088469367404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6739088469367404 | validation: 0.3592546857746498]
	TIME [epoch: 6.08 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36676301758787233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36676301758787233 | validation: 0.2678928029413447]
	TIME [epoch: 6.08 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22771779600569794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22771779600569794 | validation: 0.31034264115988514]
	TIME [epoch: 6.07 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3420365580390559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3420365580390559 | validation: 0.46330001080820815]
	TIME [epoch: 6.07 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3691753329412643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3691753329412643 | validation: 0.7155990125263273]
	TIME [epoch: 6.07 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8309725653034125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8309725653034125 | validation: 0.46769283342144785]
	TIME [epoch: 6.07 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.609232718829732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.609232718829732 | validation: 0.4546846838239696]
	TIME [epoch: 6.08 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4212107017244541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4212107017244541 | validation: 0.6102222605747087]
	TIME [epoch: 6.08 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6042428063700488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6042428063700488 | validation: 0.23815196040851935]
	TIME [epoch: 181 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2870615533368341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2870615533368341 | validation: 0.26522155169622774]
	TIME [epoch: 12.9 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2157634714580576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2157634714580576 | validation: 0.24013162506367386]
	TIME [epoch: 13 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2134213905786208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2134213905786208 | validation: 0.19348380385076164]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2211387140559933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2211387140559933 | validation: 0.35062714855574834]
	TIME [epoch: 12.9 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27508156581007304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27508156581007304 | validation: 0.4995129794518528]
	TIME [epoch: 12.9 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5590480810829231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5590480810829231 | validation: 0.26501289422104385]
	TIME [epoch: 13 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32206844616687735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32206844616687735 | validation: 0.41584129405823733]
	TIME [epoch: 13 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3148248959059164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3148248959059164 | validation: 0.2560980060972606]
	TIME [epoch: 12.9 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2671147212163048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2671147212163048 | validation: 0.33825989876936785]
	TIME [epoch: 12.9 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31997796164611136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31997796164611136 | validation: 0.5978583874080086]
	TIME [epoch: 13 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5366798819470189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5366798819470189 | validation: 0.43940548280418223]
	TIME [epoch: 12.9 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000398032429405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000398032429405 | validation: 0.3006910389885247]
	TIME [epoch: 13 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32777081850247086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32777081850247086 | validation: 0.4517418344248376]
	TIME [epoch: 12.9 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.428005787534906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.428005787534906 | validation: 0.362576804724305]
	TIME [epoch: 13 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3792391146928305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3792391146928305 | validation: 0.23103617609899293]
	TIME [epoch: 12.9 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20687203716551172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20687203716551172 | validation: 0.20496179240577872]
	TIME [epoch: 13 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20286013554635682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20286013554635682 | validation: 0.2875600712544668]
	TIME [epoch: 13 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28468384525569396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28468384525569396 | validation: 0.6552513772073969]
	TIME [epoch: 13 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097326576131584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097326576131584 | validation: 0.4417055017805718]
	TIME [epoch: 12.9 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5194298763470124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5194298763470124 | validation: 0.8127953855203995]
	TIME [epoch: 13 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200583945165892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7200583945165892 | validation: 0.8371652786149539]
	TIME [epoch: 13 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73714187479203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73714187479203 | validation: 0.4367437864508219]
	TIME [epoch: 13 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487686682928633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5487686682928633 | validation: 0.3051193259090013]
	TIME [epoch: 12.9 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38667625329273103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38667625329273103 | validation: 0.4386844091085763]
	TIME [epoch: 12.9 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4042375131516078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4042375131516078 | validation: 0.2647839737542276]
	TIME [epoch: 12.9 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2765587688494097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2765587688494097 | validation: 0.23792504374763723]
	TIME [epoch: 13 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2436434159682629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2436434159682629 | validation: 0.3030524764687069]
	TIME [epoch: 12.9 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2895267971453776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2895267971453776 | validation: 0.44301332826154527]
	TIME [epoch: 12.9 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42263463535643836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42263463535643836 | validation: 0.3473773025141871]
	TIME [epoch: 12.9 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3901583793954241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3901583793954241 | validation: 0.199861875051094]
	TIME [epoch: 12.9 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255235860348173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2255235860348173 | validation: 0.27489071210123206]
	TIME [epoch: 12.9 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22599118145635586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22599118145635586 | validation: 0.3381346562338605]
	TIME [epoch: 12.9 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3500321440147964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3500321440147964 | validation: 0.48148928228762505]
	TIME [epoch: 12.9 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417360536884581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4417360536884581 | validation: 0.3705955932694937]
	TIME [epoch: 13 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3976754868079081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3976754868079081 | validation: 0.31130265992102046]
	TIME [epoch: 12.9 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3285368448029233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3285368448029233 | validation: 0.35083994351522596]
	TIME [epoch: 12.9 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.281450077394294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.281450077394294 | validation: 0.25419369187518887]
	TIME [epoch: 12.9 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2705910508817552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2705910508817552 | validation: 0.28469413846632824]
	TIME [epoch: 13 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23720650476512606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23720650476512606 | validation: 0.3460745058581961]
	TIME [epoch: 12.9 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32031146312942727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32031146312942727 | validation: 0.3846882907474116]
	TIME [epoch: 12.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43524662416365445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43524662416365445 | validation: 0.21372210589662483]
	TIME [epoch: 12.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21170492830479956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21170492830479956 | validation: 0.2945657545826678]
	TIME [epoch: 13 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.258291842749207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.258291842749207 | validation: 0.5493529584140149]
	TIME [epoch: 12.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5723382650221432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5723382650221432 | validation: 0.21792200850184973]
	TIME [epoch: 12.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22689622430254908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22689622430254908 | validation: 0.31812173503054564]
	TIME [epoch: 12.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2370273085076015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2370273085076015 | validation: 0.2497487119854534]
	TIME [epoch: 12.9 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2834791645994589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2834791645994589 | validation: 0.40930405480415316]
	TIME [epoch: 12.9 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2932290167452029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2932290167452029 | validation: 0.24448595796089984]
	TIME [epoch: 12.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854537142251501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2854537142251501 | validation: 0.597382297114914]
	TIME [epoch: 12.9 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5575870326964707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5575870326964707 | validation: 0.38616750429274993]
	TIME [epoch: 12.9 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43572538809876976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43572538809876976 | validation: 0.22520037195406603]
	TIME [epoch: 12.9 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22997526104674978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22997526104674978 | validation: 0.22779040411956775]
	TIME [epoch: 12.9 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1801402021400156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1801402021400156 | validation: 0.23098746497103317]
	TIME [epoch: 12.9 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24174491830103237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24174491830103237 | validation: 0.6091172469343059]
	TIME [epoch: 13 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5259090983135517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5259090983135517 | validation: 0.4930616119839032]
	TIME [epoch: 12.9 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5723051040024294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5723051040024294 | validation: 0.3750175467233678]
	TIME [epoch: 12.9 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45918035310294314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45918035310294314 | validation: 0.4327942003711709]
	TIME [epoch: 12.9 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38255141480960303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38255141480960303 | validation: 0.1954528504657187]
	TIME [epoch: 12.9 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20215944739669084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20215944739669084 | validation: 0.2594321690159287]
	TIME [epoch: 12.9 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2129409057952589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2129409057952589 | validation: 0.42901436553561806]
	TIME [epoch: 12.9 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42835453331420964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42835453331420964 | validation: 0.5814150111315317]
	TIME [epoch: 12.9 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6314396796836086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6314396796836086 | validation: 0.25098067545123465]
	TIME [epoch: 12.9 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2628596759916741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2628596759916741 | validation: 0.28940355096838444]
	TIME [epoch: 12.9 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26101314529375425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26101314529375425 | validation: 0.38102090697117535]
	TIME [epoch: 12.9 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39193499456576164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39193499456576164 | validation: 0.32146712151852236]
	TIME [epoch: 12.9 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2908968103688477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2908968103688477 | validation: 0.32439536625866483]
	TIME [epoch: 12.9 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.328156721623686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.328156721623686 | validation: 0.2862469197361352]
	TIME [epoch: 12.9 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2812285221299484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2812285221299484 | validation: 0.29749762228964766]
	TIME [epoch: 12.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28714481618615345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28714481618615345 | validation: 0.3112729755842333]
	TIME [epoch: 12.9 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27571862510784445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27571862510784445 | validation: 0.2949130584002808]
	TIME [epoch: 12.9 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3186964779417955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3186964779417955 | validation: 0.37210847699758975]
	TIME [epoch: 12.9 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28406602872396475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28406602872396475 | validation: 0.285432527902403]
	TIME [epoch: 12.9 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3285952918978592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3285952918978592 | validation: 0.35075612577119464]
	TIME [epoch: 12.9 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24805224187807834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24805224187807834 | validation: 0.2585232899950648]
	TIME [epoch: 12.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28126901191935216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28126901191935216 | validation: 0.4519664575736233]
	TIME [epoch: 12.9 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44622418735817926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44622418735817926 | validation: 0.34103457575550417]
	TIME [epoch: 12.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3543100392357731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3543100392357731 | validation: 0.2261701991513496]
	TIME [epoch: 12.9 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2313061067845724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2313061067845724 | validation: 0.29097350404316996]
	TIME [epoch: 12.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23680877444459025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23680877444459025 | validation: 0.3455881704503024]
	TIME [epoch: 12.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3519341698585322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3519341698585322 | validation: 0.3817920235793483]
	TIME [epoch: 12.9 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39224027055324395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39224027055324395 | validation: 0.30820498586443235]
	TIME [epoch: 12.9 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2910736582548285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2910736582548285 | validation: 0.24512444472297618]
	TIME [epoch: 12.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26073410061666596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26073410061666596 | validation: 0.3638878353170144]
	TIME [epoch: 12.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.292613650320908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.292613650320908 | validation: 0.38691628722561067]
	TIME [epoch: 12.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4326383632637052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4326383632637052 | validation: 0.21815021465997872]
	TIME [epoch: 12.9 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20165794109997595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20165794109997595 | validation: 0.21573244545333614]
	TIME [epoch: 12.9 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1852988618870479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1852988618870479 | validation: 0.3218415700450244]
	TIME [epoch: 12.9 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3328004388837323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3328004388837323 | validation: 0.48818501406640213]
	TIME [epoch: 12.9 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42210113736177407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42210113736177407 | validation: 0.3564454593776101]
	TIME [epoch: 12.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41321590380956524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41321590380956524 | validation: 0.25568326364777766]
	TIME [epoch: 12.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666176325878011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2666176325878011 | validation: 0.35367075250775265]
	TIME [epoch: 12.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2758107962554247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2758107962554247 | validation: 0.44184650098274303]
	TIME [epoch: 13 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4544036767790425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4544036767790425 | validation: 0.2919322346461116]
	TIME [epoch: 12.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23124616938712783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23124616938712783 | validation: 0.28124411195938837]
	TIME [epoch: 12.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24408279929794158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24408279929794158 | validation: 0.35785646930140014]
	TIME [epoch: 12.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3933873923365016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3933873923365016 | validation: 0.2692137392447483]
	TIME [epoch: 12.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20425624653178887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20425624653178887 | validation: 0.17858225535473005]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1737062371425483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1737062371425483 | validation: 0.261117932040906]
	TIME [epoch: 12.9 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1886838118770813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1886838118770813 | validation: 0.37805592642497676]
	TIME [epoch: 12.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4307562969433623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4307562969433623 | validation: 0.35703693366401856]
	TIME [epoch: 12.9 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36298795519270377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36298795519270377 | validation: 0.34519218121343087]
	TIME [epoch: 12.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3145094320820215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3145094320820215 | validation: 0.3086120171036976]
	TIME [epoch: 12.9 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3010558095102351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3010558095102351 | validation: 0.3028321721031611]
	TIME [epoch: 12.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24921869451090678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24921869451090678 | validation: 0.29318617612296877]
	TIME [epoch: 13 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2532899897591693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2532899897591693 | validation: 0.345191311062895]
	TIME [epoch: 12.9 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39611386592051856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39611386592051856 | validation: 0.669042288984549]
	TIME [epoch: 12.9 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5347935164512422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5347935164512422 | validation: 0.2893724711817633]
	TIME [epoch: 12.9 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28679179574680097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28679179574680097 | validation: 0.24965290792941586]
	TIME [epoch: 12.9 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29965181482098185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29965181482098185 | validation: 0.35430696372249093]
	TIME [epoch: 12.9 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25406068091758405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25406068091758405 | validation: 0.20820835073526744]
	TIME [epoch: 12.9 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17569487041043416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17569487041043416 | validation: 0.17980653747663555]
	TIME [epoch: 12.9 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16950135402711872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16950135402711872 | validation: 0.2793690872801827]
	TIME [epoch: 12.9 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2106382956173153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2106382956173153 | validation: 0.44772296789021504]
	TIME [epoch: 12.9 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40796309497671146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40796309497671146 | validation: 0.33460260879625564]
	TIME [epoch: 12.9 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37977607864269775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37977607864269775 | validation: 0.22907959893457958]
	TIME [epoch: 12.9 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2004685839621568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2004685839621568 | validation: 0.16460017727470136]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16800735205459982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16800735205459982 | validation: 0.28850622273071674]
	TIME [epoch: 12.9 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20400745234136552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20400745234136552 | validation: 0.39041557463337234]
	TIME [epoch: 12.9 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41371472637390083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41371472637390083 | validation: 0.27581654400565514]
	TIME [epoch: 12.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27288272882782005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27288272882782005 | validation: 0.3445466279498218]
	TIME [epoch: 12.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2876472242021831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2876472242021831 | validation: 0.42916130297401467]
	TIME [epoch: 12.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4115856907389103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4115856907389103 | validation: 0.2624320804182161]
	TIME [epoch: 12.9 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2722995018504598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2722995018504598 | validation: 0.33801518721667334]
	TIME [epoch: 12.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25672027110382817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25672027110382817 | validation: 0.3193956075924238]
	TIME [epoch: 12.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3657558235541138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3657558235541138 | validation: 0.24639312983689377]
	TIME [epoch: 12.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19032575238473493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19032575238473493 | validation: 0.15624828482186276]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15781893403527597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15781893403527597 | validation: 0.19399218717649114]
	TIME [epoch: 12.9 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16470457482477038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16470457482477038 | validation: 0.33388053270228896]
	TIME [epoch: 12.9 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3174605814117201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3174605814117201 | validation: 0.5482526425047918]
	TIME [epoch: 12.9 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5901668192690137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5901668192690137 | validation: 0.23081691132315676]
	TIME [epoch: 12.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29291895835601217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29291895835601217 | validation: 0.5710289181299734]
	TIME [epoch: 12.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44170430930332644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44170430930332644 | validation: 0.5129163611046991]
	TIME [epoch: 12.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5313959061508564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5313959061508564 | validation: 0.38539590095423165]
	TIME [epoch: 12.9 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42088353510281373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42088353510281373 | validation: 0.3545207257158621]
	TIME [epoch: 12.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26903630528862926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26903630528862926 | validation: 0.1742510027984134]
	TIME [epoch: 12.9 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17078251136282596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17078251136282596 | validation: 0.16052354824815876]
	TIME [epoch: 12.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15957969442977785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15957969442977785 | validation: 0.3312270357913462]
	TIME [epoch: 12.9 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25294778395047524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25294778395047524 | validation: 0.49189118757782746]
	TIME [epoch: 12.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5355135651525623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5355135651525623 | validation: 0.1941303301292795]
	TIME [epoch: 12.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23431779121431004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23431779121431004 | validation: 0.3367821227952383]
	TIME [epoch: 12.9 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22956673042112663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22956673042112663 | validation: 0.27623413065938246]
	TIME [epoch: 12.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2830984590416832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2830984590416832 | validation: 0.43597906931979624]
	TIME [epoch: 12.9 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38484949341899033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38484949341899033 | validation: 0.3680488207676521]
	TIME [epoch: 12.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37328631533697476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37328631533697476 | validation: 0.18762891792001787]
	TIME [epoch: 12.9 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21680429925908795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21680429925908795 | validation: 0.244658576385424]
	TIME [epoch: 12.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18143422772371118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18143422772371118 | validation: 0.2142348113145141]
	TIME [epoch: 12.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21011446247010926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21011446247010926 | validation: 0.3351645943525851]
	TIME [epoch: 12.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31447488080941216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31447488080941216 | validation: 0.4028214422439229]
	TIME [epoch: 12.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3941024817787171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3941024817787171 | validation: 0.24711800245370102]
	TIME [epoch: 12.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28382878460089894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28382878460089894 | validation: 0.41845982471304466]
	TIME [epoch: 12.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2785013487988781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2785013487988781 | validation: 0.2397861473541445]
	TIME [epoch: 12.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2724255158215065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2724255158215065 | validation: 0.3382082716688225]
	TIME [epoch: 12.9 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3079455659456868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3079455659456868 | validation: 0.34136317090328305]
	TIME [epoch: 12.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3297836446049374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3297836446049374 | validation: 0.2870730866655182]
	TIME [epoch: 12.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2749975197210419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2749975197210419 | validation: 0.22165658508732156]
	TIME [epoch: 12.9 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23506737321340868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23506737321340868 | validation: 0.34035091374353016]
	TIME [epoch: 12.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2738219436380805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2738219436380805 | validation: 0.30457668518932524]
	TIME [epoch: 12.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32568401961463767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32568401961463767 | validation: 0.1960200056519217]
	TIME [epoch: 12.9 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17044706571285048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17044706571285048 | validation: 0.16716792863566787]
	TIME [epoch: 12.9 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.161153340008874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.161153340008874 | validation: 0.2970772769837537]
	TIME [epoch: 12.9 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23864906125966323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23864906125966323 | validation: 0.4226285897959852]
	TIME [epoch: 12.9 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4311494002313806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4311494002313806 | validation: 0.2325742617228284]
	TIME [epoch: 12.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27969827194150104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27969827194150104 | validation: 0.32838735518852774]
	TIME [epoch: 12.9 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23481799630732642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23481799630732642 | validation: 0.24483121704919292]
	TIME [epoch: 12.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29794373492452897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29794373492452897 | validation: 0.32026720725868246]
	TIME [epoch: 12.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23230802491734523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23230802491734523 | validation: 0.30854792332178715]
	TIME [epoch: 12.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31849363312630774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31849363312630774 | validation: 0.3015670540882039]
	TIME [epoch: 12.9 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.319665078896649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.319665078896649 | validation: 0.3168162419509983]
	TIME [epoch: 12.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2589776047055794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2589776047055794 | validation: 0.2437388043672052]
	TIME [epoch: 12.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23744657671197505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23744657671197505 | validation: 0.30725771594457063]
	TIME [epoch: 12.9 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28402590553092505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28402590553092505 | validation: 0.3814812899109048]
	TIME [epoch: 12.9 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27529253882846483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27529253882846483 | validation: 0.31719111896525515]
	TIME [epoch: 12.9 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38839066380033416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38839066380033416 | validation: 0.20109419196687095]
	TIME [epoch: 12.9 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20296528119107643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20296528119107643 | validation: 0.34485002902873074]
	TIME [epoch: 12.9 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28057458053093665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28057458053093665 | validation: 0.42317243664233906]
	TIME [epoch: 12.9 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.430265812448576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.430265812448576 | validation: 0.17196244068539734]
	TIME [epoch: 12.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15489568697637338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15489568697637338 | validation: 0.15644744703960597]
	TIME [epoch: 12.9 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14464380826402928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14464380826402928 | validation: 0.17409902584356413]
	TIME [epoch: 12.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17480469257144107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17480469257144107 | validation: 0.4528533205449704]
	TIME [epoch: 12.9 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38211441861010015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38211441861010015 | validation: 0.38867815060145383]
	TIME [epoch: 12.9 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42896507600466577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42896507600466577 | validation: 0.2561087399159761]
	TIME [epoch: 12.9 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28783260568038166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28783260568038166 | validation: 0.5728055202766811]
	TIME [epoch: 12.9 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44092907845168344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44092907845168344 | validation: 0.19173487165451633]
	TIME [epoch: 12.9 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19233236289239253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19233236289239253 | validation: 0.1662702781009538]
	TIME [epoch: 12.9 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685292574656664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685292574656664 | validation: 0.36657885608145957]
	TIME [epoch: 12.9 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30828314130807527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30828314130807527 | validation: 0.39637467173618623]
	TIME [epoch: 12.9 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4195332689610355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4195332689610355 | validation: 0.1618406029764281]
	TIME [epoch: 12.9 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1835699010042815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1835699010042815 | validation: 0.35230591507716313]
	TIME [epoch: 12.9 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24355813858367142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24355813858367142 | validation: 0.2732218322392825]
	TIME [epoch: 12.9 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28700513139431033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28700513139431033 | validation: 0.32621581448828163]
	TIME [epoch: 12.9 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2500702776186707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2500702776186707 | validation: 0.47487812433060594]
	TIME [epoch: 12.9 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45007353736201533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45007353736201533 | validation: 0.29602150000667243]
	TIME [epoch: 12.9 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3186038383999268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3186038383999268 | validation: 0.18225956557454148]
	TIME [epoch: 12.9 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17032978162699647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17032978162699647 | validation: 0.18636740943218794]
	TIME [epoch: 12.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573211047681464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1573211047681464 | validation: 0.27387377153775194]
	TIME [epoch: 12.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2802474331974287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2802474331974287 | validation: 0.5701374675706182]
	TIME [epoch: 12.9 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5076150220743385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5076150220743385 | validation: 0.22608189088884256]
	TIME [epoch: 13 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2807358567044819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2807358567044819 | validation: 0.16541110226316466]
	TIME [epoch: 12.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18722018567453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18722018567453 | validation: 0.19539150572644776]
	TIME [epoch: 12.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16416373115125008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16416373115125008 | validation: 0.3502360632015751]
	TIME [epoch: 12.9 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33595828256355575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33595828256355575 | validation: 0.34884189370772845]
	TIME [epoch: 12.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3456758604606304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3456758604606304 | validation: 0.27622270182746356]
	TIME [epoch: 12.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2742690246913245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2742690246913245 | validation: 0.22173431442199698]
	TIME [epoch: 12.9 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22140791904855894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22140791904855894 | validation: 0.2733710072273192]
	TIME [epoch: 12.9 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20721656734855315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20721656734855315 | validation: 0.31841362052989847]
	TIME [epoch: 12.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33119470823559866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33119470823559866 | validation: 0.31262658492429035]
	TIME [epoch: 13 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24915339231196473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24915339231196473 | validation: 0.21237443665190164]
	TIME [epoch: 12.9 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20569744923926891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20569744923926891 | validation: 0.23527483376263394]
	TIME [epoch: 13 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24916599051599245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24916599051599245 | validation: 0.4247289978425064]
	TIME [epoch: 13 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31978591132220535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31978591132220535 | validation: 0.30059581554297377]
	TIME [epoch: 12.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3602937550936743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3602937550936743 | validation: 0.16423763008356834]
	TIME [epoch: 13 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18092886106408557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18092886106408557 | validation: 0.25035091847983265]
	TIME [epoch: 12.9 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17561974489802115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17561974489802115 | validation: 0.1840895098667158]
	TIME [epoch: 13 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2067978162156151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2067978162156151 | validation: 0.26827734747759496]
	TIME [epoch: 12.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18729520656772372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18729520656772372 | validation: 0.3014719705510411]
	TIME [epoch: 12.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30658717528200063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30658717528200063 | validation: 0.2564391928889755]
	TIME [epoch: 12.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539342603034638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2539342603034638 | validation: 0.28222993673167135]
	TIME [epoch: 12.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24811586853879966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24811586853879966 | validation: 0.26603610660930127]
	TIME [epoch: 12.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2567767791501209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2567767791501209 | validation: 0.25582979131575795]
	TIME [epoch: 12.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25389730066128635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25389730066128635 | validation: 0.19629291076903843]
	TIME [epoch: 12.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825566298682072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825566298682072 | validation: 0.19885552121414524]
	TIME [epoch: 12.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19665450972660792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19665450972660792 | validation: 0.34600996116555294]
	TIME [epoch: 13 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2773440395788662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2773440395788662 | validation: 0.3079054532208333]
	TIME [epoch: 12.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3482648216572652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3482648216572652 | validation: 0.3149159028480177]
	TIME [epoch: 12.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24302580026345816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24302580026345816 | validation: 0.2794330466021235]
	TIME [epoch: 13 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2919168087777912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2919168087777912 | validation: 0.3007235073193692]
	TIME [epoch: 12.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28049972834049947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28049972834049947 | validation: 0.26270945969786047]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_130220/states/model_phi1_4c_v_mmd1_728.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5888.904 seconds.
