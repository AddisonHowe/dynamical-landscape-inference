Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 397321665

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.458851464118104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.458851464118104 | validation: 5.574351429878295]
	TIME [epoch: 161 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.85263186090309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.85263186090309 | validation: 4.640299374688982]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.846939659502905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.846939659502905 | validation: 4.655290439312147]
	TIME [epoch: 1.38 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.110425530589965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.110425530589965 | validation: 4.521142998989013]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.984825962378972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.984825962378972 | validation: 4.012504457900319]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.570585277788115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.570585277788115 | validation: 4.314551838234324]
	TIME [epoch: 1.38 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.74211348487441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.74211348487441 | validation: 3.94317749459316]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.460745608767775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.460745608767775 | validation: 3.917307633861861]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.425123538523601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.425123538523601 | validation: 3.8531009790853674]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.363624662989105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.363624662989105 | validation: 3.894867212234699]
	TIME [epoch: 1.38 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.293961571043799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.293961571043799 | validation: 3.830317004101363]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.222792137955656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.222792137955656 | validation: 3.7460166048266608]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.170004410971759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.170004410971759 | validation: 3.799607619730602]
	TIME [epoch: 1.38 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.113458123014736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.113458123014736 | validation: 3.7814208023486913]
	TIME [epoch: 1.37 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.075068213399057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.075068213399057 | validation: 3.8012522851132213]
	TIME [epoch: 1.38 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.185660013423771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.185660013423771 | validation: 3.918553910139176]
	TIME [epoch: 1.38 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.137613790278083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.137613790278083 | validation: 3.6337169464439896]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.008385520902809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.008385520902809 | validation: 3.6384045087870103]
	TIME [epoch: 1.38 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8873868138211662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8873868138211662 | validation: 3.60130048173576]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.856390216363528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.856390216363528 | validation: 3.5335783894956423]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8470371798380865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8470371798380865 | validation: 3.6629664154020563]
	TIME [epoch: 1.37 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.872091047641676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.872091047641676 | validation: 3.611906104052006]
	TIME [epoch: 1.37 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.977770952895476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.977770952895476 | validation: 3.553029569698788]
	TIME [epoch: 1.72 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7713820057083867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7713820057083867 | validation: 3.4014933492454404]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7251913056212063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7251913056212063 | validation: 3.5483880664711567]
	TIME [epoch: 1.38 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7445457254605117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7445457254605117 | validation: 3.434887169780751]
	TIME [epoch: 1.38 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.847799332942001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.847799332942001 | validation: 3.633801584401043]
	TIME [epoch: 1.38 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.826896372454081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.826896372454081 | validation: 3.360621271564017]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.641937193287787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.641937193287787 | validation: 3.3084618004537405]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.656732572472198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.656732572472198 | validation: 3.434396046920459]
	TIME [epoch: 1.38 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.653505340746059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.653505340746059 | validation: 3.272305392985553]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6008875203733557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6008875203733557 | validation: 3.2895782983352855]
	TIME [epoch: 1.38 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.568314152834598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.568314152834598 | validation: 3.241911022353945]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.550185147670768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.550185147670768 | validation: 3.2385903274909382]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5282732469153997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5282732469153997 | validation: 3.2022764715679415]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.518139055637437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.518139055637437 | validation: 3.255923821820531]
	TIME [epoch: 1.38 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.525332876372056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.525332876372056 | validation: 3.3253400445882755]
	TIME [epoch: 1.37 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6961242827722165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6961242827722165 | validation: 3.3975366691906568]
	TIME [epoch: 1.37 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6309988169928475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6309988169928475 | validation: 3.1907081335433]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5511699691502954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5511699691502954 | validation: 3.1246201564576874]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4445087391873095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4445087391873095 | validation: 3.171605524603227]
	TIME [epoch: 1.38 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.45224552213773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.45224552213773 | validation: 3.095357428976534]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4616531412031373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4616531412031373 | validation: 3.185158365268375]
	TIME [epoch: 1.37 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.45416830551684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.45416830551684 | validation: 3.1303934608785164]
	TIME [epoch: 1.37 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.488836417786885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.488836417786885 | validation: 3.249186371488479]
	TIME [epoch: 1.37 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.51751730082839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.51751730082839 | validation: 3.0476032534148114]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3643002567146687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3643002567146687 | validation: 3.053663017205696]
	TIME [epoch: 1.37 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3838273974444566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3838273974444566 | validation: 3.1190590597145844]
	TIME [epoch: 1.37 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3970079590016304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3970079590016304 | validation: 3.010021321337236]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3351820207662493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3351820207662493 | validation: 3.0080709063052793]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3189368574266904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3189368574266904 | validation: 2.9982171336885015]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3112935838426156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3112935838426156 | validation: 2.974282950161522]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.291049272805221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.291049272805221 | validation: 2.9785725001845447]
	TIME [epoch: 1.37 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.279994219228362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.279994219228362 | validation: 2.9633010786691116]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.262400144971815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.262400144971815 | validation: 2.958477020433071]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2473818753357966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2473818753357966 | validation: 2.938973680439732]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.235060208519055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.235060208519055 | validation: 2.967657843324114]
	TIME [epoch: 1.37 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.221509147218826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.221509147218826 | validation: 2.9308785452054416]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1830067476991633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1830067476991633 | validation: 2.9991449523652607]
	TIME [epoch: 1.37 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.05881987216054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.05881987216054 | validation: 2.8398707609744744]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8384734195356702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8384734195356702 | validation: 2.9064893097480575]
	TIME [epoch: 1.37 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.942150636986114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.942150636986114 | validation: 2.725280624582661]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0911109456555494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0911109456555494 | validation: 2.5664424928259493]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8440526143066074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8440526143066074 | validation: 2.4532476443859412]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.260867691184517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.260867691184517 | validation: 2.4458070765686655]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0764174875660184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0764174875660184 | validation: 2.268874964963887]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0896761091049845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0896761091049845 | validation: 1.579197916294918]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4095982080267095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4095982080267095 | validation: 1.806050127166675]
	TIME [epoch: 1.38 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4202781288263655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4202781288263655 | validation: 2.8694637920998747]
	TIME [epoch: 1.38 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.676141053306079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.676141053306079 | validation: 1.4528446927211682]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2656085302264801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2656085302264801 | validation: 1.9413871262560993]
	TIME [epoch: 1.38 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5367539252430544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5367539252430544 | validation: 1.3206797849434546]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.095725287981334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.095725287981334 | validation: 1.3153551465400093]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1299274367038334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1299274367038334 | validation: 1.116627301623904]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9619912368929676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9619912368929676 | validation: 1.1526927541201344]
	TIME [epoch: 1.37 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202019488583615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202019488583615 | validation: 1.0257663347032289]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971965667046985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8971965667046985 | validation: 1.0289337045168512]
	TIME [epoch: 1.38 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637558482519703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8637558482519703 | validation: 0.9863096515921186]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8378888516167823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8378888516167823 | validation: 0.9600639089836998]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8283565792085917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8283565792085917 | validation: 0.9825139304677573]
	TIME [epoch: 1.38 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7988876041037721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7988876041037721 | validation: 0.9136963263428849]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7796279273437471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7796279273437471 | validation: 0.9842377066633596]
	TIME [epoch: 1.37 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779660714550833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.779660714550833 | validation: 0.8852614717600498]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7752738599630707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7752738599630707 | validation: 0.9737108156447628]
	TIME [epoch: 1.37 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875498185163591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7875498185163591 | validation: 0.8710727470371453]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822390798896031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822390798896031 | validation: 1.2611124434160672]
	TIME [epoch: 1.38 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9629113464529837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9629113464529837 | validation: 0.856635645011383]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794280637835379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794280637835379 | validation: 0.9803195422473743]
	TIME [epoch: 1.38 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629175686684496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629175686684496 | validation: 0.8453499508197653]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487070225272217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7487070225272217 | validation: 0.9012365310113634]
	TIME [epoch: 1.37 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459020696538419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7459020696538419 | validation: 0.8292461829790533]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.737288477149032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.737288477149032 | validation: 0.961729125046805]
	TIME [epoch: 1.37 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7553763607952846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7553763607952846 | validation: 0.830854837917207]
	TIME [epoch: 1.38 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875499333160443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7875499333160443 | validation: 1.0582402804335946]
	TIME [epoch: 1.37 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8314755016245318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8314755016245318 | validation: 0.813118397506676]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7824215462683659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7824215462683659 | validation: 0.9034236760245347]
	TIME [epoch: 1.38 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7434745605755643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7434745605755643 | validation: 0.8039424897562418]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7209123083675061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7209123083675061 | validation: 0.8288891764629329]
	TIME [epoch: 1.38 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126394213488775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126394213488775 | validation: 0.8537930843970017]
	TIME [epoch: 1.38 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7161263836605621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7161263836605621 | validation: 0.8327058969059156]
	TIME [epoch: 1.38 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060029516811284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060029516811284 | validation: 0.7863908286217232]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162352832832835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162352832832835 | validation: 1.1522060983636493]
	TIME [epoch: 1.38 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015085852104781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015085852104781 | validation: 1.0233771439466814]
	TIME [epoch: 1.39 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0067624857121014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0067624857121014 | validation: 1.4298748785587994]
	TIME [epoch: 1.37 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0695027725121766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0695027725121766 | validation: 1.0020092627688961]
	TIME [epoch: 1.37 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7339024737183383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7339024737183383 | validation: 0.8302142575430393]
	TIME [epoch: 1.38 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903214521969549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7903214521969549 | validation: 0.8451106811595541]
	TIME [epoch: 1.37 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292798565440961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292798565440961 | validation: 0.8841886272153762]
	TIME [epoch: 1.38 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711703719482239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711703719482239 | validation: 0.7451496639062154]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6989895896705308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6989895896705308 | validation: 0.7457643450782911]
	TIME [epoch: 1.38 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6863757944874727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6863757944874727 | validation: 0.8101638725578131]
	TIME [epoch: 1.38 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6827535786406975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6827535786406975 | validation: 0.733987908888699]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678195555506685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.678195555506685 | validation: 0.8420234629452411]
	TIME [epoch: 1.38 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7028582178364741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7028582178364741 | validation: 0.8692971649699461]
	TIME [epoch: 1.38 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657202300257023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657202300257023 | validation: 0.7862093631898868]
	TIME [epoch: 1.38 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997040940824586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7997040940824586 | validation: 0.8356815360785714]
	TIME [epoch: 1.37 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6997958081685173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6997958081685173 | validation: 0.7078797625216895]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.620532895122424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.620532895122424 | validation: 0.6749595966067866]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6143641789717534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6143641789717534 | validation: 0.6908450482927662]
	TIME [epoch: 1.38 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6211393499492496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6211393499492496 | validation: 1.0259989135140306]
	TIME [epoch: 1.38 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328035151044674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7328035151044674 | validation: 0.7657542166587716]
	TIME [epoch: 1.38 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6612323017472511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6612323017472511 | validation: 0.7222988556251301]
	TIME [epoch: 1.37 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5960964501772822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5960964501772822 | validation: 0.7282918301397052]
	TIME [epoch: 1.38 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5777968723236253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5777968723236253 | validation: 0.6153388791446098]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658122618277819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6658122618277819 | validation: 1.331527643093112]
	TIME [epoch: 1.38 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838975289450045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.838975289450045 | validation: 0.5954530707280109]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_126.pth
	Model improved!!!
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5856198604543067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5856198604543067 | validation: 0.6464318553680912]
	TIME [epoch: 1.38 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6048351036797643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6048351036797643 | validation: 0.9111263850004152]
	TIME [epoch: 1.37 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926798256324558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5926798256324558 | validation: 1.0937773170369869]
	TIME [epoch: 1.37 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5981657108871243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5981657108871243 | validation: 0.6413789440125574]
	TIME [epoch: 1.37 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.52736898251886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.52736898251886 | validation: 0.6161001972006881]
	TIME [epoch: 1.37 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6003654038520656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6003654038520656 | validation: 0.769123388513545]
	TIME [epoch: 1.37 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.607526884646266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607526884646266 | validation: 1.1312630668571948]
	TIME [epoch: 1.37 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6096252676518324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6096252676518324 | validation: 0.629134752783798]
	TIME [epoch: 1.37 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4966469400701716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4966469400701716 | validation: 0.5777789708207799]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5323414410322856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5323414410322856 | validation: 0.6844386432347414]
	TIME [epoch: 1.38 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.491185474573217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.491185474573217 | validation: 0.7084914411436802]
	TIME [epoch: 1.38 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46639125609352916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46639125609352916 | validation: 0.5282003682897993]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4572208785055354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4572208785055354 | validation: 0.6971711568185128]
	TIME [epoch: 1.38 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5442880309845454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5442880309845454 | validation: 0.5619981618827028]
	TIME [epoch: 1.37 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5975733714214303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5975733714214303 | validation: 0.6344085542112151]
	TIME [epoch: 1.37 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47124455448436553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47124455448436553 | validation: 0.6390978806319163]
	TIME [epoch: 1.37 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5166442705937185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5166442705937185 | validation: 0.5480580446512463]
	TIME [epoch: 1.37 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4402000928400822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4402000928400822 | validation: 0.6524528812449472]
	TIME [epoch: 1.37 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42315404864412814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42315404864412814 | validation: 0.516703087419296]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43621285884634403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43621285884634403 | validation: 0.785606180438607]
	TIME [epoch: 1.37 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4425268567668331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4425268567668331 | validation: 0.524454133765528]
	TIME [epoch: 1.37 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4245918627559403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4245918627559403 | validation: 0.691578356040672]
	TIME [epoch: 1.37 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4604438986115019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4604438986115019 | validation: 0.5014673569402736]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4220648090748098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4220648090748098 | validation: 0.8052569747662948]
	TIME [epoch: 1.37 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4317710828871952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4317710828871952 | validation: 0.5113588858446185]
	TIME [epoch: 1.37 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37705713059406637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37705713059406637 | validation: 0.5696365858350108]
	TIME [epoch: 1.37 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37058897248601946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37058897248601946 | validation: 0.5695640824685672]
	TIME [epoch: 1.37 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33878020954758775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33878020954758775 | validation: 0.5026286572539136]
	TIME [epoch: 1.37 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3274325834863476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3274325834863476 | validation: 0.5486106253284619]
	TIME [epoch: 1.37 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3305578062794626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3305578062794626 | validation: 0.49492935735533367]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38236239810508665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38236239810508665 | validation: 0.965429859962309]
	TIME [epoch: 1.38 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5226323778220102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5226323778220102 | validation: 0.5401588394951156]
	TIME [epoch: 1.38 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5944797964189025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5944797964189025 | validation: 0.7266342761339222]
	TIME [epoch: 1.38 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113334835855211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6113334835855211 | validation: 0.7926568928835018]
	TIME [epoch: 1.38 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4036471637384594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4036471637384594 | validation: 0.5052672451554997]
	TIME [epoch: 1.38 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46204430758729004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46204430758729004 | validation: 0.49164055252855116]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_162.pth
	Model improved!!!
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3216063332969014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3216063332969014 | validation: 0.5065217186628298]
	TIME [epoch: 1.38 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3199861844309188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3199861844309188 | validation: 0.4821033658277072]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30661551434216167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30661551434216167 | validation: 0.5291561603971264]
	TIME [epoch: 1.38 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28616018333130805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28616018333130805 | validation: 0.4452787391869235]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2934797796191582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2934797796191582 | validation: 0.5458967473580656]
	TIME [epoch: 1.38 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840084304320073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2840084304320073 | validation: 0.43278278244545176]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_168.pth
	Model improved!!!
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34187910409600664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34187910409600664 | validation: 0.852348738129288]
	TIME [epoch: 1.37 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4417244649546767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4417244649546767 | validation: 0.5046174271994731]
	TIME [epoch: 1.37 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299718978751865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7299718978751865 | validation: 0.41179058787099426]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6261772584310958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6261772584310958 | validation: 0.5679080889631207]
	TIME [epoch: 1.38 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5884798431829904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5884798431829904 | validation: 0.39281466899124845]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4901824245579364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4901824245579364 | validation: 0.42420121547508993]
	TIME [epoch: 1.37 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34051228599207994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34051228599207994 | validation: 0.5849958754806382]
	TIME [epoch: 1.37 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32697224486758614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32697224486758614 | validation: 0.4241885660431917]
	TIME [epoch: 1.37 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3033574651432358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3033574651432358 | validation: 0.617932274389416]
	TIME [epoch: 1.37 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28310733978513697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28310733978513697 | validation: 0.47548124099014316]
	TIME [epoch: 1.38 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24957837412791065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24957837412791065 | validation: 0.43957912569834134]
	TIME [epoch: 1.38 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2508777200516438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2508777200516438 | validation: 0.4990062786470226]
	TIME [epoch: 1.37 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2347394543993456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2347394543993456 | validation: 0.4160774194718402]
	TIME [epoch: 1.38 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.257145616443012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.257145616443012 | validation: 0.7348403987610621]
	TIME [epoch: 1.37 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38801422615544034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38801422615544034 | validation: 0.5381213011653667]
	TIME [epoch: 1.37 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336201967802555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336201967802555 | validation: 0.4161911157275823]
	TIME [epoch: 1.37 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25025393520477474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25025393520477474 | validation: 0.7771926684869996]
	TIME [epoch: 1.38 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4303428726143173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4303428726143173 | validation: 0.4275943467427434]
	TIME [epoch: 1.37 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28165936748448744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28165936748448744 | validation: 0.42384664447182074]
	TIME [epoch: 1.37 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23946590164464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23946590164464 | validation: 0.584179984471945]
	TIME [epoch: 1.37 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28402760251543124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28402760251543124 | validation: 0.4290435755030217]
	TIME [epoch: 1.37 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2526882573853484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2526882573853484 | validation: 0.44449659547814097]
	TIME [epoch: 1.37 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22160490389035623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22160490389035623 | validation: 0.46320358944089346]
	TIME [epoch: 1.37 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21625048281970694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21625048281970694 | validation: 0.41539481089255237]
	TIME [epoch: 1.37 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22184415189472387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22184415189472387 | validation: 0.4910862246959714]
	TIME [epoch: 1.78 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22201276133747033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22201276133747033 | validation: 0.38745366380862634]
	TIME [epoch: 1.37 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2470536270460483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2470536270460483 | validation: 0.6980174522222281]
	TIME [epoch: 1.37 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3148977355498687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3148977355498687 | validation: 0.39652696150781935]
	TIME [epoch: 1.37 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36875934144322203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36875934144322203 | validation: 0.5809089468514843]
	TIME [epoch: 1.37 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3839288028365987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3839288028365987 | validation: 0.43662167908275595]
	TIME [epoch: 1.37 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21635128363247874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21635128363247874 | validation: 0.4006567659484488]
	TIME [epoch: 1.38 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21301062540129168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21301062540129168 | validation: 0.4567038158823582]
	TIME [epoch: 1.38 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22086809312075611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22086809312075611 | validation: 0.38339019249235956]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2180491783797713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2180491783797713 | validation: 0.5515227889518343]
	TIME [epoch: 2.73 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21906210208270074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21906210208270074 | validation: 0.3838250492101461]
	TIME [epoch: 2.73 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22822401127374203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22822401127374203 | validation: 0.5494409632995607]
	TIME [epoch: 2.73 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23877439704125308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23877439704125308 | validation: 0.39247781210850297]
	TIME [epoch: 2.73 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25784687826025976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25784687826025976 | validation: 0.5275398940675092]
	TIME [epoch: 2.73 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21145921258742978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21145921258742978 | validation: 0.37955123812702196]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18796056709397038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18796056709397038 | validation: 0.4368753722780233]
	TIME [epoch: 2.72 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18508703004129484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18508703004129484 | validation: 0.39099401845437287]
	TIME [epoch: 2.72 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15831626304409538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15831626304409538 | validation: 0.3888672143467302]
	TIME [epoch: 2.72 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15575189143267845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15575189143267845 | validation: 0.42229523838350613]
	TIME [epoch: 2.72 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17047837460065757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17047837460065757 | validation: 0.366434739954991]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2016174850991925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2016174850991925 | validation: 0.7287463946153274]
	TIME [epoch: 2.72 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33316848278284517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33316848278284517 | validation: 0.42518741525777987]
	TIME [epoch: 2.71 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46750164035335995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46750164035335995 | validation: 0.585518746434086]
	TIME [epoch: 2.72 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743380679724849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2743380679724849 | validation: 0.39244354456280756]
	TIME [epoch: 2.71 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1648126466460684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1648126466460684 | validation: 0.36164752487906193]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_217.pth
	Model improved!!!
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741472193772148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1741472193772148 | validation: 0.4759088601333288]
	TIME [epoch: 2.72 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18028842130319622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18028842130319622 | validation: 0.32842947087599383]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16257983424425113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16257983424425113 | validation: 0.4493691146285339]
	TIME [epoch: 2.72 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1610650860652099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1610650860652099 | validation: 0.33970092212355685]
	TIME [epoch: 2.72 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15072372894933606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15072372894933606 | validation: 0.40685080232235704]
	TIME [epoch: 2.71 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1444610544177719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1444610544177719 | validation: 0.3317791606163787]
	TIME [epoch: 2.72 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15011053032925264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15011053032925264 | validation: 0.4223957906178404]
	TIME [epoch: 2.71 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1441777753069198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1441777753069198 | validation: 0.3139398667799316]
	TIME [epoch: 2.71 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_225.pth
	Model improved!!!
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15746418666796488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15746418666796488 | validation: 0.5129869921704281]
	TIME [epoch: 2.73 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16886492531790423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16886492531790423 | validation: 0.29567593484574023]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_227.pth
	Model improved!!!
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17841183654916176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17841183654916176 | validation: 0.7140382854000136]
	TIME [epoch: 2.73 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30316497989240293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30316497989240293 | validation: 0.44671649108364114]
	TIME [epoch: 2.74 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26602107711320616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26602107711320616 | validation: 0.40510165316641866]
	TIME [epoch: 2.73 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15946267373869633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15946267373869633 | validation: 0.4183662155333515]
	TIME [epoch: 2.73 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14027694863240048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14027694863240048 | validation: 0.3297476095234723]
	TIME [epoch: 2.73 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13153574407598811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13153574407598811 | validation: 0.390354599857864]
	TIME [epoch: 2.73 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1382083173957073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1382083173957073 | validation: 0.3081501671771906]
	TIME [epoch: 2.73 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13377711563102232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13377711563102232 | validation: 0.4156710139359778]
	TIME [epoch: 2.73 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14806756314661956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14806756314661956 | validation: 0.3150838506085733]
	TIME [epoch: 2.74 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14836724662751063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14836724662751063 | validation: 0.52457585191553]
	TIME [epoch: 2.73 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29717244284015915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29717244284015915 | validation: 0.34672166876287236]
	TIME [epoch: 2.73 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13360079402102093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13360079402102093 | validation: 0.39301861187959397]
	TIME [epoch: 2.73 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1755203624744157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1755203624744157 | validation: 0.3848973745353823]
	TIME [epoch: 2.73 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18476561502158298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18476561502158298 | validation: 0.555040147739671]
	TIME [epoch: 2.73 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19924206931618535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19924206931618535 | validation: 0.4728749509144825]
	TIME [epoch: 2.73 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5595455374938562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5595455374938562 | validation: 0.30072070484813973]
	TIME [epoch: 2.73 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15266385199199772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15266385199199772 | validation: 0.6373975301140098]
	TIME [epoch: 2.73 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27341603649491797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27341603649491797 | validation: 0.39097754275632]
	TIME [epoch: 2.73 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17994653499862426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17994653499862426 | validation: 0.3412118479451728]
	TIME [epoch: 2.73 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1491318553503066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1491318553503066 | validation: 0.42340343873330694]
	TIME [epoch: 2.73 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328809815377855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1328809815377855 | validation: 0.36063894723544787]
	TIME [epoch: 2.73 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11897695462626427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11897695462626427 | validation: 0.3283415575655094]
	TIME [epoch: 2.73 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11824081181912206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11824081181912206 | validation: 0.3175428757659137]
	TIME [epoch: 2.73 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175896534366421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10175896534366421 | validation: 0.331708156663688]
	TIME [epoch: 2.73 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10426040920145177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10426040920145177 | validation: 0.3049197667298626]
	TIME [epoch: 2.73 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10246025662692163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10246025662692163 | validation: 0.32449215586929847]
	TIME [epoch: 2.73 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10074998758867146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10074998758867146 | validation: 0.3138744775959638]
	TIME [epoch: 2.73 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12024956047329997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12024956047329997 | validation: 0.4312935508622568]
	TIME [epoch: 2.73 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1805018503488893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1805018503488893 | validation: 0.3244738605790216]
	TIME [epoch: 2.73 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2013496995465419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2013496995465419 | validation: 0.3587746168148354]
	TIME [epoch: 2.73 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12501212840527554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12501212840527554 | validation: 0.3477059482778634]
	TIME [epoch: 2.72 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12308833981762696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12308833981762696 | validation: 0.29912301613395603]
	TIME [epoch: 2.73 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1029061161113389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1029061161113389 | validation: 0.319380904966671]
	TIME [epoch: 2.72 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12652016369455452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12652016369455452 | validation: 0.3212884627682369]
	TIME [epoch: 2.73 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11255778193099643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11255778193099643 | validation: 0.3236590941232265]
	TIME [epoch: 2.73 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10486329468291744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10486329468291744 | validation: 0.2582606908328347]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11765210839399615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11765210839399615 | validation: 0.5313381078817366]
	TIME [epoch: 2.72 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17339485874458252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17339485874458252 | validation: 0.37826595645365746]
	TIME [epoch: 2.72 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2380741151317632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2380741151317632 | validation: 0.45318973838729754]
	TIME [epoch: 2.72 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16394665978698034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16394665978698034 | validation: 0.2762167125043426]
	TIME [epoch: 2.72 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10448727846075918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10448727846075918 | validation: 0.3304424304734612]
	TIME [epoch: 2.72 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222705787534212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09222705787534212 | validation: 0.2864687615560764]
	TIME [epoch: 2.72 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08487168676917893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08487168676917893 | validation: 0.28069175767100474]
	TIME [epoch: 2.72 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08436494796003097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08436494796003097 | validation: 0.3243623038331965]
	TIME [epoch: 2.72 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09574025399876643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09574025399876643 | validation: 0.25348585625429987]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12368768719598836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12368768719598836 | validation: 0.4464816835461564]
	TIME [epoch: 2.73 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20039196995529765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20039196995529765 | validation: 0.2861127701008642]
	TIME [epoch: 2.73 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12192504057638828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12192504057638828 | validation: 0.33297514296525244]
	TIME [epoch: 2.73 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1269608280577267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1269608280577267 | validation: 0.2974418117650121]
	TIME [epoch: 2.73 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14004423146056916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14004423146056916 | validation: 0.5433737778316914]
	TIME [epoch: 2.73 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14969514030092027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14969514030092027 | validation: 0.2627565893975475]
	TIME [epoch: 2.73 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13606385171196372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13606385171196372 | validation: 0.5106642470220022]
	TIME [epoch: 2.72 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21381505777286985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21381505777286985 | validation: 0.3173731135342645]
	TIME [epoch: 2.73 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13062773240264725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13062773240264725 | validation: 0.2834863835483139]
	TIME [epoch: 2.72 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09988209835895447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09988209835895447 | validation: 0.3245152495665525]
	TIME [epoch: 2.72 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09849803489668277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09849803489668277 | validation: 0.2640038979388121]
	TIME [epoch: 2.72 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09661880444342386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09661880444342386 | validation: 0.40143373943318894]
	TIME [epoch: 2.73 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12005192748554695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12005192748554695 | validation: 0.2412978439712673]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12123772303709618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12123772303709618 | validation: 0.36931356850352215]
	TIME [epoch: 2.72 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11045666084205039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11045666084205039 | validation: 0.2567482136544658]
	TIME [epoch: 2.73 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002361359349461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1002361359349461 | validation: 0.31225131564403835]
	TIME [epoch: 2.73 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902202809019714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902202809019714 | validation: 0.2854415638108058]
	TIME [epoch: 2.73 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11352107105288445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11352107105288445 | validation: 0.3247021314574616]
	TIME [epoch: 2.72 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15289840140519517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15289840140519517 | validation: 0.3551089328939399]
	TIME [epoch: 2.73 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1569966162269291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1569966162269291 | validation: 0.2656775421686408]
	TIME [epoch: 2.73 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10875877784977828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10875877784977828 | validation: 0.43753921674497287]
	TIME [epoch: 2.72 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12867089353386177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12867089353386177 | validation: 0.27390930808659397]
	TIME [epoch: 2.72 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10215839327770745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10215839327770745 | validation: 0.3054787364655957]
	TIME [epoch: 2.73 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09383321821073969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09383321821073969 | validation: 0.24546192659939683]
	TIME [epoch: 2.74 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10748082493930593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10748082493930593 | validation: 0.3794487861214966]
	TIME [epoch: 2.73 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384241473764265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1384241473764265 | validation: 0.25492238641810455]
	TIME [epoch: 2.73 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14479757779323577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14479757779323577 | validation: 0.38997480880406493]
	TIME [epoch: 2.73 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11145350727868472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11145350727868472 | validation: 0.2227041619323539]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08095443468487921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08095443468487921 | validation: 0.2846589512846304]
	TIME [epoch: 2.72 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06893882444838254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06893882444838254 | validation: 0.24579743275754634]
	TIME [epoch: 2.71 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0681715162066958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0681715162066958 | validation: 0.2384775909221645]
	TIME [epoch: 2.71 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07606057456276247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07606057456276247 | validation: 0.28468999609932927]
	TIME [epoch: 2.72 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07439226110505562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07439226110505562 | validation: 0.22910479436854964]
	TIME [epoch: 2.72 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07786897311048095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07786897311048095 | validation: 0.4414205570876506]
	TIME [epoch: 2.72 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12942155509401182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12942155509401182 | validation: 0.31246927127875185]
	TIME [epoch: 2.71 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18716296503909796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18716296503909796 | validation: 0.36667759493069074]
	TIME [epoch: 2.72 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11729455853490378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11729455853490378 | validation: 0.2039467295260874]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08521687156467504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08521687156467504 | validation: 0.29758496686377767]
	TIME [epoch: 2.72 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254227061480998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1254227061480998 | validation: 0.29728623031685014]
	TIME [epoch: 2.72 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2095100508864031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2095100508864031 | validation: 0.3168827279698207]
	TIME [epoch: 2.72 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13883398015418977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13883398015418977 | validation: 0.23343396712764566]
	TIME [epoch: 2.71 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10000996492941999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10000996492941999 | validation: 0.23601084009644976]
	TIME [epoch: 2.72 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08517809454896134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08517809454896134 | validation: 0.2619429676647835]
	TIME [epoch: 2.71 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07920666608693043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07920666608693043 | validation: 0.22934418093409892]
	TIME [epoch: 2.72 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09587998141724534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09587998141724534 | validation: 0.6748092877738138]
	TIME [epoch: 2.72 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2290015121836077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2290015121836077 | validation: 0.2951791200405416]
	TIME [epoch: 2.72 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090287013775264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1090287013775264 | validation: 0.251828148832943]
	TIME [epoch: 2.73 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10838810520085553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10838810520085553 | validation: 0.29551936967038994]
	TIME [epoch: 2.73 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0980609939625858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0980609939625858 | validation: 0.24251113304879146]
	TIME [epoch: 2.73 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09547741146698542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09547741146698542 | validation: 0.2763888862816636]
	TIME [epoch: 2.73 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10948337335125889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10948337335125889 | validation: 0.21833064280286585]
	TIME [epoch: 2.74 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10700305859774795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10700305859774795 | validation: 0.28214271082027004]
	TIME [epoch: 2.73 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537753399923989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08537753399923989 | validation: 0.22959352859529167]
	TIME [epoch: 2.73 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0810188479541196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0810188479541196 | validation: 0.2605741283892277]
	TIME [epoch: 2.74 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07288453026777442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07288453026777442 | validation: 0.21648366678655875]
	TIME [epoch: 2.74 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07652262835564312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07652262835564312 | validation: 0.35619511310566426]
	TIME [epoch: 2.73 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09582279133084315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09582279133084315 | validation: 0.24053979337543502]
	TIME [epoch: 2.72 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10870682613200167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10870682613200167 | validation: 0.2735711589685218]
	TIME [epoch: 2.72 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10378939606742751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10378939606742751 | validation: 0.21323631769567306]
	TIME [epoch: 2.72 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08177229217235969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08177229217235969 | validation: 0.26975988129612316]
	TIME [epoch: 2.72 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08598237086038603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08598237086038603 | validation: 0.3014609413468695]
	TIME [epoch: 2.73 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17677236171451113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17677236171451113 | validation: 0.3821259795913167]
	TIME [epoch: 2.73 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14015672668576992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14015672668576992 | validation: 0.36860958803999094]
	TIME [epoch: 2.72 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176585012558127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176585012558127 | validation: 0.2771642924748646]
	TIME [epoch: 2.72 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12977967854630237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12977967854630237 | validation: 0.3675796717017898]
	TIME [epoch: 2.73 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11349308874205488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11349308874205488 | validation: 0.24593651667989633]
	TIME [epoch: 2.73 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0796603337446348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0796603337446348 | validation: 0.23990884004194415]
	TIME [epoch: 2.73 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07530965808732135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07530965808732135 | validation: 0.19897444309072496]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07636853053926376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07636853053926376 | validation: 0.2894208729028456]
	TIME [epoch: 2.72 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07745099870585831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07745099870585831 | validation: 0.21194311126252796]
	TIME [epoch: 2.72 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10191460317750578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10191460317750578 | validation: 0.29776182894643627]
	TIME [epoch: 2.72 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11161045768075101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11161045768075101 | validation: 0.22649105418615367]
	TIME [epoch: 2.73 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09600274985783809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09600274985783809 | validation: 0.2751961594719896]
	TIME [epoch: 2.73 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08114943395600602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08114943395600602 | validation: 0.22427740336150426]
	TIME [epoch: 2.73 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10632920883965975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10632920883965975 | validation: 0.24710206276394625]
	TIME [epoch: 2.73 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10236532178113546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10236532178113546 | validation: 0.28073435748494996]
	TIME [epoch: 2.72 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787799802003777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787799802003777 | validation: 0.21299025134098415]
	TIME [epoch: 2.72 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0637542584251488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0637542584251488 | validation: 0.2094502585441318]
	TIME [epoch: 2.71 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0655590734469653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0655590734469653 | validation: 0.2223030046545445]
	TIME [epoch: 2.71 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07827821643249794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07827821643249794 | validation: 0.24384365783701992]
	TIME [epoch: 2.71 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11161484710026955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11161484710026955 | validation: 0.26441050980684394]
	TIME [epoch: 2.72 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09658816599728802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09658816599728802 | validation: 0.21507600021566634]
	TIME [epoch: 2.71 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08120037877110675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08120037877110675 | validation: 0.20756109635983871]
	TIME [epoch: 2.72 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09476913928283529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09476913928283529 | validation: 0.5419551850626741]
	TIME [epoch: 2.72 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731975758122243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1731975758122243 | validation: 0.2862000862490525]
	TIME [epoch: 2.72 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1607082894613255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1607082894613255 | validation: 0.23638198961096324]
	TIME [epoch: 2.72 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06744251119050686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06744251119050686 | validation: 0.2234467560458014]
	TIME [epoch: 2.72 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09993296247022389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09993296247022389 | validation: 0.5592609135143535]
	TIME [epoch: 2.72 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19594197388647785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19594197388647785 | validation: 0.40740887070136655]
	TIME [epoch: 2.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18055763804239564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18055763804239564 | validation: 0.2376508678810815]
	TIME [epoch: 2.71 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08725260000802404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08725260000802404 | validation: 0.2108910959746021]
	TIME [epoch: 2.71 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07852418789703008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07852418789703008 | validation: 0.19773581070244522]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_364.pth
	Model improved!!!
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05818210590818211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05818210590818211 | validation: 0.22896628343399034]
	TIME [epoch: 2.73 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059371808162455644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059371808162455644 | validation: 0.18278770253483376]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065361181891971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06065361181891971 | validation: 0.18361811794467206]
	TIME [epoch: 2.72 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05933424721515132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05933424721515132 | validation: 0.18162185084940374]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_368.pth
	Model improved!!!
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06621251239458509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06621251239458509 | validation: 0.24924532970338054]
	TIME [epoch: 2.73 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08377199098806777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08377199098806777 | validation: 0.21705506779328074]
	TIME [epoch: 2.73 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13590101914194092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13590101914194092 | validation: 0.46078207206672617]
	TIME [epoch: 2.73 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1480503636817592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1480503636817592 | validation: 0.2035859216374406]
	TIME [epoch: 2.73 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09665057873015226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09665057873015226 | validation: 0.20055644344059048]
	TIME [epoch: 2.73 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06102391958110852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06102391958110852 | validation: 0.2177981962157511]
	TIME [epoch: 2.73 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07030055654481357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07030055654481357 | validation: 0.2238835087169521]
	TIME [epoch: 2.73 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09121501730427077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09121501730427077 | validation: 0.688543319806761]
	TIME [epoch: 2.73 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24479316550853022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24479316550853022 | validation: 0.2576956629679771]
	TIME [epoch: 2.73 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11697115720796494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11697115720796494 | validation: 0.21158381096738796]
	TIME [epoch: 2.72 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08523863232819041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08523863232819041 | validation: 0.25359539262346337]
	TIME [epoch: 2.72 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265871438948311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265871438948311 | validation: 0.23364685525879392]
	TIME [epoch: 2.73 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823387160753003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823387160753003 | validation: 0.21443377294358967]
	TIME [epoch: 2.73 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335331789612002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06335331789612002 | validation: 0.18445711282179159]
	TIME [epoch: 2.73 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07878619072188767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07878619072188767 | validation: 0.2517611254626816]
	TIME [epoch: 2.72 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11332400290800947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11332400290800947 | validation: 0.2406053731394167]
	TIME [epoch: 2.73 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14820555205121522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14820555205121522 | validation: 0.27057373976180205]
	TIME [epoch: 2.72 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10401503294282678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10401503294282678 | validation: 0.22894007318134746]
	TIME [epoch: 2.73 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959793439976544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0959793439976544 | validation: 0.21194678872551675]
	TIME [epoch: 2.73 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060861807843969676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060861807843969676 | validation: 0.2570726058034552]
	TIME [epoch: 2.73 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095536639613091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07095536639613091 | validation: 0.18367502080067433]
	TIME [epoch: 2.72 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06272936732090142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06272936732090142 | validation: 0.177037258937656]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06518561670170116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06518561670170116 | validation: 0.1676535043316736]
	TIME [epoch: 2.72 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420951064293705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06420951064293705 | validation: 0.21966192910745957]
	TIME [epoch: 2.73 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08129063879098133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08129063879098133 | validation: 0.1700728415389733]
	TIME [epoch: 2.73 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0662297007595854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0662297007595854 | validation: 0.22303853154461964]
	TIME [epoch: 2.73 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09344116631055265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09344116631055265 | validation: 0.2557735460225639]
	TIME [epoch: 2.73 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17176450202209803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17176450202209803 | validation: 0.3842268293481653]
	TIME [epoch: 2.73 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1289833428441911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1289833428441911 | validation: 0.31512717963672504]
	TIME [epoch: 2.73 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11461720955763782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11461720955763782 | validation: 0.21107119934744956]
	TIME [epoch: 2.73 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07257710460848171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07257710460848171 | validation: 0.2199709818483198]
	TIME [epoch: 2.73 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05838339827769511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05838339827769511 | validation: 0.17463429282246837]
	TIME [epoch: 2.73 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410556182733124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05410556182733124 | validation: 0.17477424596383945]
	TIME [epoch: 2.74 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335845059806008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06335845059806008 | validation: 0.17669293688149434]
	TIME [epoch: 2.74 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08082128848035451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08082128848035451 | validation: 0.2466448645002529]
	TIME [epoch: 2.74 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12200389652208465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12200389652208465 | validation: 0.17841776029020534]
	TIME [epoch: 2.73 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0958909012691128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0958909012691128 | validation: 0.20727661080598955]
	TIME [epoch: 2.73 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0839559254920263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0839559254920263 | validation: 0.20191480386345018]
	TIME [epoch: 2.73 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898749649760606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0898749649760606 | validation: 0.26546033790684176]
	TIME [epoch: 2.73 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0863255491192022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0863255491192022 | validation: 0.19361409970350346]
	TIME [epoch: 2.73 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09496818779818123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09496818779818123 | validation: 0.1976640892888041]
	TIME [epoch: 2.73 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07987752167645598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07987752167645598 | validation: 0.2835221280900901]
	TIME [epoch: 2.73 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0783918468438234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0783918468438234 | validation: 0.1768076668012071]
	TIME [epoch: 2.73 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05817872380117743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05817872380117743 | validation: 0.15102716743410793]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_412.pth
	Model improved!!!
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059362311411874126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059362311411874126 | validation: 0.17442469280352563]
	TIME [epoch: 2.73 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05792730739606392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05792730739606392 | validation: 0.18356050100990837]
	TIME [epoch: 2.73 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08487360366723838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08487360366723838 | validation: 0.33761271969996454]
	TIME [epoch: 2.73 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1338189300739034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1338189300739034 | validation: 0.22411561979522215]
	TIME [epoch: 2.73 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15087367794662332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15087367794662332 | validation: 0.24452105139018643]
	TIME [epoch: 2.73 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09901903762765532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09901903762765532 | validation: 0.21397167420404284]
	TIME [epoch: 2.73 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06922081746906406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06922081746906406 | validation: 0.19113017315006578]
	TIME [epoch: 2.73 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058977506678036626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058977506678036626 | validation: 0.18329636902035226]
	TIME [epoch: 2.73 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048453511913703624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048453511913703624 | validation: 0.15413101492540116]
	TIME [epoch: 2.73 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05426394306756204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05426394306756204 | validation: 0.1581156935739495]
	TIME [epoch: 2.73 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05661188958671227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05661188958671227 | validation: 0.18199842651933118]
	TIME [epoch: 2.73 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686443715106823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0686443715106823 | validation: 0.18728416851322643]
	TIME [epoch: 2.74 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09469451060427958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09469451060427958 | validation: 0.21771854864346635]
	TIME [epoch: 2.73 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910159868312941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10910159868312941 | validation: 0.182181045861497]
	TIME [epoch: 2.73 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07551985387412324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07551985387412324 | validation: 0.17667854371754918]
	TIME [epoch: 2.73 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05654620105900943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05654620105900943 | validation: 0.20254531599309067]
	TIME [epoch: 2.73 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10764809020754484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10764809020754484 | validation: 0.2740758969889642]
	TIME [epoch: 2.74 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1778621925546558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1778621925546558 | validation: 0.43287345746940753]
	TIME [epoch: 2.74 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12346358888876637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12346358888876637 | validation: 0.35690822009372936]
	TIME [epoch: 2.74 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13617571584006366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13617571584006366 | validation: 0.30933723156948134]
	TIME [epoch: 2.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11552333459852525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11552333459852525 | validation: 0.20679219523144488]
	TIME [epoch: 2.74 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325826146145678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325826146145678 | validation: 0.19879294913828122]
	TIME [epoch: 2.73 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0560473924794305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0560473924794305 | validation: 0.20782357643122762]
	TIME [epoch: 2.74 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06813226481048958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06813226481048958 | validation: 0.2100350481011377]
	TIME [epoch: 2.73 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08546599511567565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08546599511567565 | validation: 0.1991172394854455]
	TIME [epoch: 2.73 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769304991887741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0769304991887741 | validation: 0.21703125815615218]
	TIME [epoch: 2.73 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0808153057585977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0808153057585977 | validation: 0.20765235687121844]
	TIME [epoch: 2.73 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10105825422658538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10105825422658538 | validation: 0.2651316518476875]
	TIME [epoch: 2.73 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10293601337332454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10293601337332454 | validation: 0.18628314439620375]
	TIME [epoch: 2.74 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07204383081515374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07204383081515374 | validation: 0.1814814162185764]
	TIME [epoch: 2.74 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05441071087448676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05441071087448676 | validation: 0.18059542729566558]
	TIME [epoch: 2.74 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475439448987227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0475439448987227 | validation: 0.18449290061284462]
	TIME [epoch: 2.74 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07639937165125651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07639937165125651 | validation: 0.24527865507776936]
	TIME [epoch: 2.73 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06001804471710793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06001804471710793 | validation: 0.16866606412306448]
	TIME [epoch: 2.74 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0670745494236562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0670745494236562 | validation: 0.1956380797545149]
	TIME [epoch: 2.74 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267412008775033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07267412008775033 | validation: 0.18638424255855549]
	TIME [epoch: 2.73 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0769469567073835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0769469567073835 | validation: 0.20053785357670684]
	TIME [epoch: 2.73 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08950307834547967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08950307834547967 | validation: 0.16724446302400364]
	TIME [epoch: 2.73 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07069890749146959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07069890749146959 | validation: 0.16559004031895844]
	TIME [epoch: 2.73 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06327339437186205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06327339437186205 | validation: 0.21120194148134233]
	TIME [epoch: 2.74 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08342712959135487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08342712959135487 | validation: 0.20321317898259858]
	TIME [epoch: 2.74 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13308048821606291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13308048821606291 | validation: 0.25879060475540955]
	TIME [epoch: 2.73 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12035611453887557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12035611453887557 | validation: 0.18842976245094417]
	TIME [epoch: 2.73 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07837019965001124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07837019965001124 | validation: 0.28830248720731605]
	TIME [epoch: 2.73 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719376824764275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05719376824764275 | validation: 0.17007452529212028]
	TIME [epoch: 2.73 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05399051703884361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05399051703884361 | validation: 0.17512803523891785]
	TIME [epoch: 2.73 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06855615262078785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06855615262078785 | validation: 0.18500453456945706]
	TIME [epoch: 2.73 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07388242996884783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07388242996884783 | validation: 0.24503213296363757]
	TIME [epoch: 2.73 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0624144532228147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0624144532228147 | validation: 0.1548827643127318]
	TIME [epoch: 2.74 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013980505302088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05013980505302088 | validation: 0.1565340582523581]
	TIME [epoch: 2.73 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052727972001523835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052727972001523835 | validation: 0.1667132318978459]
	TIME [epoch: 2.73 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512231249255494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512231249255494 | validation: 0.17280580629701844]
	TIME [epoch: 2.73 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10352632346802804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10352632346802804 | validation: 0.2209863790072374]
	TIME [epoch: 2.73 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041779868097738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1041779868097738 | validation: 0.24226682465188878]
	TIME [epoch: 2.73 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14659270250668086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14659270250668086 | validation: 0.3204429871977845]
	TIME [epoch: 2.73 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09276430031884784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09276430031884784 | validation: 0.18882577835337788]
	TIME [epoch: 2.73 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07401053639736092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07401053639736092 | validation: 0.21328954123807176]
	TIME [epoch: 2.73 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08318309319851817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08318309319851817 | validation: 0.1834449017805117]
	TIME [epoch: 2.73 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043908873756147215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043908873756147215 | validation: 0.17615311879403503]
	TIME [epoch: 2.73 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05626850248215838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05626850248215838 | validation: 0.17792368451883434]
	TIME [epoch: 2.73 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051143657652435136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051143657652435136 | validation: 0.16683835135950434]
	TIME [epoch: 2.73 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05132359036699133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05132359036699133 | validation: 0.17541755260869235]
	TIME [epoch: 2.73 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06000103387339539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06000103387339539 | validation: 0.17006361102058354]
	TIME [epoch: 2.73 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0898624802417164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0898624802417164 | validation: 0.2799569360000584]
	TIME [epoch: 2.73 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13183002382035391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13183002382035391 | validation: 0.19592782645931633]
	TIME [epoch: 2.73 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1148619204088834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1148619204088834 | validation: 0.20045606782295528]
	TIME [epoch: 2.73 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760433433103217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07760433433103217 | validation: 0.19349207026856974]
	TIME [epoch: 2.73 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062244592184353124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062244592184353124 | validation: 0.17877503143001983]
	TIME [epoch: 2.73 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060116055580562884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060116055580562884 | validation: 0.1668127058034371]
	TIME [epoch: 2.72 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05729109492806884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05729109492806884 | validation: 0.15874070093822848]
	TIME [epoch: 2.73 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06935879617798278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06935879617798278 | validation: 0.18766242615706613]
	TIME [epoch: 2.73 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522767248311109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07522767248311109 | validation: 0.16654127786469056]
	TIME [epoch: 2.73 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054651417882345066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054651417882345066 | validation: 0.15760872683832206]
	TIME [epoch: 2.73 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04665915004177589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04665915004177589 | validation: 0.1414358482801806]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04589298221968125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04589298221968125 | validation: 0.18625403371870727]
	TIME [epoch: 2.73 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473224908059616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05473224908059616 | validation: 0.2350187093475561]
	TIME [epoch: 2.73 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14465868244288746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14465868244288746 | validation: 0.3566868768646234]
	TIME [epoch: 2.73 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1195535481030558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1195535481030558 | validation: 0.2119174100858744]
	TIME [epoch: 2.73 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10305791684985949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10305791684985949 | validation: 0.2142860583514807]
	TIME [epoch: 2.73 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0984680111852085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0984680111852085 | validation: 0.2823695637512577]
	TIME [epoch: 2.73 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06577142778931389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06577142778931389 | validation: 0.2339379860392791]
	TIME [epoch: 2.73 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08218493873005858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08218493873005858 | validation: 0.18011534326920878]
	TIME [epoch: 2.73 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07208844833056684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07208844833056684 | validation: 0.1919941677364683]
	TIME [epoch: 2.73 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565710276508379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07565710276508379 | validation: 0.17703678649245363]
	TIME [epoch: 2.73 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08699225811845311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08699225811845311 | validation: 0.17974426928446963]
	TIME [epoch: 2.73 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07488495144926521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07488495144926521 | validation: 0.14540022867922936]
	TIME [epoch: 2.73 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723800189636565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0723800189636565 | validation: 0.1842975696686219]
	TIME [epoch: 2.73 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07591697009121025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07591697009121025 | validation: 0.159838915759]
	TIME [epoch: 2.73 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08347680350369509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08347680350369509 | validation: 0.1763857625002754]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07748396516168271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07748396516168271 | validation: 0.15040565740123488]
	TIME [epoch: 5.83 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06053230454895887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06053230454895887 | validation: 0.1435960135148072]
	TIME [epoch: 5.82 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05135493190498568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05135493190498568 | validation: 0.14102158674331028]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04060474820544144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04060474820544144 | validation: 0.14386802556792216]
	TIME [epoch: 5.82 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04605031413154551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04605031413154551 | validation: 0.13959891842231006]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_506.pth
	Model improved!!!
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03809969070647728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03809969070647728 | validation: 0.12843484496859575]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_507.pth
	Model improved!!!
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902695143086069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03902695143086069 | validation: 0.15002963143074097]
	TIME [epoch: 5.82 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04902191439381204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04902191439381204 | validation: 0.18098279243512397]
	TIME [epoch: 5.83 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09914814227409988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09914814227409988 | validation: 0.3658309150981469]
	TIME [epoch: 5.82 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15369318666558573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15369318666558573 | validation: 0.2050511798989844]
	TIME [epoch: 5.82 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633701461295189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10633701461295189 | validation: 0.15914998089981514]
	TIME [epoch: 5.82 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06030963211793564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06030963211793564 | validation: 0.17573533748588466]
	TIME [epoch: 5.83 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06204890460136859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06204890460136859 | validation: 0.15749669890198376]
	TIME [epoch: 5.82 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06385451938555128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06385451938555128 | validation: 0.1702372933270759]
	TIME [epoch: 5.82 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05816942023694889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05816942023694889 | validation: 0.16786759718729038]
	TIME [epoch: 5.82 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07018927222649075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07018927222649075 | validation: 0.1688206866536779]
	TIME [epoch: 5.83 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05520338261619175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05520338261619175 | validation: 0.18402433352136477]
	TIME [epoch: 5.82 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0666042760183372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0666042760183372 | validation: 0.17743524903122238]
	TIME [epoch: 5.83 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07432151410858237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07432151410858237 | validation: 0.182825009048422]
	TIME [epoch: 5.82 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08960694338744121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08960694338744121 | validation: 0.24439392275701577]
	TIME [epoch: 5.82 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14446393555539896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14446393555539896 | validation: 0.18187280309626241]
	TIME [epoch: 5.82 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09841943426559334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09841943426559334 | validation: 0.20290880625513916]
	TIME [epoch: 5.83 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05227245173366878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05227245173366878 | validation: 0.1518434114724676]
	TIME [epoch: 5.82 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042047289990233126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042047289990233126 | validation: 0.1403440831704068]
	TIME [epoch: 5.83 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042165669521735706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042165669521735706 | validation: 0.13509742147158363]
	TIME [epoch: 5.82 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041749635432496233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041749635432496233 | validation: 0.1374154192981842]
	TIME [epoch: 5.84 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04303295167962859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04303295167962859 | validation: 0.14112163777807082]
	TIME [epoch: 5.82 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424041956899655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04424041956899655 | validation: 0.14913138810175333]
	TIME [epoch: 5.83 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06495985690503189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06495985690503189 | validation: 0.17161994764893446]
	TIME [epoch: 5.82 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09428683597358237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09428683597358237 | validation: 0.19781271263350164]
	TIME [epoch: 5.83 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08375933122805021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08375933122805021 | validation: 0.18326507481024534]
	TIME [epoch: 5.81 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12126740418559312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12126740418559312 | validation: 0.2667458706919287]
	TIME [epoch: 5.82 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09458827278894984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09458827278894984 | validation: 0.13802387788432124]
	TIME [epoch: 5.82 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625309876311636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625309876311636 | validation: 0.14288493046356707]
	TIME [epoch: 5.82 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05340167292595841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05340167292595841 | validation: 0.16654359750927092]
	TIME [epoch: 5.81 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565456448538047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565456448538047 | validation: 0.15388470327314727]
	TIME [epoch: 5.83 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04879484195032191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04879484195032191 | validation: 0.15528378814652283]
	TIME [epoch: 5.81 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04520068614110032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04520068614110032 | validation: 0.1498517021942934]
	TIME [epoch: 5.83 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520626850773202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0520626850773202 | validation: 0.15975820099335714]
	TIME [epoch: 5.81 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06846323804021989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06846323804021989 | validation: 0.17949839883277338]
	TIME [epoch: 5.82 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07051590961914961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07051590961914961 | validation: 0.15726225210043504]
	TIME [epoch: 5.81 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06408617415286973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06408617415286973 | validation: 0.17587269579329382]
	TIME [epoch: 5.83 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061618493651468875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061618493651468875 | validation: 0.17617116084045856]
	TIME [epoch: 5.82 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365570414404704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10365570414404704 | validation: 0.2519361126373738]
	TIME [epoch: 5.82 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16313268493443975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16313268493443975 | validation: 0.15076966206166822]
	TIME [epoch: 5.82 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800549814012563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06800549814012563 | validation: 0.1648336237847855]
	TIME [epoch: 5.82 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047521669694524864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047521669694524864 | validation: 0.16518096244461988]
	TIME [epoch: 5.82 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654736374937902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0654736374937902 | validation: 0.1858246124598263]
	TIME [epoch: 5.82 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547562361190344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04547562361190344 | validation: 0.15483141689775445]
	TIME [epoch: 5.82 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04799731045984882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04799731045984882 | validation: 0.16450242882162996]
	TIME [epoch: 5.83 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04799411475863725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04799411475863725 | validation: 0.1400721072208526]
	TIME [epoch: 5.82 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04279604957167295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04279604957167295 | validation: 0.13756926977588307]
	TIME [epoch: 5.82 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377623580074633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377623580074633 | validation: 0.14409802622468684]
	TIME [epoch: 5.82 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567659998203336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0567659998203336 | validation: 0.17132714978310654]
	TIME [epoch: 5.82 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0849698442067141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0849698442067141 | validation: 0.16830575957513177]
	TIME [epoch: 5.81 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07934184180565743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07934184180565743 | validation: 0.135644254052684]
	TIME [epoch: 5.82 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998157266824368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04998157266824368 | validation: 0.15347880012216134]
	TIME [epoch: 5.82 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04645351344463911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04645351344463911 | validation: 0.21963353658551155]
	TIME [epoch: 5.83 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06418558567947305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06418558567947305 | validation: 0.22757219590185862]
	TIME [epoch: 5.82 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16261663186880526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16261663186880526 | validation: 0.3019673677905592]
	TIME [epoch: 5.82 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096199811235966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11096199811235966 | validation: 0.23422883408374898]
	TIME [epoch: 5.82 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09715671928523348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09715671928523348 | validation: 0.2023451562373283]
	TIME [epoch: 5.83 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08055082706875206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08055082706875206 | validation: 0.16000861685557355]
	TIME [epoch: 5.82 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04374970890007855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04374970890007855 | validation: 0.15491606388964985]
	TIME [epoch: 5.82 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05188654070791424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05188654070791424 | validation: 0.14735795483469544]
	TIME [epoch: 5.82 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04518840084474311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04518840084474311 | validation: 0.1511239812219402]
	TIME [epoch: 5.82 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04331839112341388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04331839112341388 | validation: 0.16051938979444969]
	TIME [epoch: 5.81 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042045907691902855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042045907691902855 | validation: 0.13725667438471706]
	TIME [epoch: 5.83 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04359136009253108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04359136009253108 | validation: 0.1304646038356024]
	TIME [epoch: 5.82 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04122521278250577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04122521278250577 | validation: 0.13887580120988083]
	TIME [epoch: 5.83 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930356277643605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04930356277643605 | validation: 0.1518198299759782]
	TIME [epoch: 5.82 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0618020055656987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0618020055656987 | validation: 0.19296233306338847]
	TIME [epoch: 5.82 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08434054515091145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08434054515091145 | validation: 0.26559152056293067]
	TIME [epoch: 5.82 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10398954562552971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10398954562552971 | validation: 0.18719562905814363]
	TIME [epoch: 5.81 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095506965825147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1095506965825147 | validation: 0.19107917744242742]
	TIME [epoch: 5.82 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08835818264083813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08835818264083813 | validation: 0.1672864602655593]
	TIME [epoch: 5.82 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07445421466452753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07445421466452753 | validation: 0.1555757986237841]
	TIME [epoch: 5.82 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06210864470402609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06210864470402609 | validation: 0.18570425653041756]
	TIME [epoch: 5.83 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039496247828320466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039496247828320466 | validation: 0.16841754349960214]
	TIME [epoch: 5.82 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047952298444628595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047952298444628595 | validation: 0.14182110541432347]
	TIME [epoch: 5.82 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03960223255756327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03960223255756327 | validation: 0.1316551140825419]
	TIME [epoch: 5.82 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04244593097901127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04244593097901127 | validation: 0.1387080193296538]
	TIME [epoch: 5.82 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05094329561662754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05094329561662754 | validation: 0.14778850698029658]
	TIME [epoch: 5.82 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07166826957736057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07166826957736057 | validation: 0.15531320569414195]
	TIME [epoch: 5.83 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09777390746890334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09777390746890334 | validation: 0.1592500433346089]
	TIME [epoch: 5.82 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814097629994492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06814097629994492 | validation: 0.1413349883539888]
	TIME [epoch: 5.82 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04854817950046509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04854817950046509 | validation: 0.13214103897317334]
	TIME [epoch: 5.82 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047702474142710935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047702474142710935 | validation: 0.17518796186502583]
	TIME [epoch: 5.82 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06796883158148875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06796883158148875 | validation: 0.17864582801514994]
	TIME [epoch: 5.82 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009348087148825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08009348087148825 | validation: 0.1410674540883253]
	TIME [epoch: 5.82 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054385834352456905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054385834352456905 | validation: 0.14328740856895616]
	TIME [epoch: 5.82 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04549023751399083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04549023751399083 | validation: 0.16637001762714043]
	TIME [epoch: 5.82 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047664613525364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047664613525364 | validation: 0.13782478047004754]
	TIME [epoch: 5.83 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041947876652297184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041947876652297184 | validation: 0.14401483674681442]
	TIME [epoch: 5.82 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051371991748174736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051371991748174736 | validation: 0.14282066040004004]
	TIME [epoch: 5.83 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761072844650551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0761072844650551 | validation: 0.14782862827770224]
	TIME [epoch: 5.82 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252713724557998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252713724557998 | validation: 0.1508626738551628]
	TIME [epoch: 5.82 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813533724465652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0813533724465652 | validation: 0.17387437513031914]
	TIME [epoch: 5.82 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06831056406486097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06831056406486097 | validation: 0.14270794616166813]
	TIME [epoch: 5.82 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0558617881651495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0558617881651495 | validation: 0.14365030320038788]
	TIME [epoch: 5.82 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559901045168621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04559901045168621 | validation: 0.11890973824994835]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03644312846117106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03644312846117106 | validation: 0.1162226827248726]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_603.pth
	Model improved!!!
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498465687968899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03498465687968899 | validation: 0.10600478580413104]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_604.pth
	Model improved!!!
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03954300813766128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03954300813766128 | validation: 0.15750844935940675]
	TIME [epoch: 5.82 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540578785020561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05540578785020561 | validation: 0.219785212528208]
	TIME [epoch: 5.83 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06646453328471622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06646453328471622 | validation: 0.15391515508117395]
	TIME [epoch: 5.82 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07524111515119039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07524111515119039 | validation: 0.16928546542749723]
	TIME [epoch: 5.83 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060943765584422174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060943765584422174 | validation: 0.14104410256885846]
	TIME [epoch: 5.82 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990173837494239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06990173837494239 | validation: 0.16762087689794836]
	TIME [epoch: 5.83 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07557182720224702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07557182720224702 | validation: 0.1589535382591521]
	TIME [epoch: 5.82 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06925295357986432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06925295357986432 | validation: 0.14823601207586187]
	TIME [epoch: 5.83 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055402389387018615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055402389387018615 | validation: 0.12230824416871017]
	TIME [epoch: 5.83 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603160205708843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03603160205708843 | validation: 0.11924985326274798]
	TIME [epoch: 5.82 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030958743076320384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030958743076320384 | validation: 0.10642153505697327]
	TIME [epoch: 5.82 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386494248209668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03386494248209668 | validation: 0.11509811034896772]
	TIME [epoch: 5.84 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031084925908085658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031084925908085658 | validation: 0.1036408340764464]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051284478648968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051284478648968 | validation: 0.1569234613377343]
	TIME [epoch: 5.85 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08214180492778636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08214180492778636 | validation: 0.20116608686556248]
	TIME [epoch: 5.85 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08503453828677689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08503453828677689 | validation: 0.17563407582398816]
	TIME [epoch: 5.85 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08427333571798894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08427333571798894 | validation: 0.1801751455574878]
	TIME [epoch: 5.84 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13346778786686264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13346778786686264 | validation: 0.18533740344690977]
	TIME [epoch: 5.85 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09140602831547387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09140602831547387 | validation: 0.1408375470105778]
	TIME [epoch: 5.84 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05107497941229875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05107497941229875 | validation: 0.15067268518845564]
	TIME [epoch: 5.84 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04508818212732486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04508818212732486 | validation: 0.14243679867657494]
	TIME [epoch: 5.86 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048057391518953385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048057391518953385 | validation: 0.13739179976735402]
	TIME [epoch: 5.85 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04441042626930895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04441042626930895 | validation: 0.12708123327959328]
	TIME [epoch: 5.85 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038109394693534415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038109394693534415 | validation: 0.1416381265212999]
	TIME [epoch: 5.83 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057481750563678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04057481750563678 | validation: 0.14648684949355184]
	TIME [epoch: 5.82 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0556853508861091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0556853508861091 | validation: 0.21645571180547774]
	TIME [epoch: 5.83 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07823277399780708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07823277399780708 | validation: 0.1791740714140932]
	TIME [epoch: 5.82 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09544015944635251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09544015944635251 | validation: 0.22223778226389862]
	TIME [epoch: 5.82 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219978552971993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05219978552971993 | validation: 0.1934820797867559]
	TIME [epoch: 5.82 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04799121078443225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04799121078443225 | validation: 0.14435862676445532]
	TIME [epoch: 5.82 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041564246030196726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041564246030196726 | validation: 0.1264289819087229]
	TIME [epoch: 5.82 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662388622297872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03662388622297872 | validation: 0.12000765624456407]
	TIME [epoch: 5.82 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359834425953842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03359834425953842 | validation: 0.11177757239656949]
	TIME [epoch: 5.81 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342399114701329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0342399114701329 | validation: 0.13853132246012384]
	TIME [epoch: 5.83 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04379126467376961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04379126467376961 | validation: 0.15033537019708623]
	TIME [epoch: 5.82 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947521646022799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947521646022799 | validation: 0.16913097065181804]
	TIME [epoch: 5.84 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10331438756799266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10331438756799266 | validation: 0.13770095252316686]
	TIME [epoch: 5.83 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056830899530677825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056830899530677825 | validation: 0.15203989362430903]
	TIME [epoch: 5.83 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07230306133392148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07230306133392148 | validation: 0.23099008018177258]
	TIME [epoch: 5.82 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14735501802616519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14735501802616519 | validation: 0.16514788262834798]
	TIME [epoch: 5.82 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09213160010134328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09213160010134328 | validation: 0.133759819658478]
	TIME [epoch: 5.82 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04145274543144154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04145274543144154 | validation: 0.1401366025123103]
	TIME [epoch: 5.84 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039703677911635406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039703677911635406 | validation: 0.1420393600663691]
	TIME [epoch: 5.83 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03698591323934494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03698591323934494 | validation: 0.1340069689294839]
	TIME [epoch: 5.82 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02968845330702637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02968845330702637 | validation: 0.12944830915454744]
	TIME [epoch: 5.82 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02665091898927314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02665091898927314 | validation: 0.11413867172499055]
	TIME [epoch: 5.82 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029941679636096476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029941679636096476 | validation: 0.12126702657960364]
	TIME [epoch: 5.83 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859720442102405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03859720442102405 | validation: 0.14552275334720108]
	TIME [epoch: 5.82 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06296161888934795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06296161888934795 | validation: 0.14937464640823303]
	TIME [epoch: 5.83 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07813065646222754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07813065646222754 | validation: 0.1412958938723247]
	TIME [epoch: 5.82 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07969952976266453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07969952976266453 | validation: 0.14955100778380193]
	TIME [epoch: 5.83 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06615055278690095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06615055278690095 | validation: 0.1475781375805231]
	TIME [epoch: 5.82 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0651166754987272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0651166754987272 | validation: 0.21741509392397265]
	TIME [epoch: 5.83 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06726893499612038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06726893499612038 | validation: 0.15310286511961568]
	TIME [epoch: 5.82 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047274141116124435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047274141116124435 | validation: 0.12452749254950024]
	TIME [epoch: 5.82 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038169765298817396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038169765298817396 | validation: 0.13225015698233036]
	TIME [epoch: 5.83 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039761530272652154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039761530272652154 | validation: 0.1695044367610831]
	TIME [epoch: 5.82 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08007979718353951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08007979718353951 | validation: 0.23271255777117866]
	TIME [epoch: 5.82 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058720677023167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05058720677023167 | validation: 0.15356535205595195]
	TIME [epoch: 5.82 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04442618986531293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04442618986531293 | validation: 0.13712355342626995]
	TIME [epoch: 5.83 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054272287409035926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054272287409035926 | validation: 0.16989514309446055]
	TIME [epoch: 5.82 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07791614645375586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07791614645375586 | validation: 0.15016517390394407]
	TIME [epoch: 5.83 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08364363290646924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08364363290646924 | validation: 0.21560918324048953]
	TIME [epoch: 5.83 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742202393281365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07742202393281365 | validation: 0.1422218953131277]
	TIME [epoch: 5.83 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0804374490434546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0804374490434546 | validation: 0.14945015869163408]
	TIME [epoch: 5.82 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04538215175226005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04538215175226005 | validation: 0.14571271014660173]
	TIME [epoch: 5.81 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03947302340154724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03947302340154724 | validation: 0.135593561255104]
	TIME [epoch: 5.82 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03384384207366792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03384384207366792 | validation: 0.13912193490745958]
	TIME [epoch: 5.82 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02895485696286743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02895485696286743 | validation: 0.14227294874791782]
	TIME [epoch: 5.82 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046355618838834885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046355618838834885 | validation: 0.12418866558580444]
	TIME [epoch: 5.82 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048352652096594353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048352652096594353 | validation: 0.16504494177166776]
	TIME [epoch: 5.82 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08176143840301688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08176143840301688 | validation: 0.1885901236031196]
	TIME [epoch: 5.82 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11163628679619265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11163628679619265 | validation: 0.16566348327016214]
	TIME [epoch: 5.83 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05616698298318271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05616698298318271 | validation: 0.10799105444490426]
	TIME [epoch: 5.81 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026548320695708682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026548320695708682 | validation: 0.1355722150092259]
	TIME [epoch: 5.82 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0536044983768697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0536044983768697 | validation: 0.1348306029527869]
	TIME [epoch: 5.81 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052748832371845654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052748832371845654 | validation: 0.14125820021888497]
	TIME [epoch: 5.82 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07132398268954282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07132398268954282 | validation: 0.13990966489625914]
	TIME [epoch: 5.82 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07120651888790629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07120651888790629 | validation: 0.10292161114037264]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03953091674885823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03953091674885823 | validation: 0.1001839301854147]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_684.pth
	Model improved!!!
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038958895504983125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038958895504983125 | validation: 0.11794734416489346]
	TIME [epoch: 5.83 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497712459432759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04497712459432759 | validation: 0.12067996266158856]
	TIME [epoch: 5.82 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061494260622928607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061494260622928607 | validation: 0.1613129760773147]
	TIME [epoch: 5.83 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190200543398964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07190200543398964 | validation: 0.12094944141402485]
	TIME [epoch: 5.82 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05678703469439733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05678703469439733 | validation: 0.1127069909403859]
	TIME [epoch: 5.83 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0391710482983656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0391710482983656 | validation: 0.11988726050125882]
	TIME [epoch: 5.83 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03254365440144083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03254365440144083 | validation: 0.15096844427368344]
	TIME [epoch: 5.82 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0465880852478385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0465880852478385 | validation: 0.11597005209383233]
	TIME [epoch: 5.83 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03799071299844783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03799071299844783 | validation: 0.10018869595624143]
	TIME [epoch: 5.83 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039212991991512944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039212991991512944 | validation: 0.09560422567934232]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04119650850965973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04119650850965973 | validation: 0.10240063206473829]
	TIME [epoch: 5.82 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0411246351405841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0411246351405841 | validation: 0.11340120900152129]
	TIME [epoch: 5.82 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040461620997793324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040461620997793324 | validation: 0.12031888620079828]
	TIME [epoch: 5.84 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843291021198892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03843291021198892 | validation: 0.11014139096910387]
	TIME [epoch: 5.83 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06661712289807058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06661712289807058 | validation: 0.18420515841199736]
	TIME [epoch: 5.83 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10414258692045106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10414258692045106 | validation: 0.14271252028556064]
	TIME [epoch: 5.83 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08297668990557522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08297668990557522 | validation: 0.12765558094596272]
	TIME [epoch: 5.85 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04302498884717481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04302498884717481 | validation: 0.12650956182169534]
	TIME [epoch: 5.85 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028718789739894685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028718789739894685 | validation: 0.11490246642584122]
	TIME [epoch: 5.84 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03592659546375158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03592659546375158 | validation: 0.13616624493143337]
	TIME [epoch: 5.84 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0428838543911231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0428838543911231 | validation: 0.13723667048259294]
	TIME [epoch: 5.85 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07162605505146392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07162605505146392 | validation: 0.11678443635988661]
	TIME [epoch: 5.84 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06447104700235269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06447104700235269 | validation: 0.10835588842452955]
	TIME [epoch: 5.85 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04391025648852484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04391025648852484 | validation: 0.0721805189127521]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029296832427108104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029296832427108104 | validation: 0.14276133268318386]
	TIME [epoch: 5.83 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04780787530097197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04780787530097197 | validation: 0.13059422181499944]
	TIME [epoch: 5.82 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06981740381567035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06981740381567035 | validation: 0.14755505777294362]
	TIME [epoch: 5.82 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07993417269777188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07993417269777188 | validation: 0.13894449834196082]
	TIME [epoch: 5.83 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06518181489510975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06518181489510975 | validation: 0.10554450687864787]
	TIME [epoch: 5.83 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029419502866348264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029419502866348264 | validation: 0.11226157197093914]
	TIME [epoch: 5.82 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025552843293195817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025552843293195817 | validation: 0.09129738507881874]
	TIME [epoch: 5.82 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027033146605964325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027033146605964325 | validation: 0.10827068676831456]
	TIME [epoch: 5.83 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025268229603622138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025268229603622138 | validation: 0.09369827797626704]
	TIME [epoch: 5.83 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027345178963439385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027345178963439385 | validation: 0.11444465146269156]
	TIME [epoch: 5.84 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038575238899227725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038575238899227725 | validation: 0.12272643239270742]
	TIME [epoch: 5.82 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05485292472680362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05485292472680362 | validation: 0.11320752202861216]
	TIME [epoch: 5.82 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05232085696918658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05232085696918658 | validation: 0.1495210478946814]
	TIME [epoch: 5.82 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056063108489762564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056063108489762564 | validation: 0.14477500421848888]
	TIME [epoch: 5.83 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022466724162614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1022466724162614 | validation: 0.17201278548090906]
	TIME [epoch: 5.82 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09570920585225037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09570920585225037 | validation: 0.16617987763937464]
	TIME [epoch: 5.82 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09984843213131812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09984843213131812 | validation: 0.2597639928276933]
	TIME [epoch: 5.82 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07212116893602075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07212116893602075 | validation: 0.12966770901231586]
	TIME [epoch: 5.82 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03167513599190634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03167513599190634 | validation: 0.11524559286267971]
	TIME [epoch: 5.83 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03613063420535531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03613063420535531 | validation: 0.11723503873703148]
	TIME [epoch: 5.82 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02939466223362361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02939466223362361 | validation: 0.09042047738113589]
	TIME [epoch: 5.82 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024620687282083616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024620687282083616 | validation: 0.10185438706269326]
	TIME [epoch: 5.82 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02004812325005001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02004812325005001 | validation: 0.09468017172720347]
	TIME [epoch: 5.82 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020658126459812246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020658126459812246 | validation: 0.07867586493149888]
	TIME [epoch: 5.82 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019626379557449013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019626379557449013 | validation: 0.08122611434588972]
	TIME [epoch: 5.82 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019892770459032198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019892770459032198 | validation: 0.0936523619768617]
	TIME [epoch: 5.82 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024350803835855625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024350803835855625 | validation: 0.10484047513859358]
	TIME [epoch: 5.84 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05408156145278898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05408156145278898 | validation: 0.22567428704936754]
	TIME [epoch: 5.84 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14501994053492992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14501994053492992 | validation: 0.17207421322783847]
	TIME [epoch: 5.83 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12440407716298513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12440407716298513 | validation: 0.11916450110551975]
	TIME [epoch: 5.83 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03226334676340471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03226334676340471 | validation: 0.11830168496032334]
	TIME [epoch: 5.82 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03130867525978415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03130867525978415 | validation: 0.10870736982782747]
	TIME [epoch: 5.83 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04067153390173352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04067153390173352 | validation: 0.1008756943342538]
	TIME [epoch: 5.82 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039044814587499824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039044814587499824 | validation: 0.12296364898191302]
	TIME [epoch: 5.82 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05573726699931632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05573726699931632 | validation: 0.1250448055214188]
	TIME [epoch: 5.82 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05859442856700205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05859442856700205 | validation: 0.10946158544405203]
	TIME [epoch: 5.83 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04048028573226395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04048028573226395 | validation: 0.08380025558930299]
	TIME [epoch: 5.82 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024585949432025203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024585949432025203 | validation: 0.08664840354428516]
	TIME [epoch: 5.83 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0203109394511566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0203109394511566 | validation: 0.08461290051074943]
	TIME [epoch: 5.82 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02254863418410546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02254863418410546 | validation: 0.07901460478386373]
	TIME [epoch: 5.84 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023177161633818114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023177161633818114 | validation: 0.09384388921773795]
	TIME [epoch: 5.83 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03004721700984474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03004721700984474 | validation: 0.10284967627106444]
	TIME [epoch: 5.83 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034995126215532156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034995126215532156 | validation: 0.12757377370653275]
	TIME [epoch: 5.82 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04578477490532385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04578477490532385 | validation: 0.12268205161260326]
	TIME [epoch: 5.82 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07570160327060521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07570160327060521 | validation: 0.1978955820754824]
	TIME [epoch: 5.83 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1416304395002824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1416304395002824 | validation: 0.1901043993406158]
	TIME [epoch: 5.82 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17184298659197947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17184298659197947 | validation: 0.11011078793446988]
	TIME [epoch: 5.83 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048708459616079544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048708459616079544 | validation: 0.09694355755801147]
	TIME [epoch: 5.82 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449767732270336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03449767732270336 | validation: 0.10397199897855952]
	TIME [epoch: 5.83 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03622319475272052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03622319475272052 | validation: 0.10823825277070794]
	TIME [epoch: 5.82 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02803330569133644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02803330569133644 | validation: 0.08531695599711299]
	TIME [epoch: 5.82 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020607970529894378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020607970529894378 | validation: 0.08304178839368098]
	TIME [epoch: 5.82 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018068718861893363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018068718861893363 | validation: 0.07961861872221641]
	TIME [epoch: 5.82 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01834327138155186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01834327138155186 | validation: 0.07907462924186753]
	TIME [epoch: 5.82 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021250918941507494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021250918941507494 | validation: 0.10780965980044135]
	TIME [epoch: 5.82 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03768086611065339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03768086611065339 | validation: 0.11579574698741767]
	TIME [epoch: 5.83 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0684333515460348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0684333515460348 | validation: 0.14341642925977519]
	TIME [epoch: 5.82 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805911931832588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07805911931832588 | validation: 0.14676869225012354]
	TIME [epoch: 5.82 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0587063224142545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0587063224142545 | validation: 0.13941753319641742]
	TIME [epoch: 5.82 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08119728323668206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08119728323668206 | validation: 0.1384950961692024]
	TIME [epoch: 5.82 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583652437242872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0583652437242872 | validation: 0.09669286421482229]
	TIME [epoch: 5.82 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159281208183945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03159281208183945 | validation: 0.10263541588198893]
	TIME [epoch: 5.82 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034212794876850444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034212794876850444 | validation: 0.1476570617947416]
	TIME [epoch: 5.81 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045810415907987005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045810415907987005 | validation: 0.07091088522449529]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_772.pth
	Model improved!!!
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024277478541109804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024277478541109804 | validation: 0.0663247050360513]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01855566830734045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01855566830734045 | validation: 0.050321484475477456]
	TIME [epoch: 5.82 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_774.pth
	Model improved!!!
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019631705671747376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019631705671747376 | validation: 0.08081202407326558]
	TIME [epoch: 5.82 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025863905677853335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025863905677853335 | validation: 0.0904342234043071]
	TIME [epoch: 5.82 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047800105104414695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047800105104414695 | validation: 0.10610320358455826]
	TIME [epoch: 5.83 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06542574033781282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06542574033781282 | validation: 0.09833449711263359]
	TIME [epoch: 5.83 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05452239712540575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05452239712540575 | validation: 0.14763575242273386]
	TIME [epoch: 5.82 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09075820361286395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09075820361286395 | validation: 0.24202815587103632]
	TIME [epoch: 5.82 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13494239600211957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13494239600211957 | validation: 0.10851520539584417]
	TIME [epoch: 5.82 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03868119985439592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03868119985439592 | validation: 0.11984442697358438]
	TIME [epoch: 5.82 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030431893735913054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030431893735913054 | validation: 0.09421953498806966]
	TIME [epoch: 5.83 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03182335083211323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03182335083211323 | validation: 0.08243172977493196]
	TIME [epoch: 5.82 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02307285986227127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02307285986227127 | validation: 0.0786703664646903]
	TIME [epoch: 5.82 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0299471316230397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0299471316230397 | validation: 0.08710603022436599]
	TIME [epoch: 5.82 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027780007125684073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027780007125684073 | validation: 0.08780020725730733]
	TIME [epoch: 5.82 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037596437467610246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037596437467610246 | validation: 0.10695151726190302]
	TIME [epoch: 5.83 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05271549285582593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05271549285582593 | validation: 0.10402184652522882]
	TIME [epoch: 5.82 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07785160779810141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07785160779810141 | validation: 0.13294935349316014]
	TIME [epoch: 5.82 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06781676776775349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06781676776775349 | validation: 0.09502353806217184]
	TIME [epoch: 5.83 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03716942347319342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03716942347319342 | validation: 0.0837451661085226]
	TIME [epoch: 5.82 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022376648952074786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022376648952074786 | validation: 0.083051764482924]
	TIME [epoch: 5.83 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017499252617830953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017499252617830953 | validation: 0.0704226697401129]
	TIME [epoch: 5.82 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01792640119609912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01792640119609912 | validation: 0.07013552152263457]
	TIME [epoch: 5.82 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02178096814814781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02178096814814781 | validation: 0.07807199452254905]
	TIME [epoch: 5.83 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02518389869993574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02518389869993574 | validation: 0.09814525279187515]
	TIME [epoch: 5.82 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03831121200896547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03831121200896547 | validation: 0.15859147036458962]
	TIME [epoch: 5.82 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12107492300349475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12107492300349475 | validation: 0.14873574910346996]
	TIME [epoch: 5.83 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598178237267415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04598178237267415 | validation: 0.18274331477954303]
	TIME [epoch: 5.82 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06353821352467293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06353821352467293 | validation: 0.14442462672781628]
	TIME [epoch: 5.82 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06800782266820159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06800782266820159 | validation: 0.17828057328883476]
	TIME [epoch: 5.82 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12282805618132768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12282805618132768 | validation: 0.1334069056396574]
	TIME [epoch: 5.83 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06200330340258895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06200330340258895 | validation: 0.11804847448306442]
	TIME [epoch: 5.83 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046620170027305445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046620170027305445 | validation: 0.09827286979205864]
	TIME [epoch: 5.83 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030447104426370785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030447104426370785 | validation: 0.08264745920152239]
	TIME [epoch: 5.82 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.015786510645430844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.015786510645430844 | validation: 0.07369110302273597]
	TIME [epoch: 5.82 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022534707834773356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022534707834773356 | validation: 0.08434657768632632]
	TIME [epoch: 5.82 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026518999917724707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026518999917724707 | validation: 0.08457991538866531]
	TIME [epoch: 5.82 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031601948318971326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031601948318971326 | validation: 0.13028279673674958]
	TIME [epoch: 5.82 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05877168180732501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05877168180732501 | validation: 0.1005150722092715]
	TIME [epoch: 5.83 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0602955951781367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0602955951781367 | validation: 0.09131041982397023]
	TIME [epoch: 5.82 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04425587576942527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04425587576942527 | validation: 0.11417939719611021]
	TIME [epoch: 5.83 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044555427623129765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044555427623129765 | validation: 0.10133591878811186]
	TIME [epoch: 5.82 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0311748616625132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0311748616625132 | validation: 0.0631141647921245]
	TIME [epoch: 5.82 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022079942983601657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022079942983601657 | validation: 0.09685269551750562]
	TIME [epoch: 5.82 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05486020975386605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05486020975386605 | validation: 0.15445786593890465]
	TIME [epoch: 5.82 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13455416967612052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13455416967612052 | validation: 0.09877294017055162]
	TIME [epoch: 5.81 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053727171694721125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053727171694721125 | validation: 0.07777225189027659]
	TIME [epoch: 5.83 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02208618634048059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02208618634048059 | validation: 0.07903262241302128]
	TIME [epoch: 5.82 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019257570680942138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019257570680942138 | validation: 0.35598029132695336]
	TIME [epoch: 5.82 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3085117926415768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3085117926415768 | validation: 0.24130083066123387]
	TIME [epoch: 5.82 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17020087870685743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17020087870685743 | validation: 0.2421174051196336]
	TIME [epoch: 5.82 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09196773943233742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09196773943233742 | validation: 0.24033724943880452]
	TIME [epoch: 5.82 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04168355988065192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04168355988065192 | validation: 0.3039402094412303]
	TIME [epoch: 5.82 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04677073149381552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04677073149381552 | validation: 0.288924788928907]
	TIME [epoch: 5.82 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159883575652836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04159883575652836 | validation: 0.26071390912125375]
	TIME [epoch: 5.82 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036127862564670585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036127862564670585 | validation: 0.21967760185390528]
	TIME [epoch: 5.82 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03396319553541084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03396319553541084 | validation: 0.20204462891406588]
	TIME [epoch: 5.83 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03720409914599012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03720409914599012 | validation: 0.20009468602683]
	TIME [epoch: 5.82 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544132044819152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05544132044819152 | validation: 0.19669879440734195]
	TIME [epoch: 5.83 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06604160290702725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06604160290702725 | validation: 0.1728966475119404]
	TIME [epoch: 5.82 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998718800792815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04998718800792815 | validation: 0.14077332013152377]
	TIME [epoch: 5.82 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031493256748873745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031493256748873745 | validation: 0.14147301807862508]
	TIME [epoch: 5.82 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02033351620176514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02033351620176514 | validation: 0.13453470354263306]
	TIME [epoch: 5.82 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019840264252904236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019840264252904236 | validation: 0.13380843633281533]
	TIME [epoch: 5.82 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019068577613314805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019068577613314805 | validation: 0.11011327679868287]
	TIME [epoch: 5.83 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023822583010059857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023822583010059857 | validation: 0.12216637615510431]
	TIME [epoch: 5.83 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04283810088289491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04283810088289491 | validation: 0.1354083939220576]
	TIME [epoch: 5.83 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630057190642428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0630057190642428 | validation: 0.13183892548682374]
	TIME [epoch: 5.82 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06282652141658604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06282652141658604 | validation: 0.12798774161564452]
	TIME [epoch: 5.83 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060675923796240386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060675923796240386 | validation: 0.12150235199453555]
	TIME [epoch: 5.82 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05045439628251664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05045439628251664 | validation: 0.10283266531073539]
	TIME [epoch: 5.82 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04527339106803277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04527339106803277 | validation: 0.10703175263608791]
	TIME [epoch: 5.82 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719453102369563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03719453102369563 | validation: 0.08746755630783432]
	TIME [epoch: 5.83 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021283206608993827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021283206608993827 | validation: 0.08865185798801589]
	TIME [epoch: 5.84 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01547781760378793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01547781760378793 | validation: 0.06445658271282018]
	TIME [epoch: 5.82 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013997899874581094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013997899874581094 | validation: 0.07678754505503284]
	TIME [epoch: 5.82 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013594934399189318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013594934399189318 | validation: 0.07046682903783473]
	TIME [epoch: 5.83 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014483182566293412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014483182566293412 | validation: 0.06160910078483087]
	TIME [epoch: 5.82 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016776265498228686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016776265498228686 | validation: 0.07325118848382232]
	TIME [epoch: 5.83 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026228865625628223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026228865625628223 | validation: 0.08430979039854901]
	TIME [epoch: 5.82 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788444412393767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04788444412393767 | validation: 0.09794748532752978]
	TIME [epoch: 5.83 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055008298432287134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055008298432287134 | validation: 0.06890483613037662]
	TIME [epoch: 5.82 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03367196665831254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03367196665831254 | validation: 0.07835344630883409]
	TIME [epoch: 5.83 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0429889023313088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0429889023313088 | validation: 0.1665382458668034]
	TIME [epoch: 5.82 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12280172305287274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12280172305287274 | validation: 0.13557939517136663]
	TIME [epoch: 5.84 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09185255504499913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09185255504499913 | validation: 0.11152537771376343]
	TIME [epoch: 5.83 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112500082306753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05112500082306753 | validation: 0.07779173974703595]
	TIME [epoch: 5.83 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0236535330001857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0236535330001857 | validation: 0.07860128953172033]
	TIME [epoch: 5.83 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025139430653421505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025139430653421505 | validation: 0.0750250954015422]
	TIME [epoch: 5.84 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018333928356509547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018333928356509547 | validation: 0.0819366468036079]
	TIME [epoch: 5.82 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022865201109296577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022865201109296577 | validation: 0.06690569287970179]
	TIME [epoch: 5.83 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018585025278791148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018585025278791148 | validation: 0.0728367479110144]
	TIME [epoch: 5.82 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019169504159354846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019169504159354846 | validation: 0.09181157867436708]
	TIME [epoch: 5.82 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023694101248226244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023694101248226244 | validation: 0.08130619145881868]
	TIME [epoch: 5.82 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031358939218227064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031358939218227064 | validation: 0.08502398831100583]
	TIME [epoch: 5.82 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040178209535370976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040178209535370976 | validation: 0.11275417985539976]
	TIME [epoch: 5.82 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07617558391750212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07617558391750212 | validation: 0.11622159155704022]
	TIME [epoch: 5.83 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08069377882294566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08069377882294566 | validation: 0.08692673023176155]
	TIME [epoch: 5.82 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0408073231411224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0408073231411224 | validation: 0.07576852634654635]
	TIME [epoch: 5.83 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024364341609974378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024364341609974378 | validation: 0.09828779221665732]
	TIME [epoch: 5.82 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04071581302974226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04071581302974226 | validation: 0.12537915592223176]
	TIME [epoch: 5.82 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05712988856973226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05712988856973226 | validation: 0.11239988828510175]
	TIME [epoch: 5.82 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05009631480447541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05009631480447541 | validation: 0.07085979821736776]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_121724/states/model_phi1_4b_v_mmd1_875.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 3856.335 seconds.
