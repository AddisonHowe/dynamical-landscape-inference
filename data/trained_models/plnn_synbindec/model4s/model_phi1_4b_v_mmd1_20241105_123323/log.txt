Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3671867036

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.796553318362023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.796553318362023 | validation: 5.022507627363259]
	TIME [epoch: 164 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.165026822715918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.165026822715918 | validation: 4.129176320895131]
	TIME [epoch: 1.47 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.592363900141278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.592363900141278 | validation: 4.347592722506299]
	TIME [epoch: 1.41 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.371014031847811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.371014031847811 | validation: 3.7603153243883796]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.145459081493105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.145459081493105 | validation: 3.7402402384294504]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.05047731247438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.05047731247438 | validation: 3.5432075075157985]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8513988301149022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8513988301149022 | validation: 3.562538223905017]
	TIME [epoch: 1.41 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.864688328730496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.864688328730496 | validation: 3.5752182818609053]
	TIME [epoch: 1.41 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8512959637406086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8512959637406086 | validation: 3.4360297298552873]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.756578916796984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.756578916796984 | validation: 3.4547576596310807]
	TIME [epoch: 1.41 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7121766232454525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7121766232454525 | validation: 3.379252253210895]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.677935101333046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.677935101333046 | validation: 3.3966828403801603]
	TIME [epoch: 1.41 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6523149570225657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6523149570225657 | validation: 3.3267211870253677]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6345531140481433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6345531140481433 | validation: 3.3849286301152874]
	TIME [epoch: 1.41 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6404221526077776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6404221526077776 | validation: 3.33540778035769]
	TIME [epoch: 1.41 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6779832568379276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6779832568379276 | validation: 3.5107878320972845]
	TIME [epoch: 1.41 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.733955204974364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.733955204974364 | validation: 3.213311170222168]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5650081081090366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5650081081090366 | validation: 3.2735712922244193]
	TIME [epoch: 1.41 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5299857753663977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5299857753663977 | validation: 3.2153963988242342]
	TIME [epoch: 1.41 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5269632700898366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5269632700898366 | validation: 3.210075903519712]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4953409677326137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4953409677326137 | validation: 3.205534859649964]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4761144096405907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4761144096405907 | validation: 3.1277304849693666]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.446623187571804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.446623187571804 | validation: 3.233954852037853]
	TIME [epoch: 1.4 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4566214480632027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4566214480632027 | validation: 3.138384853081703]
	TIME [epoch: 1.4 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5241095108644425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5241095108644425 | validation: 3.173519985969629]
	TIME [epoch: 1.4 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4051720341759624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4051720341759624 | validation: 3.056511325193155]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3407683316799184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3407683316799184 | validation: 3.0450250898243434]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.304811720248775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.304811720248775 | validation: 3.0035413837528524]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.268597117717001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.268597117717001 | validation: 3.029778638181421]
	TIME [epoch: 1.41 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.270375541614529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.270375541614529 | validation: 3.0636422478781635]
	TIME [epoch: 1.4 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3755909538616162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3755909538616162 | validation: 3.1603882920093342]
	TIME [epoch: 1.4 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4013901586234545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4013901586234545 | validation: 2.961682098469232]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2085594840990685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2085594840990685 | validation: 2.9492133009867647]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.16676802507184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.16676802507184 | validation: 2.9457424488059396]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.167209209225608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.167209209225608 | validation: 2.957786767019758]
	TIME [epoch: 1.41 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1819330643341517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1819330643341517 | validation: 2.9924151711981595]
	TIME [epoch: 1.41 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2146750272283606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2146750272283606 | validation: 2.9490395328145533]
	TIME [epoch: 1.4 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.122710128325246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.122710128325246 | validation: 2.914328258843223]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0352720652000733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0352720652000733 | validation: 2.838912006185812]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8698722440598807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8698722440598807 | validation: 2.7844075993700956]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7137951896547623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7137951896547623 | validation: 3.246592124772682]
	TIME [epoch: 1.41 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1451793217217934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1451793217217934 | validation: 2.638009735806123]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9447741896800648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9447741896800648 | validation: 2.5125401953816153]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3284795536920018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3284795536920018 | validation: 2.894484866376617]
	TIME [epoch: 1.41 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4261924504177013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4261924504177013 | validation: 2.4557938870695644]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.02474795289865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.02474795289865 | validation: 2.2624426939437647]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9213419325222243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9213419325222243 | validation: 2.071620742750685]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6786780202601816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6786780202601816 | validation: 1.6778627107380357]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3422644981325726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3422644981325726 | validation: 1.3148818772001]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9776163774929768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9776163774929768 | validation: 1.3953575911580893]
	TIME [epoch: 1.41 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.436564993512099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.436564993512099 | validation: 2.5902595576025895]
	TIME [epoch: 1.41 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1709226321007127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1709226321007127 | validation: 1.2826856576698942]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.047909232297472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.047909232297472 | validation: 1.3767407950839463]
	TIME [epoch: 1.41 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3793727971395513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3793727971395513 | validation: 1.2118814045875217]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.945329324933928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.945329324933928 | validation: 1.1492180044555897]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8864171509126336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8864171509126336 | validation: 0.9467459336217591]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195256274268959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195256274268959 | validation: 0.9380573768221137]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933267597370156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933267597370156 | validation: 0.8376064529032718]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746560675515093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7746560675515093 | validation: 0.9025755677962526]
	TIME [epoch: 1.41 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765315753063788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765315753063788 | validation: 0.8435470696184076]
	TIME [epoch: 1.41 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641992339280026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641992339280026 | validation: 0.9140625685121982]
	TIME [epoch: 1.4 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653672502059514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653672502059514 | validation: 0.8609194115549571]
	TIME [epoch: 1.4 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7714667745084632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7714667745084632 | validation: 1.0487807740355313]
	TIME [epoch: 1.4 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048909029046298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8048909029046298 | validation: 0.8323626807421388]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033857797851949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8033857797851949 | validation: 1.2597433903051092]
	TIME [epoch: 1.41 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.867323800371415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.867323800371415 | validation: 0.8376772102062051]
	TIME [epoch: 1.41 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641999586931146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641999586931146 | validation: 0.8697895531456012]
	TIME [epoch: 1.4 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7431825272368994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7431825272368994 | validation: 0.8667369362750872]
	TIME [epoch: 1.4 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7448669663043088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7448669663043088 | validation: 0.8911728454495759]
	TIME [epoch: 1.4 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439628028554752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439628028554752 | validation: 0.8250720313724831]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750424074702243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750424074702243 | validation: 1.2728432384449175]
	TIME [epoch: 1.41 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754595334141795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8754595334141795 | validation: 0.8694845058643355]
	TIME [epoch: 1.41 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300762919671141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300762919671141 | validation: 1.4076655179768338]
	TIME [epoch: 1.4 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9354128143786271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9354128143786271 | validation: 0.8721591680231905]
	TIME [epoch: 1.4 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695637237379984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695637237379984 | validation: 0.8803059330807195]
	TIME [epoch: 1.4 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7939661727743033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7939661727743033 | validation: 1.0203189418444258]
	TIME [epoch: 1.4 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.837766733883551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.837766733883551 | validation: 0.810059489348319]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763975442965239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763975442965239 | validation: 0.9766146532976365]
	TIME [epoch: 1.41 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648537497884133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648537497884133 | validation: 0.8179896986739057]
	TIME [epoch: 1.41 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7679808736748953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7679808736748953 | validation: 1.13733167671245]
	TIME [epoch: 1.41 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8254136445440429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8254136445440429 | validation: 0.9118485272965476]
	TIME [epoch: 1.41 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789482850934922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789482850934922 | validation: 1.2507038882606725]
	TIME [epoch: 1.4 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642174691860558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642174691860558 | validation: 0.8371997100782356]
	TIME [epoch: 1.4 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680470482609062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7680470482609062 | validation: 0.9148627220414431]
	TIME [epoch: 1.41 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7440352628126585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7440352628126585 | validation: 0.8388784863010725]
	TIME [epoch: 1.4 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7458170651176858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7458170651176858 | validation: 0.8926781146991261]
	TIME [epoch: 1.41 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7550107727728249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7550107727728249 | validation: 0.8360695280822242]
	TIME [epoch: 1.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7635103939499763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7635103939499763 | validation: 0.9875441646778989]
	TIME [epoch: 1.4 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7917351155799347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7917351155799347 | validation: 0.8267963391253677]
	TIME [epoch: 1.4 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957163815677082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7957163815677082 | validation: 1.2762308939258507]
	TIME [epoch: 1.4 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9209095145314607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9209095145314607 | validation: 0.8508768521417931]
	TIME [epoch: 1.4 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886163678067183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886163678067183 | validation: 0.9789779160122581]
	TIME [epoch: 1.41 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061267417794327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8061267417794327 | validation: 0.9057395747963626]
	TIME [epoch: 1.4 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894155232617707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7894155232617707 | validation: 0.852784790518057]
	TIME [epoch: 1.4 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402729323706957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7402729323706957 | validation: 0.8804869486572561]
	TIME [epoch: 1.4 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7319893281975854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7319893281975854 | validation: 0.7941457142620231]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491239270803197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491239270803197 | validation: 0.9770451598438439]
	TIME [epoch: 1.41 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495541118895054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495541118895054 | validation: 0.8319441993513831]
	TIME [epoch: 1.41 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8060023879057622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8060023879057622 | validation: 1.4080815364893624]
	TIME [epoch: 1.41 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9852230206303795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9852230206303795 | validation: 0.8749863290469497]
	TIME [epoch: 1.4 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467233460123995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467233460123995 | validation: 0.9685179753237408]
	TIME [epoch: 1.42 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7570093580536204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7570093580536204 | validation: 0.8207898848712749]
	TIME [epoch: 1.4 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7254997381639788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7254997381639788 | validation: 0.8220109123501858]
	TIME [epoch: 1.4 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7258735750206204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7258735750206204 | validation: 0.8603223130799326]
	TIME [epoch: 1.4 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733770185709194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.733770185709194 | validation: 0.8342206114348376]
	TIME [epoch: 1.4 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7569072681925585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7569072681925585 | validation: 0.9545857041784788]
	TIME [epoch: 1.41 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640467366012615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7640467366012615 | validation: 0.8462815287804646]
	TIME [epoch: 1.41 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688775684534752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688775684534752 | validation: 0.914667296739957]
	TIME [epoch: 1.41 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443493715590468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7443493715590468 | validation: 0.8766739691118112]
	TIME [epoch: 1.41 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7415263856842202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7415263856842202 | validation: 0.8218668556759079]
	TIME [epoch: 1.4 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489050837429241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8489050837429241 | validation: 1.55163946538015]
	TIME [epoch: 1.41 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2112543082988456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2112543082988456 | validation: 1.0023766123190156]
	TIME [epoch: 1.41 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168582483040018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168582483040018 | validation: 1.0631396039723624]
	TIME [epoch: 1.4 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.150276330924951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.150276330924951 | validation: 1.1564532659372249]
	TIME [epoch: 1.4 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8209355381486091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8209355381486091 | validation: 1.0048023003919995]
	TIME [epoch: 1.4 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717579640781111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8717579640781111 | validation: 0.7945595186286307]
	TIME [epoch: 1.4 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365224317332847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7365224317332847 | validation: 0.8632412288515575]
	TIME [epoch: 1.4 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7497134014103621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7497134014103621 | validation: 0.8338240737125926]
	TIME [epoch: 1.4 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.74429186633488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.74429186633488 | validation: 0.8215982237048629]
	TIME [epoch: 1.4 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7183534903973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7183534903973 | validation: 0.7906040435123813]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142041279349244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142041279349244 | validation: 0.8488984947482685]
	TIME [epoch: 1.41 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7190919764385797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7190919764385797 | validation: 0.782302645019853]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129856779312967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7129856779312967 | validation: 0.8172487894983643]
	TIME [epoch: 1.41 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7095321381873929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7095321381873929 | validation: 0.7980547366464155]
	TIME [epoch: 1.4 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7164939182841272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7164939182841272 | validation: 0.8294170307741716]
	TIME [epoch: 1.4 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7260664543872503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7260664543872503 | validation: 0.8822836458880956]
	TIME [epoch: 1.4 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7676579226733264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7676579226733264 | validation: 0.9206371738455279]
	TIME [epoch: 1.4 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82660843465476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82660843465476 | validation: 0.8764384886588314]
	TIME [epoch: 1.4 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668296808703385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668296808703385 | validation: 0.9610468785693045]
	TIME [epoch: 1.4 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7535199275412461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7535199275412461 | validation: 0.7963858054408822]
	TIME [epoch: 1.4 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393305839654687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8393305839654687 | validation: 1.462888862346917]
	TIME [epoch: 1.4 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0297587682062885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0297587682062885 | validation: 0.8723219269100696]
	TIME [epoch: 1.41 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7784835067146451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7784835067146451 | validation: 0.7905916760787675]
	TIME [epoch: 1.41 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832286201076514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7832286201076514 | validation: 1.0984472995310492]
	TIME [epoch: 1.41 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068266532156397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068266532156397 | validation: 0.8017086567945291]
	TIME [epoch: 1.41 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7146803515887814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7146803515887814 | validation: 0.7715836178120707]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7132649661951049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7132649661951049 | validation: 0.9085488859446879]
	TIME [epoch: 1.41 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243454031362228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7243454031362228 | validation: 0.7632440601282008]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_138.pth
	Model improved!!!
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168068283480804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7168068283480804 | validation: 0.9253504238353822]
	TIME [epoch: 1.41 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7186649119628445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7186649119628445 | validation: 0.7660138923776203]
	TIME [epoch: 1.41 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717850018165833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717850018165833 | validation: 0.968390673970098]
	TIME [epoch: 1.41 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7320146492240861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7320146492240861 | validation: 0.7701806825494015]
	TIME [epoch: 1.41 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266207054488354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7266207054488354 | validation: 0.989023976293542]
	TIME [epoch: 1.42 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7379584997649306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7379584997649306 | validation: 0.7614923333450492]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.728628805995056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.728628805995056 | validation: 1.107507009098226]
	TIME [epoch: 1.41 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025873634478963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025873634478963 | validation: 0.821086465162895]
	TIME [epoch: 1.41 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8740683654508068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8740683654508068 | validation: 1.0567202932235282]
	TIME [epoch: 1.41 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8274522931116198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8274522931116198 | validation: 0.8261742852651314]
	TIME [epoch: 1.41 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7142259827658333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7142259827658333 | validation: 0.7752483638198581]
	TIME [epoch: 1.42 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7362275986287267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7362275986287267 | validation: 1.0587126778885305]
	TIME [epoch: 1.41 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7646140185880611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7646140185880611 | validation: 0.7594120003134444]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717206442092859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717206442092859 | validation: 0.829604716669855]
	TIME [epoch: 1.41 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6821905454507871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6821905454507871 | validation: 0.8029474949093173]
	TIME [epoch: 1.42 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6789799765427077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6789799765427077 | validation: 0.7589654984541129]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69756634477554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.69756634477554 | validation: 0.9497620420219743]
	TIME [epoch: 1.41 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7991613899155986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7991613899155986 | validation: 0.8601313607734412]
	TIME [epoch: 1.41 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8367794481417693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8367794481417693 | validation: 0.9045820830217104]
	TIME [epoch: 1.41 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7369946438476204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7369946438476204 | validation: 0.7917636742613123]
	TIME [epoch: 1.41 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6715594943915353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6715594943915353 | validation: 0.7514639081021928]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6557873604220689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6557873604220689 | validation: 0.7531816416304474]
	TIME [epoch: 1.41 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590678357707597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6590678357707597 | validation: 0.8336737549318152]
	TIME [epoch: 1.42 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804195246460334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6804195246460334 | validation: 0.8211146719679623]
	TIME [epoch: 1.41 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736840376552985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736840376552985 | validation: 1.1586018134020457]
	TIME [epoch: 1.41 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819034775388172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819034775388172 | validation: 0.8088945791978475]
	TIME [epoch: 1.41 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6545232576728438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6545232576728438 | validation: 0.7475069991391236]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6529674916219835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6529674916219835 | validation: 0.8504941100368266]
	TIME [epoch: 1.41 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.659454114266783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.659454114266783 | validation: 0.7032904306748217]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7333931494488034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7333931494488034 | validation: 1.3960800860504052]
	TIME [epoch: 1.41 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1196723596811282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1196723596811282 | validation: 0.7654883486721219]
	TIME [epoch: 1.42 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482254931503246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482254931503246 | validation: 0.7176881359029439]
	TIME [epoch: 1.41 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292372784394654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292372784394654 | validation: 1.0369913900451466]
	TIME [epoch: 1.41 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855022908048147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855022908048147 | validation: 0.7091476424385723]
	TIME [epoch: 1.41 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.655449522740972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.655449522740972 | validation: 0.7420006988640196]
	TIME [epoch: 1.41 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6409964147594625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6409964147594625 | validation: 0.8017018109680969]
	TIME [epoch: 1.41 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446291758019203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446291758019203 | validation: 0.7054038311658264]
	TIME [epoch: 1.41 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6514872366849208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6514872366849208 | validation: 0.9374893619482354]
	TIME [epoch: 1.41 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919675078648334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6919675078648334 | validation: 0.7758337290581403]
	TIME [epoch: 1.41 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7065672045337894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7065672045337894 | validation: 0.9378699226401764]
	TIME [epoch: 1.41 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7322739169684341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7322739169684341 | validation: 1.1806070446968504]
	TIME [epoch: 1.41 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8341756230918239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8341756230918239 | validation: 0.7057792365542601]
	TIME [epoch: 1.41 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.72966071235345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.72966071235345 | validation: 0.7890504602904331]
	TIME [epoch: 1.41 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6401149648949559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6401149648949559 | validation: 0.7319302917865933]
	TIME [epoch: 1.41 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6164652734217922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6164652734217922 | validation: 0.6689488045043186]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6073664521856594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6073664521856594 | validation: 0.7663401620886362]
	TIME [epoch: 1.41 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6166049313894796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6166049313894796 | validation: 0.6996486797219976]
	TIME [epoch: 1.41 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962417273940023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6962417273940023 | validation: 1.240584137743033]
	TIME [epoch: 1.41 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763465074008723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763465074008723 | validation: 0.9160635079360695]
	TIME [epoch: 1.41 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6437659099097027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6437659099097027 | validation: 0.6925111650149252]
	TIME [epoch: 1.42 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.686857319622815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.686857319622815 | validation: 0.9552166950003103]
	TIME [epoch: 1.41 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713194278509911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713194278509911 | validation: 0.6938176518038651]
	TIME [epoch: 1.41 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6788010791463402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6788010791463402 | validation: 0.7887687755039122]
	TIME [epoch: 1.41 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6704109366428963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6704109366428963 | validation: 0.8116406713606445]
	TIME [epoch: 1.41 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6642447712628682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6642447712628682 | validation: 0.8129730325651056]
	TIME [epoch: 1.41 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6138073174474761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6138073174474761 | validation: 0.6258471119929326]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6014020957475951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6014020957475951 | validation: 0.7541050198790101]
	TIME [epoch: 1.41 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.59673362797038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.59673362797038 | validation: 0.6178448653398079]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5897019058055988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5897019058055988 | validation: 0.755144328223933]
	TIME [epoch: 1.41 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5859620073451235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5859620073451235 | validation: 0.6349217963705203]
	TIME [epoch: 1.41 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5993925361090193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5993925361090193 | validation: 0.7881670668587261]
	TIME [epoch: 1.41 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467590289502877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6467590289502877 | validation: 1.0127156332433955]
	TIME [epoch: 1.41 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7162961127441099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7162961127441099 | validation: 0.7155127761649924]
	TIME [epoch: 172 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5937329321308373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5937329321308373 | validation: 0.7088881451450476]
	TIME [epoch: 2.79 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444553292106531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5444553292106531 | validation: 0.6145807699618353]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5499290638649655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5499290638649655 | validation: 0.6893222319069845]
	TIME [epoch: 2.78 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5708644982593452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5708644982593452 | validation: 1.0269871497557965]
	TIME [epoch: 2.78 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6874717126026215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6874717126026215 | validation: 0.8793623939757137]
	TIME [epoch: 2.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6075544536982844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6075544536982844 | validation: 0.6594202712976278]
	TIME [epoch: 2.78 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5415461261803902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5415461261803902 | validation: 0.6614119714440085]
	TIME [epoch: 2.79 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5291718486011281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5291718486011281 | validation: 0.5722198453702658]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5446020054156546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5446020054156546 | validation: 1.1258751576493018]
	TIME [epoch: 2.79 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667523114872823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.667523114872823 | validation: 0.581686199876415]
	TIME [epoch: 2.78 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5897896147269149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5897896147269149 | validation: 0.7543884985994037]
	TIME [epoch: 2.78 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5144285748543252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5144285748543252 | validation: 0.6476747417371596]
	TIME [epoch: 2.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5293141612747313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5293141612747313 | validation: 0.7743175067383128]
	TIME [epoch: 2.78 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6650053918444544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6650053918444544 | validation: 0.7854643996160054]
	TIME [epoch: 2.78 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5227019994678775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5227019994678775 | validation: 0.6188604932446389]
	TIME [epoch: 2.79 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48895572270586457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48895572270586457 | validation: 0.7004710497453059]
	TIME [epoch: 2.78 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5772571073257735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5772571073257735 | validation: 0.6567592054101108]
	TIME [epoch: 2.78 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49680913689296136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49680913689296136 | validation: 0.6462674604804978]
	TIME [epoch: 2.78 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4733523788208321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4733523788208321 | validation: 0.5757307795949151]
	TIME [epoch: 2.79 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4423551091638461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4423551091638461 | validation: 0.7723579043173295]
	TIME [epoch: 2.79 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47405912502730047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47405912502730047 | validation: 0.6314381074391351]
	TIME [epoch: 2.78 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4311395391900619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4311395391900619 | validation: 0.5377488189220453]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45390797817465584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45390797817465584 | validation: 0.681872490961688]
	TIME [epoch: 2.8 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5133558051611816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5133558051611816 | validation: 0.7194756020596229]
	TIME [epoch: 2.8 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5524397956022025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5524397956022025 | validation: 0.6073222727408782]
	TIME [epoch: 2.79 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.465463947572787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.465463947572787 | validation: 0.6398565424862627]
	TIME [epoch: 2.8 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4691150587333952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4691150587333952 | validation: 0.9307740065622001]
	TIME [epoch: 2.8 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5443772264039681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5443772264039681 | validation: 0.5203988206622959]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592763879471989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592763879471989 | validation: 0.7137176517444308]
	TIME [epoch: 2.8 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4280916600461974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4280916600461974 | validation: 0.5004509380980385]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3596026719647143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3596026719647143 | validation: 0.5646637008119805]
	TIME [epoch: 2.78 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34864006073508985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34864006073508985 | validation: 0.49768542685965067]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34845005425417747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34845005425417747 | validation: 0.6471673623887438]
	TIME [epoch: 2.78 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44198371075577053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44198371075577053 | validation: 0.650152690736068]
	TIME [epoch: 2.78 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4515805573841848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4515805573841848 | validation: 0.5235644491069349]
	TIME [epoch: 2.78 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5177131612433392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5177131612433392 | validation: 0.49013475931497796]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_237.pth
	Model improved!!!
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3406563234821919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3406563234821919 | validation: 0.6077249607540987]
	TIME [epoch: 2.78 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48481427676318173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48481427676318173 | validation: 1.029831906377585]
	TIME [epoch: 2.78 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260834701275857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6260834701275857 | validation: 0.45526887596025345]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5192475006515447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5192475006515447 | validation: 0.5646387188441481]
	TIME [epoch: 2.79 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39016475296505887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39016475296505887 | validation: 0.7520354423572956]
	TIME [epoch: 2.8 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4133033032783604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4133033032783604 | validation: 0.5143126948122341]
	TIME [epoch: 2.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3130811996278895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3130811996278895 | validation: 0.44363321851340975]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3456178327205575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3456178327205575 | validation: 0.6761843621168845]
	TIME [epoch: 2.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39659059788937123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39659059788937123 | validation: 0.6366710027510689]
	TIME [epoch: 2.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3752441843456646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3752441843456646 | validation: 0.44203380025264]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_247.pth
	Model improved!!!
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40759124237717315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40759124237717315 | validation: 0.47671439231407065]
	TIME [epoch: 2.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297637012244382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.297637012244382 | validation: 0.5539189064650492]
	TIME [epoch: 2.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29653214326370453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29653214326370453 | validation: 0.4109958665855117]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_250.pth
	Model improved!!!
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2878993026234408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2878993026234408 | validation: 0.6077119280466422]
	TIME [epoch: 2.79 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30587492542907335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30587492542907335 | validation: 0.501864639818407]
	TIME [epoch: 2.78 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555665019542565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3555665019542565 | validation: 0.6478556330725654]
	TIME [epoch: 2.78 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5694941859807366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5694941859807366 | validation: 0.5468118788238066]
	TIME [epoch: 2.78 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42844670983185396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42844670983185396 | validation: 0.6770043274415047]
	TIME [epoch: 2.78 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5978951473873104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5978951473873104 | validation: 1.051661299851373]
	TIME [epoch: 2.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6323885044640721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6323885044640721 | validation: 0.5355522748190612]
	TIME [epoch: 2.79 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38064830206577155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38064830206577155 | validation: 1.4896648210090064]
	TIME [epoch: 2.79 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6257005480445474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6257005480445474 | validation: 0.9849465985712418]
	TIME [epoch: 2.79 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5352962309780419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5352962309780419 | validation: 0.7877200602316516]
	TIME [epoch: 2.79 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47349857588499944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47349857588499944 | validation: 0.6841379903404491]
	TIME [epoch: 2.79 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.408671597564665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.408671597564665 | validation: 0.508301146549118]
	TIME [epoch: 2.79 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32870127295586765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32870127295586765 | validation: 0.43491773661294764]
	TIME [epoch: 2.79 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3084916456891958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3084916456891958 | validation: 0.4631221329264141]
	TIME [epoch: 2.79 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30396553682225985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30396553682225985 | validation: 0.4801036195381557]
	TIME [epoch: 2.79 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30065749831660193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30065749831660193 | validation: 0.47978241345033984]
	TIME [epoch: 2.79 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30334390860856547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30334390860856547 | validation: 0.44288673780606835]
	TIME [epoch: 2.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2885623628773554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2885623628773554 | validation: 0.46161500087051927]
	TIME [epoch: 2.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25988601169230796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25988601169230796 | validation: 0.45184643186984924]
	TIME [epoch: 2.79 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24580868816765475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24580868816765475 | validation: 0.4563406076324313]
	TIME [epoch: 2.79 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24730651286960512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24730651286960512 | validation: 0.4388319042001525]
	TIME [epoch: 2.79 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27118842057734344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27118842057734344 | validation: 0.5139393807472751]
	TIME [epoch: 2.79 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32733575065508624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32733575065508624 | validation: 0.5669827545794137]
	TIME [epoch: 2.79 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38056893610613174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38056893610613174 | validation: 0.5868759725328841]
	TIME [epoch: 2.79 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2840358322787669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2840358322787669 | validation: 0.37516807409048336]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_275.pth
	Model improved!!!
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25079243920697747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25079243920697747 | validation: 0.516266170692569]
	TIME [epoch: 2.78 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23007055738835697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23007055738835697 | validation: 0.36355658546116393]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23130738099379097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23130738099379097 | validation: 0.4814678624009927]
	TIME [epoch: 2.78 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21519088824910035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21519088824910035 | validation: 0.4304109878601013]
	TIME [epoch: 2.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2529238529183439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2529238529183439 | validation: 0.5642530256658871]
	TIME [epoch: 2.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4196443758884878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4196443758884878 | validation: 0.5718262601518598]
	TIME [epoch: 2.79 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2600403088552655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2600403088552655 | validation: 0.4517834020785495]
	TIME [epoch: 2.79 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.351897342669962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.351897342669962 | validation: 0.6815593566062339]
	TIME [epoch: 2.79 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34040704755472434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34040704755472434 | validation: 0.5921313779872744]
	TIME [epoch: 2.8 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2765695951667641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2765695951667641 | validation: 0.3626221621184736]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32859204581406537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32859204581406537 | validation: 0.5344218327224224]
	TIME [epoch: 2.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36214957880889304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36214957880889304 | validation: 0.5926279145117217]
	TIME [epoch: 2.78 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39579482324683296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39579482324683296 | validation: 0.42535688678724537]
	TIME [epoch: 2.78 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26795970587184065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26795970587184065 | validation: 0.46868585139854624]
	TIME [epoch: 2.78 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2401642368944638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2401642368944638 | validation: 0.35502850041668016]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21714977709448932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21714977709448932 | validation: 0.39864084577617476]
	TIME [epoch: 2.79 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20281757804881706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20281757804881706 | validation: 0.44502631845457563]
	TIME [epoch: 2.79 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018029840631723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2018029840631723 | validation: 0.41581567842481226]
	TIME [epoch: 2.79 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20729986228001676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20729986228001676 | validation: 0.39902465174149765]
	TIME [epoch: 2.79 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2997876671343665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2997876671343665 | validation: 0.5081364337032234]
	TIME [epoch: 2.79 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26174493855152997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26174493855152997 | validation: 0.3415703231269853]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18244113220308592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18244113220308592 | validation: 0.39467516886773213]
	TIME [epoch: 2.79 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16668297807983357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16668297807983357 | validation: 0.35252498436734225]
	TIME [epoch: 2.78 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17407934987704596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17407934987704596 | validation: 0.4829233526882932]
	TIME [epoch: 2.78 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2265282702473867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2265282702473867 | validation: 0.456699887924105]
	TIME [epoch: 2.78 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29677536818409295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29677536818409295 | validation: 0.7922853737818638]
	TIME [epoch: 2.78 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4310026030492074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4310026030492074 | validation: 0.6784004958403118]
	TIME [epoch: 2.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30400410396530764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30400410396530764 | validation: 0.41418393448349095]
	TIME [epoch: 2.78 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29802420999044876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29802420999044876 | validation: 0.4430100203741084]
	TIME [epoch: 2.78 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41196134022104247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41196134022104247 | validation: 0.5324786759158038]
	TIME [epoch: 2.78 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2954094991201572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2954094991201572 | validation: 0.39106851624311]
	TIME [epoch: 2.78 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21911574728404115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21911574728404115 | validation: 0.5028023187206799]
	TIME [epoch: 2.78 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2643897965432886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2643897965432886 | validation: 0.3739317221833155]
	TIME [epoch: 2.78 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4805133128187924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4805133128187924 | validation: 0.4210951993328064]
	TIME [epoch: 2.78 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28464850385194346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28464850385194346 | validation: 0.5364015715523609]
	TIME [epoch: 2.78 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28219856504386004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28219856504386004 | validation: 0.38503769978512636]
	TIME [epoch: 2.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19262911848855246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19262911848855246 | validation: 0.38804602146025613]
	TIME [epoch: 2.78 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16904884790417238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16904884790417238 | validation: 0.38999608683104486]
	TIME [epoch: 2.78 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15219450671699894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15219450671699894 | validation: 0.38509363694287524]
	TIME [epoch: 2.78 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15256107086269782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15256107086269782 | validation: 0.3679248481910986]
	TIME [epoch: 2.79 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13962983487643976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13962983487643976 | validation: 0.37152722955453427]
	TIME [epoch: 2.78 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369244371757692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1369244371757692 | validation: 0.40853588924817663]
	TIME [epoch: 2.79 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14819715429176397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14819715429176397 | validation: 0.514562425705381]
	TIME [epoch: 2.78 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3216921396173728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3216921396173728 | validation: 0.8750122263083556]
	TIME [epoch: 2.78 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5636797254965495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5636797254965495 | validation: 1.4487739999848348]
	TIME [epoch: 2.78 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0945818241814282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0945818241814282 | validation: 0.9809365891164519]
	TIME [epoch: 2.78 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5823968201798991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5823968201798991 | validation: 0.6077261413614728]
	TIME [epoch: 2.78 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5028799831208439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5028799831208439 | validation: 0.35904234287017467]
	TIME [epoch: 2.78 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2502038851292228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2502038851292228 | validation: 0.5130106441344878]
	TIME [epoch: 2.78 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41754923258003845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41754923258003845 | validation: 0.6595539552149399]
	TIME [epoch: 2.78 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35636514322430934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35636514322430934 | validation: 0.41048160809559314]
	TIME [epoch: 2.78 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19528716575099672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19528716575099672 | validation: 0.46772967760606526]
	TIME [epoch: 2.78 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25136074490293353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25136074490293353 | validation: 0.5172073759984984]
	TIME [epoch: 2.79 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23106792025820624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23106792025820624 | validation: 0.46779197633602154]
	TIME [epoch: 2.78 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1806346031527084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1806346031527084 | validation: 0.35388061185931985]
	TIME [epoch: 2.78 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17765099882779875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17765099882779875 | validation: 0.4398184430223963]
	TIME [epoch: 2.79 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16033447771768458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16033447771768458 | validation: 0.3721741842114304]
	TIME [epoch: 2.78 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13955820039296324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13955820039296324 | validation: 0.37396281432419676]
	TIME [epoch: 2.79 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13977128300001357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13977128300001357 | validation: 0.352869959040067]
	TIME [epoch: 2.78 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1561354716283689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1561354716283689 | validation: 0.5338577140366205]
	TIME [epoch: 2.78 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2983130614494786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2983130614494786 | validation: 0.4324188513306177]
	TIME [epoch: 2.78 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4162212409500383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4162212409500383 | validation: 0.5323925345326763]
	TIME [epoch: 2.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19686635039500539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19686635039500539 | validation: 0.338266641086659]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_338.pth
	Model improved!!!
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18617319164646848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18617319164646848 | validation: 0.5837546097455324]
	TIME [epoch: 2.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2930456271175653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2930456271175653 | validation: 0.3750829975132453]
	TIME [epoch: 2.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.152821360001784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.152821360001784 | validation: 0.3421866250984041]
	TIME [epoch: 2.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17037738398175784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17037738398175784 | validation: 0.5451189271438176]
	TIME [epoch: 2.79 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018483238456873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2018483238456873 | validation: 0.3498095733210775]
	TIME [epoch: 2.8 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.158375698548957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.158375698548957 | validation: 0.36854146035015767]
	TIME [epoch: 2.79 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15334080136571743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15334080136571743 | validation: 0.38481136027810536]
	TIME [epoch: 2.81 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14687967145616287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14687967145616287 | validation: 0.4026282169846713]
	TIME [epoch: 2.8 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19890261616859178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19890261616859178 | validation: 0.36953857776657717]
	TIME [epoch: 2.8 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2917771356299433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2917771356299433 | validation: 0.6177472252238615]
	TIME [epoch: 2.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28469371854053177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28469371854053177 | validation: 0.3297321378431801]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2070625144909026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2070625144909026 | validation: 0.516197100567559]
	TIME [epoch: 2.78 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1699537230718917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1699537230718917 | validation: 0.3759086371572521]
	TIME [epoch: 2.79 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14563723461704112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14563723461704112 | validation: 0.3152302231182664]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14949769781059408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14949769781059408 | validation: 0.43094929745231864]
	TIME [epoch: 2.79 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193349217566901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2193349217566901 | validation: 0.5347605786136757]
	TIME [epoch: 2.78 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551260070940497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551260070940497 | validation: 0.3016268834276454]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13306086185985883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13306086185985883 | validation: 0.39820337878515527]
	TIME [epoch: 2.78 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14553766430071466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14553766430071466 | validation: 0.2727852234165574]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_357.pth
	Model improved!!!
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1717071408701242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1717071408701242 | validation: 0.6861602084956198]
	TIME [epoch: 2.79 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3590476772474554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3590476772474554 | validation: 0.42981028056092563]
	TIME [epoch: 2.79 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14545839006591624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14545839006591624 | validation: 0.3477338501461103]
	TIME [epoch: 2.79 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25981769514762126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25981769514762126 | validation: 0.5453120756729108]
	TIME [epoch: 2.78 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2101642182892369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2101642182892369 | validation: 0.40515019217734766]
	TIME [epoch: 2.79 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19837047949206527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19837047949206527 | validation: 0.3544183056436804]
	TIME [epoch: 2.78 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20026521837405092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20026521837405092 | validation: 0.381582151762754]
	TIME [epoch: 2.78 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18417016441235298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18417016441235298 | validation: 0.3632635817003479]
	TIME [epoch: 2.78 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17542382196091905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17542382196091905 | validation: 0.3535106751062229]
	TIME [epoch: 2.79 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16153557547705155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16153557547705155 | validation: 0.4194385136677745]
	TIME [epoch: 2.78 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18488872148473973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18488872148473973 | validation: 0.3400611643231022]
	TIME [epoch: 2.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1265645220287279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1265645220287279 | validation: 0.34804775889794215]
	TIME [epoch: 2.78 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11982217470741385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11982217470741385 | validation: 0.30964748538102616]
	TIME [epoch: 2.79 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1171199329875991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1171199329875991 | validation: 0.3316319776070067]
	TIME [epoch: 2.79 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.111931681749967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.111931681749967 | validation: 0.2949364135938891]
	TIME [epoch: 2.79 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11964743349653924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11964743349653924 | validation: 0.5943073687586449]
	TIME [epoch: 2.78 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2528293395079049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2528293395079049 | validation: 0.40689791152970767]
	TIME [epoch: 2.79 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1986885114018879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1986885114018879 | validation: 0.3601595659063743]
	TIME [epoch: 2.78 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21610030429904245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21610030429904245 | validation: 0.5791883446510576]
	TIME [epoch: 2.79 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2777606682164016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2777606682164016 | validation: 0.37570204778560257]
	TIME [epoch: 2.79 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22303356741825184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22303356741825184 | validation: 0.4131674606703243]
	TIME [epoch: 2.79 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13047878702493645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13047878702493645 | validation: 0.3085560653142093]
	TIME [epoch: 2.78 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12761386772858344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12761386772858344 | validation: 0.40508005042822914]
	TIME [epoch: 2.78 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16045837375565877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16045837375565877 | validation: 0.3608254051483469]
	TIME [epoch: 2.78 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157382382952697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157382382952697 | validation: 0.5117908144317992]
	TIME [epoch: 2.78 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19245398140302605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19245398140302605 | validation: 0.40292565598636243]
	TIME [epoch: 2.78 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14368053684796428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14368053684796428 | validation: 0.27534959629761097]
	TIME [epoch: 2.79 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531295499589926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531295499589926 | validation: 0.37400643118641896]
	TIME [epoch: 2.78 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1334922235741933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1334922235741933 | validation: 0.2944171997242209]
	TIME [epoch: 2.78 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1331149490728426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1331149490728426 | validation: 0.33188836339531413]
	TIME [epoch: 2.79 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14083443826037587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14083443826037587 | validation: 0.323342456365112]
	TIME [epoch: 2.79 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16577929326188387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16577929326188387 | validation: 0.38669676091079874]
	TIME [epoch: 2.78 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1824166834578588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1824166834578588 | validation: 0.4081769916582683]
	TIME [epoch: 2.79 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17426173492193647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17426173492193647 | validation: 0.3812964743753545]
	TIME [epoch: 2.78 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16273062043839198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16273062043839198 | validation: 0.5685858938942638]
	TIME [epoch: 2.79 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21769668844214454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21769668844214454 | validation: 0.3188342042370651]
	TIME [epoch: 2.79 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573335282707308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1573335282707308 | validation: 0.42545615538849557]
	TIME [epoch: 2.79 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16279640253229188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16279640253229188 | validation: 0.33346604364495774]
	TIME [epoch: 2.78 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1530967627970898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1530967627970898 | validation: 0.36465466492477616]
	TIME [epoch: 2.78 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14346760448032927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14346760448032927 | validation: 0.4440251547754544]
	TIME [epoch: 2.78 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20802424317218177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20802424317218177 | validation: 0.2851195310118088]
	TIME [epoch: 2.79 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15841422621217235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15841422621217235 | validation: 0.45710654503462317]
	TIME [epoch: 2.78 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14809686792990667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14809686792990667 | validation: 0.29351540352200545]
	TIME [epoch: 2.78 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10776208006112128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10776208006112128 | validation: 0.312211194839792]
	TIME [epoch: 2.78 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13347558660934844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13347558660934844 | validation: 0.4894966581615447]
	TIME [epoch: 2.79 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18326853356049236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18326853356049236 | validation: 0.31283075260827503]
	TIME [epoch: 2.79 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14061573425438467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14061573425438467 | validation: 0.34589481849292186]
	TIME [epoch: 2.79 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12581171430523683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12581171430523683 | validation: 0.3088555394542723]
	TIME [epoch: 2.79 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09939515920135034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09939515920135034 | validation: 0.3391830578277055]
	TIME [epoch: 2.78 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09887788637808811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09887788637808811 | validation: 0.3012524309122185]
	TIME [epoch: 2.79 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11180153271113628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11180153271113628 | validation: 0.4948166505059087]
	TIME [epoch: 2.78 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21440261878082598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21440261878082598 | validation: 0.47069926740563184]
	TIME [epoch: 2.78 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20883866127477638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20883866127477638 | validation: 0.3083608333347636]
	TIME [epoch: 2.79 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31861386461714936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31861386461714936 | validation: 0.40225539959211193]
	TIME [epoch: 2.79 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21564155774206342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21564155774206342 | validation: 0.466729120579793]
	TIME [epoch: 2.79 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14355323166557593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14355323166557593 | validation: 0.2630643376178395]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_413.pth
	Model improved!!!
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11291885246851224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11291885246851224 | validation: 0.28992041151714204]
	TIME [epoch: 2.78 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11027685688844838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11027685688844838 | validation: 0.25846780998923474]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09054790498371855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09054790498371855 | validation: 0.4404101397517251]
	TIME [epoch: 2.78 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15218104529780382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15218104529780382 | validation: 0.4546484414931311]
	TIME [epoch: 2.79 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21087710160448064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21087710160448064 | validation: 0.531809852667943]
	TIME [epoch: 2.79 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1872475491210627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1872475491210627 | validation: 0.3249098528180407]
	TIME [epoch: 2.78 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012045845483091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12012045845483091 | validation: 0.36198691063760685]
	TIME [epoch: 2.78 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15565632486174807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15565632486174807 | validation: 0.3341870800690965]
	TIME [epoch: 2.78 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431480871219814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1431480871219814 | validation: 0.3135171938233378]
	TIME [epoch: 2.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12828571269002714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12828571269002714 | validation: 0.32959617482053855]
	TIME [epoch: 2.78 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1282667785272207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1282667785272207 | validation: 0.25119903030771734]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_424.pth
	Model improved!!!
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1147135559477973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1147135559477973 | validation: 0.33494539135006973]
	TIME [epoch: 2.78 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11464001733569709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11464001733569709 | validation: 0.25521867940303905]
	TIME [epoch: 2.8 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10576868347267195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10576868347267195 | validation: 0.33766440884538723]
	TIME [epoch: 2.78 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476610111901642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1476610111901642 | validation: 0.2734456196674633]
	TIME [epoch: 2.78 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12584299852689632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12584299852689632 | validation: 0.32317172335018896]
	TIME [epoch: 2.79 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14984036423728664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14984036423728664 | validation: 0.48893208324681425]
	TIME [epoch: 2.79 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1904523914640151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1904523914640151 | validation: 0.31923330116828486]
	TIME [epoch: 2.78 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1507685386959439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1507685386959439 | validation: 0.37844230004042323]
	TIME [epoch: 2.78 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13762457817036094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13762457817036094 | validation: 0.31106987740923553]
	TIME [epoch: 2.78 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13543308374394633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13543308374394633 | validation: 0.621806728299943]
	TIME [epoch: 2.78 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2048651822143034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2048651822143034 | validation: 0.30025517498091886]
	TIME [epoch: 2.78 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08687452056009566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08687452056009566 | validation: 0.2757690464643098]
	TIME [epoch: 2.79 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902713527835019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902713527835019 | validation: 0.3355181033478367]
	TIME [epoch: 2.78 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11381445218510688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11381445218510688 | validation: 0.3706551316749416]
	TIME [epoch: 2.78 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33706523219735235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33706523219735235 | validation: 0.4732738276612616]
	TIME [epoch: 2.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504094296806736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2504094296806736 | validation: 0.39309668681381127]
	TIME [epoch: 2.79 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17881349372258443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17881349372258443 | validation: 0.2954547626565806]
	TIME [epoch: 2.78 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541083632272972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08541083632272972 | validation: 0.31440697978134824]
	TIME [epoch: 2.78 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08806260422956715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08806260422956715 | validation: 0.2592227540561362]
	TIME [epoch: 2.79 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09298898189172788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09298898189172788 | validation: 0.3041003306358739]
	TIME [epoch: 2.78 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11234904843587305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11234904843587305 | validation: 0.34664598926059925]
	TIME [epoch: 2.78 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1525054040192767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1525054040192767 | validation: 0.4094753993989699]
	TIME [epoch: 2.78 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20599038266198919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20599038266198919 | validation: 0.4038740344565641]
	TIME [epoch: 2.78 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1703716287135407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1703716287135407 | validation: 0.2886781061104184]
	TIME [epoch: 2.78 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11450499487881043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11450499487881043 | validation: 0.39383343293217626]
	TIME [epoch: 2.79 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14621472672986383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14621472672986383 | validation: 0.3381194186715037]
	TIME [epoch: 2.79 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16822457110787034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16822457110787034 | validation: 0.3910312465791121]
	TIME [epoch: 2.78 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15807696614749017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15807696614749017 | validation: 0.2834463995337345]
	TIME [epoch: 2.78 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08998060322135025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08998060322135025 | validation: 0.29592175204685023]
	TIME [epoch: 2.78 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816570070836978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08816570070836978 | validation: 0.2858436268246142]
	TIME [epoch: 2.78 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256766154653728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10256766154653728 | validation: 0.35629852562111963]
	TIME [epoch: 2.78 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.151669606956766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.151669606956766 | validation: 0.34730938525348054]
	TIME [epoch: 2.78 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498899765349344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498899765349344 | validation: 0.2983719246684641]
	TIME [epoch: 2.78 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12287073119003004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12287073119003004 | validation: 0.3539866089481625]
	TIME [epoch: 2.78 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1351695438414777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1351695438414777 | validation: 0.30898738151629423]
	TIME [epoch: 2.78 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14688324409779013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14688324409779013 | validation: 0.29620273765914323]
	TIME [epoch: 2.79 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11037712781055498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11037712781055498 | validation: 0.38646342820968205]
	TIME [epoch: 2.79 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12501141198900917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12501141198900917 | validation: 0.5638488595199281]
	TIME [epoch: 2.79 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4323369643825762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4323369643825762 | validation: 0.4051077482874467]
	TIME [epoch: 2.78 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1357882004883571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1357882004883571 | validation: 0.39359832859083355]
	TIME [epoch: 2.78 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2849090989678975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2849090989678975 | validation: 0.6129200117457483]
	TIME [epoch: 2.79 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3096366750352144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3096366750352144 | validation: 0.5784702176179941]
	TIME [epoch: 2.78 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1660874045956443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1660874045956443 | validation: 0.40721681983859614]
	TIME [epoch: 2.78 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4870672584546371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4870672584546371 | validation: 0.4161147787134871]
	TIME [epoch: 2.78 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857910874934136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2857910874934136 | validation: 0.5254662313529188]
	TIME [epoch: 2.78 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31188219847373744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31188219847373744 | validation: 0.4982655603540132]
	TIME [epoch: 2.79 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.194740379326382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.194740379326382 | validation: 0.3527993365378014]
	TIME [epoch: 2.78 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10769444014831361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10769444014831361 | validation: 0.3111194932800485]
	TIME [epoch: 2.78 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10848070228891925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10848070228891925 | validation: 0.29176727661779206]
	TIME [epoch: 2.78 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0993204269570926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0993204269570926 | validation: 0.33296548525457653]
	TIME [epoch: 2.78 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1221975490957513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1221975490957513 | validation: 0.33573314775081275]
	TIME [epoch: 2.79 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13707645727280032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13707645727280032 | validation: 0.36693625676802916]
	TIME [epoch: 2.79 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12561362638691065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12561362638691065 | validation: 0.28563354006557057]
	TIME [epoch: 2.78 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0922522773271519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0922522773271519 | validation: 0.3171629448067246]
	TIME [epoch: 2.78 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08574357707490951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08574357707490951 | validation: 0.29207326824847185]
	TIME [epoch: 2.78 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08516785010835996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08516785010835996 | validation: 0.3549940788063528]
	TIME [epoch: 2.78 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511031833172318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09511031833172318 | validation: 0.3033445129829709]
	TIME [epoch: 2.78 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1389881677257787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1389881677257787 | validation: 0.5005442177416631]
	TIME [epoch: 2.79 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18784127141940288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18784127141940288 | validation: 0.34072915861445086]
	TIME [epoch: 2.78 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12687736013867856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12687736013867856 | validation: 0.31590890049987586]
	TIME [epoch: 2.78 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11742290724775573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11742290724775573 | validation: 0.2851661060411869]
	TIME [epoch: 2.78 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09206411714094424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09206411714094424 | validation: 0.2625306473649473]
	TIME [epoch: 2.78 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0926988785645657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0926988785645657 | validation: 0.32750431196138025]
	TIME [epoch: 2.78 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11510988364636099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11510988364636099 | validation: 0.24430220566719305]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11131097687962475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11131097687962475 | validation: 0.28070041284842673]
	TIME [epoch: 2.78 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10122392093173961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10122392093173961 | validation: 0.22484713382834412]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_490.pth
	Model improved!!!
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09339532242763796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09339532242763796 | validation: 0.25562099902611973]
	TIME [epoch: 2.79 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11028114830077897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11028114830077897 | validation: 0.4204807168441178]
	TIME [epoch: 2.78 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1876514984106292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1876514984106292 | validation: 0.3885410564626999]
	TIME [epoch: 2.78 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21611016506971217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21611016506971217 | validation: 0.490863385638278]
	TIME [epoch: 2.78 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14136311375943472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14136311375943472 | validation: 0.26127708909171804]
	TIME [epoch: 2.78 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08926078488132032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08926078488132032 | validation: 0.3178180707000533]
	TIME [epoch: 2.78 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12531959669758783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12531959669758783 | validation: 0.3060109967707412]
	TIME [epoch: 2.78 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10273295636690911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10273295636690911 | validation: 0.29043139448385785]
	TIME [epoch: 2.79 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08700760672093083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08700760672093083 | validation: 0.2872345416174465]
	TIME [epoch: 2.78 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12355277923622782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12355277923622782 | validation: 0.3091321939240498]
	TIME [epoch: 2.78 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13451932358760454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13451932358760454 | validation: 0.2932393225356355]
	TIME [epoch: 177 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11774672481106258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11774672481106258 | validation: 0.2654829806001564]
	TIME [epoch: 6.01 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0943714031882643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0943714031882643 | validation: 0.316166707156316]
	TIME [epoch: 6.01 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12976367256835197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12976367256835197 | validation: 0.27922196669788296]
	TIME [epoch: 6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11320959619282402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11320959619282402 | validation: 0.2765471891310161]
	TIME [epoch: 6.01 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1042711479906331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1042711479906331 | validation: 0.35952316286221003]
	TIME [epoch: 6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15528869393544817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15528869393544817 | validation: 0.276742350385995]
	TIME [epoch: 6.01 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135327090880373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135327090880373 | validation: 0.3986034064418139]
	TIME [epoch: 6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10700729442783567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10700729442783567 | validation: 0.22859349503875181]
	TIME [epoch: 6.01 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07302717867179567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07302717867179567 | validation: 0.279044801347283]
	TIME [epoch: 6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438522444849326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09438522444849326 | validation: 0.3118379935435855]
	TIME [epoch: 6.01 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16317464056263523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16317464056263523 | validation: 0.34864734225488947]
	TIME [epoch: 6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21561583998027453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21561583998027453 | validation: 0.29435140039303587]
	TIME [epoch: 6.01 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15790062368408084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15790062368408084 | validation: 0.39517197133387194]
	TIME [epoch: 6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10943668909725009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10943668909725009 | validation: 0.2980034755951572]
	TIME [epoch: 6.01 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11218481746515753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11218481746515753 | validation: 0.3462331254129072]
	TIME [epoch: 6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.081991675819608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.081991675819608 | validation: 0.2622328322371245]
	TIME [epoch: 6.01 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953331973639056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07953331973639056 | validation: 0.2504780370095415]
	TIME [epoch: 6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08684233343416235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08684233343416235 | validation: 0.24764053207695702]
	TIME [epoch: 6.01 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09243890287093155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09243890287093155 | validation: 0.24285405086898157]
	TIME [epoch: 6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0974713008214107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0974713008214107 | validation: 0.2820402750151705]
	TIME [epoch: 6.01 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11403873726123848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11403873726123848 | validation: 0.23646217197987102]
	TIME [epoch: 5.99 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1655209065730567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1655209065730567 | validation: 0.3377528069520465]
	TIME [epoch: 6.02 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18230580577343491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18230580577343491 | validation: 0.4583598427593426]
	TIME [epoch: 6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40226850747068454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40226850747068454 | validation: 0.3903690169301435]
	TIME [epoch: 6.02 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3549695474222163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3549695474222163 | validation: 0.7213530622042568]
	TIME [epoch: 6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42118754891024046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42118754891024046 | validation: 0.5409340994044375]
	TIME [epoch: 6.02 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13633106209715037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13633106209715037 | validation: 0.5216272326613945]
	TIME [epoch: 6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2998463357910602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2998463357910602 | validation: 0.5581111222505382]
	TIME [epoch: 6.02 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260347339283964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.260347339283964 | validation: 0.4477897442354521]
	TIME [epoch: 6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1487506980664403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1487506980664403 | validation: 0.3167667852770896]
	TIME [epoch: 6.01 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10903903693750228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10903903693750228 | validation: 0.2913944101566767]
	TIME [epoch: 6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07671510531539691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07671510531539691 | validation: 0.2842601061348872]
	TIME [epoch: 6.01 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08066569897407135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08066569897407135 | validation: 0.2582838347268863]
	TIME [epoch: 6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641597431958453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641597431958453 | validation: 0.2517295610821882]
	TIME [epoch: 6.01 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07677114570763754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07677114570763754 | validation: 0.3428832476652417]
	TIME [epoch: 6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12807092936205527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12807092936205527 | validation: 0.3895188385711886]
	TIME [epoch: 6.02 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24107876933875985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24107876933875985 | validation: 0.4397404101175932]
	TIME [epoch: 6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.201727335383139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.201727335383139 | validation: 0.2765683953356512]
	TIME [epoch: 6.01 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08759815857470074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08759815857470074 | validation: 0.3084276188578274]
	TIME [epoch: 6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546285726178598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06546285726178598 | validation: 0.23956194798114616]
	TIME [epoch: 6.01 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061414564712213945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061414564712213945 | validation: 0.28198371702292857]
	TIME [epoch: 6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10095689287599964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10095689287599964 | validation: 0.2959714068548163]
	TIME [epoch: 6.01 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15182732011486383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15182732011486383 | validation: 0.33214266891001953]
	TIME [epoch: 6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14177244844306341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14177244844306341 | validation: 0.2725205016290845]
	TIME [epoch: 6.01 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10736084193598151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10736084193598151 | validation: 0.25413133278894107]
	TIME [epoch: 6.01 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06553981366534774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06553981366534774 | validation: 0.23167048411697533]
	TIME [epoch: 6.01 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054644957044734654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054644957044734654 | validation: 0.23036084561274783]
	TIME [epoch: 6.01 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053503625217240364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053503625217240364 | validation: 0.23810347798283404]
	TIME [epoch: 6.01 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05832224906907568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05832224906907568 | validation: 0.26046675956939175]
	TIME [epoch: 6.01 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09673933156071218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09673933156071218 | validation: 0.6294881613643725]
	TIME [epoch: 6.01 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28173139603160524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28173139603160524 | validation: 0.47646819625319364]
	TIME [epoch: 6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2447128074351366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2447128074351366 | validation: 0.3111895210430616]
	TIME [epoch: 6.01 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12530104244211804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12530104244211804 | validation: 0.2286019357615386]
	TIME [epoch: 6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08932233104460988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08932233104460988 | validation: 0.26853533198631147]
	TIME [epoch: 6.02 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040006119664137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1040006119664137 | validation: 0.29937308845629484]
	TIME [epoch: 6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0857052388753036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0857052388753036 | validation: 0.22815814086254982]
	TIME [epoch: 6.01 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076426908686365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06076426908686365 | validation: 0.20212445260556322]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_558.pth
	Model improved!!!
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058089623921099356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058089623921099356 | validation: 0.24214601206398056]
	TIME [epoch: 6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083341221642493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07083341221642493 | validation: 0.22551446643918993]
	TIME [epoch: 6.01 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10920846886794841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10920846886794841 | validation: 0.4533044587569588]
	TIME [epoch: 5.99 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1633306633149909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1633306633149909 | validation: 0.24432123497037433]
	TIME [epoch: 6.01 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11578793626525714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11578793626525714 | validation: 0.3468993761847261]
	TIME [epoch: 6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13922370378189658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13922370378189658 | validation: 0.29017045697031385]
	TIME [epoch: 6.01 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16191184137335612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16191184137335612 | validation: 0.3027186784017561]
	TIME [epoch: 6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14440159958611318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14440159958611318 | validation: 0.2603399422309889]
	TIME [epoch: 6.02 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08951530630974502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08951530630974502 | validation: 0.21049006461158318]
	TIME [epoch: 6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059786041698407344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059786041698407344 | validation: 0.22543931921955612]
	TIME [epoch: 6.01 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519649659553643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06519649659553643 | validation: 0.3043024111107637]
	TIME [epoch: 6.01 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06658349887447199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06658349887447199 | validation: 0.2256884042471822]
	TIME [epoch: 6.01 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057981265585470156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057981265585470156 | validation: 0.2483140123822874]
	TIME [epoch: 6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06054424053080249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06054424053080249 | validation: 0.2275625222873044]
	TIME [epoch: 6.01 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08546957459507819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08546957459507819 | validation: 0.43284058696924443]
	TIME [epoch: 6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2069027071165753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2069027071165753 | validation: 0.33578812037543826]
	TIME [epoch: 6.01 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2252348448104079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2252348448104079 | validation: 0.25112970430422216]
	TIME [epoch: 6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11825194943856104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11825194943856104 | validation: 0.21409466808734737]
	TIME [epoch: 6.01 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06603155469054171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06603155469054171 | validation: 0.3017897321542649]
	TIME [epoch: 6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07652243980048253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07652243980048253 | validation: 0.25280988703144763]
	TIME [epoch: 6.01 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07174902747488664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07174902747488664 | validation: 0.19877969711249396]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07366225903664864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07366225903664864 | validation: 0.26894187754557863]
	TIME [epoch: 6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08205381194921436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08205381194921436 | validation: 0.2203328738993484]
	TIME [epoch: 6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09769154111952082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09769154111952082 | validation: 0.2722659728497689]
	TIME [epoch: 6.01 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327328440344125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12327328440344125 | validation: 0.2600166215826544]
	TIME [epoch: 6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12314956618989964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12314956618989964 | validation: 0.22877777597046164]
	TIME [epoch: 6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1019648133754085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1019648133754085 | validation: 0.3654251273463199]
	TIME [epoch: 6 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10579315156815522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10579315156815522 | validation: 0.21030479391068152]
	TIME [epoch: 6.02 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08324117272225295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08324117272225295 | validation: 0.23984443621830112]
	TIME [epoch: 6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137494174949821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09137494174949821 | validation: 0.2018786518640747]
	TIME [epoch: 6.01 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797663888624762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09797663888624762 | validation: 0.35667349039590446]
	TIME [epoch: 6.01 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12270467292548963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12270467292548963 | validation: 0.21550047142273554]
	TIME [epoch: 6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09552768514601166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09552768514601166 | validation: 0.23653758725586532]
	TIME [epoch: 6.01 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07959017343216263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07959017343216263 | validation: 0.20318358536634726]
	TIME [epoch: 6.01 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07674975738969593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07674975738969593 | validation: 0.27530467349619797]
	TIME [epoch: 6.01 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20417785361991136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20417785361991136 | validation: 0.27057350295868526]
	TIME [epoch: 6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12355853987084732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12355853987084732 | validation: 0.28298962853535414]
	TIME [epoch: 6.01 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10984680927928496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10984680927928496 | validation: 0.2636862659774535]
	TIME [epoch: 6.01 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954501165296322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0954501165296322 | validation: 0.22584995952491763]
	TIME [epoch: 6 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08142287602295693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08142287602295693 | validation: 0.23223849897470894]
	TIME [epoch: 6.01 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07840001688163534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07840001688163534 | validation: 0.2393948451682154]
	TIME [epoch: 6.01 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12876457447126244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12876457447126244 | validation: 0.2645725591409569]
	TIME [epoch: 6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887703617636701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08887703617636701 | validation: 0.2669531600885519]
	TIME [epoch: 5.99 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06644656940798528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06644656940798528 | validation: 0.21272973552717778]
	TIME [epoch: 6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058893617572094124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058893617572094124 | validation: 0.21762474058983636]
	TIME [epoch: 6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067653471217401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.067653471217401 | validation: 0.25780431189116254]
	TIME [epoch: 6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06752315468308355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06752315468308355 | validation: 0.23650166929795927]
	TIME [epoch: 6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07393301527175844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07393301527175844 | validation: 0.2133786680287528]
	TIME [epoch: 6 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10029744685920702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10029744685920702 | validation: 0.5062186917251713]
	TIME [epoch: 6.01 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23015134700022566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23015134700022566 | validation: 0.21421031267061577]
	TIME [epoch: 6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12267532104708004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12267532104708004 | validation: 0.2050426874802463]
	TIME [epoch: 6.01 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05290403860234502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05290403860234502 | validation: 0.20587189237911196]
	TIME [epoch: 6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05343531122174028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05343531122174028 | validation: 0.19390299972969563]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_611.pth
	Model improved!!!
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904878380850595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07904878380850595 | validation: 0.3795828065237743]
	TIME [epoch: 6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17541153490922176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17541153490922176 | validation: 0.24426507614536908]
	TIME [epoch: 6.01 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10256631104777242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10256631104777242 | validation: 0.26212220811532294]
	TIME [epoch: 6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08124913314908891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08124913314908891 | validation: 0.20729014279302216]
	TIME [epoch: 6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08231213350794934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08231213350794934 | validation: 0.21025638260432272]
	TIME [epoch: 6.01 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06880988575508921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06880988575508921 | validation: 0.23029403407977558]
	TIME [epoch: 6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06548896006538554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06548896006538554 | validation: 0.23244432483385255]
	TIME [epoch: 6.01 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09991560891166837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09991560891166837 | validation: 0.30449430398952326]
	TIME [epoch: 6.01 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1706364162895184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706364162895184 | validation: 0.3964220060169963]
	TIME [epoch: 6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.130283939401081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.130283939401081 | validation: 0.23967013323850353]
	TIME [epoch: 6.01 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.064530936285937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.064530936285937 | validation: 0.21855728304155886]
	TIME [epoch: 6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05919050921790639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05919050921790639 | validation: 0.1889621450946955]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07817596171772241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07817596171772241 | validation: 0.27302234218744503]
	TIME [epoch: 6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11708327964369826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11708327964369826 | validation: 0.2513981378503064]
	TIME [epoch: 6.01 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15135876172214452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15135876172214452 | validation: 0.42784144497447074]
	TIME [epoch: 6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15163758813850436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15163758813850436 | validation: 0.22236088515639604]
	TIME [epoch: 6.01 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06513712825957996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06513712825957996 | validation: 0.20414759224306006]
	TIME [epoch: 6.01 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03696569245097549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03696569245097549 | validation: 0.1654469837667068]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04523486377254822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04523486377254822 | validation: 0.4063603852027088]
	TIME [epoch: 5.99 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28850094131511816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28850094131511816 | validation: 0.4680902165945979]
	TIME [epoch: 6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12829202140341542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12829202140341542 | validation: 0.3297077831990554]
	TIME [epoch: 6.01 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08410692620349257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08410692620349257 | validation: 0.2510246118981325]
	TIME [epoch: 5.99 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11931799358309261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11931799358309261 | validation: 0.5597539584649392]
	TIME [epoch: 6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18740389582468972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18740389582468972 | validation: 0.2901266919616962]
	TIME [epoch: 6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705449482939774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0705449482939774 | validation: 0.2216988287267396]
	TIME [epoch: 6 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06196499635662913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06196499635662913 | validation: 0.1817740016995176]
	TIME [epoch: 5.99 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06726832295964845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06726832295964845 | validation: 0.26212885697031657]
	TIME [epoch: 6.01 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925834797577102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09925834797577102 | validation: 0.23853383037931739]
	TIME [epoch: 6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1116098438780657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1116098438780657 | validation: 0.2922186918808416]
	TIME [epoch: 6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09802993342510906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09802993342510906 | validation: 0.21203906692676444]
	TIME [epoch: 5.99 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477342836513992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07477342836513992 | validation: 0.24236798914848168]
	TIME [epoch: 6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09717240418680158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09717240418680158 | validation: 0.33078968630993333]
	TIME [epoch: 6.01 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336779742355009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336779742355009 | validation: 0.278085902361376]
	TIME [epoch: 6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13767585828293377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13767585828293377 | validation: 0.2800684655091394]
	TIME [epoch: 6.01 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08614499798538787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08614499798538787 | validation: 0.17659337557236413]
	TIME [epoch: 6.01 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04762145755472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04762145755472 | validation: 0.19275902362416067]
	TIME [epoch: 6.02 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046031449087718994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046031449087718994 | validation: 0.16834965753564415]
	TIME [epoch: 6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042101888721573597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042101888721573597 | validation: 0.21137807646327458]
	TIME [epoch: 6.01 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04119869765105559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04119869765105559 | validation: 0.16909294174728126]
	TIME [epoch: 6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05259214232456448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05259214232456448 | validation: 0.21135139615344528]
	TIME [epoch: 6.01 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07503908526993704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07503908526993704 | validation: 0.18464988154715142]
	TIME [epoch: 6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09140184106435985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09140184106435985 | validation: 0.2796458201881326]
	TIME [epoch: 6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08569404621142883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08569404621142883 | validation: 0.1899450941210181]
	TIME [epoch: 6.01 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0754850207792817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0754850207792817 | validation: 0.3111235321752317]
	TIME [epoch: 6.01 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15023580378643087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15023580378643087 | validation: 0.2806327386431939]
	TIME [epoch: 6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1700766091956948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1700766091956948 | validation: 0.22153826687433817]
	TIME [epoch: 6.01 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07280203647259866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07280203647259866 | validation: 0.18172525275246967]
	TIME [epoch: 6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03475071486248192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03475071486248192 | validation: 0.16103950021340904]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_659.pth
	Model improved!!!
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03732813948848677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03732813948848677 | validation: 0.16658241937006155]
	TIME [epoch: 6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046391907540267174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046391907540267174 | validation: 0.16195338541756268]
	TIME [epoch: 6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055103503228697746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055103503228697746 | validation: 0.41780294894126746]
	TIME [epoch: 6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11991485890985214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11991485890985214 | validation: 0.20195487725904934]
	TIME [epoch: 6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626667679547438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12626667679547438 | validation: 0.23967159616036035]
	TIME [epoch: 6.01 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12425537046337069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12425537046337069 | validation: 0.1663859147680718]
	TIME [epoch: 6.01 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570588846774253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06570588846774253 | validation: 0.19847097363079985]
	TIME [epoch: 6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04578169814833867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04578169814833867 | validation: 0.1667486374876337]
	TIME [epoch: 6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042715547550070614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042715547550070614 | validation: 0.17632699885189346]
	TIME [epoch: 5.99 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04820418400151073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04820418400151073 | validation: 0.17940257807828136]
	TIME [epoch: 6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396983300775769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05396983300775769 | validation: 0.1984073253988331]
	TIME [epoch: 5.99 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06762196923758271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06762196923758271 | validation: 0.20359401704064484]
	TIME [epoch: 6.01 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0871332008738985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0871332008738985 | validation: 0.2639257298123788]
	TIME [epoch: 5.99 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19283449233443947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19283449233443947 | validation: 0.2639219381323817]
	TIME [epoch: 6.01 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19584256136153388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19584256136153388 | validation: 0.3060880450112975]
	TIME [epoch: 6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07672055387073694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07672055387073694 | validation: 0.1869993523603477]
	TIME [epoch: 6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058574270002962774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058574270002962774 | validation: 0.19185251744117574]
	TIME [epoch: 5.99 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06350346398313458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06350346398313458 | validation: 0.3446369563571907]
	TIME [epoch: 6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1015743775004447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1015743775004447 | validation: 0.2508760253333637]
	TIME [epoch: 5.99 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13212809023987623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13212809023987623 | validation: 0.32345081594480346]
	TIME [epoch: 6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11307550709417001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11307550709417001 | validation: 0.2100347841464012]
	TIME [epoch: 6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07880446008805864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07880446008805864 | validation: 0.20214729122180916]
	TIME [epoch: 6.01 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05595979438979132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05595979438979132 | validation: 0.2002462935054463]
	TIME [epoch: 6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03657348428911952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03657348428911952 | validation: 0.18143825155621937]
	TIME [epoch: 6.01 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03287931333402837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03287931333402837 | validation: 0.16271082961645533]
	TIME [epoch: 5.99 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030122177875360184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030122177875360184 | validation: 0.16859173390507637]
	TIME [epoch: 6.01 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03561316765372674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03561316765372674 | validation: 0.18166147580810313]
	TIME [epoch: 5.99 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05482162108409593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05482162108409593 | validation: 0.19902210221938696]
	TIME [epoch: 5.99 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06843592096197237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06843592096197237 | validation: 0.36813360599165207]
	TIME [epoch: 5.99 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11514795174327873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11514795174327873 | validation: 0.2452998567337129]
	TIME [epoch: 5.99 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15386741510021001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15386741510021001 | validation: 0.2422427873390146]
	TIME [epoch: 6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.121053829546957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.121053829546957 | validation: 0.2805245865321055]
	TIME [epoch: 5.99 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10865265817654671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10865265817654671 | validation: 0.2146090943150541]
	TIME [epoch: 5.99 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11533349750340367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11533349750340367 | validation: 0.23384098475827006]
	TIME [epoch: 5.99 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669214329318581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10669214329318581 | validation: 0.147941086450821]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_694.pth
	Model improved!!!
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053100324659352785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053100324659352785 | validation: 0.1655036298042286]
	TIME [epoch: 5.97 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028739475017099662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028739475017099662 | validation: 0.14405715670180305]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_696.pth
	Model improved!!!
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030254442518184536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030254442518184536 | validation: 0.16311429768647995]
	TIME [epoch: 6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0319845740891778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0319845740891778 | validation: 0.13823565338667]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028100895458618682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028100895458618682 | validation: 0.14367704892750313]
	TIME [epoch: 6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026455787348433703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026455787348433703 | validation: 0.12589129205494132]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_700.pth
	Model improved!!!
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03660561697179102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03660561697179102 | validation: 0.22263224244066673]
	TIME [epoch: 5.99 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1095932953365336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1095932953365336 | validation: 0.365272124061005]
	TIME [epoch: 6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2255302852514168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2255302852514168 | validation: 0.7346907730867249]
	TIME [epoch: 6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3881574335723259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3881574335723259 | validation: 0.7796399563829679]
	TIME [epoch: 6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3959556984475246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3959556984475246 | validation: 0.5821559995201752]
	TIME [epoch: 5.99 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1698802310131458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1698802310131458 | validation: 0.25340196472125626]
	TIME [epoch: 6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08448093994882278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08448093994882278 | validation: 0.24892155922073794]
	TIME [epoch: 6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0963199783438818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0963199783438818 | validation: 0.2448624567716393]
	TIME [epoch: 6 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09264364747253347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09264364747253347 | validation: 0.3025339870537485]
	TIME [epoch: 6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12331527189312919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12331527189312919 | validation: 0.311925738740119]
	TIME [epoch: 6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09716333495001718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09716333495001718 | validation: 0.28313134068443446]
	TIME [epoch: 6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07858973543733613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07858973543733613 | validation: 0.2353216538886013]
	TIME [epoch: 6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062210149914193595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062210149914193595 | validation: 0.26144205004368826]
	TIME [epoch: 5.99 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06582402480798884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06582402480798884 | validation: 0.2207922329931209]
	TIME [epoch: 6.01 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06365248446355913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06365248446355913 | validation: 0.2532749583293831]
	TIME [epoch: 5.99 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08093739503990786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08093739503990786 | validation: 0.2414065169985367]
	TIME [epoch: 6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08495066527603047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08495066527603047 | validation: 0.2504092891731942]
	TIME [epoch: 5.99 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08798280081177158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08798280081177158 | validation: 0.22840117292571005]
	TIME [epoch: 6 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07559061665892572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07559061665892572 | validation: 0.22995530739431147]
	TIME [epoch: 5.99 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07949569843399734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07949569843399734 | validation: 0.2188712693954174]
	TIME [epoch: 6 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06667419792223986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06667419792223986 | validation: 0.25413854360736327]
	TIME [epoch: 5.99 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09035540584512385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09035540584512385 | validation: 0.2237561247598987]
	TIME [epoch: 6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1114738138647137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1114738138647137 | validation: 0.27521716602817853]
	TIME [epoch: 5.99 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11824233441923855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11824233441923855 | validation: 0.20627071385351803]
	TIME [epoch: 6.01 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06429815569425917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06429815569425917 | validation: 0.19412732913884045]
	TIME [epoch: 6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03837917332901733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03837917332901733 | validation: 0.19117834221675453]
	TIME [epoch: 6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040001564964305895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040001564964305895 | validation: 0.17785901454381348]
	TIME [epoch: 6 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043740426495223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05043740426495223 | validation: 0.2727378803277653]
	TIME [epoch: 6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08510225625576984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08510225625576984 | validation: 0.23989382531619485]
	TIME [epoch: 5.99 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12005474097031424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12005474097031424 | validation: 0.2845240844420316]
	TIME [epoch: 6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13282640944678065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13282640944678065 | validation: 0.2069179453072937]
	TIME [epoch: 6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08626406322488744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08626406322488744 | validation: 0.19952650648658654]
	TIME [epoch: 6.01 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0424358409935267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0424358409935267 | validation: 0.1783033881975436]
	TIME [epoch: 6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03414289846224326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03414289846224326 | validation: 0.15659078857278913]
	TIME [epoch: 6.01 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038940271627412454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038940271627412454 | validation: 0.22576353202564833]
	TIME [epoch: 6.01 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08473839856876098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08473839856876098 | validation: 0.19665812340843497]
	TIME [epoch: 6.01 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1162810438177097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1162810438177097 | validation: 0.24110002063633784]
	TIME [epoch: 6.01 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1255586937964609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1255586937964609 | validation: 0.19513830113059524]
	TIME [epoch: 6.02 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09601783434955835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09601783434955835 | validation: 0.2053111745719035]
	TIME [epoch: 6 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08975753426938818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08975753426938818 | validation: 0.28616906895575295]
	TIME [epoch: 6.02 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11409952963984439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11409952963984439 | validation: 0.18850392485332296]
	TIME [epoch: 6.01 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299467089247527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07299467089247527 | validation: 0.19580477296147572]
	TIME [epoch: 6.02 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044575156165588936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044575156165588936 | validation: 0.150026686375189]
	TIME [epoch: 6 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04395753378282422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04395753378282422 | validation: 0.18290097964362106]
	TIME [epoch: 6.01 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056124320108602375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056124320108602375 | validation: 0.17691461635675765]
	TIME [epoch: 6 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07444478159937407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07444478159937407 | validation: 0.20186177459591065]
	TIME [epoch: 6.02 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07050873703173954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07050873703173954 | validation: 0.149251201906365]
	TIME [epoch: 6.01 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05824078456316275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05824078456316275 | validation: 0.17909673583612534]
	TIME [epoch: 6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05760992998623106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05760992998623106 | validation: 0.15751282535208364]
	TIME [epoch: 6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06737152276633739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06737152276633739 | validation: 0.22475810094709]
	TIME [epoch: 6.02 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11761996162748808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11761996162748808 | validation: 0.27313526449992737]
	TIME [epoch: 6.01 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16020367983071224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16020367983071224 | validation: 0.19244867001652863]
	TIME [epoch: 6.01 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08339302001582162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08339302001582162 | validation: 0.22196589524825838]
	TIME [epoch: 6 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05320234066705231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05320234066705231 | validation: 0.12763911955799193]
	TIME [epoch: 6.01 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05795602985413408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05795602985413408 | validation: 0.22322169891634364]
	TIME [epoch: 6.01 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07974085226854119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07974085226854119 | validation: 0.14823559512421877]
	TIME [epoch: 6.01 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0645637011169059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0645637011169059 | validation: 0.17958403558583416]
	TIME [epoch: 6.01 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04288759337481705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04288759337481705 | validation: 0.13397295459877878]
	TIME [epoch: 6.01 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0316210343539716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0316210343539716 | validation: 0.1855019246898698]
	TIME [epoch: 6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06007784990587016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06007784990587016 | validation: 0.2022045226927213]
	TIME [epoch: 6.01 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06324685318758247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06324685318758247 | validation: 0.2106539104988189]
	TIME [epoch: 6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08413283338051404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08413283338051404 | validation: 0.22410077151095528]
	TIME [epoch: 6.01 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771503542220366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08771503542220366 | validation: 0.19650704356658577]
	TIME [epoch: 6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08295625433165553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08295625433165553 | validation: 0.1479820259378427]
	TIME [epoch: 6.01 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05884347972366702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05884347972366702 | validation: 0.24943509555769686]
	TIME [epoch: 6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12938491319865691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12938491319865691 | validation: 0.2282354558092032]
	TIME [epoch: 6.01 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12934865989780975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12934865989780975 | validation: 0.25693650163212145]
	TIME [epoch: 6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09008903931794172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09008903931794172 | validation: 0.1656591124611153]
	TIME [epoch: 6.01 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04473949927910807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04473949927910807 | validation: 0.12859664898184878]
	TIME [epoch: 6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03422138980082815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03422138980082815 | validation: 0.13463305898369732]
	TIME [epoch: 6.01 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02235818475152488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02235818475152488 | validation: 0.11198859285224037]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_771.pth
	Model improved!!!
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02187130638635692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02187130638635692 | validation: 0.1279622570156508]
	TIME [epoch: 5.96 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024483525270394005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024483525270394005 | validation: 0.11873482526659251]
	TIME [epoch: 5.97 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04387722773220141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04387722773220141 | validation: 0.238058467939391]
	TIME [epoch: 5.97 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11998756496075057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11998756496075057 | validation: 0.1941208832029362]
	TIME [epoch: 5.97 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15859530934811733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15859530934811733 | validation: 0.2479292434211705]
	TIME [epoch: 5.97 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17595457049921442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17595457049921442 | validation: 0.3225384483257286]
	TIME [epoch: 5.97 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264075817049765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11264075817049765 | validation: 0.1683511224286527]
	TIME [epoch: 5.98 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09083253045070729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09083253045070729 | validation: 0.18300285304286057]
	TIME [epoch: 5.97 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07846071398909785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07846071398909785 | validation: 0.14450468985066509]
	TIME [epoch: 5.98 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0393259295833655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0393259295833655 | validation: 0.19723316762637877]
	TIME [epoch: 5.96 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032882082452137855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032882082452137855 | validation: 0.13728652966644903]
	TIME [epoch: 5.98 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02880219848448335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02880219848448335 | validation: 0.11163441651356663]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028737169356504992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028737169356504992 | validation: 0.1300584136952343]
	TIME [epoch: 5.97 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043995065427699334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043995065427699334 | validation: 0.1537444280642899]
	TIME [epoch: 5.97 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07691843212146264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07691843212146264 | validation: 0.21035896499651333]
	TIME [epoch: 5.97 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12254342498817812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12254342498817812 | validation: 0.2784247932125009]
	TIME [epoch: 5.97 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12027148387584145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12027148387584145 | validation: 0.21430285916540834]
	TIME [epoch: 5.98 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05546031711027171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05546031711027171 | validation: 0.154546958617965]
	TIME [epoch: 5.97 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561508669615794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05561508669615794 | validation: 0.1705965888482749]
	TIME [epoch: 5.98 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07453238111371054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07453238111371054 | validation: 0.20592323011068656]
	TIME [epoch: 5.98 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10169240463694271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10169240463694271 | validation: 0.15616225728212063]
	TIME [epoch: 5.97 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653356304763879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06653356304763879 | validation: 0.2964865684072899]
	TIME [epoch: 5.98 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06017068986124692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06017068986124692 | validation: 0.14085592481311413]
	TIME [epoch: 5.97 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04520299397862679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04520299397862679 | validation: 0.12759922432422452]
	TIME [epoch: 5.97 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049535561119315424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049535561119315424 | validation: 0.16413576259089538]
	TIME [epoch: 5.96 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07402524830662886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07402524830662886 | validation: 0.4654740021494439]
	TIME [epoch: 5.96 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1421821154302335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1421821154302335 | validation: 0.28875989557456405]
	TIME [epoch: 5.96 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08571300925296439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08571300925296439 | validation: 0.14862044073297417]
	TIME [epoch: 5.96 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07086438691555579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07086438691555579 | validation: 0.11003284644053074]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_800.pth
	Model improved!!!
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07256057302578277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07256057302578277 | validation: 0.17784222365329455]
	TIME [epoch: 5.99 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07393479630799955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07393479630799955 | validation: 0.150728745826316]
	TIME [epoch: 5.99 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0654790485892777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0654790485892777 | validation: 0.2056887544798908]
	TIME [epoch: 5.99 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08070881518462407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08070881518462407 | validation: 0.170635194143176]
	TIME [epoch: 6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09203485957435614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09203485957435614 | validation: 0.16065496025327622]
	TIME [epoch: 5.99 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08434312176986347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08434312176986347 | validation: 0.1437282561088916]
	TIME [epoch: 5.99 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396670963317332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05396670963317332 | validation: 0.11305628549292651]
	TIME [epoch: 6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03113950810760345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03113950810760345 | validation: 0.1125542739729434]
	TIME [epoch: 5.99 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031046716103191773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031046716103191773 | validation: 0.13444792387737087]
	TIME [epoch: 6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050692584593982855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050692584593982855 | validation: 0.21798704977233352]
	TIME [epoch: 5.97 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09223376193502186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09223376193502186 | validation: 0.14567774132620745]
	TIME [epoch: 5.98 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08733832379287249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08733832379287249 | validation: 0.1947934965985868]
	TIME [epoch: 5.97 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07441197190193961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07441197190193961 | validation: 0.13633602887567683]
	TIME [epoch: 5.97 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055127160398861105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055127160398861105 | validation: 0.12165877603178164]
	TIME [epoch: 5.97 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047250575626733814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047250575626733814 | validation: 0.12401388603042117]
	TIME [epoch: 5.97 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036866340479230514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036866340479230514 | validation: 0.13524351925312847]
	TIME [epoch: 5.96 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434148170314977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04434148170314977 | validation: 0.16915527583103854]
	TIME [epoch: 5.98 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07470433821766105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07470433821766105 | validation: 0.20719930780824086]
	TIME [epoch: 5.98 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061089385858483565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061089385858483565 | validation: 0.1491079455547474]
	TIME [epoch: 5.96 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06704019731167891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06704019731167891 | validation: 0.15191748799412175]
	TIME [epoch: 5.97 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0739840876483836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0739840876483836 | validation: 0.1616755244555804]
	TIME [epoch: 5.97 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05920517227752581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05920517227752581 | validation: 0.15761641216182026]
	TIME [epoch: 5.97 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056599034260571564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056599034260571564 | validation: 0.16614217774900172]
	TIME [epoch: 5.97 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05758474121326201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05758474121326201 | validation: 0.10407093197872941]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_824.pth
	Model improved!!!
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047385484282172284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047385484282172284 | validation: 0.1126168693629509]
	TIME [epoch: 5.99 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791723896589391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03791723896589391 | validation: 0.117601194769635]
	TIME [epoch: 5.99 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04713610166414666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04713610166414666 | validation: 0.21502855596316733]
	TIME [epoch: 6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11624987445137709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11624987445137709 | validation: 0.2370466805189204]
	TIME [epoch: 5.99 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.153332938494505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.153332938494505 | validation: 0.1915629643830306]
	TIME [epoch: 5.99 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.070648483160429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.070648483160429 | validation: 0.10726201594201808]
	TIME [epoch: 5.99 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03976233644767851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03976233644767851 | validation: 0.1384080844029532]
	TIME [epoch: 5.99 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035508574118971396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035508574118971396 | validation: 0.11464076307308756]
	TIME [epoch: 5.99 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03569162128776762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03569162128776762 | validation: 0.18186188745024423]
	TIME [epoch: 6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14381365499351975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14381365499351975 | validation: 0.2686496557534514]
	TIME [epoch: 6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05973629852224592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05973629852224592 | validation: 0.11991943420483778]
	TIME [epoch: 5.99 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030424088977467022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030424088977467022 | validation: 0.1477308884938593]
	TIME [epoch: 6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02819602060743307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02819602060743307 | validation: 0.8057515004766694]
	TIME [epoch: 6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35953042814265035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35953042814265035 | validation: 0.3199158574603052]
	TIME [epoch: 5.99 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27420179004308315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27420179004308315 | validation: 0.12733485188519]
	TIME [epoch: 6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060105961392822184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060105961392822184 | validation: 0.1547303894554357]
	TIME [epoch: 6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06022685841369214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06022685841369214 | validation: 0.13856177755594434]
	TIME [epoch: 6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05054339238445256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05054339238445256 | validation: 0.18285122612453703]
	TIME [epoch: 6.01 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06177044391250793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06177044391250793 | validation: 0.1482615318931572]
	TIME [epoch: 5.99 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06983932195838598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06983932195838598 | validation: 0.16844483664676246]
	TIME [epoch: 6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06984329134139823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06984329134139823 | validation: 0.15895346369966312]
	TIME [epoch: 6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559025241690843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08559025241690843 | validation: 0.1902744060076682]
	TIME [epoch: 6.01 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08817071621441229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08817071621441229 | validation: 0.12921110699267613]
	TIME [epoch: 6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049145880599652116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049145880599652116 | validation: 0.11434411812112565]
	TIME [epoch: 5.99 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030587790379304744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030587790379304744 | validation: 0.10585376920439221]
	TIME [epoch: 6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026073228649982915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026073228649982915 | validation: 0.11934419485513202]
	TIME [epoch: 6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01915927202472437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01915927202472437 | validation: 0.09824580626190668]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_851.pth
	Model improved!!!
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01802696651298324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01802696651298324 | validation: 0.10042032294837161]
	TIME [epoch: 5.98 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017091848835859937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017091848835859937 | validation: 0.08818885881739356]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_853.pth
	Model improved!!!
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017264892770421866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017264892770421866 | validation: 0.09431186403677021]
	TIME [epoch: 5.99 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020279958019088745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020279958019088745 | validation: 0.11114231653601887]
	TIME [epoch: 5.99 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05415020448321668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05415020448321668 | validation: 0.2747749832901752]
	TIME [epoch: 5.99 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17487775699499764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17487775699499764 | validation: 0.24726532242582594]
	TIME [epoch: 5.98 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18259515286440883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18259515286440883 | validation: 0.2509930547875747]
	TIME [epoch: 5.99 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13702527505383222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13702527505383222 | validation: 0.14065693843814855]
	TIME [epoch: 5.98 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04882935696383962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04882935696383962 | validation: 0.13296519204461704]
	TIME [epoch: 5.99 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06295999140509687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06295999140509687 | validation: 0.1415427840760494]
	TIME [epoch: 5.99 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04262545190391778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04262545190391778 | validation: 0.10199226825069059]
	TIME [epoch: 5.99 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027475405887865657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027475405887865657 | validation: 0.09660756535242891]
	TIME [epoch: 5.99 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017927829131719634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017927829131719634 | validation: 0.11540109174099214]
	TIME [epoch: 5.99 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028662133546677914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028662133546677914 | validation: 0.3308555217748579]
	TIME [epoch: 5.98 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06195537902877653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06195537902877653 | validation: 0.1856733981223928]
	TIME [epoch: 6.01 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057352350408348976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057352350408348976 | validation: 0.18165179972552856]
	TIME [epoch: 5.99 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1052865105830149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1052865105830149 | validation: 0.15217649784527854]
	TIME [epoch: 5.99 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08107569179774711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08107569179774711 | validation: 0.11297055733017809]
	TIME [epoch: 5.99 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04983404517357788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04983404517357788 | validation: 0.1544523939685113]
	TIME [epoch: 6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239385344400903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05239385344400903 | validation: 0.14944256938588593]
	TIME [epoch: 6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04323786720532774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04323786720532774 | validation: 0.12108708779464866]
	TIME [epoch: 6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04608883005479246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04608883005479246 | validation: 0.11910151442594762]
	TIME [epoch: 6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475891798917666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0475891798917666 | validation: 0.17050743147601888]
	TIME [epoch: 5.99 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446947441276046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446947441276046 | validation: 0.17113624654611492]
	TIME [epoch: 5.99 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09523765738496497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09523765738496497 | validation: 0.1872226870298492]
	TIME [epoch: 6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09641661808084845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09641661808084845 | validation: 0.1277225832331128]
	TIME [epoch: 6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053367416682351135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053367416682351135 | validation: 0.11301862375299372]
	TIME [epoch: 6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028974978418553742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028974978418553742 | validation: 1.0503771409020073]
	TIME [epoch: 6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41705236577194726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41705236577194726 | validation: 1.0557065968697195]
	TIME [epoch: 6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3925200909557185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3925200909557185 | validation: 0.950008288815634]
	TIME [epoch: 6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30658368586449863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30658368586449863 | validation: 0.38485689438298043]
	TIME [epoch: 5.99 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19733419259995985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19733419259995985 | validation: 0.4560086356321311]
	TIME [epoch: 6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091590336086786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2091590336086786 | validation: 0.2747288901147466]
	TIME [epoch: 6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14582529263705268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14582529263705268 | validation: 0.2344945141222748]
	TIME [epoch: 5.99 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09687053541867108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09687053541867108 | validation: 0.14645415621669697]
	TIME [epoch: 5.99 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05234027494823987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05234027494823987 | validation: 0.15197259781074943]
	TIME [epoch: 6.01 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050486736973889855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050486736973889855 | validation: 0.10654425862210329]
	TIME [epoch: 5.99 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059391776827033345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059391776827033345 | validation: 0.18622545322262546]
	TIME [epoch: 5.99 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07392425010477494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07392425010477494 | validation: 0.17271150138842772]
	TIME [epoch: 5.99 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0761343727470208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0761343727470208 | validation: 0.2198398811218225]
	TIME [epoch: 6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298422610583144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298422610583144 | validation: 0.11502212234372666]
	TIME [epoch: 5.99 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06709124764992824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06709124764992824 | validation: 0.15876048815115984]
	TIME [epoch: 5.98 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945901180776641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06945901180776641 | validation: 0.09133148030646085]
	TIME [epoch: 5.98 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05077765383580975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05077765383580975 | validation: 0.13506764474339092]
	TIME [epoch: 5.98 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04604158726463884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04604158726463884 | validation: 0.08873925675819977]
	TIME [epoch: 5.98 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033583736935654016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033583736935654016 | validation: 0.10054157094207228]
	TIME [epoch: 5.98 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03744961631789296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03744961631789296 | validation: 0.10503333264266448]
	TIME [epoch: 5.98 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06467258366762108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06467258366762108 | validation: 0.2048315003158626]
	TIME [epoch: 5.98 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11009978700082779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11009978700082779 | validation: 0.35180505595195677]
	TIME [epoch: 5.99 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1068934264252724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1068934264252724 | validation: 0.11933547216185747]
	TIME [epoch: 5.97 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0779997180809932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0779997180809932 | validation: 0.16585859954812385]
	TIME [epoch: 5.97 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08251706062652316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08251706062652316 | validation: 0.10087690259698126]
	TIME [epoch: 5.97 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045794527443122006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045794527443122006 | validation: 0.1547685047027989]
	TIME [epoch: 5.97 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04058576940145936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04058576940145936 | validation: 0.11723100921250845]
	TIME [epoch: 5.96 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03599010912481992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03599010912481992 | validation: 0.13553866714398258]
	TIME [epoch: 5.97 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040278960823522496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040278960823522496 | validation: 0.18818561415190327]
	TIME [epoch: 5.97 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06471765549393116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06471765549393116 | validation: 0.13307540003008206]
	TIME [epoch: 5.98 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06850003866332648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06850003866332648 | validation: 0.2206347516021565]
	TIME [epoch: 5.98 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07504210325675566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07504210325675566 | validation: 0.10719965997210035]
	TIME [epoch: 5.98 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642580620268605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0642580620268605 | validation: 0.12573808738343925]
	TIME [epoch: 5.98 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05174731958144176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05174731958144176 | validation: 0.09099757333197496]
	TIME [epoch: 5.99 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03322110603325106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03322110603325106 | validation: 0.10466839926917111]
	TIME [epoch: 5.99 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02427593289368679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02427593289368679 | validation: 0.0869491984025893]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02883497217077782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02883497217077782 | validation: 0.1921894102889059]
	TIME [epoch: 5.97 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0432602489203291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0432602489203291 | validation: 0.20179470355324025]
	TIME [epoch: 5.97 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477262613000744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07477262613000744 | validation: 0.34112322195202555]
	TIME [epoch: 5.97 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08080075375035005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08080075375035005 | validation: 0.10862675644688863]
	TIME [epoch: 5.99 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04152966555876127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04152966555876127 | validation: 0.09664031043582806]
	TIME [epoch: 5.98 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02812093174776789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02812093174776789 | validation: 0.08800734384459384]
	TIME [epoch: 5.99 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026814993978983744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026814993978983744 | validation: 0.11172673513260459]
	TIME [epoch: 5.99 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03611841364105962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03611841364105962 | validation: 0.12110263712384986]
	TIME [epoch: 5.99 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707444875276894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03707444875276894 | validation: 0.10760889598116341]
	TIME [epoch: 5.98 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044669378816106796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044669378816106796 | validation: 0.12085288960171346]
	TIME [epoch: 5.98 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07646955556079232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07646955556079232 | validation: 0.2879300619644186]
	TIME [epoch: 5.98 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18880057238752915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18880057238752915 | validation: 0.29559966632863016]
	TIME [epoch: 5.99 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16956877826495773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16956877826495773 | validation: 0.32832534989292644]
	TIME [epoch: 5.99 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05776307393946269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05776307393946269 | validation: 0.14186127340068622]
	TIME [epoch: 5.98 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02823943713994206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02823943713994206 | validation: 0.12546107003292387]
	TIME [epoch: 5.98 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03926840974700785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03926840974700785 | validation: 0.0999961386434227]
	TIME [epoch: 5.98 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04111284738828235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04111284738828235 | validation: 0.14149644651831728]
	TIME [epoch: 5.98 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046404693155251454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046404693155251454 | validation: 0.12581056179626746]
	TIME [epoch: 5.99 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04389473922115295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04389473922115295 | validation: 0.11760128007754274]
	TIME [epoch: 5.99 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054097554320563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04054097554320563 | validation: 0.11217403134173701]
	TIME [epoch: 5.99 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055329589959410744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055329589959410744 | validation: 0.20585170972559094]
	TIME [epoch: 5.99 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08110957106850979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08110957106850979 | validation: 0.12014871019615332]
	TIME [epoch: 5.97 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791720760798678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0791720760798678 | validation: 0.1402437869931944]
	TIME [epoch: 5.97 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10138968632634952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10138968632634952 | validation: 0.212033818709023]
	TIME [epoch: 5.96 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05089752667323543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05089752667323543 | validation: 0.13937711699519637]
	TIME [epoch: 5.99 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027695472699407614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027695472699407614 | validation: 0.08848385624860727]
	TIME [epoch: 5.98 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029463686692522608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029463686692522608 | validation: 0.1387795281228635]
	TIME [epoch: 5.98 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05604085092947291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05604085092947291 | validation: 0.22577018603648452]
	TIME [epoch: 5.98 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12928931155730544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12928931155730544 | validation: 0.2955783581399797]
	TIME [epoch: 5.98 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19903942245861975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19903942245861975 | validation: 0.20727088898352913]
	TIME [epoch: 5.99 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06329699542182719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06329699542182719 | validation: 0.16553398190433805]
	TIME [epoch: 5.98 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026497122097362054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026497122097362054 | validation: 0.08422543274313128]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_946.pth
	Model improved!!!
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028076351177631804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028076351177631804 | validation: 0.09783914001795281]
	TIME [epoch: 5.98 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026227852269004332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026227852269004332 | validation: 0.10197533160223034]
	TIME [epoch: 5.99 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028971323573958405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028971323573958405 | validation: 0.15854451596657265]
	TIME [epoch: 5.96 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050819563611011385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050819563611011385 | validation: 0.22210401296977578]
	TIME [epoch: 5.96 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10965791100978271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10965791100978271 | validation: 0.200911882293504]
	TIME [epoch: 5.96 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08930171740833799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08930171740833799 | validation: 0.13588347514405805]
	TIME [epoch: 5.96 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04297847422173205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04297847422173205 | validation: 0.1173456842391326]
	TIME [epoch: 5.96 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013346519861558609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013346519861558609 | validation: 0.09784181605179795]
	TIME [epoch: 5.96 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01598101614859042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01598101614859042 | validation: 0.08866180259554979]
	TIME [epoch: 5.96 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019401955904770445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019401955904770445 | validation: 0.13298445215430818]
	TIME [epoch: 5.97 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02672286859205091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02672286859205091 | validation: 0.10532132258338806]
	TIME [epoch: 5.96 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034474533101556654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034474533101556654 | validation: 0.15645739404557224]
	TIME [epoch: 5.96 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0712881770832382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0712881770832382 | validation: 0.2246737016426691]
	TIME [epoch: 5.96 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16104435181738438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16104435181738438 | validation: 0.17636968924648097]
	TIME [epoch: 5.96 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12498954638058252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12498954638058252 | validation: 0.28850431441073]
	TIME [epoch: 5.97 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19766032969412334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19766032969412334 | validation: 0.28119336784837784]
	TIME [epoch: 5.97 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11126651273968978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11126651273968978 | validation: 0.3569025857875556]
	TIME [epoch: 5.97 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07849049082662791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07849049082662791 | validation: 0.15595604907620997]
	TIME [epoch: 5.97 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039178862407186904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039178862407186904 | validation: 0.12743589917201445]
	TIME [epoch: 5.99 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03599424961178468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03599424961178468 | validation: 0.1649498520688212]
	TIME [epoch: 5.98 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03572241162571327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03572241162571327 | validation: 0.13381918087821532]
	TIME [epoch: 5.98 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024614555198945457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024614555198945457 | validation: 0.1186451262596826]
	TIME [epoch: 5.99 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021510030128967888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021510030128967888 | validation: 0.11650072340393369]
	TIME [epoch: 5.99 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028067672430947176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028067672430947176 | validation: 0.11632979650759663]
	TIME [epoch: 5.99 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03651432511612879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03651432511612879 | validation: 0.14022935648106724]
	TIME [epoch: 5.98 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06701352132273906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06701352132273906 | validation: 0.17745433462473975]
	TIME [epoch: 5.99 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10100619720433854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10100619720433854 | validation: 0.17704772978380953]
	TIME [epoch: 5.99 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09929902203217945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09929902203217945 | validation: 0.12255542738958694]
	TIME [epoch: 5.99 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050066419354106766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050066419354106766 | validation: 0.08929320555655078]
	TIME [epoch: 5.98 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023382315115634444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023382315115634444 | validation: 0.09676966620009862]
	TIME [epoch: 5.99 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020423077952037846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020423077952037846 | validation: 0.0904334942614636]
	TIME [epoch: 5.98 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022937767642425895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022937767642425895 | validation: 0.11905063190019474]
	TIME [epoch: 5.98 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038049200643738484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038049200643738484 | validation: 0.11137442026974967]
	TIME [epoch: 5.98 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060303899068664006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060303899068664006 | validation: 0.14499003124237336]
	TIME [epoch: 5.98 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07270181062532509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07270181062532509 | validation: 0.11522279511039507]
	TIME [epoch: 5.98 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07687918953403257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07687918953403257 | validation: 0.14802105864930817]
	TIME [epoch: 5.99 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05967315849559097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05967315849559097 | validation: 0.13757236392021596]
	TIME [epoch: 5.99 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04740776515010952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04740776515010952 | validation: 0.0931833997378774]
	TIME [epoch: 5.99 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03126642032557857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03126642032557857 | validation: 0.09448545867735281]
	TIME [epoch: 5.98 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137548457942525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04137548457942525 | validation: 0.16290635942736903]
	TIME [epoch: 5.99 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08010678215175165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08010678215175165 | validation: 0.2585661375757414]
	TIME [epoch: 5.99 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543312643438906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07543312643438906 | validation: 0.10851884315206446]
	TIME [epoch: 5.99 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05968598943117504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05968598943117504 | validation: 0.11324253861135919]
	TIME [epoch: 6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04743896902514922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04743896902514922 | validation: 0.10343438955085299]
	TIME [epoch: 5.99 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03486654539723229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03486654539723229 | validation: 0.1120978529964528]
	TIME [epoch: 5.99 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02874806134169976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02874806134169976 | validation: 0.09163353463775846]
	TIME [epoch: 6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022505181217720268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022505181217720268 | validation: 0.09093267709391473]
	TIME [epoch: 5.97 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02742421316242305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02742421316242305 | validation: 0.19991053175882662]
	TIME [epoch: 5.98 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08811005002894888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08811005002894888 | validation: 0.2670254032327799]
	TIME [epoch: 5.98 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08860636314805569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08860636314805569 | validation: 0.15208307514868094]
	TIME [epoch: 5.97 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06158068729341258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06158068729341258 | validation: 0.1130469752349303]
	TIME [epoch: 5.97 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03965540886206649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03965540886206649 | validation: 0.08978105134802125]
	TIME [epoch: 5.97 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029248386270791907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029248386270791907 | validation: 0.09998476924701642]
	TIME [epoch: 5.97 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02476123903516428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02476123903516428 | validation: 0.09706150286396291]
	TIME [epoch: 5.98 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04148873572267405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04148873572267405 | validation: 0.15226672669060104]
	TIME [epoch: 186 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09097914600290409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09097914600290409 | validation: 0.13746323418819328]
	TIME [epoch: 12.7 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08990275855427399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08990275855427399 | validation: 0.12348851343677232]
	TIME [epoch: 12.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03856729159875611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03856729159875611 | validation: 0.07526228615258565]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_1004.pth
	Model improved!!!
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018792667441148228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018792667441148228 | validation: 0.08315204242069135]
	TIME [epoch: 12.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018263655252435233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018263655252435233 | validation: 0.10583899638640991]
	TIME [epoch: 12.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03479803782678696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03479803782678696 | validation: 0.13466528094373378]
	TIME [epoch: 12.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08044288377059938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08044288377059938 | validation: 0.18424588210435644]
	TIME [epoch: 12.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12343727343819538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12343727343819538 | validation: 0.13400048284059998]
	TIME [epoch: 12.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0694772666728494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0694772666728494 | validation: 0.12522522970756678]
	TIME [epoch: 12.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031481528855599905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031481528855599905 | validation: 0.08644385776231213]
	TIME [epoch: 12.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018620478753870216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018620478753870216 | validation: 0.089184709298124]
	TIME [epoch: 12.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02117914248630259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02117914248630259 | validation: 0.08681761114716989]
	TIME [epoch: 12.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02387862266890688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02387862266890688 | validation: 0.1231826818963864]
	TIME [epoch: 12.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033409245386979904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033409245386979904 | validation: 0.10884180079266614]
	TIME [epoch: 12.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03942289978450291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03942289978450291 | validation: 0.13413779924061273]
	TIME [epoch: 12.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03859706334612896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03859706334612896 | validation: 0.10321236638293417]
	TIME [epoch: 12.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05300999808788241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05300999808788241 | validation: 0.11199454204773276]
	TIME [epoch: 12.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05845279044702082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05845279044702082 | validation: 0.09754812249262397]
	TIME [epoch: 12.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04777820353309118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04777820353309118 | validation: 0.09089329359369457]
	TIME [epoch: 12.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034053333906146704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034053333906146704 | validation: 0.10280869984786634]
	TIME [epoch: 12.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05448978252033786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05448978252033786 | validation: 0.14410764117358194]
	TIME [epoch: 12.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09080486183569676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09080486183569676 | validation: 0.16906013230056033]
	TIME [epoch: 12.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0903526758662294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0903526758662294 | validation: 0.2346841870131261]
	TIME [epoch: 12.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07724950559057916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07724950559057916 | validation: 0.11218582933767036]
	TIME [epoch: 12.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055133539467889035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055133539467889035 | validation: 0.0861482669725724]
	TIME [epoch: 12.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023138440899482807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023138440899482807 | validation: 0.094313268727082]
	TIME [epoch: 12.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018461036954855008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018461036954855008 | validation: 0.07934817917539688]
	TIME [epoch: 12.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01732152018140401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01732152018140401 | validation: 0.10974742625652252]
	TIME [epoch: 12.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026979275839396993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026979275839396993 | validation: 0.1106244484315684]
	TIME [epoch: 12.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05282397522652878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05282397522652878 | validation: 0.18096918669309]
	TIME [epoch: 12.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09647827576396774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09647827576396774 | validation: 0.10095769489486779]
	TIME [epoch: 12.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0583190805636507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0583190805636507 | validation: 0.10443032462112356]
	TIME [epoch: 12.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040576688586334855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040576688586334855 | validation: 0.11357825079453421]
	TIME [epoch: 12.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04842578252882146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04842578252882146 | validation: 0.1331177073579166]
	TIME [epoch: 12.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06418506568503292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06418506568503292 | validation: 0.11160736586930647]
	TIME [epoch: 12.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06115643751068573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06115643751068573 | validation: 0.08274031046629765]
	TIME [epoch: 12.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037573672968723576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037573672968723576 | validation: 0.08112993139661169]
	TIME [epoch: 12.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020207137808291035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020207137808291035 | validation: 0.07322274173414779]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_1039.pth
	Model improved!!!
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016115341762607167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016115341762607167 | validation: 0.06931394872643452]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_1040.pth
	Model improved!!!
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0192860078570371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0192860078570371 | validation: 0.09097065387395764]
	TIME [epoch: 12.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04094397656460604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04094397656460604 | validation: 0.12110957771553388]
	TIME [epoch: 12.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05965724301320419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05965724301320419 | validation: 0.12738566977872998]
	TIME [epoch: 12.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06502707895540316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06502707895540316 | validation: 0.12230998177509556]
	TIME [epoch: 12.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122941925172093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05122941925172093 | validation: 0.1296212851470538]
	TIME [epoch: 12.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056613486441514906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056613486441514906 | validation: 0.10690152047863277]
	TIME [epoch: 12.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04347621420656031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04347621420656031 | validation: 0.09590155358605508]
	TIME [epoch: 12.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04881346959429708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04881346959429708 | validation: 0.11751885326854956]
	TIME [epoch: 12.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06592281935995344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06592281935995344 | validation: 0.1668743175378666]
	TIME [epoch: 12.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09269899869298677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09269899869298677 | validation: 0.10287959842919255]
	TIME [epoch: 12.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05716331009631248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05716331009631248 | validation: 0.13364615923554454]
	TIME [epoch: 12.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052996614333893105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052996614333893105 | validation: 0.08373214867316828]
	TIME [epoch: 12.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03856411604585447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03856411604585447 | validation: 0.08642796730739233]
	TIME [epoch: 12.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025578743262389243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025578743262389243 | validation: 0.08430845613584925]
	TIME [epoch: 12.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020134585334503675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020134585334503675 | validation: 0.07310571135404396]
	TIME [epoch: 12.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013495985261026074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013495985261026074 | validation: 0.10645096136324411]
	TIME [epoch: 12.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03248190919026778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03248190919026778 | validation: 0.14305651197313987]
	TIME [epoch: 12.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025287925178540473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025287925178540473 | validation: 0.056426567785347416]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_1058.pth
	Model improved!!!
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020130823249306103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020130823249306103 | validation: 0.1043938839107689]
	TIME [epoch: 12.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057799426418057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04057799426418057 | validation: 0.1342535330288965]
	TIME [epoch: 12.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07695221577419714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07695221577419714 | validation: 0.18605364466337693]
	TIME [epoch: 12.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12664794947394448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12664794947394448 | validation: 0.12741853199627992]
	TIME [epoch: 12.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07883890631284775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07883890631284775 | validation: 0.07092557555949044]
	TIME [epoch: 12.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02353380741471888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02353380741471888 | validation: 0.08937459325545684]
	TIME [epoch: 12.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017489497814946613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017489497814946613 | validation: 0.07990465665576721]
	TIME [epoch: 12.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01699590077467234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01699590077467234 | validation: 0.07199593184261632]
	TIME [epoch: 12.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019169474956379044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019169474956379044 | validation: 0.09436939438770435]
	TIME [epoch: 12.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026564251641461496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026564251641461496 | validation: 0.16782745211831407]
	TIME [epoch: 12.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06170761411924366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06170761411924366 | validation: 0.19166423019463863]
	TIME [epoch: 12.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13345334066163594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13345334066163594 | validation: 0.18332736346359324]
	TIME [epoch: 12.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12112649323296504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12112649323296504 | validation: 0.09550671502792116]
	TIME [epoch: 12.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04479733481430027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04479733481430027 | validation: 0.0742782789318585]
	TIME [epoch: 12.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02055242676241082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02055242676241082 | validation: 0.09718120886404927]
	TIME [epoch: 12.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021021477175698192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021021477175698192 | validation: 0.09196296928826281]
	TIME [epoch: 12.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029728298677168998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029728298677168998 | validation: 0.07958655832674928]
	TIME [epoch: 12.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021378550320468107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021378550320468107 | validation: 0.06412187362105229]
	TIME [epoch: 12.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020927156785464595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020927156785464595 | validation: 0.07942694438343569]
	TIME [epoch: 12.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02901109481547773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02901109481547773 | validation: 0.08379653416861385]
	TIME [epoch: 12.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031556115527408286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031556115527408286 | validation: 0.08195118923145439]
	TIME [epoch: 12.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03338496686155218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03338496686155218 | validation: 0.11546632495867198]
	TIME [epoch: 12.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057689474880508955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057689474880508955 | validation: 0.18185803999052025]
	TIME [epoch: 12.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10304614711527153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10304614711527153 | validation: 0.17576739110254902]
	TIME [epoch: 12.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054186483393945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08054186483393945 | validation: 0.08118951343724873]
	TIME [epoch: 12.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027197046275609586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027197046275609586 | validation: 0.07579738706213324]
	TIME [epoch: 12.7 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019082534151857743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019082534151857743 | validation: 0.07380209727002889]
	TIME [epoch: 12.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023023825521935874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023023825521935874 | validation: 0.09451122130063189]
	TIME [epoch: 12.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04192427903995886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04192427903995886 | validation: 0.14165237536077488]
	TIME [epoch: 12.7 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697899272715011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08697899272715011 | validation: 0.12827112205356858]
	TIME [epoch: 12.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08780728076838765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08780728076838765 | validation: 0.14269610543563488]
	TIME [epoch: 12.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05015541994769732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05015541994769732 | validation: 0.091701605555663]
	TIME [epoch: 12.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027771295861872006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027771295861872006 | validation: 0.0838648615756103]
	TIME [epoch: 12.7 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025857136230619766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025857136230619766 | validation: 0.07653324437373499]
	TIME [epoch: 12.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029581343181976395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029581343181976395 | validation: 0.0960872981799949]
	TIME [epoch: 12.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03296220208661194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03296220208661194 | validation: 0.09698455087878655]
	TIME [epoch: 12.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026797310849927456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026797310849927456 | validation: 0.07305417881607948]
	TIME [epoch: 12.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029631593516828945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029631593516828945 | validation: 0.09362431234577266]
	TIME [epoch: 12.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03801340698468825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03801340698468825 | validation: 0.12273662628759263]
	TIME [epoch: 12.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07747250864917106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07747250864917106 | validation: 0.1038365568177157]
	TIME [epoch: 12.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05719655335305368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05719655335305368 | validation: 0.09660653761517873]
	TIME [epoch: 12.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046347923470188485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046347923470188485 | validation: 0.07805044178262094]
	TIME [epoch: 12.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029707508821610915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029707508821610915 | validation: 0.08611580704983099]
	TIME [epoch: 12.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025635590816300093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025635590816300093 | validation: 0.09571101905116323]
	TIME [epoch: 12.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022184290806069918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022184290806069918 | validation: 0.06900179723430874]
	TIME [epoch: 12.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029439235895789388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029439235895789388 | validation: 0.10407623160972741]
	TIME [epoch: 12.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047177994337533294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047177994337533294 | validation: 0.10004259255008706]
	TIME [epoch: 12.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810021327822155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06810021327822155 | validation: 0.13960761340233702]
	TIME [epoch: 12.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08330353926193772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08330353926193772 | validation: 0.13403131575921484]
	TIME [epoch: 12.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08712156069817198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08712156069817198 | validation: 0.1798625035822179]
	TIME [epoch: 12.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08499200663162423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08499200663162423 | validation: 0.09633997388368994]
	TIME [epoch: 12.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028661906519287947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028661906519287947 | validation: 0.07853097546363508]
	TIME [epoch: 12.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018635861372470072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018635861372470072 | validation: 0.06559222868697528]
	TIME [epoch: 12.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01704129338394333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01704129338394333 | validation: 0.0778617550663411]
	TIME [epoch: 12.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018654885767992388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018654885767992388 | validation: 0.11641077320729193]
	TIME [epoch: 12.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01984700774006231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01984700774006231 | validation: 0.10234793775400948]
	TIME [epoch: 12.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03547603934515189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03547603934515189 | validation: 0.12015135460004532]
	TIME [epoch: 12.7 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823216533069712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823216533069712 | validation: 0.12619161942234963]
	TIME [epoch: 12.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07013338222844796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07013338222844796 | validation: 0.10669746532443031]
	TIME [epoch: 12.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043177797601367765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043177797601367765 | validation: 0.07805197942222022]
	TIME [epoch: 12.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03136162215394036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03136162215394036 | validation: 0.11812096124370915]
	TIME [epoch: 12.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06439886268262976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06439886268262976 | validation: 0.1340863362310228]
	TIME [epoch: 12.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08768207671643434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08768207671643434 | validation: 0.11666719509941732]
	TIME [epoch: 12.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051792125780541584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051792125780541584 | validation: 0.07062226228758149]
	TIME [epoch: 12.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023891802772880864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023891802772880864 | validation: 0.06748058461229635]
	TIME [epoch: 12.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012775690096447953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012775690096447953 | validation: 0.0601223177119426]
	TIME [epoch: 12.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012942689496578093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012942689496578093 | validation: 0.06512833318132386]
	TIME [epoch: 12.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01794486515644739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01794486515644739 | validation: 0.06764578642353757]
	TIME [epoch: 12.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023797483376440477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023797483376440477 | validation: 0.0962183713047909]
	TIME [epoch: 12.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04499838559270867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04499838559270867 | validation: 0.18180420757428095]
	TIME [epoch: 12.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10733490908063453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10733490908063453 | validation: 0.1850670936424435]
	TIME [epoch: 12.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12176052849025566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12176052849025566 | validation: 0.15301440739495184]
	TIME [epoch: 12.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04396263927769245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04396263927769245 | validation: 0.07270177825895945]
	TIME [epoch: 12.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019033050482850703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019033050482850703 | validation: 0.08183309882713688]
	TIME [epoch: 12.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024292975513342384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024292975513342384 | validation: 0.10219724684231846]
	TIME [epoch: 12.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022901551923537707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022901551923537707 | validation: 0.08825145906198001]
	TIME [epoch: 12.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023911690116427527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023911690116427527 | validation: 0.10553142811510091]
	TIME [epoch: 12.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027098092843476508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027098092843476508 | validation: 0.08808528648792238]
	TIME [epoch: 12.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020624153443253617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020624153443253617 | validation: 0.07635136819579935]
	TIME [epoch: 12.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017669317559849552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017669317559849552 | validation: 0.10226249242170456]
	TIME [epoch: 12.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036635736053164225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036635736053164225 | validation: 0.20530112445956317]
	TIME [epoch: 12.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0883497856667667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0883497856667667 | validation: 0.13906078888339152]
	TIME [epoch: 12.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1036683310100198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1036683310100198 | validation: 0.12897229492554044]
	TIME [epoch: 12.7 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06600754198180496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06600754198180496 | validation: 0.06037372722368132]
	TIME [epoch: 12.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029984577400482017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029984577400482017 | validation: 0.07588870466377573]
	TIME [epoch: 12.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022345416841425606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022345416841425606 | validation: 0.08293206403127072]
	TIME [epoch: 12.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029073096910536486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029073096910536486 | validation: 0.09329834185429925]
	TIME [epoch: 12.7 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04800796145016069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04800796145016069 | validation: 0.1337199423983579]
	TIME [epoch: 12.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07592721017765434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07592721017765434 | validation: 0.1612217968104626]
	TIME [epoch: 12.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08099526978080861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08099526978080861 | validation: 0.09681977146083436]
	TIME [epoch: 12.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053427586654336674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053427586654336674 | validation: 0.09336149885266329]
	TIME [epoch: 12.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021625940023057826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021625940023057826 | validation: 0.07969182540961676]
	TIME [epoch: 12.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01656232764374114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01656232764374114 | validation: 0.0713768869851567]
	TIME [epoch: 12.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017414129941525067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017414129941525067 | validation: 0.07929878303010535]
	TIME [epoch: 12.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019447246456274225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019447246456274225 | validation: 0.08672897097862566]
	TIME [epoch: 12.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029387485390954825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029387485390954825 | validation: 0.10016135368494249]
	TIME [epoch: 12.7 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052700705499221005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052700705499221005 | validation: 0.14658805146530993]
	TIME [epoch: 12.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07216313816474228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07216313816474228 | validation: 0.09606155000488314]
	TIME [epoch: 12.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04641819018581551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04641819018581551 | validation: 0.06920927809292407]
	TIME [epoch: 12.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022639360159050353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022639360159050353 | validation: 0.09681699574469255]
	TIME [epoch: 12.7 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021929020301445643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021929020301445643 | validation: 0.08210545278480148]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_123323/states/model_phi1_4b_v_mmd1_1159.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 6901.748 seconds.
