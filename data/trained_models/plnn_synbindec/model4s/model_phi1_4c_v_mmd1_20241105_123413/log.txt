Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1985139181

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.9377623945550075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.9377623945550075 | validation: 4.302185014289678]
	TIME [epoch: 178 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.543082128392583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.543082128392583 | validation: 4.68364397798601]
	TIME [epoch: 2.63 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.850736185512444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.850736185512444 | validation: 3.970430473366716]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.085470800643594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.085470800643594 | validation: 3.8915177653765087]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.040276961821593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.040276961821593 | validation: 3.702401386538317]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8749336407542025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8749336407542025 | validation: 3.4207100987439887]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.598035299866823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.598035299866823 | validation: 2.979938815667427]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.196331187669249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.196331187669249 | validation: 2.5582135701604827]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.765507877042197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.765507877042197 | validation: 2.1690525777284617]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6880447026113816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6880447026113816 | validation: 3.3590992989875463]
	TIME [epoch: 2.62 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.611396278505188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.611396278505188 | validation: 2.7788195375319638]
	TIME [epoch: 2.62 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9843870896201374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9843870896201374 | validation: 1.6362240967286636]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9845868412334824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9845868412334824 | validation: 1.6863316588692532]
	TIME [epoch: 2.6 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0834843317268383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0834843317268383 | validation: 1.3847931412792347]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6483475049381087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6483475049381087 | validation: 1.0803167922998869]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.303291236692986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.303291236692986 | validation: 1.259548081114615]
	TIME [epoch: 2.62 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.357271681798985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.357271681798985 | validation: 1.4794965201170365]
	TIME [epoch: 2.61 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6227312188228729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6227312188228729 | validation: 1.8895342267290447]
	TIME [epoch: 2.62 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.03343201373361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.03343201373361 | validation: 1.1817842944923562]
	TIME [epoch: 2.6 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.358008212058296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.358008212058296 | validation: 1.298026203593562]
	TIME [epoch: 2.6 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5324152818082957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5324152818082957 | validation: 1.1083844781799896]
	TIME [epoch: 2.61 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2750690448978392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2750690448978392 | validation: 1.1077395582594942]
	TIME [epoch: 2.6 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1625724811927303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1625724811927303 | validation: 1.1889547341793152]
	TIME [epoch: 2.61 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4635624356322856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4635624356322856 | validation: 1.192163098970214]
	TIME [epoch: 2.6 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2701728002211459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2701728002211459 | validation: 1.0189857028241611]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2067929396060977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2067929396060977 | validation: 1.0021918320113732]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1206761626862998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1206761626862998 | validation: 1.0292768364748903]
	TIME [epoch: 2.62 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.142273203843157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.142273203843157 | validation: 0.9706450500341894]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1443004243454515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1443004243454515 | validation: 0.9784122799769635]
	TIME [epoch: 2.62 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.130147732487792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.130147732487792 | validation: 0.9618719045514393]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1122764434880605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1122764434880605 | validation: 1.0056745621425873]
	TIME [epoch: 2.62 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0863515550155436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0863515550155436 | validation: 0.9337855940852532]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1128875487120866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1128875487120866 | validation: 1.075009858012945]
	TIME [epoch: 2.62 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1910611049481952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1910611049481952 | validation: 1.0357802757745018]
	TIME [epoch: 2.61 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.169726031674843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.169726031674843 | validation: 1.0165973084879567]
	TIME [epoch: 2.62 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2143784466517225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2143784466517225 | validation: 0.9167934479034234]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0938716333152838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0938716333152838 | validation: 0.9941092260912392]
	TIME [epoch: 2.62 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124913566456123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.124913566456123 | validation: 0.8763530576375571]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0839940205101604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0839940205101604 | validation: 0.8948892066172061]
	TIME [epoch: 2.64 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.046924905488183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.046924905488183 | validation: 0.9249873715258752]
	TIME [epoch: 2.63 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0569774721415532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0569774721415532 | validation: 0.9333412601265589]
	TIME [epoch: 2.65 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0102941009688349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0102941009688349 | validation: 0.9240950855448281]
	TIME [epoch: 2.62 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0003026116531315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0003026116531315 | validation: 0.9048590739607596]
	TIME [epoch: 2.62 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0002779158308415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0002779158308415 | validation: 0.952931419925676]
	TIME [epoch: 2.62 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2130369936198693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2130369936198693 | validation: 0.9608311156598041]
	TIME [epoch: 2.62 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1115696821381835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1115696821381835 | validation: 0.9486923788210273]
	TIME [epoch: 2.62 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1385043358206337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1385043358206337 | validation: 0.9059613200096248]
	TIME [epoch: 2.62 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9784377626244165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9784377626244165 | validation: 0.8343561473505373]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0413087519894122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0413087519894122 | validation: 0.804126353260762]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9741611404139369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9741611404139369 | validation: 0.8325415607559382]
	TIME [epoch: 2.62 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0124440203071985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0124440203071985 | validation: 0.8501874505328408]
	TIME [epoch: 2.62 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9596692616204512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9596692616204512 | validation: 0.8594106197013928]
	TIME [epoch: 2.62 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0927627095572674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0927627095572674 | validation: 0.8819086906615431]
	TIME [epoch: 2.62 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0564583996541432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0564583996541432 | validation: 0.8505520371784089]
	TIME [epoch: 2.62 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0771588218631611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0771588218631611 | validation: 0.8069518771395526]
	TIME [epoch: 2.62 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9348646001236546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9348646001236546 | validation: 0.7965638103818037]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9500321352991091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9500321352991091 | validation: 0.7851691387625778]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9329914962132503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9329914962132503 | validation: 0.8227045774466561]
	TIME [epoch: 2.63 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291303300872431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9291303300872431 | validation: 0.7677396277877127]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9487481484305386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9487481484305386 | validation: 0.776658937509302]
	TIME [epoch: 2.63 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9423066470930346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9423066470930346 | validation: 0.7971794055362473]
	TIME [epoch: 2.62 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.967973412999657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.967973412999657 | validation: 0.7878366315914699]
	TIME [epoch: 2.62 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9224977260333245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9224977260333245 | validation: 0.7562012130561981]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9156064277906905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9156064277906905 | validation: 0.7735258461946076]
	TIME [epoch: 2.63 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9107649678894042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9107649678894042 | validation: 0.7519767993029902]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9067319298567992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9067319298567992 | validation: 0.7626810131643897]
	TIME [epoch: 2.62 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9118561971396628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9118561971396628 | validation: 0.8054816776042655]
	TIME [epoch: 2.63 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606245124172201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9606245124172201 | validation: 0.7105466042829802]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8877197462743251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8877197462743251 | validation: 0.7194619987851398]
	TIME [epoch: 2.63 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919378747785285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8919378747785285 | validation: 0.7495390897030018]
	TIME [epoch: 2.62 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8874157997983193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8874157997983193 | validation: 0.7570735169269697]
	TIME [epoch: 2.62 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9306632051668652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9306632051668652 | validation: 0.755032603074977]
	TIME [epoch: 2.63 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327318261632044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9327318261632044 | validation: 0.7350691968205697]
	TIME [epoch: 2.62 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8817857327940359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8817857327940359 | validation: 0.8613847173507749]
	TIME [epoch: 2.62 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9801758557592908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9801758557592908 | validation: 0.9108346177355809]
	TIME [epoch: 2.62 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0642523936581012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0642523936581012 | validation: 0.7432752113077238]
	TIME [epoch: 2.62 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9100903286213392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9100903286213392 | validation: 0.710056368016637]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881739781846118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.881739781846118 | validation: 0.7901639339912903]
	TIME [epoch: 2.62 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9461639708183108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9461639708183108 | validation: 0.711059027390071]
	TIME [epoch: 2.63 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.863075426719821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.863075426719821 | validation: 0.7295509055541679]
	TIME [epoch: 2.62 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865422937567865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8865422937567865 | validation: 0.7617387354944724]
	TIME [epoch: 2.63 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131533996573614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9131533996573614 | validation: 0.7005215302421741]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.855253556565991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.855253556565991 | validation: 0.740048696176857]
	TIME [epoch: 2.62 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723035006391788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723035006391788 | validation: 0.7183141263317497]
	TIME [epoch: 2.62 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526864896972651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526864896972651 | validation: 0.700905281299505]
	TIME [epoch: 2.63 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850417252628661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.850417252628661 | validation: 0.7960758355264789]
	TIME [epoch: 2.62 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9540841679381171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9540841679381171 | validation: 0.721822850867366]
	TIME [epoch: 2.62 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.84981459265383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.84981459265383 | validation: 0.756596754889065]
	TIME [epoch: 2.63 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9172024373683195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9172024373683195 | validation: 0.7429967268514339]
	TIME [epoch: 2.62 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092754654974143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9092754654974143 | validation: 0.7540943071749072]
	TIME [epoch: 2.62 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007681197738049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9007681197738049 | validation: 0.7385191642114163]
	TIME [epoch: 2.62 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789194954977464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789194954977464 | validation: 0.7147734281820203]
	TIME [epoch: 2.62 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860695439473101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860695439473101 | validation: 0.7280335117984671]
	TIME [epoch: 2.62 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691134836430453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8691134836430453 | validation: 0.707176963749391]
	TIME [epoch: 2.62 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8498094012190387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8498094012190387 | validation: 0.7326957769318695]
	TIME [epoch: 2.62 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533097538459663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533097538459663 | validation: 0.7245347204489949]
	TIME [epoch: 2.63 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8519770690448755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8519770690448755 | validation: 0.6972714365889265]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8514098603584421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8514098603584421 | validation: 0.6944950086257384]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516711452125841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516711452125841 | validation: 0.7459483178850896]
	TIME [epoch: 2.61 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892973999322529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8892973999322529 | validation: 0.7945948745214605]
	TIME [epoch: 2.6 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9971990691300533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9971990691300533 | validation: 0.7556893966455721]
	TIME [epoch: 2.63 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9178465951989558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9178465951989558 | validation: 0.7376871044645478]
	TIME [epoch: 2.62 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885195188144747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885195188144747 | validation: 0.7567266600566969]
	TIME [epoch: 2.63 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9227282211712864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9227282211712864 | validation: 0.6885430932701267]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453789901922193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453789901922193 | validation: 0.7370350244894113]
	TIME [epoch: 2.63 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8908037073422099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908037073422099 | validation: 0.702196535402337]
	TIME [epoch: 2.62 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.870487848217592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.870487848217592 | validation: 0.6999406412924176]
	TIME [epoch: 2.63 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526864589932929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8526864589932929 | validation: 0.7385530767116263]
	TIME [epoch: 2.61 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9060331045958796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9060331045958796 | validation: 0.6891427946933985]
	TIME [epoch: 2.63 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8284673269807545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8284673269807545 | validation: 0.7227524321883433]
	TIME [epoch: 2.62 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426152210895851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426152210895851 | validation: 0.6903257291748613]
	TIME [epoch: 2.63 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358953659197178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358953659197178 | validation: 0.7684507452033209]
	TIME [epoch: 2.62 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9464424470600068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9464424470600068 | validation: 0.6911394534218807]
	TIME [epoch: 2.63 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8428996366516894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8428996366516894 | validation: 0.8521107143660323]
	TIME [epoch: 2.62 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0500100341332677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0500100341332677 | validation: 0.7568425920151908]
	TIME [epoch: 2.63 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9023547800710572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9023547800710572 | validation: 0.7541145906396348]
	TIME [epoch: 2.62 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9325593653774672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9325593653774672 | validation: 0.7054644655212208]
	TIME [epoch: 2.62 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8403704984288765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8403704984288765 | validation: 0.7051830732195388]
	TIME [epoch: 2.62 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495238004889824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8495238004889824 | validation: 0.6968767110601712]
	TIME [epoch: 2.62 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459535010009935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8459535010009935 | validation: 0.6767421140979254]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_120.pth
	Model improved!!!
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370643127403398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8370643127403398 | validation: 0.688929110739332]
	TIME [epoch: 2.63 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339056267321863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8339056267321863 | validation: 0.706775760926077]
	TIME [epoch: 2.62 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885982380089845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885982380089845 | validation: 0.6984417856835492]
	TIME [epoch: 2.62 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579243797650699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8579243797650699 | validation: 0.7574221835621794]
	TIME [epoch: 2.62 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.923779864652844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.923779864652844 | validation: 0.7457128827057447]
	TIME [epoch: 2.62 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9287992571443602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9287992571443602 | validation: 0.6796555620368541]
	TIME [epoch: 2.62 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8293532505510561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8293532505510561 | validation: 0.6894482870999465]
	TIME [epoch: 2.62 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430739504307095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430739504307095 | validation: 0.6691987030375861]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8624944703447329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8624944703447329 | validation: 0.6827296673683385]
	TIME [epoch: 2.62 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273113219082815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8273113219082815 | validation: 0.673750180027372]
	TIME [epoch: 2.61 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8284893734602881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8284893734602881 | validation: 0.7204054458256699]
	TIME [epoch: 2.62 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501509455941876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8501509455941876 | validation: 0.741054341179629]
	TIME [epoch: 2.62 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.891091491672683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.891091491672683 | validation: 0.708598840374466]
	TIME [epoch: 2.62 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729255836023091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729255836023091 | validation: 0.8046361638403792]
	TIME [epoch: 2.62 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0049545703531169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0049545703531169 | validation: 0.6915292464524083]
	TIME [epoch: 2.62 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525419992797172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8525419992797172 | validation: 0.7543638389446294]
	TIME [epoch: 2.61 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077654120988601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9077654120988601 | validation: 0.7122393750279159]
	TIME [epoch: 2.61 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8664996144564779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8664996144564779 | validation: 0.7130838956312973]
	TIME [epoch: 2.61 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681217677329958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681217677329958 | validation: 0.6857340795333015]
	TIME [epoch: 2.62 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8276502382302887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8276502382302887 | validation: 0.7055707632490258]
	TIME [epoch: 2.61 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8406048556537888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8406048556537888 | validation: 0.6804632397171094]
	TIME [epoch: 2.64 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832130174434681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.832130174434681 | validation: 0.6890083621436907]
	TIME [epoch: 2.61 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8325421716042071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8325421716042071 | validation: 0.7540403909447989]
	TIME [epoch: 2.62 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9177723214222505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9177723214222505 | validation: 0.6763748419031224]
	TIME [epoch: 2.61 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528455913929729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528455913929729 | validation: 0.6863530217795996]
	TIME [epoch: 2.63 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358583645360317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358583645360317 | validation: 0.6772252285350138]
	TIME [epoch: 2.62 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8287960681870519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8287960681870519 | validation: 0.6742883418133743]
	TIME [epoch: 2.63 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467259533980738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467259533980738 | validation: 0.7875723683631953]
	TIME [epoch: 2.62 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9998111565814602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9998111565814602 | validation: 0.6724137485085159]
	TIME [epoch: 2.62 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8397168883986735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8397168883986735 | validation: 0.6825958772095433]
	TIME [epoch: 2.61 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8595599394048303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8595599394048303 | validation: 0.7552005123612675]
	TIME [epoch: 2.62 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.972665686370132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.972665686370132 | validation: 0.6889730991849975]
	TIME [epoch: 2.62 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991185464052879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991185464052879 | validation: 0.696479958847165]
	TIME [epoch: 2.62 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656863214489542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8656863214489542 | validation: 0.670357427256907]
	TIME [epoch: 2.62 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393050889794222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8393050889794222 | validation: 0.6894814405345135]
	TIME [epoch: 2.62 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799922924886101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8799922924886101 | validation: 0.6689789536847061]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8224415010496506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8224415010496506 | validation: 0.6762958819197891]
	TIME [epoch: 2.61 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8370942825013182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8370942825013182 | validation: 0.6911764916190407]
	TIME [epoch: 2.61 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8811566784506356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8811566784506356 | validation: 0.6582821238811084]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8421165365161635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8421165365161635 | validation: 0.6930115089730354]
	TIME [epoch: 2.61 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8541612209261814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8541612209261814 | validation: 0.6716048435486112]
	TIME [epoch: 2.62 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453673702171333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453673702171333 | validation: 0.67455705970141]
	TIME [epoch: 2.61 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.838008438624789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.838008438624789 | validation: 0.6743668295479837]
	TIME [epoch: 2.61 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358078733836886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358078733836886 | validation: 0.689998188358588]
	TIME [epoch: 2.61 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8722618058047007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8722618058047007 | validation: 0.7276691835029857]
	TIME [epoch: 2.62 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621791720579626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621791720579626 | validation: 0.6711960045866072]
	TIME [epoch: 2.62 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8167158566281134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8167158566281134 | validation: 0.7058342547668229]
	TIME [epoch: 2.62 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8778450951860649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8778450951860649 | validation: 0.6605674494545135]
	TIME [epoch: 2.61 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8193112482914225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8193112482914225 | validation: 0.7411890053503266]
	TIME [epoch: 2.61 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9779519751221734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9779519751221734 | validation: 0.6865895796093685]
	TIME [epoch: 2.61 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8782557912834407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8782557912834407 | validation: 0.6885267834248124]
	TIME [epoch: 2.61 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650761733227644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8650761733227644 | validation: 0.6620696985162388]
	TIME [epoch: 2.61 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161627913742788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161627913742788 | validation: 0.6574312575136863]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8143887048505037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8143887048505037 | validation: 0.670274653828728]
	TIME [epoch: 2.61 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152116056997838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152116056997838 | validation: 0.6905306162387663]
	TIME [epoch: 2.61 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840804059348088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.840804059348088 | validation: 0.6732108398045716]
	TIME [epoch: 2.61 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349062914091974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8349062914091974 | validation: 0.658689571054115]
	TIME [epoch: 2.6 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8374824778710727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8374824778710727 | validation: 0.7735896558013184]
	TIME [epoch: 2.61 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9875412430468501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9875412430468501 | validation: 0.7211628833413647]
	TIME [epoch: 2.6 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353760654336832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9353760654336832 | validation: 0.6720341728497377]
	TIME [epoch: 2.61 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183263565731518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183263565731518 | validation: 0.6738497917736105]
	TIME [epoch: 2.61 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430852558836907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430852558836907 | validation: 0.6715733283703548]
	TIME [epoch: 2.61 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8444576000794015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8444576000794015 | validation: 0.6785169532508117]
	TIME [epoch: 2.61 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8189959096541597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8189959096541597 | validation: 0.6677935898573384]
	TIME [epoch: 2.61 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8182090670488819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8182090670488819 | validation: 0.6895126132174823]
	TIME [epoch: 2.62 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8567560186010356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8567560186010356 | validation: 0.664857956942096]
	TIME [epoch: 2.61 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393926762875478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8393926762875478 | validation: 0.6727763367578705]
	TIME [epoch: 2.61 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164914455931289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8164914455931289 | validation: 0.6496400464961234]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_188.pth
	Model improved!!!
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117657945010212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8117657945010212 | validation: 0.6868170283909463]
	TIME [epoch: 2.6 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853931387812643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.853931387812643 | validation: 0.7146509015233379]
	TIME [epoch: 2.6 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.83524000360006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.83524000360006 | validation: 0.7422866882686382]
	TIME [epoch: 2.6 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956068360780626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956068360780626 | validation: 0.6742521201042007]
	TIME [epoch: 2.6 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8621707481259899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8621707481259899 | validation: 0.6707546025503552]
	TIME [epoch: 2.6 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.817195016565546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.817195016565546 | validation: 0.7217481166288416]
	TIME [epoch: 2.6 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8524879521570786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8524879521570786 | validation: 0.7049424251610351]
	TIME [epoch: 2.6 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803299432544885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803299432544885 | validation: 0.6813730717613338]
	TIME [epoch: 2.6 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732782080154655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8732782080154655 | validation: 0.6547015942830643]
	TIME [epoch: 2.61 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095518305221162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8095518305221162 | validation: 0.6704652938860804]
	TIME [epoch: 2.6 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8192548742043849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8192548742043849 | validation: 0.6374261679374329]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_199.pth
	Model improved!!!
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152363684185427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152363684185427 | validation: 0.6564200171147342]
	TIME [epoch: 2.63 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024572627674671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8024572627674671 | validation: 0.6514361034192842]
	TIME [epoch: 187 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8054817616189556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8054817616189556 | validation: 0.6505498142379019]
	TIME [epoch: 5.67 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8025384040351247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8025384040351247 | validation: 0.6281485220454354]
	TIME [epoch: 5.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047466912639692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8047466912639692 | validation: 0.6533162895953439]
	TIME [epoch: 5.64 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8008536147836465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8008536147836465 | validation: 0.9076291213077923]
	TIME [epoch: 5.64 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.110086243109205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.110086243109205 | validation: 0.7375036143260538]
	TIME [epoch: 5.65 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896362041674892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.896362041674892 | validation: 0.6628753444227341]
	TIME [epoch: 5.62 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404307929022212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8404307929022212 | validation: 0.71798718890551]
	TIME [epoch: 5.63 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320035712875921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320035712875921 | validation: 0.6624698677777154]
	TIME [epoch: 5.62 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7952924758953199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7952924758953199 | validation: 0.6597007161765819]
	TIME [epoch: 5.63 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8455660402993844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8455660402993844 | validation: 0.6498894480545565]
	TIME [epoch: 5.64 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015507011456907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015507011456907 | validation: 0.6484106889143801]
	TIME [epoch: 5.62 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8157458121442196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8157458121442196 | validation: 0.667106209671553]
	TIME [epoch: 5.63 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093130081363379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093130081363379 | validation: 0.6444723260106985]
	TIME [epoch: 5.63 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7967918851739532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7967918851739532 | validation: 0.6669671997736245]
	TIME [epoch: 5.64 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829421681591693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.829421681591693 | validation: 0.6574393246196881]
	TIME [epoch: 5.65 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221968238398074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221968238398074 | validation: 0.6724925150721437]
	TIME [epoch: 5.68 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8196089192537287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8196089192537287 | validation: 0.7464526581533775]
	TIME [epoch: 5.63 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8745630636900944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8745630636900944 | validation: 0.7078059884012294]
	TIME [epoch: 5.65 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985389380463994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8985389380463994 | validation: 0.6325077040361956]
	TIME [epoch: 5.63 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959215309660277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959215309660277 | validation: 0.684518329299899]
	TIME [epoch: 5.67 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445020732160983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445020732160983 | validation: 0.6502721720848976]
	TIME [epoch: 5.64 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011660999826415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8011660999826415 | validation: 0.6351369426059205]
	TIME [epoch: 5.66 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799706269907878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799706269907878 | validation: 0.6411068294400911]
	TIME [epoch: 5.63 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7977656916778706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7977656916778706 | validation: 0.6418802490616531]
	TIME [epoch: 5.65 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8024407200136091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8024407200136091 | validation: 0.6414340414063245]
	TIME [epoch: 5.64 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986922128024125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7986922128024125 | validation: 0.7996592936640424]
	TIME [epoch: 5.67 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9823085245528898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9823085245528898 | validation: 0.7041710529837953]
	TIME [epoch: 5.67 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857026018515378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8857026018515378 | validation: 0.6512183709250826]
	TIME [epoch: 5.69 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8190899978251686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8190899978251686 | validation: 0.7074773427544498]
	TIME [epoch: 5.67 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381229272323644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381229272323644 | validation: 0.6561761409517173]
	TIME [epoch: 5.65 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7981754365151287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7981754365151287 | validation: 0.656689923323309]
	TIME [epoch: 5.66 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8170774823471486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8170774823471486 | validation: 0.6661283028916762]
	TIME [epoch: 5.64 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8046725150679851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8046725150679851 | validation: 0.6994033695972874]
	TIME [epoch: 5.66 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420519783827206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420519783827206 | validation: 0.6848282014075159]
	TIME [epoch: 5.76 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400809819776284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8400809819776284 | validation: 0.6895527740364703]
	TIME [epoch: 5.67 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440903234121356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8440903234121356 | validation: 0.6680045433328607]
	TIME [epoch: 5.67 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234661323486461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234661323486461 | validation: 0.6380560959378903]
	TIME [epoch: 5.65 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7958145349857142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7958145349857142 | validation: 0.6399530941872529]
	TIME [epoch: 5.63 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7948142927447092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7948142927447092 | validation: 0.6305397395663443]
	TIME [epoch: 5.66 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7937158718627348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7937158718627348 | validation: 0.7053883883288896]
	TIME [epoch: 5.64 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9323215269485879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9323215269485879 | validation: 0.6408880192161681]
	TIME [epoch: 5.66 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109755050508849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109755050508849 | validation: 0.7682946785801376]
	TIME [epoch: 5.66 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470710554584612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9470710554584612 | validation: 0.7133248556708636]
	TIME [epoch: 5.68 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8467234572694667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8467234572694667 | validation: 0.6544367683139536]
	TIME [epoch: 5.64 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244769949177299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244769949177299 | validation: 0.6795668546126358]
	TIME [epoch: 5.64 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8153117327987343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8153117327987343 | validation: 0.6492211532624858]
	TIME [epoch: 5.63 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785715947530978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.785715947530978 | validation: 0.6329192417753542]
	TIME [epoch: 5.63 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799699661063813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799699661063813 | validation: 0.6557401857990256]
	TIME [epoch: 5.63 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7997303214482642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7997303214482642 | validation: 0.6606854891816378]
	TIME [epoch: 5.66 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271617495309576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8271617495309576 | validation: 0.6330376131332114]
	TIME [epoch: 5.65 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.788335869030618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.788335869030618 | validation: 0.6588518947478461]
	TIME [epoch: 5.65 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266742207571335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266742207571335 | validation: 0.6303826236497572]
	TIME [epoch: 5.67 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065066678911446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8065066678911446 | validation: 0.673998248538196]
	TIME [epoch: 5.67 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8333619640987985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8333619640987985 | validation: 0.7179754047084413]
	TIME [epoch: 5.66 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8574019322380895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8574019322380895 | validation: 0.6416463325965767]
	TIME [epoch: 5.67 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.799358043013702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.799358043013702 | validation: 0.6365044099409616]
	TIME [epoch: 5.67 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942705524426603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942705524426603 | validation: 0.6289704222781105]
	TIME [epoch: 5.65 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808220683192415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7808220683192415 | validation: 0.632208991382484]
	TIME [epoch: 5.68 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834714017921998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7834714017921998 | validation: 0.6296330638795864]
	TIME [epoch: 5.62 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943741294643684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7943741294643684 | validation: 0.6398775798032198]
	TIME [epoch: 5.67 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8017231235520503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8017231235520503 | validation: 0.6273522885106138]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_262.pth
	Model improved!!!
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869687868957317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869687868957317 | validation: 0.6390839708846221]
	TIME [epoch: 5.7 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019818785158447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019818785158447 | validation: 0.6786898356858746]
	TIME [epoch: 5.67 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666531419871077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8666531419871077 | validation: 0.6839590651723296]
	TIME [epoch: 5.64 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8497154060509197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8497154060509197 | validation: 0.6523467791403166]
	TIME [epoch: 5.65 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8181365672819871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8181365672819871 | validation: 0.6474911185829536]
	TIME [epoch: 5.65 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7876212218985987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7876212218985987 | validation: 0.6456182683373871]
	TIME [epoch: 5.65 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809455392769846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809455392769846 | validation: 0.6348528260402834]
	TIME [epoch: 5.63 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809155947241786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809155947241786 | validation: 0.6489225909515085]
	TIME [epoch: 5.67 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822470490701862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7822470490701862 | validation: 0.6339563834564694]
	TIME [epoch: 5.63 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.793414088651923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.793414088651923 | validation: 0.6624699957593422]
	TIME [epoch: 5.67 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990077166275836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990077166275836 | validation: 0.729936852818152]
	TIME [epoch: 5.63 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8814389783402892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8814389783402892 | validation: 0.6948581920901479]
	TIME [epoch: 5.66 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655833465880702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8655833465880702 | validation: 0.660177945145455]
	TIME [epoch: 5.63 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7998571566300461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998571566300461 | validation: 0.71060587732096]
	TIME [epoch: 5.67 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666298133939958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8666298133939958 | validation: 0.636009844312598]
	TIME [epoch: 5.66 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7909063457208915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7909063457208915 | validation: 0.6387060156597464]
	TIME [epoch: 5.66 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.814649969746765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.814649969746765 | validation: 0.6552675808720139]
	TIME [epoch: 5.65 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849652792644699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7849652792644699 | validation: 0.6304858865477716]
	TIME [epoch: 5.67 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7921218441448243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7921218441448243 | validation: 0.6442396462692063]
	TIME [epoch: 5.65 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857823634637862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857823634637862 | validation: 0.6462025539066109]
	TIME [epoch: 5.66 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042311609566405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8042311609566405 | validation: 0.6393722729388144]
	TIME [epoch: 5.64 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7882228679801804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7882228679801804 | validation: 0.6405623474086621]
	TIME [epoch: 5.66 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806053749649897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806053749649897 | validation: 0.6588415100035938]
	TIME [epoch: 5.65 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8091429971042883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8091429971042883 | validation: 0.6203306976063188]
	TIME [epoch: 5.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_286.pth
	Model improved!!!
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8011699936580238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8011699936580238 | validation: 0.6157210455251664]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7674957753824952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7674957753824952 | validation: 0.6437287267682585]
	TIME [epoch: 5.66 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037667218688545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8037667218688545 | validation: 0.6697566834698601]
	TIME [epoch: 5.66 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.835648681326868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.835648681326868 | validation: 0.6475870175499582]
	TIME [epoch: 5.66 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996277411828365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7996277411828365 | validation: 0.6808731887637491]
	TIME [epoch: 5.68 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917577612779281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917577612779281 | validation: 0.6462820521324479]
	TIME [epoch: 5.68 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135744342969727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135744342969727 | validation: 0.6427244137483064]
	TIME [epoch: 5.69 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063668048878762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8063668048878762 | validation: 0.6273909894917052]
	TIME [epoch: 5.68 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807112766892832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7807112766892832 | validation: 0.6332237773881013]
	TIME [epoch: 5.67 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7827645816238356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7827645816238356 | validation: 0.6068298124909496]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021957190196577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021957190196577 | validation: 0.6291604128271797]
	TIME [epoch: 5.65 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7721068487820694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7721068487820694 | validation: 0.6196543793030282]
	TIME [epoch: 5.63 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746199046897249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7746199046897249 | validation: 0.6500014187177285]
	TIME [epoch: 5.64 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933468153471351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933468153471351 | validation: 0.6581892260886143]
	TIME [epoch: 5.61 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8159951152230953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8159951152230953 | validation: 0.6428407569917729]
	TIME [epoch: 5.71 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7987336396352949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7987336396352949 | validation: 0.6325456683407694]
	TIME [epoch: 5.67 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8300298943156483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8300298943156483 | validation: 0.6075053716199064]
	TIME [epoch: 5.73 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7668068200692844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7668068200692844 | validation: 0.6125667446119034]
	TIME [epoch: 5.7 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7763135328187457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7763135328187457 | validation: 0.6429665020106357]
	TIME [epoch: 5.72 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7964107046846904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7964107046846904 | validation: 0.7578833400971641]
	TIME [epoch: 5.71 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0589728999872485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0589728999872485 | validation: 0.7802258143162883]
	TIME [epoch: 5.72 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0249295115815555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0249295115815555 | validation: 0.65726350267893]
	TIME [epoch: 5.67 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8749384501281026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8749384501281026 | validation: 0.6627625648246996]
	TIME [epoch: 5.7 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966814263477309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7966814263477309 | validation: 0.6973490340955133]
	TIME [epoch: 5.71 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551860060321357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8551860060321357 | validation: 0.6273814335774784]
	TIME [epoch: 5.72 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891864638197637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7891864638197637 | validation: 0.6557869626836683]
	TIME [epoch: 5.64 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8223508932988435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8223508932988435 | validation: 0.619070775319775]
	TIME [epoch: 5.65 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811859962854572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811859962854572 | validation: 0.610700537561182]
	TIME [epoch: 5.69 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7837954902918122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7837954902918122 | validation: 0.6277472021945765]
	TIME [epoch: 5.65 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7797552344128951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7797552344128951 | validation: 0.6237402555218122]
	TIME [epoch: 5.64 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77393893925495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.77393893925495 | validation: 0.6205407980302798]
	TIME [epoch: 5.67 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015455973081389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015455973081389 | validation: 0.6692649747728624]
	TIME [epoch: 5.66 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163449416958228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8163449416958228 | validation: 0.6259938787064604]
	TIME [epoch: 5.65 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7920625827552206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7920625827552206 | validation: 0.6726876029064432]
	TIME [epoch: 5.65 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8570516435983703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8570516435983703 | validation: 0.6409482369698101]
	TIME [epoch: 5.67 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347869336415363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8347869336415363 | validation: 0.5906486126200902]
	TIME [epoch: 5.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_322.pth
	Model improved!!!
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962008051025007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962008051025007 | validation: 0.6873921648178233]
	TIME [epoch: 5.65 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839958958867964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8839958958867964 | validation: 0.6135399971285191]
	TIME [epoch: 5.62 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7725784443072101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7725784443072101 | validation: 0.6098232629616677]
	TIME [epoch: 5.66 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965250105957411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965250105957411 | validation: 0.6194962158242048]
	TIME [epoch: 5.64 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7689136201918494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7689136201918494 | validation: 0.6261890071342964]
	TIME [epoch: 5.63 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.795788752606945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.795788752606945 | validation: 0.60093378961759]
	TIME [epoch: 5.63 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738681588480608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738681588480608 | validation: 0.6777004684587403]
	TIME [epoch: 5.63 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445954805641678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445954805641678 | validation: 0.6064020623336304]
	TIME [epoch: 5.64 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801211958178539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7801211958178539 | validation: 0.6006540246490588]
	TIME [epoch: 5.65 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678503446153576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7678503446153576 | validation: 0.6002197293104884]
	TIME [epoch: 5.63 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778076086583005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778076086583005 | validation: 0.6030545788784316]
	TIME [epoch: 5.63 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891046404279642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7891046404279642 | validation: 0.6036115274966223]
	TIME [epoch: 5.65 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7790163141187242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7790163141187242 | validation: 0.6120462597109193]
	TIME [epoch: 5.65 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775485964666687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775485964666687 | validation: 0.6214571382720102]
	TIME [epoch: 5.64 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7961025458363669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7961025458363669 | validation: 0.621590730089967]
	TIME [epoch: 5.63 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901108782016519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7901108782016519 | validation: 0.6124940009042354]
	TIME [epoch: 5.66 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8148209376931165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8148209376931165 | validation: 0.6178451946839746]
	TIME [epoch: 5.63 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7794106459614071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7794106459614071 | validation: 0.5983358210175725]
	TIME [epoch: 5.68 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665212370594473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7665212370594473 | validation: 0.6268403794135761]
	TIME [epoch: 5.67 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895791582461698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895791582461698 | validation: 0.6172970814411285]
	TIME [epoch: 5.66 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654786784442357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654786784442357 | validation: 0.641867484840393]
	TIME [epoch: 5.68 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738268301838196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738268301838196 | validation: 0.6161651432438662]
	TIME [epoch: 5.65 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8233417009362946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8233417009362946 | validation: 0.6063577572755023]
	TIME [epoch: 5.67 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7785407293692947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7785407293692947 | validation: 0.6212838938318214]
	TIME [epoch: 5.66 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746725138200224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7746725138200224 | validation: 0.619275850778665]
	TIME [epoch: 5.68 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7940547661689741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7940547661689741 | validation: 0.645862819989233]
	TIME [epoch: 5.68 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980655091576614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980655091576614 | validation: 0.6954314857454172]
	TIME [epoch: 5.66 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9294172614659635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9294172614659635 | validation: 0.6412136590554959]
	TIME [epoch: 5.68 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8668659910878879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8668659910878879 | validation: 0.6122140360901243]
	TIME [epoch: 5.66 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7786829085858624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7786829085858624 | validation: 0.654398321273959]
	TIME [epoch: 5.68 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8236369989586197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8236369989586197 | validation: 0.638054621508445]
	TIME [epoch: 5.68 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7866428439429188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7866428439429188 | validation: 0.6473185605631276]
	TIME [epoch: 5.69 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7915792474284971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7915792474284971 | validation: 0.6143035507520488]
	TIME [epoch: 5.69 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857014370545461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857014370545461 | validation: 0.6094430536057329]
	TIME [epoch: 5.67 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7669086233153991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7669086233153991 | validation: 0.5971092112951545]
	TIME [epoch: 5.65 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7603562145373565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7603562145373565 | validation: 0.6278931682955659]
	TIME [epoch: 5.68 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996385982015406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7996385982015406 | validation: 0.8893210102228748]
	TIME [epoch: 5.71 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0252758877051642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0252758877051642 | validation: 0.8413200065248994]
	TIME [epoch: 5.7 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9611863601325255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9611863601325255 | validation: 0.6579949482038376]
	TIME [epoch: 5.71 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8074597684637819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8074597684637819 | validation: 0.6346054167066746]
	TIME [epoch: 5.68 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7996395779899063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7996395779899063 | validation: 0.6166967679453172]
	TIME [epoch: 5.69 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7676678377101308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7676678377101308 | validation: 0.6288369501365954]
	TIME [epoch: 5.65 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7844347893249924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7844347893249924 | validation: 0.609236129462461]
	TIME [epoch: 5.67 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7891822017195114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7891822017195114 | validation: 0.6309196674491896]
	TIME [epoch: 5.65 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756362075176044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7756362075176044 | validation: 0.6899309057774135]
	TIME [epoch: 5.67 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8400011105050679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8400011105050679 | validation: 0.6539625289591293]
	TIME [epoch: 5.66 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8075561335476051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8075561335476051 | validation: 0.6384455310937679]
	TIME [epoch: 5.66 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803671109922659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803671109922659 | validation: 0.6205018480823649]
	TIME [epoch: 5.7 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7807277984305117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7807277984305117 | validation: 0.6080098722066333]
	TIME [epoch: 5.69 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800402883011214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800402883011214 | validation: 0.6655943216591627]
	TIME [epoch: 5.64 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858470823886702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.858470823886702 | validation: 0.5976154712946611]
	TIME [epoch: 5.65 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684058982186437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684058982186437 | validation: 0.6396293147808181]
	TIME [epoch: 5.68 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.784500031468385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.784500031468385 | validation: 0.6010052542249325]
	TIME [epoch: 5.66 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773892593177902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773892593177902 | validation: 0.6161402913577458]
	TIME [epoch: 5.63 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7601448028787056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7601448028787056 | validation: 0.6409198757830694]
	TIME [epoch: 5.65 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7811082695864059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7811082695864059 | validation: 0.9178133160656808]
	TIME [epoch: 5.65 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0432145689607017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0432145689607017 | validation: 0.8871505526207649]
	TIME [epoch: 5.66 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9929061499098013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9929061499098013 | validation: 0.719786627613821]
	TIME [epoch: 5.68 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8616658834425267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8616658834425267 | validation: 0.613988549907891]
	TIME [epoch: 5.66 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7648241739050053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7648241739050053 | validation: 0.6776727771628996]
	TIME [epoch: 5.67 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199545323425435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8199545323425435 | validation: 0.6127801671646962]
	TIME [epoch: 5.66 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872603287045837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872603287045837 | validation: 0.6356638357777543]
	TIME [epoch: 5.62 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713265642271216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713265642271216 | validation: 0.624268807139217]
	TIME [epoch: 5.65 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7820137809639804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7820137809639804 | validation: 0.6258022531188877]
	TIME [epoch: 5.65 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7833976241361497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7833976241361497 | validation: 0.6289386087847957]
	TIME [epoch: 5.64 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857846306626429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857846306626429 | validation: 0.6364472690086938]
	TIME [epoch: 5.66 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266871147009076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266871147009076 | validation: 0.602724256622829]
	TIME [epoch: 5.64 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759076236311584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759076236311584 | validation: 0.6421470786417595]
	TIME [epoch: 5.66 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8044588353671075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8044588353671075 | validation: 0.6467352712509418]
	TIME [epoch: 5.64 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7797008236425756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7797008236425756 | validation: 0.618418247267801]
	TIME [epoch: 5.66 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038531592638666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8038531592638666 | validation: 0.6041353139650667]
	TIME [epoch: 5.63 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7819747113633225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7819747113633225 | validation: 0.6366292024121523]
	TIME [epoch: 5.66 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7788543699869033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7788543699869033 | validation: 0.639660165777294]
	TIME [epoch: 5.64 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7838387030597553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7838387030597553 | validation: 0.6042078432274772]
	TIME [epoch: 5.64 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767126858291542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.767126858291542 | validation: 0.5993615033162925]
	TIME [epoch: 5.67 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7611790618150898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7611790618150898 | validation: 0.6089419876916251]
	TIME [epoch: 5.66 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651879843902484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651879843902484 | validation: 0.6649841899990921]
	TIME [epoch: 5.69 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7979235768088612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7979235768088612 | validation: 0.6287064002669889]
	TIME [epoch: 5.65 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821053212448384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7821053212448384 | validation: 0.6125285793976109]
	TIME [epoch: 5.65 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680063631325311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7680063631325311 | validation: 0.6220215333657482]
	TIME [epoch: 5.68 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202505615754122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202505615754122 | validation: 0.601393428814946]
	TIME [epoch: 5.68 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654914402132659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654914402132659 | validation: 0.6292710394160572]
	TIME [epoch: 5.65 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7810996642214797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7810996642214797 | validation: 0.6163943464425271]
	TIME [epoch: 5.66 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783337461385246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783337461385246 | validation: 0.6084856373491889]
	TIME [epoch: 5.68 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593173785496174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593173785496174 | validation: 0.6134306450884438]
	TIME [epoch: 5.67 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610548271678574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610548271678574 | validation: 0.6036170635158435]
	TIME [epoch: 5.68 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962232635405129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962232635405129 | validation: 0.5895461646902519]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_409.pth
	Model improved!!!
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758470299354875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758470299354875 | validation: 0.5978187149991196]
	TIME [epoch: 5.64 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536464838617487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536464838617487 | validation: 0.6677858317612805]
	TIME [epoch: 5.66 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8146294790022199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8146294790022199 | validation: 0.6129906099169151]
	TIME [epoch: 5.66 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626088373152311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7626088373152311 | validation: 0.6081250392139099]
	TIME [epoch: 5.66 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7615340259479918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7615340259479918 | validation: 0.6094588726285322]
	TIME [epoch: 5.65 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7746674881727219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7746674881727219 | validation: 0.6031708066044845]
	TIME [epoch: 5.69 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642404292230174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642404292230174 | validation: 0.6212782316923783]
	TIME [epoch: 5.66 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695762392424425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695762392424425 | validation: 0.6200614227438535]
	TIME [epoch: 5.67 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542248789495366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542248789495366 | validation: 0.7160166473891938]
	TIME [epoch: 5.63 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8373641659519616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8373641659519616 | validation: 0.6221828878024187]
	TIME [epoch: 5.66 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619054563702008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619054563702008 | validation: 0.6701153369506094]
	TIME [epoch: 5.64 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689859107601801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8689859107601801 | validation: 0.6314566909178239]
	TIME [epoch: 5.63 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8019845697733788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8019845697733788 | validation: 0.676762946165066]
	TIME [epoch: 5.62 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.79967715398135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.79967715398135 | validation: 0.6378359680660171]
	TIME [epoch: 5.64 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639407547179181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7639407547179181 | validation: 0.6281809552958899]
	TIME [epoch: 5.66 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581612964927388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581612964927388 | validation: 0.6337124879267867]
	TIME [epoch: 5.66 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7641497702008259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7641497702008259 | validation: 0.6107483990718372]
	TIME [epoch: 5.63 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7700510953182997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7700510953182997 | validation: 0.6128050577584914]
	TIME [epoch: 5.65 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7536256317143071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7536256317143071 | validation: 0.6081184459514803]
	TIME [epoch: 5.65 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756954747879384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756954747879384 | validation: 0.615645710885786]
	TIME [epoch: 5.64 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581389723980553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581389723980553 | validation: 0.6022368018689439]
	TIME [epoch: 5.69 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582124682692504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7582124682692504 | validation: 0.6466131189067603]
	TIME [epoch: 5.64 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7856245224620128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7856245224620128 | validation: 0.6113066318076531]
	TIME [epoch: 5.69 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7739353236200771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7739353236200771 | validation: 0.610091838137305]
	TIME [epoch: 5.68 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730259755177502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7730259755177502 | validation: 0.6090074241722031]
	TIME [epoch: 5.66 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551552829522118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7551552829522118 | validation: 0.6003985586457086]
	TIME [epoch: 5.65 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7531353392833449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7531353392833449 | validation: 0.599913473742481]
	TIME [epoch: 5.67 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.761238531980677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.761238531980677 | validation: 0.6426589508154223]
	TIME [epoch: 5.64 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7963738554202192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7963738554202192 | validation: 0.6086357423089842]
	TIME [epoch: 5.64 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597687923072454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597687923072454 | validation: 0.6089424657264085]
	TIME [epoch: 5.66 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572624396855279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572624396855279 | validation: 0.5964040812805411]
	TIME [epoch: 5.67 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564653250396827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564653250396827 | validation: 0.59643655526737]
	TIME [epoch: 5.67 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537582120896318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537582120896318 | validation: 0.6118094876730572]
	TIME [epoch: 5.69 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7715798632032117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7715798632032117 | validation: 0.6128483281793313]
	TIME [epoch: 5.65 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305565371406569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305565371406569 | validation: 0.6107315946566502]
	TIME [epoch: 5.67 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507007556198328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507007556198328 | validation: 0.6363510825299278]
	TIME [epoch: 5.65 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589225739545268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589225739545268 | validation: 0.6051829472184487]
	TIME [epoch: 5.68 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757029964435624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.757029964435624 | validation: 0.5926382024416893]
	TIME [epoch: 5.64 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517896658046607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517896658046607 | validation: 0.6040732976402454]
	TIME [epoch: 5.67 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521104937857374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7521104937857374 | validation: 0.5922659689387284]
	TIME [epoch: 5.64 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518382372697896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518382372697896 | validation: 0.6231805344900212]
	TIME [epoch: 5.66 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681950084209284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7681950084209284 | validation: 0.6487397799333582]
	TIME [epoch: 5.66 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306097596794823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306097596794823 | validation: 0.6343398340371357]
	TIME [epoch: 5.66 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769527821818254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769527821818254 | validation: 0.6180020611474822]
	TIME [epoch: 5.66 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7662637302876658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7662637302876658 | validation: 0.6470485589981234]
	TIME [epoch: 5.67 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8538633941803428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8538633941803428 | validation: 0.6007479792100163]
	TIME [epoch: 5.65 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839851617700824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7839851617700824 | validation: 0.6210323111135609]
	TIME [epoch: 5.65 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537446833775944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537446833775944 | validation: 0.6761487940117787]
	TIME [epoch: 5.65 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.807026857443135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.807026857443135 | validation: 0.6142302754329387]
	TIME [epoch: 5.66 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7776015800889998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7776015800889998 | validation: 0.5947733559101612]
	TIME [epoch: 5.63 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7582646402889486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7582646402889486 | validation: 0.6812663011479501]
	TIME [epoch: 5.68 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8867074091410407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8867074091410407 | validation: 0.6199731361421863]
	TIME [epoch: 5.62 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145974704293443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145974704293443 | validation: 0.5909059961204907]
	TIME [epoch: 5.65 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588948323582619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588948323582619 | validation: 0.6589412976327765]
	TIME [epoch: 5.65 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240970701408321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240970701408321 | validation: 0.6156768128623047]
	TIME [epoch: 5.65 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.744281135925773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.744281135925773 | validation: 0.6016960172792875]
	TIME [epoch: 5.64 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7808546228804799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7808546228804799 | validation: 0.6058637426119168]
	TIME [epoch: 5.68 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562811012684594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562811012684594 | validation: 0.5973431016173617]
	TIME [epoch: 5.64 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598220578846894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7598220578846894 | validation: 0.6102661474216673]
	TIME [epoch: 5.65 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630942684348051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630942684348051 | validation: 0.5861250224177219]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7538404499628224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7538404499628224 | validation: 0.7124621550002456]
	TIME [epoch: 5.67 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8360090851947695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8360090851947695 | validation: 0.6655552098515898]
	TIME [epoch: 5.65 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983703142593297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7983703142593297 | validation: 0.6065438530851588]
	TIME [epoch: 5.64 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7522020279638193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7522020279638193 | validation: 0.6086515217757722]
	TIME [epoch: 5.66 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768600535207576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7768600535207576 | validation: 0.5939619278255924]
	TIME [epoch: 5.66 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7599398456408427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7599398456408427 | validation: 0.6495010060455526]
	TIME [epoch: 5.64 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7861420651809846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7861420651809846 | validation: 0.6268992519212738]
	TIME [epoch: 5.62 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7554706000004108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7554706000004108 | validation: 0.6140646276836765]
	TIME [epoch: 5.63 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455094270242569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7455094270242569 | validation: 0.6054765774262075]
	TIME [epoch: 5.64 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528789121080389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528789121080389 | validation: 0.5957039558920318]
	TIME [epoch: 5.63 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552467424915623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552467424915623 | validation: 0.6027373999347749]
	TIME [epoch: 5.66 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747362815488963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7747362815488963 | validation: 0.5898652269078052]
	TIME [epoch: 5.67 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528213976473606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528213976473606 | validation: 0.6253865591435391]
	TIME [epoch: 5.65 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520782940231123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520782940231123 | validation: 0.6123029321382665]
	TIME [epoch: 5.68 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759263717982051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759263717982051 | validation: 0.637926960520403]
	TIME [epoch: 5.64 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.774249314091625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.774249314091625 | validation: 0.6048655695132051]
	TIME [epoch: 5.68 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7718426882246837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7718426882246837 | validation: 0.5839313180975204]
	TIME [epoch: 5.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_486.pth
	Model improved!!!
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7537458276720349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7537458276720349 | validation: 0.7312454741894372]
	TIME [epoch: 5.68 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551885759164037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8551885759164037 | validation: 0.6929056979016309]
	TIME [epoch: 5.64 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7943716656190505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7943716656190505 | validation: 0.5894684884737158]
	TIME [epoch: 5.65 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528638537958233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528638537958233 | validation: 0.6463135539589135]
	TIME [epoch: 5.67 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.795078414985923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.795078414985923 | validation: 0.5868021676251628]
	TIME [epoch: 5.67 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7552844318491068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7552844318491068 | validation: 0.6098956068037061]
	TIME [epoch: 5.67 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7560689073295568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7560689073295568 | validation: 0.5822404216658051]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754173633474668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754173633474668 | validation: 0.5950467620209884]
	TIME [epoch: 5.68 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7447033182196904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7447033182196904 | validation: 0.6455823933842122]
	TIME [epoch: 5.63 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8666283002335194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8666283002335194 | validation: 0.6682477058130414]
	TIME [epoch: 5.63 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445194154327359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445194154327359 | validation: 0.6355619880857915]
	TIME [epoch: 5.66 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.790606911821089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.790606911821089 | validation: 0.6800409292977074]
	TIME [epoch: 5.67 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8093789614815652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8093789614815652 | validation: 0.6312719772893727]
	TIME [epoch: 5.68 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7564441634303402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7564441634303402 | validation: 0.5926081703344742]
	TIME [epoch: 5.67 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759076914225966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759076914225966 | validation: 0.6021021992419467]
	TIME [epoch: 195 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520876889438972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520876889438972 | validation: 0.5981085259747897]
	TIME [epoch: 12.1 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7576681104739627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7576681104739627 | validation: 0.5899487787332935]
	TIME [epoch: 12 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481774487064067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7481774487064067 | validation: 0.6246757288638386]
	TIME [epoch: 12.1 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533915622203731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7533915622203731 | validation: 0.6068360847386924]
	TIME [epoch: 12 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7556003437127058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7556003437127058 | validation: 0.6091488928785594]
	TIME [epoch: 12.1 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.764152073681894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.764152073681894 | validation: 0.6419270328721954]
	TIME [epoch: 12 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7769774836771557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7769774836771557 | validation: 0.5987119914042308]
	TIME [epoch: 12 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491670553993748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491670553993748 | validation: 0.6033949985200453]
	TIME [epoch: 12 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7675053681445142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7675053681445142 | validation: 0.5915430660281377]
	TIME [epoch: 12.1 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517824345960866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517824345960866 | validation: 0.6285964670974485]
	TIME [epoch: 11.9 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765438727312792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765438727312792 | validation: 0.617220279076578]
	TIME [epoch: 12.1 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7628933612275047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7628933612275047 | validation: 0.6330473727029853]
	TIME [epoch: 12 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8181809195135558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8181809195135558 | validation: 0.583856927826721]
	TIME [epoch: 12.1 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.754770174581211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.754770174581211 | validation: 0.6193949969224222]
	TIME [epoch: 12 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7653710103029113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7653710103029113 | validation: 0.6780483031502031]
	TIME [epoch: 12.1 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8543466520755496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8543466520755496 | validation: 0.6078102319710332]
	TIME [epoch: 12 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8280006092330839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8280006092330839 | validation: 0.6041156617226815]
	TIME [epoch: 12 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728094330285962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7728094330285962 | validation: 0.612577012624167]
	TIME [epoch: 12 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677464314733546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677464314733546 | validation: 0.6296741114710055]
	TIME [epoch: 12 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765557874320277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765557874320277 | validation: 0.593275876515293]
	TIME [epoch: 12 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7724449323095816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7724449323095816 | validation: 0.6179965659189162]
	TIME [epoch: 12.1 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852549716726444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852549716726444 | validation: 0.6028413962716916]
	TIME [epoch: 12.1 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541906113880676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541906113880676 | validation: 0.586171747744735]
	TIME [epoch: 12.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752953182570924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752953182570924 | validation: 0.5944663139524229]
	TIME [epoch: 12.1 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502294255714922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502294255714922 | validation: 0.6216451712262345]
	TIME [epoch: 12.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460746035659644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7460746035659644 | validation: 0.6024523982337562]
	TIME [epoch: 12 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602309981080293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602309981080293 | validation: 0.6206357280055805]
	TIME [epoch: 12 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.777770714460863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.777770714460863 | validation: 0.5725888014789721]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7461473673420251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7461473673420251 | validation: 0.5820731836108916]
	TIME [epoch: 12 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7441591317045567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7441591317045567 | validation: 0.601164340307339]
	TIME [epoch: 12 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7643420334536062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7643420334536062 | validation: 0.613265922135769]
	TIME [epoch: 12.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649976058813063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649976058813063 | validation: 0.723740593689195]
	TIME [epoch: 12 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480395275393347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9480395275393347 | validation: 0.628472340846558]
	TIME [epoch: 12 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8402443024760272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8402443024760272 | validation: 0.5807805872169443]
	TIME [epoch: 12 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7496928920477309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496928920477309 | validation: 0.6425232596243154]
	TIME [epoch: 12 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7957682630475492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7957682630475492 | validation: 0.5787659279896359]
	TIME [epoch: 12 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7681056344106572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7681056344106572 | validation: 0.5859568872289076]
	TIME [epoch: 12 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475578280674823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7475578280674823 | validation: 0.593267800654888]
	TIME [epoch: 12 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598891313385008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7598891313385008 | validation: 0.6274014607705793]
	TIME [epoch: 12 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557602379709988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7557602379709988 | validation: 0.5717861544921304]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406929423629766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7406929423629766 | validation: 0.6408492811991205]
	TIME [epoch: 11.9 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7725085318167425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7725085318167425 | validation: 0.5784583735896657]
	TIME [epoch: 12 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546968519436072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7546968519436072 | validation: 0.5801741429086941]
	TIME [epoch: 11.9 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7559898414981447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7559898414981447 | validation: 0.5948480159638799]
	TIME [epoch: 11.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589667811288305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589667811288305 | validation: 0.5851934741710648]
	TIME [epoch: 11.9 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7594602257494302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7594602257494302 | validation: 0.5688834800570417]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7686786144944285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7686786144944285 | validation: 0.5788143694075094]
	TIME [epoch: 12 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562052048068787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562052048068787 | validation: 0.6530561830136339]
	TIME [epoch: 11.9 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8312651602403296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8312651602403296 | validation: 1.4002306896219134]
	TIME [epoch: 12 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8709549798504839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8709549798504839 | validation: 0.9226607436611354]
	TIME [epoch: 12 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2206833312280756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2206833312280756 | validation: 0.8155045028712724]
	TIME [epoch: 12 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0581186338149389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0581186338149389 | validation: 0.6914419567131486]
	TIME [epoch: 12 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9516819965027938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9516819965027938 | validation: 0.620190759942032]
	TIME [epoch: 12 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8268722939668556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8268722939668556 | validation: 0.59816568927357]
	TIME [epoch: 12 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7617841028284648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7617841028284648 | validation: 0.631267463726688]
	TIME [epoch: 12 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7960130635758103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7960130635758103 | validation: 0.6175565324987942]
	TIME [epoch: 12 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740699050473788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740699050473788 | validation: 0.6322053145922247]
	TIME [epoch: 12 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218843581010339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8218843581010339 | validation: 0.6225125709305637]
	TIME [epoch: 12 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8221153830615231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8221153830615231 | validation: 0.5911350778750025]
	TIME [epoch: 12 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923888844458512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923888844458512 | validation: 0.5834484177888161]
	TIME [epoch: 12 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636992968065015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7636992968065015 | validation: 0.6132803446531541]
	TIME [epoch: 12 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514997511700421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514997511700421 | validation: 0.6463902055321651]
	TIME [epoch: 12 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8244921574445212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8244921574445212 | validation: 0.6615824064087695]
	TIME [epoch: 12 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8473925357330404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8473925357330404 | validation: 0.6489702680967364]
	TIME [epoch: 12 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.797709239691527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.797709239691527 | validation: 0.5871197297219448]
	TIME [epoch: 12 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7649264881865177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7649264881865177 | validation: 0.5955823031748715]
	TIME [epoch: 12 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7639536194387312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7639536194387312 | validation: 0.5731332651952044]
	TIME [epoch: 12 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7527530974645185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7527530974645185 | validation: 0.6517560767565178]
	TIME [epoch: 12 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712237875857437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712237875857437 | validation: 0.6515648324368218]
	TIME [epoch: 12 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588559455869904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588559455869904 | validation: 0.6099930519750076]
	TIME [epoch: 12 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8055458870490761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8055458870490761 | validation: 0.575429983747115]
	TIME [epoch: 12 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485308208998102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485308208998102 | validation: 0.6126206450587106]
	TIME [epoch: 12 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736584307478751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7736584307478751 | validation: 0.6286031407710685]
	TIME [epoch: 12 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806885562673372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806885562673372 | validation: 0.6778241403120596]
	TIME [epoch: 12 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723140353715592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723140353715592 | validation: 0.6266331689450342]
	TIME [epoch: 12 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864265698582853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7864265698582853 | validation: 0.5996049506881411]
	TIME [epoch: 12 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7644089847820217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7644089847820217 | validation: 0.6120708969659121]
	TIME [epoch: 12 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7783457575773269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7783457575773269 | validation: 0.5752477665633335]
	TIME [epoch: 11.9 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7507411041759072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7507411041759072 | validation: 0.6283008220516754]
	TIME [epoch: 12 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860701209946747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7860701209946747 | validation: 0.5870340768593414]
	TIME [epoch: 12 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7613282640451865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7613282640451865 | validation: 0.5838929551550314]
	TIME [epoch: 12 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495581741829724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495581741829724 | validation: 0.5720543681942399]
	TIME [epoch: 11.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518153935537825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518153935537825 | validation: 0.6125705947240466]
	TIME [epoch: 12 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7767599814919188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7767599814919188 | validation: 0.6016901398564293]
	TIME [epoch: 11.9 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7606388324863858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7606388324863858 | validation: 0.5945272891236758]
	TIME [epoch: 12 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472884548510211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472884548510211 | validation: 0.5852850687195453]
	TIME [epoch: 12 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7795901346775034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7795901346775034 | validation: 0.5969963366193505]
	TIME [epoch: 12 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683617178131968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683617178131968 | validation: 0.6097309471728654]
	TIME [epoch: 11.9 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618783316325556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618783316325556 | validation: 0.5897722853283714]
	TIME [epoch: 12 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7542792704870676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7542792704870676 | validation: 0.6077403795283948]
	TIME [epoch: 11.9 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771823876984691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771823876984691 | validation: 0.6088394041975218]
	TIME [epoch: 12 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062842981802618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062842981802618 | validation: 0.6128310166818051]
	TIME [epoch: 11.9 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023794288333781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8023794288333781 | validation: 0.5851437712271447]
	TIME [epoch: 12 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7435548094141086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7435548094141086 | validation: 0.6161996547515738]
	TIME [epoch: 11.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765918774037149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765918774037149 | validation: 0.5911652513355392]
	TIME [epoch: 12 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7587886594072083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7587886594072083 | validation: 0.5924314776100673]
	TIME [epoch: 12 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7607602558191138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7607602558191138 | validation: 0.5675045127223942]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_598.pth
	Model improved!!!
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402127013461623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7402127013461623 | validation: 0.5915032466421124]
	TIME [epoch: 12 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7504043013755378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7504043013755378 | validation: 0.5876989046619399]
	TIME [epoch: 12 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7545008817236947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7545008817236947 | validation: 0.6427187989849908]
	TIME [epoch: 12.1 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8453089426729112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8453089426729112 | validation: 0.6427883066114478]
	TIME [epoch: 12.1 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454942352103008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8454942352103008 | validation: 0.5893769802036019]
	TIME [epoch: 12.1 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.779451376768736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.779451376768736 | validation: 0.6158119571218794]
	TIME [epoch: 12 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581003660822043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581003660822043 | validation: 0.758697617668895]
	TIME [epoch: 12.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.901904539362516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.901904539362516 | validation: 0.830880227999074]
	TIME [epoch: 12.1 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9422087078096274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9422087078096274 | validation: 0.7234840043574275]
	TIME [epoch: 12.1 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8421837137577556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8421837137577556 | validation: 0.5773345172363868]
	TIME [epoch: 12.1 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7555343476132016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7555343476132016 | validation: 0.6126150244227082]
	TIME [epoch: 12.1 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678560475944454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7678560475944454 | validation: 0.585436315849458]
	TIME [epoch: 12.1 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7589030525674636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7589030525674636 | validation: 0.5841656502420142]
	TIME [epoch: 12.1 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439297707473402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439297707473402 | validation: 0.6120543178077137]
	TIME [epoch: 12 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7480393301437268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7480393301437268 | validation: 0.65042213158204]
	TIME [epoch: 12.1 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906583280038979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906583280038979 | validation: 0.6429968810637203]
	TIME [epoch: 12.1 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7825960324932456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7825960324932456 | validation: 0.6177809953660307]
	TIME [epoch: 12.1 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7612336249789587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7612336249789587 | validation: 0.5951463263440137]
	TIME [epoch: 12.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7479132563275872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7479132563275872 | validation: 0.600631055766481]
	TIME [epoch: 12 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.751123640484494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.751123640484494 | validation: 0.5955806116702884]
	TIME [epoch: 12.1 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7496366014730955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496366014730955 | validation: 0.572613417245025]
	TIME [epoch: 12 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456062041169499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7456062041169499 | validation: 0.5983423040446244]
	TIME [epoch: 12.1 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.75887807435168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.75887807435168 | validation: 0.6166593249360074]
	TIME [epoch: 11.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7499614211243983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7499614211243983 | validation: 0.610759854335508]
	TIME [epoch: 12.1 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7978200540529236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7978200540529236 | validation: 0.6030567040464455]
	TIME [epoch: 12 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7623346053248407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7623346053248407 | validation: 0.6503778636861416]
	TIME [epoch: 12.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081592141759291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8081592141759291 | validation: 0.6102245486027854]
	TIME [epoch: 11.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965542727205709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965542727205709 | validation: 0.6066824201518197]
	TIME [epoch: 12.1 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8042899801694333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8042899801694333 | validation: 0.6039378942448305]
	TIME [epoch: 12 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7950786610690266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7950786610690266 | validation: 0.5934706927418212]
	TIME [epoch: 12 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7800018700791503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7800018700791503 | validation: 0.5752908238781788]
	TIME [epoch: 12 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7462145284933854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7462145284933854 | validation: 0.6062875599160163]
	TIME [epoch: 12 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7571383708207611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7571383708207611 | validation: 0.5977470955320138]
	TIME [epoch: 12 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588924707465854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588924707465854 | validation: 0.5826169264785982]
	TIME [epoch: 12 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520035114155976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520035114155976 | validation: 0.5805539217152559]
	TIME [epoch: 12 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412586720370117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412586720370117 | validation: 0.5898280491636843]
	TIME [epoch: 12 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7797359655736804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7797359655736804 | validation: 0.577589852107681]
	TIME [epoch: 11.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7750923377878849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7750923377878849 | validation: 0.5719439028044947]
	TIME [epoch: 12 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7451341582115765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7451341582115765 | validation: 0.5975117455745969]
	TIME [epoch: 11.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771024700500913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771024700500913 | validation: 0.5934380363620523]
	TIME [epoch: 12 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7491465143717589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7491465143717589 | validation: 0.5855872802218846]
	TIME [epoch: 12 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7472534590959861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7472534590959861 | validation: 0.5743443123330824]
	TIME [epoch: 12 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574909360154892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574909360154892 | validation: 0.5869520233997687]
	TIME [epoch: 12 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393583942647151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393583942647151 | validation: 0.5843456977952711]
	TIME [epoch: 12 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419455636036075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7419455636036075 | validation: 0.5825557427338701]
	TIME [epoch: 12 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7396465209964675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7396465209964675 | validation: 0.612834625261025]
	TIME [epoch: 12 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775168970371039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775168970371039 | validation: 0.6152347271571534]
	TIME [epoch: 12 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7695611746768876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7695611746768876 | validation: 0.5953906408838812]
	TIME [epoch: 12 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409591981915518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7409591981915518 | validation: 0.5927520762328585]
	TIME [epoch: 12 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7393265862283913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7393265862283913 | validation: 0.5853083449946388]
	TIME [epoch: 12 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548007245366444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7548007245366444 | validation: 0.6111430313393942]
	TIME [epoch: 12.1 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7645057169871547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7645057169871547 | validation: 0.5882696167115892]
	TIME [epoch: 12 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7412629022487076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7412629022487076 | validation: 0.6331993211663131]
	TIME [epoch: 12 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883361996248343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7883361996248343 | validation: 0.6066503303818482]
	TIME [epoch: 12 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7680731388879142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7680731388879142 | validation: 0.5812197513979525]
	TIME [epoch: 12.1 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454055941982094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7454055941982094 | validation: 0.6123768957785001]
	TIME [epoch: 12 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7734106520109929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7734106520109929 | validation: 0.5922009800527633]
	TIME [epoch: 12 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7596330617596392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7596330617596392 | validation: 0.5963575959749836]
	TIME [epoch: 12 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514628654576339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514628654576339 | validation: 0.5900789390826097]
	TIME [epoch: 12 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7532531424186946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7532531424186946 | validation: 0.5765114577825284]
	TIME [epoch: 12 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7354425981586384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7354425981586384 | validation: 0.5943384904991766]
	TIME [epoch: 12 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7421695327022456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7421695327022456 | validation: 0.594859979189301]
	TIME [epoch: 12.1 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7547669536027791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7547669536027791 | validation: 0.5669173917781943]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_661.pth
	Model improved!!!
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7465333332997176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7465333332997176 | validation: 0.5847486687230669]
	TIME [epoch: 12 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7476539525890732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7476539525890732 | validation: 0.6755496530383348]
	TIME [epoch: 12.1 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9121946938198059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9121946938198059 | validation: 0.7030722524083661]
	TIME [epoch: 12.1 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682853253894473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9682853253894473 | validation: 0.6553155423961684]
	TIME [epoch: 12 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8879416356429757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8879416356429757 | validation: 0.602825609686962]
	TIME [epoch: 12 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7944880404739766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7944880404739766 | validation: 0.5780022360280984]
	TIME [epoch: 12 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749490738421141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749490738421141 | validation: 0.5919497907753614]
	TIME [epoch: 12 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566377987023327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7566377987023327 | validation: 0.6068938783078576]
	TIME [epoch: 12.1 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7456240265774965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7456240265774965 | validation: 0.5939770749582606]
	TIME [epoch: 12 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501111302130536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7501111302130536 | validation: 0.6333537430895815]
	TIME [epoch: 12.1 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8462346821539491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8462346821539491 | validation: 0.6529535779260488]
	TIME [epoch: 12 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8727218387045877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8727218387045877 | validation: 0.5925019273529676]
	TIME [epoch: 12.1 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854858709697871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854858709697871 | validation: 0.5749386142675448]
	TIME [epoch: 12 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498321920870211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7498321920870211 | validation: 0.592571288151551]
	TIME [epoch: 12.1 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437034558931142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437034558931142 | validation: 0.5713288610726291]
	TIME [epoch: 12 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.743884423699955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.743884423699955 | validation: 0.5936365962505306]
	TIME [epoch: 12.1 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430899289736749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7430899289736749 | validation: 0.5782850428901559]
	TIME [epoch: 12 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7374280292453963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7374280292453963 | validation: 0.6048498388884213]
	TIME [epoch: 12 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183197505507479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183197505507479 | validation: 0.6085598947064017]
	TIME [epoch: 12 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336652792566759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336652792566759 | validation: 0.5768856014455415]
	TIME [epoch: 12 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740685511485063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740685511485063 | validation: 0.5741402196982079]
	TIME [epoch: 12 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509946157846039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7509946157846039 | validation: 0.6096275572592623]
	TIME [epoch: 12 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549740310231033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549740310231033 | validation: 0.5981483945483993]
	TIME [epoch: 12.1 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7509886554563096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7509886554563096 | validation: 0.6027626416808264]
	TIME [epoch: 12 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7487476379944374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7487476379944374 | validation: 0.5992346118836421]
	TIME [epoch: 12 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7489852302330243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7489852302330243 | validation: 0.5848420362261049]
	TIME [epoch: 12 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466622093796872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466622093796872 | validation: 0.6080615358137601]
	TIME [epoch: 12 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7638538977423537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7638538977423537 | validation: 0.6249355316045668]
	TIME [epoch: 12 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760750055351268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7760750055351268 | validation: 0.6392067829323812]
	TIME [epoch: 12 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8261353337825585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8261353337825585 | validation: 0.6485337140331928]
	TIME [epoch: 12 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472164327289811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8472164327289811 | validation: 0.6207946329797293]
	TIME [epoch: 12.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070632481701473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8070632481701473 | validation: 0.5974774749197677]
	TIME [epoch: 12 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7594300030906398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7594300030906398 | validation: 0.60874153330762]
	TIME [epoch: 12 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7823543280449198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7823543280449198 | validation: 0.6136057086413165]
	TIME [epoch: 12 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7742392502519513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7742392502519513 | validation: 0.5933053152391355]
	TIME [epoch: 12.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365644872937626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7365644872937626 | validation: 0.5885884363324178]
	TIME [epoch: 12 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7467169029153758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7467169029153758 | validation: 0.606005799296133]
	TIME [epoch: 12.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636046861994578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7636046861994578 | validation: 0.6122087194141757]
	TIME [epoch: 12 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7603285408669795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7603285408669795 | validation: 0.6055378135037458]
	TIME [epoch: 12 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546228870599734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7546228870599734 | validation: 0.5770479099180859]
	TIME [epoch: 12 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7378276902749505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7378276902749505 | validation: 0.5864028351600151]
	TIME [epoch: 12.1 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455525623218798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7455525623218798 | validation: 0.602236654474758]
	TIME [epoch: 12.1 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517800275672277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517800275672277 | validation: 0.6226338529719302]
	TIME [epoch: 12.1 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7651951676876536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7651951676876536 | validation: 0.606094282722871]
	TIME [epoch: 12.1 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762646007124707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762646007124707 | validation: 0.6499234238619715]
	TIME [epoch: 12.1 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7809885636379629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7809885636379629 | validation: 0.646652678826921]
	TIME [epoch: 12.1 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754681933673533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7754681933673533 | validation: 0.5960227755318046]
	TIME [epoch: 12 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7402142744540493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7402142744540493 | validation: 0.5688328518858431]
	TIME [epoch: 12 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7541478764198223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7541478764198223 | validation: 0.6035662018186726]
	TIME [epoch: 12 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518329731139792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518329731139792 | validation: 0.5916930705846565]
	TIME [epoch: 12 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7496800293097384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496800293097384 | validation: 0.5765854494437058]
	TIME [epoch: 12 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7470295922383605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470295922383605 | validation: 0.5826752410603929]
	TIME [epoch: 12.1 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449494721087832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449494721087832 | validation: 0.602836239949525]
	TIME [epoch: 12 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514735001321836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514735001321836 | validation: 0.5851653325782166]
	TIME [epoch: 12.1 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762497859240147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.762497859240147 | validation: 0.6262607712893381]
	TIME [epoch: 12 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320194889952675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320194889952675 | validation: 0.6253168722717265]
	TIME [epoch: 12.1 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8021171987599692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8021171987599692 | validation: 0.5803675705981566]
	TIME [epoch: 12 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7510534069665724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7510534069665724 | validation: 0.6151542266456943]
	TIME [epoch: 12.1 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7484716488478163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7484716488478163 | validation: 0.6357829477914562]
	TIME [epoch: 12 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7908520292075062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7908520292075062 | validation: 0.6078299362632583]
	TIME [epoch: 12.1 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549468991556322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549468991556322 | validation: 0.5875030491217507]
	TIME [epoch: 12 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457874204533359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457874204533359 | validation: 0.5862835462307361]
	TIME [epoch: 12.1 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7516933435958407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7516933435958407 | validation: 0.5858120706433598]
	TIME [epoch: 12.1 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.739527900625831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.739527900625831 | validation: 0.614485809820278]
	TIME [epoch: 12.1 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.775059704002685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.775059704002685 | validation: 0.5826357074109003]
	TIME [epoch: 12.1 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502203183710182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502203183710182 | validation: 0.5892749674818308]
	TIME [epoch: 12.1 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7455177101248361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7455177101248361 | validation: 0.592118586767877]
	TIME [epoch: 12.1 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424626750135483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7424626750135483 | validation: 0.5921468879317835]
	TIME [epoch: 12.1 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959690344686305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959690344686305 | validation: 0.6143443921446113]
	TIME [epoch: 12.1 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886119950287604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886119950287604 | validation: 0.6032781627956343]
	TIME [epoch: 12.1 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432822161091793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7432822161091793 | validation: 0.6373763759620017]
	TIME [epoch: 12.1 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694970472089917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694970472089917 | validation: 0.644451584570674]
	TIME [epoch: 12 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.757698415890037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.757698415890037 | validation: 0.6163579708052945]
	TIME [epoch: 12.1 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7523974544713663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7523974544713663 | validation: 0.6082722848983826]
	TIME [epoch: 12.1 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720209884362054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720209884362054 | validation: 0.5883482167816058]
	TIME [epoch: 12 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7422644572503393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7422644572503393 | validation: 0.6112387038890116]
	TIME [epoch: 12.1 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591306742811895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591306742811895 | validation: 0.6079016109661952]
	TIME [epoch: 12 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7519176656363947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7519176656363947 | validation: 0.5857561297581537]
	TIME [epoch: 12.1 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485187450853857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485187450853857 | validation: 0.5867885616788525]
	TIME [epoch: 12 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7359973462596415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7359973462596415 | validation: 0.6174874326467612]
	TIME [epoch: 12.1 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424311653990097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7424311653990097 | validation: 0.5906365313704696]
	TIME [epoch: 12 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7454898069066133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7454898069066133 | validation: 0.5974775275570866]
	TIME [epoch: 12.1 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7494177047646505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7494177047646505 | validation: 0.5974584567172201]
	TIME [epoch: 12 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424472140613206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7424472140613206 | validation: 0.6200917393378143]
	TIME [epoch: 12.1 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8155965669902818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8155965669902818 | validation: 0.6010750170549042]
	TIME [epoch: 12 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7834022897350054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7834022897350054 | validation: 0.6032476674666493]
	TIME [epoch: 12.1 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332250593477088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7332250593477088 | validation: 0.6248789999453825]
	TIME [epoch: 12 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481840031344561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7481840031344561 | validation: 0.6467253409887042]
	TIME [epoch: 12.1 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7633154772695321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7633154772695321 | validation: 0.638625082721059]
	TIME [epoch: 12 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7496828122310646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7496828122310646 | validation: 0.6268470567556]
	TIME [epoch: 12.1 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740775768761766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.740775768761766 | validation: 0.6196414706341022]
	TIME [epoch: 12 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7367048050729211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7367048050729211 | validation: 0.6322177224066803]
	TIME [epoch: 12.1 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591371648682048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591371648682048 | validation: 0.641763276465013]
	TIME [epoch: 12 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753239668290943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753239668290943 | validation: 0.6089984739319236]
	TIME [epoch: 12 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7382651210328652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7382651210328652 | validation: 0.5838779488839186]
	TIME [epoch: 12 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7485388343729048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7485388343729048 | validation: 0.6089826251684252]
	TIME [epoch: 12.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483546001137192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7483546001137192 | validation: 0.6290303823860184]
	TIME [epoch: 12 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7470289554888998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7470289554888998 | validation: 0.6474371590623584]
	TIME [epoch: 12 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7603309963463345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7603309963463345 | validation: 0.6213057230575225]
	TIME [epoch: 12 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7626581315276504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7626581315276504 | validation: 0.6326078120470937]
	TIME [epoch: 12 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573253023886365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573253023886365 | validation: 0.6401931051445615]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_123413/states/model_phi1_4c_v_mmd1_762.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5966.181 seconds.
