Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1219912402

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.414116247053283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.414116247053283 | validation: 3.536745103200156]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.677736480527374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.677736480527374 | validation: 3.70203774803562]
	TIME [epoch: 0.81 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.443592377550293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.443592377550293 | validation: 3.762666234516745]
	TIME [epoch: 0.711 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.927822838992587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.927822838992587 | validation: 4.94851247445218]
	TIME [epoch: 0.709 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.745738492884133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.745738492884133 | validation: 3.2974419821920637]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4954505788629557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4954505788629557 | validation: 2.940337227627652]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.239219871291672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.239219871291672 | validation: 2.7970316675537825]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1268258345656714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1268258345656714 | validation: 2.676333999518552]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1453775164170823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1453775164170823 | validation: 2.45831216163488]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0184602219273753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0184602219273753 | validation: 1.7641044504247774]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.658381646000488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.658381646000488 | validation: 1.4247567745214729]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4628188944576612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4628188944576612 | validation: 1.3914559604119214]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4209606411258802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4209606411258802 | validation: 1.463446365415113]
	TIME [epoch: 0.715 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6397371312572364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6397371312572364 | validation: 1.3818329927304427]
	TIME [epoch: 0.712 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4417534935095828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4417534935095828 | validation: 1.153364630679216]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2455172104504766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2455172104504766 | validation: 0.9933447977723429]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.201116263048366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.201116263048366 | validation: 1.0357742600809878]
	TIME [epoch: 0.713 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1813386979580915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1813386979580915 | validation: 0.9068464293455523]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.16915768980895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.16915768980895 | validation: 0.9600480606211541]
	TIME [epoch: 0.709 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1677792802260727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1677792802260727 | validation: 0.9031580897255156]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.188439057039591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.188439057039591 | validation: 1.146345242356056]
	TIME [epoch: 0.713 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.26763795468889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.26763795468889 | validation: 0.9304237536901528]
	TIME [epoch: 0.71 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2376644159617527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2376644159617527 | validation: 0.9807714128569679]
	TIME [epoch: 0.71 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2116422980672636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2116422980672636 | validation: 0.8544856152818687]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1425463003508023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1425463003508023 | validation: 0.8342473638570364]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1150588000982034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1150588000982034 | validation: 0.8402180209893078]
	TIME [epoch: 0.717 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1122052438331798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1122052438331798 | validation: 0.8363902411113154]
	TIME [epoch: 0.717 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1091581602195044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1091581602195044 | validation: 0.8113108297160105]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1163618504158581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1163618504158581 | validation: 0.9786384120657315]
	TIME [epoch: 0.715 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.167693247401902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.167693247401902 | validation: 0.8567351077386803]
	TIME [epoch: 0.714 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1740438103094026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1740438103094026 | validation: 1.0188414402860138]
	TIME [epoch: 0.711 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1982956591840452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1982956591840452 | validation: 0.7955740407203944]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.091990549437063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.091990549437063 | validation: 0.747967897903487]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0618450051093422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0618450051093422 | validation: 0.8079072823902526]
	TIME [epoch: 0.714 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0498658234563605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0498658234563605 | validation: 0.7079876955845052]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0386422916514062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0386422916514062 | validation: 0.8176719621786686]
	TIME [epoch: 0.713 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0319327468072792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0319327468072792 | validation: 0.7352067504114678]
	TIME [epoch: 0.711 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.029953024191398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.029953024191398 | validation: 0.7606878309637218]
	TIME [epoch: 0.712 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0434186511831336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0434186511831336 | validation: 1.0618306137779832]
	TIME [epoch: 0.709 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1543984148809159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1543984148809159 | validation: 0.8523077254145003]
	TIME [epoch: 0.71 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1542996596660076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1542996596660076 | validation: 0.9535231038874701]
	TIME [epoch: 0.708 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1596088042653994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1596088042653994 | validation: 0.7939157490881588]
	TIME [epoch: 0.711 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9947445637233757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9947445637233757 | validation: 0.7120615863065439]
	TIME [epoch: 0.715 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9843794463127882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9843794463127882 | validation: 0.7813962666699157]
	TIME [epoch: 0.71 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9827131336887013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9827131336887013 | validation: 0.8424123543260233]
	TIME [epoch: 0.711 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.986567194110136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.986567194110136 | validation: 0.9878416612343653]
	TIME [epoch: 0.709 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9956533075078661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9956533075078661 | validation: 0.7765545217362656]
	TIME [epoch: 0.71 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0444458090212283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0444458090212283 | validation: 1.2651816229037738]
	TIME [epoch: 0.711 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0905997437593502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0905997437593502 | validation: 0.6738931282355272]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9311148815632657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9311148815632657 | validation: 0.8307808042381567]
	TIME [epoch: 0.713 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.935956590483481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.935956590483481 | validation: 0.9295698486034244]
	TIME [epoch: 0.71 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9866003309383874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9866003309383874 | validation: 0.8883826653704837]
	TIME [epoch: 0.711 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9524757891882193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9524757891882193 | validation: 0.7300822500111005]
	TIME [epoch: 0.713 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9688294320273559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9688294320273559 | validation: 0.9514404370082151]
	TIME [epoch: 0.714 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9367008003964681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9367008003964681 | validation: 0.6112052761710023]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.967450550243581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.967450550243581 | validation: 0.9308974226849465]
	TIME [epoch: 0.711 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9070476464420258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9070476464420258 | validation: 0.6773639539260359]
	TIME [epoch: 0.715 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9021059938719579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9021059938719579 | validation: 0.9350396675324621]
	TIME [epoch: 0.715 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9015248964920598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9015248964920598 | validation: 0.7250658370686427]
	TIME [epoch: 0.712 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9014624860197361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9014624860197361 | validation: 0.9730430251213877]
	TIME [epoch: 0.714 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9426799070676374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9426799070676374 | validation: 0.8148468514963986]
	TIME [epoch: 0.714 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9236817079324608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9236817079324608 | validation: 0.8639666481112069]
	TIME [epoch: 0.714 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9797525713564329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9797525713564329 | validation: 0.8435638659158791]
	TIME [epoch: 0.713 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006230810337816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9006230810337816 | validation: 0.6793363843575707]
	TIME [epoch: 0.709 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007594656842145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9007594656842145 | validation: 0.8609619787412378]
	TIME [epoch: 0.718 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8668787827829283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8668787827829283 | validation: 0.6887148493070591]
	TIME [epoch: 0.715 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663431094838441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663431094838441 | validation: 0.7965780926849921]
	TIME [epoch: 0.712 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512473769266049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8512473769266049 | validation: 0.7230314423136757]
	TIME [epoch: 0.709 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8614309693102022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8614309693102022 | validation: 0.8026147978810354]
	TIME [epoch: 0.709 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8817224412165663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8817224412165663 | validation: 0.8731028974965316]
	TIME [epoch: 0.709 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9661166405296752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9661166405296752 | validation: 0.8678100653995451]
	TIME [epoch: 0.71 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9680269353493398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9680269353493398 | validation: 1.0854958295929908]
	TIME [epoch: 0.709 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9938108682616397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9938108682616397 | validation: 0.6430426528920177]
	TIME [epoch: 0.708 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516097408500983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516097408500983 | validation: 0.8004128597629427]
	TIME [epoch: 0.71 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8629039884721691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8629039884721691 | validation: 0.7500584060553204]
	TIME [epoch: 0.712 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9031060837463751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9031060837463751 | validation: 0.8568771165087707]
	TIME [epoch: 0.714 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788054808974697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8788054808974697 | validation: 0.6429634736422922]
	TIME [epoch: 0.714 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885920913720037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8885920913720037 | validation: 0.8851335440909637]
	TIME [epoch: 0.714 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8739643150142757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8739643150142757 | validation: 0.6229043686399859]
	TIME [epoch: 0.711 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9063572978008141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9063572978008141 | validation: 0.883510144040573]
	TIME [epoch: 0.71 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.876974174941373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.876974174941373 | validation: 0.6668836681867106]
	TIME [epoch: 0.708 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525064029661431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8525064029661431 | validation: 0.8353933240329424]
	TIME [epoch: 0.709 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8671627799315055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8671627799315055 | validation: 0.7649117136725189]
	TIME [epoch: 0.709 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8818563654965736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8818563654965736 | validation: 0.8625239761132941]
	TIME [epoch: 0.711 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9803553665572571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9803553665572571 | validation: 0.7949835164726455]
	TIME [epoch: 0.711 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842927725001011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842927725001011 | validation: 0.7305801274535781]
	TIME [epoch: 0.711 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500493734006959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8500493734006959 | validation: 0.6960988291608518]
	TIME [epoch: 0.711 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8335165409329972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8335165409329972 | validation: 0.8073756731208406]
	TIME [epoch: 0.71 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8304827132967049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8304827132967049 | validation: 0.6305541759165473]
	TIME [epoch: 0.715 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481048495175765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8481048495175765 | validation: 0.9112861845562386]
	TIME [epoch: 0.71 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651626127521058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8651626127521058 | validation: 0.6136400355263292]
	TIME [epoch: 0.71 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761624481883805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8761624481883805 | validation: 0.9084592277369086]
	TIME [epoch: 0.708 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8721467470331834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8721467470331834 | validation: 0.6157321908795098]
	TIME [epoch: 0.704 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8479822611629971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8479822611629971 | validation: 0.8082556676428139]
	TIME [epoch: 0.707 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838426456386603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8838426456386603 | validation: 0.9231699125024451]
	TIME [epoch: 0.707 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0399990361551568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0399990361551568 | validation: 0.7246412931743317]
	TIME [epoch: 0.705 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8547074911655784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8547074911655784 | validation: 0.7561835925127792]
	TIME [epoch: 0.705 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266732554279638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266732554279638 | validation: 0.695406641750369]
	TIME [epoch: 0.705 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8070635091784653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8070635091784653 | validation: 0.6708497707014056]
	TIME [epoch: 0.706 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119997428809487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119997428809487 | validation: 0.7541663623985916]
	TIME [epoch: 1.1 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8098124679960546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8098124679960546 | validation: 0.622091832161726]
	TIME [epoch: 0.71 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8217300711829446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8217300711829446 | validation: 0.7968593606474865]
	TIME [epoch: 0.707 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8177122838988106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8177122838988106 | validation: 0.5802630125192038]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8710738841521259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8710738841521259 | validation: 0.9421549591965175]
	TIME [epoch: 0.711 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8909554034726316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8909554034726316 | validation: 0.6140425107640874]
	TIME [epoch: 0.71 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9350465704728318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9350465704728318 | validation: 0.81763095297874]
	TIME [epoch: 0.708 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9047440954748419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9047440954748419 | validation: 1.1358060636823175]
	TIME [epoch: 0.708 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0697888790857366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0697888790857366 | validation: 0.6609715418399867]
	TIME [epoch: 0.707 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8130783619968243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8130783619968243 | validation: 0.6576635007390912]
	TIME [epoch: 0.708 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120269250746501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8120269250746501 | validation: 0.8243570466571764]
	TIME [epoch: 0.708 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8568240893307666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8568240893307666 | validation: 0.745175570940903]
	TIME [epoch: 0.708 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.855033040332204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.855033040332204 | validation: 0.7481911638330678]
	TIME [epoch: 0.713 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8770690524189919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8770690524189919 | validation: 0.7597925541569285]
	TIME [epoch: 0.711 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8418585596107372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8418585596107372 | validation: 0.6974102417357624]
	TIME [epoch: 0.708 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8328520553631111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8328520553631111 | validation: 0.7530882884980572]
	TIME [epoch: 0.707 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8149340231161961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8149340231161961 | validation: 0.7207473279782655]
	TIME [epoch: 0.707 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8321760321959099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8321760321959099 | validation: 0.7103964045193102]
	TIME [epoch: 0.707 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445742381744495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8445742381744495 | validation: 0.9578024983734775]
	TIME [epoch: 0.708 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171831679218105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9171831679218105 | validation: 0.6936345446469778]
	TIME [epoch: 0.707 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320700017912421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320700017912421 | validation: 0.6968483040148418]
	TIME [epoch: 0.708 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264127025734085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8264127025734085 | validation: 0.7859844492263331]
	TIME [epoch: 0.707 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839311167345768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839311167345768 | validation: 0.6068036477731882]
	TIME [epoch: 0.711 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.890821176194054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.890821176194054 | validation: 0.778333385279061]
	TIME [epoch: 0.708 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273479447626021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8273479447626021 | validation: 0.6754244747093742]
	TIME [epoch: 0.711 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7954423630641632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7954423630641632 | validation: 0.6828847811912833]
	TIME [epoch: 0.711 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965380974571912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7965380974571912 | validation: 0.7776642429664578]
	TIME [epoch: 0.708 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179443939274578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8179443939274578 | validation: 0.7286800248183138]
	TIME [epoch: 0.71 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9058070536580072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9058070536580072 | validation: 1.0408526157058244]
	TIME [epoch: 0.711 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9796257373266039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9796257373266039 | validation: 0.7210584339686552]
	TIME [epoch: 0.712 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8212821782851418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8212821782851418 | validation: 0.5669878244681953]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.854938366311002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.854938366311002 | validation: 0.7975918033906363]
	TIME [epoch: 0.71 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806871259790584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806871259790584 | validation: 0.6232775904391843]
	TIME [epoch: 0.713 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773931621176253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7773931621176253 | validation: 0.6439526465293135]
	TIME [epoch: 0.749 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787637061393636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787637061393636 | validation: 0.70752051366789]
	TIME [epoch: 0.711 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7756055542629927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7756055542629927 | validation: 0.5983923891999883]
	TIME [epoch: 0.715 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7839268391434608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7839268391434608 | validation: 0.8250188441781408]
	TIME [epoch: 0.712 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105137600661375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8105137600661375 | validation: 0.6632261127976488]
	TIME [epoch: 0.711 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8532735823501983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8532735823501983 | validation: 0.9581747256671774]
	TIME [epoch: 0.711 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9715800634844723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9715800634844723 | validation: 0.8881695227353164]
	TIME [epoch: 0.712 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0268201002162718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0268201002162718 | validation: 0.6048695749956726]
	TIME [epoch: 0.709 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9435845370560805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9435845370560805 | validation: 0.647692284629013]
	TIME [epoch: 0.711 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7759219336179343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7759219336179343 | validation: 0.750207765705617]
	TIME [epoch: 0.711 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8186582059374814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8186582059374814 | validation: 0.6496838376558263]
	TIME [epoch: 0.712 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8297776083158498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8297776083158498 | validation: 0.6410153855400157]
	TIME [epoch: 0.712 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7889081492565887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7889081492565887 | validation: 0.7534689587387476]
	TIME [epoch: 0.712 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7806468460516208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7806468460516208 | validation: 0.6245435933719692]
	TIME [epoch: 0.711 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785979897339029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.785979897339029 | validation: 0.7492112404564161]
	TIME [epoch: 0.714 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7901349454725539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7901349454725539 | validation: 0.6750302935874557]
	TIME [epoch: 0.714 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199482359360135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8199482359360135 | validation: 0.9077735441478257]
	TIME [epoch: 0.711 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011033450165181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9011033450165181 | validation: 0.7288894670075187]
	TIME [epoch: 0.709 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8742539278402888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8742539278402888 | validation: 0.7208290376059984]
	TIME [epoch: 0.712 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8878971034059242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8878971034059242 | validation: 0.755956443565064]
	TIME [epoch: 0.712 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082176383247714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082176383247714 | validation: 0.5862166749967698]
	TIME [epoch: 0.711 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786322943604366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.786322943604366 | validation: 0.7245622512982808]
	TIME [epoch: 0.711 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7822865558605143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7822865558605143 | validation: 0.5808632468222523]
	TIME [epoch: 0.709 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764601556063218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764601556063218 | validation: 0.7364973611590933]
	TIME [epoch: 0.711 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7710158900156102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7710158900156102 | validation: 0.6059991152828736]
	TIME [epoch: 0.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911560017856152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911560017856152 | validation: 0.7594462692444696]
	TIME [epoch: 0.711 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904072004487886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904072004487886 | validation: 0.6613763224132465]
	TIME [epoch: 0.711 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8262790511762463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8262790511762463 | validation: 0.86959388676298]
	TIME [epoch: 0.713 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9610562636999258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9610562636999258 | validation: 0.8171911128939016]
	TIME [epoch: 0.708 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9043810988376834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043810988376834 | validation: 0.6016517571015254]
	TIME [epoch: 0.71 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461793931006929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461793931006929 | validation: 0.7033139854850085]
	TIME [epoch: 0.711 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7706764547075398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7706764547075398 | validation: 0.6694793424545163]
	TIME [epoch: 0.713 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7597978399429512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7597978399429512 | validation: 0.613708532994639]
	TIME [epoch: 0.708 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573922149403359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573922149403359 | validation: 0.6764385631206068]
	TIME [epoch: 0.709 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7764843828087553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7764843828087553 | validation: 0.6254523031030796]
	TIME [epoch: 0.711 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7803948220256078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7803948220256078 | validation: 0.7394921358210311]
	TIME [epoch: 0.713 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8336860521164041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8336860521164041 | validation: 0.768890712983083]
	TIME [epoch: 0.712 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9452219825398407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9452219825398407 | validation: 0.7198135135617569]
	TIME [epoch: 0.71 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375018419283521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8375018419283521 | validation: 0.9522805168078268]
	TIME [epoch: 0.711 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8670955908111099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8670955908111099 | validation: 0.6325849434526696]
	TIME [epoch: 0.709 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794647214796425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.794647214796425 | validation: 0.6334178382294202]
	TIME [epoch: 0.712 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.753944170550727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.753944170550727 | validation: 0.6773351082386374]
	TIME [epoch: 0.711 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7466406534404294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7466406534404294 | validation: 0.611478798371477]
	TIME [epoch: 0.714 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7549326395391407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7549326395391407 | validation: 0.7148860123217737]
	TIME [epoch: 0.714 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767324740860563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.767324740860563 | validation: 0.5950099135847258]
	TIME [epoch: 0.712 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7689096920366016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7689096920366016 | validation: 0.7548958415244571]
	TIME [epoch: 0.711 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985544809865996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7985544809865996 | validation: 0.7997461613555157]
	TIME [epoch: 0.712 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282421397711129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9282421397711129 | validation: 0.8185371341515341]
	TIME [epoch: 0.711 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0752443198869475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0752443198869475 | validation: 0.6560289199038941]
	TIME [epoch: 0.712 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745468545205412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.745468545205412 | validation: 0.7199602861628196]
	TIME [epoch: 0.711 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358587854198182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358587854198182 | validation: 0.7133669593170332]
	TIME [epoch: 0.711 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8980107447926017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8980107447926017 | validation: 0.6484555071144972]
	TIME [epoch: 0.718 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566729037622288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7566729037622288 | validation: 0.6699420780333214]
	TIME [epoch: 0.711 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746733129121586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746733129121586 | validation: 0.6405382286096549]
	TIME [epoch: 0.711 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7624945742242798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7624945742242798 | validation: 0.648095963423453]
	TIME [epoch: 0.71 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677653155662296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677653155662296 | validation: 0.6946222530887344]
	TIME [epoch: 0.71 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872591900064023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872591900064023 | validation: 0.693192881286096]
	TIME [epoch: 0.709 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8106015004939419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8106015004939419 | validation: 0.8778692175553474]
	TIME [epoch: 0.709 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769905612421218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8769905612421218 | validation: 0.6370552443219781]
	TIME [epoch: 0.707 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081690449576414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8081690449576414 | validation: 0.7249372669310411]
	TIME [epoch: 0.712 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7656985156308288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7656985156308288 | validation: 0.6077586148210753]
	TIME [epoch: 0.711 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7436230120177948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7436230120177948 | validation: 0.6128226725010658]
	TIME [epoch: 0.713 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7395224811419133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7395224811419133 | validation: 0.6562791602143268]
	TIME [epoch: 0.712 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7397865332717959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7397865332717959 | validation: 0.563820038624973]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872642260594183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872642260594183 | validation: 0.8384647845979771]
	TIME [epoch: 0.713 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9278035083602996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9278035083602996 | validation: 0.617384386214744]
	TIME [epoch: 0.713 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9136286102309368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9136286102309368 | validation: 0.6250553279521187]
	TIME [epoch: 0.711 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503350052532496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503350052532496 | validation: 0.849544437133253]
	TIME [epoch: 0.713 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8067864316624381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8067864316624381 | validation: 0.5594202574350413]
	TIME [epoch: 174 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657410886636311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657410886636311 | validation: 0.66323309674493]
	TIME [epoch: 1.41 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286974927568792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7286974927568792 | validation: 0.6302924192169533]
	TIME [epoch: 1.39 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7301783454683003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7301783454683003 | validation: 0.6148518824073728]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433594862905705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433594862905705 | validation: 0.6531048088140788]
	TIME [epoch: 1.4 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540435429716611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7540435429716611 | validation: 0.8057285486054293]
	TIME [epoch: 1.4 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920262334990178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920262334990178 | validation: 0.8015895509773503]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9396867897623291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9396867897623291 | validation: 0.8340542626937586]
	TIME [epoch: 1.4 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8590732520893049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8590732520893049 | validation: 0.5931044337515899]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290099349356666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7290099349356666 | validation: 0.6123071260467232]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7327628056401911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7327628056401911 | validation: 0.6669965621410668]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610919767545642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610919767545642 | validation: 0.651657787578332]
	TIME [epoch: 1.4 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732472959380431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7732472959380431 | validation: 0.6925259182422796]
	TIME [epoch: 1.4 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894114872789157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7894114872789157 | validation: 0.6528009664910598]
	TIME [epoch: 1.4 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7741682528596752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7741682528596752 | validation: 0.6951007454241266]
	TIME [epoch: 1.4 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7732516934217648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7732516934217648 | validation: 0.6481152347247422]
	TIME [epoch: 1.39 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524252703120061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524252703120061 | validation: 0.6554358226272154]
	TIME [epoch: 1.4 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7599239812893377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7599239812893377 | validation: 0.6775219626824076]
	TIME [epoch: 1.4 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7644104819933007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7644104819933007 | validation: 0.6313014314737768]
	TIME [epoch: 1.4 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8029840260357733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8029840260357733 | validation: 0.7010741077253523]
	TIME [epoch: 1.4 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7798919179632773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7798919179632773 | validation: 0.652893145310288]
	TIME [epoch: 1.4 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.785920833463282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.785920833463282 | validation: 0.6496824627410794]
	TIME [epoch: 1.4 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7584100513669076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7584100513669076 | validation: 0.6917111194557161]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528467600717399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7528467600717399 | validation: 0.615235490204507]
	TIME [epoch: 1.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452692916214069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452692916214069 | validation: 0.8271127695831186]
	TIME [epoch: 1.4 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859513819408197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859513819408197 | validation: 0.6011911798289792]
	TIME [epoch: 1.39 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.787210035929228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.787210035929228 | validation: 0.7671582747912646]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756819436857725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756819436857725 | validation: 0.6264538609881074]
	TIME [epoch: 1.4 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7257200650391613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7257200650391613 | validation: 0.5428510095752986]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_229.pth
	Model improved!!!
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7740276500378522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7740276500378522 | validation: 0.7327734919500349]
	TIME [epoch: 1.4 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8291398774949599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8291398774949599 | validation: 0.6199733209888942]
	TIME [epoch: 1.4 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290456957376912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8290456957376912 | validation: 0.6280775614024923]
	TIME [epoch: 1.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7511364628303993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7511364628303993 | validation: 0.7120305612036046]
	TIME [epoch: 1.4 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7446179124560182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7446179124560182 | validation: 0.5834507782746174]
	TIME [epoch: 1.4 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7159757833368204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7159757833368204 | validation: 0.6174714658710478]
	TIME [epoch: 1.4 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252553707537696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7252553707537696 | validation: 0.6124201855832457]
	TIME [epoch: 1.4 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7243764234758889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7243764234758889 | validation: 0.6306643678526902]
	TIME [epoch: 1.4 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773673114354838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773673114354838 | validation: 0.7023110552764789]
	TIME [epoch: 1.4 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.810668937291262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.810668937291262 | validation: 0.6559331408400237]
	TIME [epoch: 1.4 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326912637312409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326912637312409 | validation: 0.6106589502374753]
	TIME [epoch: 1.4 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.731986891207768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.731986891207768 | validation: 0.6483450795537502]
	TIME [epoch: 1.4 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7117935478618845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7117935478618845 | validation: 0.5821168720556982]
	TIME [epoch: 1.4 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7015194846141067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7015194846141067 | validation: 0.6820832202085209]
	TIME [epoch: 1.4 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7146025744217316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7146025744217316 | validation: 0.5563871482118158]
	TIME [epoch: 1.41 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076191302693294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7076191302693294 | validation: 0.6987281545277352]
	TIME [epoch: 1.4 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7166482473817715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7166482473817715 | validation: 0.5422190894818234]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278784200829657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278784200829657 | validation: 0.7607836842983213]
	TIME [epoch: 1.4 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7384084642050796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7384084642050796 | validation: 0.6555617499119889]
	TIME [epoch: 1.4 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7938386443644649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7938386443644649 | validation: 0.7534496828919106]
	TIME [epoch: 1.4 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9641583971252393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9641583971252393 | validation: 0.6817606866161059]
	TIME [epoch: 1.4 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7376112005500602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7376112005500602 | validation: 0.5641488826334333]
	TIME [epoch: 1.4 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832259380816096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6832259380816096 | validation: 0.5669764023806672]
	TIME [epoch: 1.4 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755813628512837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6755813628512837 | validation: 0.6227355075604625]
	TIME [epoch: 1.4 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6912032961692555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6912032961692555 | validation: 0.628046326139809]
	TIME [epoch: 1.4 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6982908437274504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6982908437274504 | validation: 0.6341187360692238]
	TIME [epoch: 1.4 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7383607938086038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7383607938086038 | validation: 0.7027638246333567]
	TIME [epoch: 1.4 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8179543388160991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8179543388160991 | validation: 0.6621069361470951]
	TIME [epoch: 1.4 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7514695834152575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7514695834152575 | validation: 0.6378680202861488]
	TIME [epoch: 1.4 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.717134574918555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.717134574918555 | validation: 0.5883084318747657]
	TIME [epoch: 1.4 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966829983731496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6966829983731496 | validation: 0.515504729532096]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6919087448846443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6919087448846443 | validation: 0.659758533354653]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906644243068655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6906644243068655 | validation: 0.5787604585384593]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6731723645137373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6731723645137373 | validation: 0.549141085643677]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6792431935991619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6792431935991619 | validation: 0.6146217042576223]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946252663677857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946252663677857 | validation: 0.6795755454093568]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801483912606991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7801483912606991 | validation: 0.7251044888377347]
	TIME [epoch: 1.4 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8010066172141319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8010066172141319 | validation: 0.5685192674526212]
	TIME [epoch: 1.4 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7105510533823939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7105510533823939 | validation: 0.5039933044635654]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_268.pth
	Model improved!!!
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6524339916706581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6524339916706581 | validation: 0.6339983078762831]
	TIME [epoch: 1.4 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6774894394808996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6774894394808996 | validation: 0.5454354374391165]
	TIME [epoch: 1.4 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6735709216608183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6735709216608183 | validation: 0.4958773181173177]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_271.pth
	Model improved!!!
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6561850943649862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6561850943649862 | validation: 0.6259672439064832]
	TIME [epoch: 1.4 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6708592935390008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6708592935390008 | validation: 0.5664094840083573]
	TIME [epoch: 1.4 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6702529031014818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6702529031014818 | validation: 0.5448727871396347]
	TIME [epoch: 1.4 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7129973699679849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7129973699679849 | validation: 0.7232449389733608]
	TIME [epoch: 1.4 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7427694727162505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7427694727162505 | validation: 0.5547909296888828]
	TIME [epoch: 1.39 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.699656633925089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.699656633925089 | validation: 0.49367893193010914]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_277.pth
	Model improved!!!
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6613497332643533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6613497332643533 | validation: 0.6016087092712419]
	TIME [epoch: 1.39 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427479500174971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427479500174971 | validation: 0.48536307173269516]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_279.pth
	Model improved!!!
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6277738589074767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6277738589074767 | validation: 0.5825631435196018]
	TIME [epoch: 1.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6349509439058699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6349509439058699 | validation: 0.5219926306195236]
	TIME [epoch: 1.39 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.651657074466095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.651657074466095 | validation: 0.5080120539762756]
	TIME [epoch: 1.39 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.651382777501127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.651382777501127 | validation: 0.5724894187291699]
	TIME [epoch: 1.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330910234117514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6330910234117514 | validation: 0.5026844857500862]
	TIME [epoch: 1.39 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6359886108184688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6359886108184688 | validation: 0.5570352756378899]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6400254803488354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6400254803488354 | validation: 0.5485137551506885]
	TIME [epoch: 1.39 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6996861652576931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6996861652576931 | validation: 0.6151988901893689]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630394888815769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630394888815769 | validation: 0.5130553172184297]
	TIME [epoch: 1.39 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667912578240786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.667912578240786 | validation: 0.6024372131552519]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161791367289265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6161791367289265 | validation: 0.5146033393789157]
	TIME [epoch: 1.4 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6179841599986275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6179841599986275 | validation: 0.45669737817484013]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234835974568033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6234835974568033 | validation: 0.5950310022721368]
	TIME [epoch: 1.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6120827255373869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6120827255373869 | validation: 0.4869890822508638]
	TIME [epoch: 1.4 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6260486683502507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6260486683502507 | validation: 0.5191518283717753]
	TIME [epoch: 1.4 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6097880486587702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6097880486587702 | validation: 0.5163082011185689]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592736801508082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592736801508082 | validation: 0.5055662257069944]
	TIME [epoch: 1.39 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5894373775786031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5894373775786031 | validation: 0.5242673118089194]
	TIME [epoch: 1.39 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6138696572714103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6138696572714103 | validation: 0.49289976951787084]
	TIME [epoch: 1.38 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6264580551051587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6264580551051587 | validation: 0.5806401191344629]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6080035706520304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6080035706520304 | validation: 0.4455944833870617]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6043316602717053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6043316602717053 | validation: 0.5760258599819694]
	TIME [epoch: 1.39 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5888121488727466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5888121488727466 | validation: 0.46023595479836604]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5899181158090067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5899181158090067 | validation: 0.47716141571678483]
	TIME [epoch: 1.4 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5638719393565088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5638719393565088 | validation: 0.4958236920166524]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5458473486514391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5458473486514391 | validation: 0.4104915129860349]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5303195743544962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5303195743544962 | validation: 0.4844541871543456]
	TIME [epoch: 1.4 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326448385329261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5326448385329261 | validation: 0.4200450623615491]
	TIME [epoch: 1.4 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5734333883727504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5734333883727504 | validation: 0.7541837636376754]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7334702735704052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7334702735704052 | validation: 0.5011913244528818]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6441513506787939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6441513506787939 | validation: 0.38281593625542615]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5228495915419898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5228495915419898 | validation: 0.5808844283165072]
	TIME [epoch: 1.38 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5624800713163053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5624800713163053 | validation: 0.4449269190428579]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5700383703452208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5700383703452208 | validation: 0.45565477405527527]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5350793264506435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5350793264506435 | validation: 0.4472754390063786]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.499432567118229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.499432567118229 | validation: 0.461668276283356]
	TIME [epoch: 1.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4737767552648514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4737767552648514 | validation: 0.3591695380258192]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46680517159262636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46680517159262636 | validation: 0.47559935193052444]
	TIME [epoch: 1.4 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45275335269819095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45275335269819095 | validation: 0.34594275788667805]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47239511314325555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47239511314325555 | validation: 0.7494294311925093]
	TIME [epoch: 1.39 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6883014597101732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6883014597101732 | validation: 0.6513105951521261]
	TIME [epoch: 1.4 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7640875834633175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7640875834633175 | validation: 0.4436383951408356]
	TIME [epoch: 1.39 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5941180183196659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5941180183196659 | validation: 0.6581844994790457]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6631093756761591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6631093756761591 | validation: 0.3948935080963703]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4386436373066927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4386436373066927 | validation: 0.39470121540982867]
	TIME [epoch: 1.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44869200481357047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44869200481357047 | validation: 0.4434599644368395]
	TIME [epoch: 1.4 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45156748688178594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45156748688178594 | validation: 0.39305912172094426]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4256372408118047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4256372408118047 | validation: 0.4295155962414232]
	TIME [epoch: 1.4 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42097458994078535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42097458994078535 | validation: 0.4176132272799617]
	TIME [epoch: 1.39 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453162784051297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.453162784051297 | validation: 0.5203414573586562]
	TIME [epoch: 1.39 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5140386759159445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5140386759159445 | validation: 0.40384313004238903]
	TIME [epoch: 1.39 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5660065903244669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5660065903244669 | validation: 0.4403281951837572]
	TIME [epoch: 1.39 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37821443961311757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37821443961311757 | validation: 0.3764820929113011]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35330496830703584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35330496830703584 | validation: 0.3666599801834551]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3601608349357122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3601608349357122 | validation: 0.44204169944794913]
	TIME [epoch: 1.4 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4151611459128599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4151611459128599 | validation: 0.5161775446453354]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6138371353883736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6138371353883736 | validation: 0.3392688641314057]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3183912614415194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3183912614415194 | validation: 0.494365379010668]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39908813357277406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39908813357277406 | validation: 0.4805253040609877]
	TIME [epoch: 1.4 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6507607042358646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6507607042358646 | validation: 0.36319404791500265]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3005301350636914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3005301350636914 | validation: 0.5448631007808736]
	TIME [epoch: 1.39 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4721742123512266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4721742123512266 | validation: 0.41554492345261357]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441424343204345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5441424343204345 | validation: 0.4353986535128682]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.338611909661458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.338611909661458 | validation: 0.4478548592207493]
	TIME [epoch: 1.39 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4027037709535935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4027037709535935 | validation: 0.35454861531981874]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3925001881116685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3925001881116685 | validation: 0.4236078981784843]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31073330645032143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31073330645032143 | validation: 0.3482311963319701]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2821258333519795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2821258333519795 | validation: 0.3459997098671928]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740105416039467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2740105416039467 | validation: 0.3787963817528443]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3786004517217383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3786004517217383 | validation: 0.47439781569734685]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6177821030469964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6177821030469964 | validation: 0.3295591928633711]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25164302856135634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25164302856135634 | validation: 0.5495527220039994]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4260292986471926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4260292986471926 | validation: 0.4441552474118294]
	TIME [epoch: 1.4 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.589355593720533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.589355593720533 | validation: 0.38329872726142356]
	TIME [epoch: 1.4 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3059287502009604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3059287502009604 | validation: 0.6522157482593762]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604507109745362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.604507109745362 | validation: 0.285072550927206]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2892929940496588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2892929940496588 | validation: 0.38826286435022284]
	TIME [epoch: 1.39 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28037709589103926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28037709589103926 | validation: 0.42029176561534726]
	TIME [epoch: 1.4 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35186340871934746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35186340871934746 | validation: 0.3652677512115839]
	TIME [epoch: 1.4 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43028117552273537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43028117552273537 | validation: 0.4080638446848843]
	TIME [epoch: 1.39 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2612470064585115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2612470064585115 | validation: 0.28147406157626026]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24200670709332492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24200670709332492 | validation: 0.3812340532136088]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24095023883400654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24095023883400654 | validation: 0.29701617516283346]
	TIME [epoch: 1.4 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24385604501420102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24385604501420102 | validation: 0.3671005371339342]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3339785370701286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3339785370701286 | validation: 0.663518856013324]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7118969367963689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7118969367963689 | validation: 0.2813649498679216]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_365.pth
	Model improved!!!
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41006996878321517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41006996878321517 | validation: 0.5402393483421586]
	TIME [epoch: 1.39 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3783232204301736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3783232204301736 | validation: 0.5910273702595568]
	TIME [epoch: 1.44 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3744764773946941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3744764773946941 | validation: 0.3917788480306983]
	TIME [epoch: 1.39 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3608550842456632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3608550842456632 | validation: 0.3812935382838214]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3187740847004017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3187740847004017 | validation: 0.3300557768237961]
	TIME [epoch: 1.39 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22389784076586697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22389784076586697 | validation: 0.34553203667673094]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21632032663945425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21632032663945425 | validation: 0.2902256664798604]
	TIME [epoch: 1.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24157663819765673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24157663819765673 | validation: 0.50351361332684]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.385174485619638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.385174485619638 | validation: 0.39204955376279754]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5434836702619235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5434836702619235 | validation: 0.2985162005924404]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23782977400489966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23782977400489966 | validation: 0.5943169110867164]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5384357701786288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5384357701786288 | validation: 0.31471742591279406]
	TIME [epoch: 1.39 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.268183202883251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.268183202883251 | validation: 0.33846603162902733]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23602514616095807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23602514616095807 | validation: 0.3646064240550656]
	TIME [epoch: 1.39 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23616099214298858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23616099214298858 | validation: 0.29083112339425116]
	TIME [epoch: 1.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22248516049298117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22248516049298117 | validation: 0.3633189937972831]
	TIME [epoch: 1.39 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23844721543673855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23844721543673855 | validation: 0.3319889305155261]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3079477632854821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3079477632854821 | validation: 0.4437597777655242]
	TIME [epoch: 1.39 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3371211071607179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3371211071607179 | validation: 0.289444211316443]
	TIME [epoch: 1.39 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27207083128421145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27207083128421145 | validation: 0.44268697802097884]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3415998145138316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3415998145138316 | validation: 0.3390496283297968]
	TIME [epoch: 1.39 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30502365808366216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30502365808366216 | validation: 0.2901633035129652]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1974725809345344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1974725809345344 | validation: 0.34596023429857065]
	TIME [epoch: 1.39 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1875977822121193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1875977822121193 | validation: 0.2867748030838996]
	TIME [epoch: 1.39 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20298839696467127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20298839696467127 | validation: 0.37367156764652454]
	TIME [epoch: 1.39 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2588318357151326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2588318357151326 | validation: 0.30653129757751313]
	TIME [epoch: 1.39 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30598244294441157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30598244294441157 | validation: 0.45815418966193583]
	TIME [epoch: 1.39 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3288459599651927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3288459599651927 | validation: 0.31516491396631574]
	TIME [epoch: 1.39 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25308247109359505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25308247109359505 | validation: 0.32830046385703016]
	TIME [epoch: 1.4 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19000661363061933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19000661363061933 | validation: 0.2565048507119939]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1694677990178362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1694677990178362 | validation: 0.3100980058771743]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16106160683926152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16106160683926152 | validation: 0.20958291107873872]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_397.pth
	Model improved!!!
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17698811905585948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17698811905585948 | validation: 0.42695724884486475]
	TIME [epoch: 1.4 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2598360447022564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2598360447022564 | validation: 0.33725322525079826]
	TIME [epoch: 1.4 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5118331277638195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5118331277638195 | validation: 0.2958188707045034]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22063084139762268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22063084139762268 | validation: 0.5259442564604644]
	TIME [epoch: 1.39 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36399352265767676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36399352265767676 | validation: 0.36761367907836945]
	TIME [epoch: 1.4 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32029580271891916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32029580271891916 | validation: 0.2897986840773282]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19704943672481506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19704943672481506 | validation: 0.33707748766172146]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17431302948702232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17431302948702232 | validation: 0.26717823373697996]
	TIME [epoch: 1.4 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1914135609353031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1914135609353031 | validation: 0.32422269574298723]
	TIME [epoch: 1.4 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2018822939033091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2018822939033091 | validation: 0.3491387810822173]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26481937460295657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26481937460295657 | validation: 0.3671317908848121]
	TIME [epoch: 1.4 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40401225511863703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40401225511863703 | validation: 0.35369299802154064]
	TIME [epoch: 1.39 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21331182155921607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21331182155921607 | validation: 0.27305745885402033]
	TIME [epoch: 1.39 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15052794250507043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15052794250507043 | validation: 0.2816654403633913]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599180589202002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1599180589202002 | validation: 0.2929831386111712]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17657690466498877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17657690466498877 | validation: 0.24979508604909914]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17779570999059097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17779570999059097 | validation: 0.30721486694270467]
	TIME [epoch: 1.39 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2155604220249508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2155604220249508 | validation: 0.2967623255182194]
	TIME [epoch: 1.39 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.255296088999676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.255296088999676 | validation: 0.3757160194130305]
	TIME [epoch: 1.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30175799893802707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30175799893802707 | validation: 0.35706520132842845]
	TIME [epoch: 1.39 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2905137005324944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2905137005324944 | validation: 0.6121267001076955]
	TIME [epoch: 1.39 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4834931387544073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4834931387544073 | validation: 0.32751302559061096]
	TIME [epoch: 1.39 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22536735698085406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22536735698085406 | validation: 0.3253644392786017]
	TIME [epoch: 1.39 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16404365516679975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16404365516679975 | validation: 0.29476538442772465]
	TIME [epoch: 1.39 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1819753575409772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1819753575409772 | validation: 0.25960010093313207]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.160734592590511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.160734592590511 | validation: 0.30537805134069124]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531580299885621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531580299885621 | validation: 0.21840822560780193]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17058517753352032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17058517753352032 | validation: 0.3712127730645396]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2748665639031721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2748665639031721 | validation: 0.3785292304617858]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4404639236892767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4404639236892767 | validation: 0.3247425299919963]
	TIME [epoch: 1.39 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1938111803872483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1938111803872483 | validation: 0.3539369835191056]
	TIME [epoch: 1.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23551646972865142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23551646972865142 | validation: 0.2786069535228943]
	TIME [epoch: 1.39 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3469634878225874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3469634878225874 | validation: 0.4138106086387011]
	TIME [epoch: 1.39 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26968297767205257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26968297767205257 | validation: 0.3068498599898769]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1438857112304567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1438857112304567 | validation: 0.2506383015558759]
	TIME [epoch: 1.39 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15747417510322587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15747417510322587 | validation: 0.2759360762986025]
	TIME [epoch: 1.4 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14516190246296728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14516190246296728 | validation: 0.2521359322081536]
	TIME [epoch: 1.39 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1276473674775624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1276473674775624 | validation: 0.22243569291900897]
	TIME [epoch: 1.39 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12649224213115826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12649224213115826 | validation: 0.2671521040433561]
	TIME [epoch: 1.39 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1480877518208515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1480877518208515 | validation: 0.2415219329745978]
	TIME [epoch: 1.39 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2445089417128079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2445089417128079 | validation: 0.30571573006926517]
	TIME [epoch: 1.4 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1899742491884018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1899742491884018 | validation: 0.24021028815067402]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16129069653011327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16129069653011327 | validation: 0.3891863763867711]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2995799915747873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2995799915747873 | validation: 0.4813857565840469]
	TIME [epoch: 1.39 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3911213541861481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3911213541861481 | validation: 0.47632701467037086]
	TIME [epoch: 1.39 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38390085505113236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38390085505113236 | validation: 0.3073332822549799]
	TIME [epoch: 1.39 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22666739719908047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22666739719908047 | validation: 0.36914131612733736]
	TIME [epoch: 1.39 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24123944593483607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24123944593483607 | validation: 0.31134069384333174]
	TIME [epoch: 1.39 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16482043185962056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16482043185962056 | validation: 0.2624713679506934]
	TIME [epoch: 1.39 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13688528953275675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13688528953275675 | validation: 0.2524599701543535]
	TIME [epoch: 1.39 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13524006350775633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13524006350775633 | validation: 0.24408831004748333]
	TIME [epoch: 1.39 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13613159267793448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13613159267793448 | validation: 0.28022767254227027]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15817778696794715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15817778696794715 | validation: 0.3114837316403606]
	TIME [epoch: 1.39 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4370684961440907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4370684961440907 | validation: 0.44073377504049693]
	TIME [epoch: 1.39 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002609447320833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3002609447320833 | validation: 0.2579142284235754]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1541691889308616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1541691889308616 | validation: 0.23535081256226303]
	TIME [epoch: 1.39 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1476071081180616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1476071081180616 | validation: 0.2661408027557844]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14688566676860626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14688566676860626 | validation: 0.27834069158505603]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15650399289272784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15650399289272784 | validation: 0.23989078643671846]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1731146058498328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1731146058498328 | validation: 0.31572853549881064]
	TIME [epoch: 1.39 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21180957883829393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21180957883829393 | validation: 0.2983351821331016]
	TIME [epoch: 1.39 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23715359888751805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23715359888751805 | validation: 0.3470155340210139]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22859085081414698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22859085081414698 | validation: 0.22045447727791406]
	TIME [epoch: 1.39 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14701416391139424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14701416391139424 | validation: 0.3851887886800044]
	TIME [epoch: 1.39 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22375790049969124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22375790049969124 | validation: 0.3140273364557921]
	TIME [epoch: 1.39 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26393352125801045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26393352125801045 | validation: 0.32290807464926985]
	TIME [epoch: 1.39 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2459048380811541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2459048380811541 | validation: 0.2719852672336504]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15574612057994014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15574612057994014 | validation: 0.20986358895990312]
	TIME [epoch: 1.39 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2105318066097961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2105318066097961 | validation: 0.28853253454173183]
	TIME [epoch: 1.4 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18377183838334843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18377183838334843 | validation: 0.2684885567658245]
	TIME [epoch: 1.4 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17034276044278351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17034276044278351 | validation: 0.25239028298753846]
	TIME [epoch: 1.4 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1498091815248231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1498091815248231 | validation: 0.22643452142297768]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11948281165861209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11948281165861209 | validation: 0.21471131360485587]
	TIME [epoch: 1.39 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11600942365612409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11600942365612409 | validation: 0.22293289992297927]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1301528785897585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1301528785897585 | validation: 0.2698022159183527]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1970083130535775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1970083130535775 | validation: 0.3989610314655629]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.303805358027936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.303805358027936 | validation: 0.33562122373808273]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.283516014949829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.283516014949829 | validation: 0.24701656486171852]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17709373778790288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17709373778790288 | validation: 0.36543770236835926]
	TIME [epoch: 1.39 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603211267304993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2603211267304993 | validation: 0.24743210132615168]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20748639440251818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20748639440251818 | validation: 0.2821705315659494]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14945203187145223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14945203187145223 | validation: 0.22523333370391063]
	TIME [epoch: 1.39 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1224479657601453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1224479657601453 | validation: 0.21799285398821744]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149251325016331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149251325016331 | validation: 0.2334013512096752]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11646667095021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11646667095021 | validation: 0.17663074487388322]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15042069281750556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15042069281750556 | validation: 0.4106781358245097]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37216102792930567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37216102792930567 | validation: 0.3746055353847444]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3439048473893224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3439048473893224 | validation: 0.3371726109335802]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1959551210949795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1959551210949795 | validation: 0.3934335666072197]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27009377022332376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27009377022332376 | validation: 0.3028999196746362]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24002188323143217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24002188323143217 | validation: 0.30332487011626463]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2314793825696993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2314793825696993 | validation: 0.2807332434622209]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15111041478220702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15111041478220702 | validation: 0.24448134354288986]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379598674570666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1379598674570666 | validation: 0.22956849525065107]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11098903829693037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11098903829693037 | validation: 0.24224942000890648]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10981571485257138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10981571485257138 | validation: 0.1935522353015854]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551840606346362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10551840606346362 | validation: 0.2341338737647615]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11920351201718173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11920351201718173 | validation: 0.1942946312258881]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16374773132831089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16374773132831089 | validation: 0.5072675331160084]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3984586326564147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3984586326564147 | validation: 0.1991789198086749]
	TIME [epoch: 1.4 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2308128293024938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2308128293024938 | validation: 0.2684482721490987]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13878946687141708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13878946687141708 | validation: 0.2950422753923347]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1910174388055812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1910174388055812 | validation: 0.3173605803571602]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24153505382449897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24153505382449897 | validation: 0.3847133687454246]
	TIME [epoch: 176 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22971306978297176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22971306978297176 | validation: 0.275850219075078]
	TIME [epoch: 2.76 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16041702070022754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16041702070022754 | validation: 0.2239547489624626]
	TIME [epoch: 2.74 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13408686646985396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13408686646985396 | validation: 0.22837073615969974]
	TIME [epoch: 2.75 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11903463135089717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11903463135089717 | validation: 0.21900527107599443]
	TIME [epoch: 2.75 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10085306086127126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10085306086127126 | validation: 0.19860495821943347]
	TIME [epoch: 2.75 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10754970818456366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10754970818456366 | validation: 0.2329972912079689]
	TIME [epoch: 2.75 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14068606315884793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14068606315884793 | validation: 0.2343786031545368]
	TIME [epoch: 2.75 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2389819923217422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2389819923217422 | validation: 0.4633234808113361]
	TIME [epoch: 2.75 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36665956056755533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36665956056755533 | validation: 0.24087236900551856]
	TIME [epoch: 2.75 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13070448158147518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13070448158147518 | validation: 0.26977807437341933]
	TIME [epoch: 2.75 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21879780313680341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21879780313680341 | validation: 0.36711525789276367]
	TIME [epoch: 2.75 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2614141169506722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2614141169506722 | validation: 0.3407766915709715]
	TIME [epoch: 2.75 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22113617937632096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22113617937632096 | validation: 0.28284898796163327]
	TIME [epoch: 2.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15051572413868738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15051572413868738 | validation: 0.2258048474504507]
	TIME [epoch: 2.75 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12318398912414152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12318398912414152 | validation: 0.2107544392013826]
	TIME [epoch: 2.75 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10217549756935508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10217549756935508 | validation: 0.21036816679645756]
	TIME [epoch: 2.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09807939602704163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09807939602704163 | validation: 0.17901583145569552]
	TIME [epoch: 2.75 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10318465967600533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10318465967600533 | validation: 0.2215950307240595]
	TIME [epoch: 2.75 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10732171143157469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10732171143157469 | validation: 0.2169208332909244]
	TIME [epoch: 2.76 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563576935216794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1563576935216794 | validation: 0.43540308305982817]
	TIME [epoch: 2.74 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25920318214767063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25920318214767063 | validation: 0.3323122253698729]
	TIME [epoch: 2.75 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27132230577569383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27132230577569383 | validation: 0.24182026159838976]
	TIME [epoch: 2.75 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33534004849589805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33534004849589805 | validation: 0.4466867328929688]
	TIME [epoch: 2.74 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31060315697277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31060315697277 | validation: 0.24116615340766603]
	TIME [epoch: 2.75 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1521964337290783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1521964337290783 | validation: 0.23759165636437451]
	TIME [epoch: 2.75 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709516307110203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1709516307110203 | validation: 0.2503556939753467]
	TIME [epoch: 2.75 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349309242185273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1349309242185273 | validation: 0.2275097302328788]
	TIME [epoch: 2.75 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132860625050439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1132860625050439 | validation: 0.20026169035500124]
	TIME [epoch: 2.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1153988617558996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1153988617558996 | validation: 0.22671859632542093]
	TIME [epoch: 2.76 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12706455747508882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12706455747508882 | validation: 0.26613304131377596]
	TIME [epoch: 2.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15655740299264992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15655740299264992 | validation: 0.27468554751244234]
	TIME [epoch: 2.75 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21177681512776494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21177681512776494 | validation: 0.2992735057357758]
	TIME [epoch: 2.75 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1927179860446536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927179860446536 | validation: 0.20124084276006338]
	TIME [epoch: 2.75 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13636445340685652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13636445340685652 | validation: 0.21604579724475803]
	TIME [epoch: 2.75 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11792025710293651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11792025710293651 | validation: 0.22734314709528558]
	TIME [epoch: 2.76 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13418135065791895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13418135065791895 | validation: 0.29264878546160006]
	TIME [epoch: 2.75 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972654802280576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972654802280576 | validation: 0.299846335732216]
	TIME [epoch: 2.75 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2523581966017081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2523581966017081 | validation: 0.34740036062025137]
	TIME [epoch: 2.75 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20503635528746578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20503635528746578 | validation: 0.1708797885959318]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10864346387658418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10864346387658418 | validation: 0.2111496041067667]
	TIME [epoch: 2.75 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10441367695319037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10441367695319037 | validation: 0.21372062770500314]
	TIME [epoch: 2.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1290692555534711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1290692555534711 | validation: 0.3115303262962579]
	TIME [epoch: 2.75 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19678496522816502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19678496522816502 | validation: 0.3086738680717138]
	TIME [epoch: 2.75 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24506724226219853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24506724226219853 | validation: 0.26283662592708973]
	TIME [epoch: 2.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19302128722009101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19302128722009101 | validation: 0.2058752755440062]
	TIME [epoch: 2.75 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12878460634597644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12878460634597644 | validation: 0.23693098940070545]
	TIME [epoch: 2.76 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13840509312664895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13840509312664895 | validation: 0.2174200011344321]
	TIME [epoch: 2.78 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622115977876704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1622115977876704 | validation: 0.36248768897619094]
	TIME [epoch: 2.75 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24388013532849565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24388013532849565 | validation: 0.24017163855728785]
	TIME [epoch: 2.76 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738083920604385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1738083920604385 | validation: 0.21548737879954138]
	TIME [epoch: 2.75 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1064822097426272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1064822097426272 | validation: 0.18385864728699897]
	TIME [epoch: 2.75 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09505795999966654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09505795999966654 | validation: 0.2083444427494031]
	TIME [epoch: 2.75 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10604206658912428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10604206658912428 | validation: 0.20435545061371307]
	TIME [epoch: 2.75 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1250721388010766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1250721388010766 | validation: 0.2811317354249318]
	TIME [epoch: 2.75 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17037274814693198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17037274814693198 | validation: 0.30127385850173755]
	TIME [epoch: 2.75 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2299416492489778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2299416492489778 | validation: 0.3476751262081367]
	TIME [epoch: 2.75 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22813161938506107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22813161938506107 | validation: 0.209215093998239]
	TIME [epoch: 2.75 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16736520246884887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16736520246884887 | validation: 0.3467880896689574]
	TIME [epoch: 2.75 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21767329315397263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21767329315397263 | validation: 0.22446108318925484]
	TIME [epoch: 2.75 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1914935674014818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1914935674014818 | validation: 0.24443394742726215]
	TIME [epoch: 2.75 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12336948290898432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12336948290898432 | validation: 0.19902292666599003]
	TIME [epoch: 2.74 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09496957449338876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09496957449338876 | validation: 0.17382952570932453]
	TIME [epoch: 2.75 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09365230712041996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09365230712041996 | validation: 0.18756803887962345]
	TIME [epoch: 2.75 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09727422763939167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09727422763939167 | validation: 0.19138504589821037]
	TIME [epoch: 2.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15463383306841594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15463383306841594 | validation: 0.3977944520918699]
	TIME [epoch: 2.75 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30276355231500074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30276355231500074 | validation: 0.19755125917308314]
	TIME [epoch: 2.75 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13850260562383224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13850260562383224 | validation: 0.21771539491153177]
	TIME [epoch: 2.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10329409182577058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10329409182577058 | validation: 0.18194535526730576]
	TIME [epoch: 2.75 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12450869381302218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12450869381302218 | validation: 0.3067873724861798]
	TIME [epoch: 2.75 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24068189405426793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24068189405426793 | validation: 0.30090011509557557]
	TIME [epoch: 2.75 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19519827172615314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19519827172615314 | validation: 0.19509779003183556]
	TIME [epoch: 2.75 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380422649610629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1380422649610629 | validation: 0.33987816936143567]
	TIME [epoch: 2.75 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19117645287971688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19117645287971688 | validation: 0.30286867599850725]
	TIME [epoch: 2.75 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1961392143836652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1961392143836652 | validation: 0.2283708561712374]
	TIME [epoch: 2.75 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14837195143516346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14837195143516346 | validation: 0.19670305895027107]
	TIME [epoch: 2.75 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11862460296670037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11862460296670037 | validation: 0.19598166770463954]
	TIME [epoch: 2.75 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09619321158445673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09619321158445673 | validation: 0.1710504112359577]
	TIME [epoch: 2.75 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08684907316815259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08684907316815259 | validation: 0.17087678069759782]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08354647003818913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08354647003818913 | validation: 0.1708328440056823]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08556145177659115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08556145177659115 | validation: 0.1654146768685847]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_581.pth
	Model improved!!!
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10248723097720808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10248723097720808 | validation: 0.3446053875344138]
	TIME [epoch: 2.75 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2450662323895658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2450662323895658 | validation: 0.36401603799741566]
	TIME [epoch: 2.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38710227175929623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38710227175929623 | validation: 0.36629519189847864]
	TIME [epoch: 2.76 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22877548398370617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22877548398370617 | validation: 0.17986141909925626]
	TIME [epoch: 2.75 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09336098750522405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09336098750522405 | validation: 0.21086362768404399]
	TIME [epoch: 2.75 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12307133862704905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12307133862704905 | validation: 0.20585270555581384]
	TIME [epoch: 2.75 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11877472551892555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11877472551892555 | validation: 0.2201027935133063]
	TIME [epoch: 2.75 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13615410214275486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13615410214275486 | validation: 0.2898803896626733]
	TIME [epoch: 2.75 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.257108397048578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.257108397048578 | validation: 0.4051646162236633]
	TIME [epoch: 2.74 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2424227821573883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2424227821573883 | validation: 0.1776941766729687]
	TIME [epoch: 2.75 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11819282810919368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11819282810919368 | validation: 0.18369869833595254]
	TIME [epoch: 2.75 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09301401054296697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09301401054296697 | validation: 0.2074846388878281]
	TIME [epoch: 2.75 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10755108709824014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10755108709824014 | validation: 0.17688086588432053]
	TIME [epoch: 2.75 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10189285495641479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10189285495641479 | validation: 0.18123105048805874]
	TIME [epoch: 2.75 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09677643577157852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09677643577157852 | validation: 0.1676612202822379]
	TIME [epoch: 2.75 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08711325358368108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08711325358368108 | validation: 0.17780912455243114]
	TIME [epoch: 2.75 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10317750724961719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10317750724961719 | validation: 0.18886851970749363]
	TIME [epoch: 2.75 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14832264677339824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14832264677339824 | validation: 0.23676112421876128]
	TIME [epoch: 2.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18025500829982385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18025500829982385 | validation: 0.21034663176199875]
	TIME [epoch: 2.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1991936983747332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1991936983747332 | validation: 0.28785820266262246]
	TIME [epoch: 2.75 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18377067481905157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18377067481905157 | validation: 0.18438985897318616]
	TIME [epoch: 2.75 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18562892280721358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18562892280721358 | validation: 0.2586475343215357]
	TIME [epoch: 2.75 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16611306726020938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16611306726020938 | validation: 0.3237462853595968]
	TIME [epoch: 2.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18387959192335454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18387959192335454 | validation: 0.22622412297677494]
	TIME [epoch: 2.75 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15185834505674328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15185834505674328 | validation: 0.2067910698079073]
	TIME [epoch: 2.75 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09952100243763887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09952100243763887 | validation: 0.16289127596497815]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_607.pth
	Model improved!!!
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08181575278985777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08181575278985777 | validation: 0.1572631052693917]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07853303046822113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07853303046822113 | validation: 0.15297376884589342]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_609.pth
	Model improved!!!
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848297773911492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07848297773911492 | validation: 0.13424665685739617]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_610.pth
	Model improved!!!
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07764144393889161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07764144393889161 | validation: 0.1498067361438263]
	TIME [epoch: 2.74 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08283744953820989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08283744953820989 | validation: 0.15367037649685736]
	TIME [epoch: 2.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14599622769180337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14599622769180337 | validation: 0.48637235766784015]
	TIME [epoch: 2.75 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40638749131498897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40638749131498897 | validation: 0.3236933694414208]
	TIME [epoch: 2.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2324044594396637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2324044594396637 | validation: 0.20921096810318762]
	TIME [epoch: 2.75 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10660630356031905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10660630356031905 | validation: 0.1795615321487998]
	TIME [epoch: 2.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08516038239413078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08516038239413078 | validation: 0.18271050551553014]
	TIME [epoch: 2.75 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08870075686610154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08870075686610154 | validation: 0.18014872496784162]
	TIME [epoch: 2.75 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09195819636174633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09195819636174633 | validation: 0.20227746144523284]
	TIME [epoch: 2.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09548983440881482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09548983440881482 | validation: 0.21979470018747077]
	TIME [epoch: 2.76 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15495588560034929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15495588560034929 | validation: 0.4468391377572667]
	TIME [epoch: 2.75 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3067003772097453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3067003772097453 | validation: 0.244796164095197]
	TIME [epoch: 2.75 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23655680317454345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23655680317454345 | validation: 0.19450419035918395]
	TIME [epoch: 2.76 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10684030731147168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10684030731147168 | validation: 0.23306536644685893]
	TIME [epoch: 2.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11424766222739972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11424766222739972 | validation: 0.20570334577256935]
	TIME [epoch: 2.74 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11179813400214199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11179813400214199 | validation: 0.1813323832518337]
	TIME [epoch: 2.75 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09216749424897773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09216749424897773 | validation: 0.17329257844033696]
	TIME [epoch: 2.75 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09510703285712566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09510703285712566 | validation: 0.16280837104885107]
	TIME [epoch: 2.75 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1264660401595266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1264660401595266 | validation: 0.30828011850144893]
	TIME [epoch: 2.75 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20289257091142276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20289257091142276 | validation: 0.24341851095559602]
	TIME [epoch: 2.75 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20317419533689282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20317419533689282 | validation: 0.22652658780995383]
	TIME [epoch: 2.75 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1333625572456793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1333625572456793 | validation: 0.20009772971578058]
	TIME [epoch: 2.74 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08547891779881348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08547891779881348 | validation: 0.14172581408291246]
	TIME [epoch: 2.76 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801652796062767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0801652796062767 | validation: 0.18445733053020485]
	TIME [epoch: 2.75 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707416616913285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09707416616913285 | validation: 0.16580974946705762]
	TIME [epoch: 2.74 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12011234894675507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12011234894675507 | validation: 0.3079396023286567]
	TIME [epoch: 2.76 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17513010865039338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17513010865039338 | validation: 0.2942300070827787]
	TIME [epoch: 2.75 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22595265636543208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22595265636543208 | validation: 0.20808990748250233]
	TIME [epoch: 2.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20598283126450384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20598283126450384 | validation: 0.21307563631151838]
	TIME [epoch: 2.76 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10435579351381659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10435579351381659 | validation: 0.1585116659574838]
	TIME [epoch: 2.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0868091599365308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0868091599365308 | validation: 0.1762998285729673]
	TIME [epoch: 2.75 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10241345675870912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10241345675870912 | validation: 0.1705696854960438]
	TIME [epoch: 2.76 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340781320631531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11340781320631531 | validation: 0.2331171491575025]
	TIME [epoch: 2.76 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1433025526113667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1433025526113667 | validation: 0.24314175240574404]
	TIME [epoch: 2.75 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20551871271092928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20551871271092928 | validation: 0.2537209912155942]
	TIME [epoch: 2.75 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615197059929637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615197059929637 | validation: 0.17498672275567395]
	TIME [epoch: 2.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09527782361495715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09527782361495715 | validation: 0.14449736198754737]
	TIME [epoch: 2.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09147480337185215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09147480337185215 | validation: 0.22101321905843607]
	TIME [epoch: 2.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1097074740898509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1097074740898509 | validation: 0.18700102716843547]
	TIME [epoch: 2.76 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13877009122982378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13877009122982378 | validation: 0.24542475668496713]
	TIME [epoch: 2.75 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13998974731257596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13998974731257596 | validation: 0.1899116552785232]
	TIME [epoch: 2.75 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11585241433353183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11585241433353183 | validation: 0.1793946023404537]
	TIME [epoch: 2.75 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1120375957718969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1120375957718969 | validation: 0.1929368195204305]
	TIME [epoch: 2.75 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12623773212314915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12623773212314915 | validation: 0.19026194804759883]
	TIME [epoch: 2.75 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13705098784710443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13705098784710443 | validation: 0.35763876289401564]
	TIME [epoch: 2.76 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21861282313271385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21861282313271385 | validation: 0.20490611094488337]
	TIME [epoch: 2.76 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1683674588498078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1683674588498078 | validation: 0.19461346129947055]
	TIME [epoch: 2.75 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10945545293102502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10945545293102502 | validation: 0.16916940653277682]
	TIME [epoch: 2.76 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09131291021925267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09131291021925267 | validation: 0.18250093373797038]
	TIME [epoch: 2.76 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08925516146092502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08925516146092502 | validation: 0.16627377967602244]
	TIME [epoch: 2.75 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10962570497002258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10962570497002258 | validation: 0.2505522209609527]
	TIME [epoch: 2.75 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.134040822727949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.134040822727949 | validation: 0.23730821558964]
	TIME [epoch: 2.76 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14817063570316447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14817063570316447 | validation: 0.21299357550282094]
	TIME [epoch: 2.75 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13388320980690396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13388320980690396 | validation: 0.187407275952122]
	TIME [epoch: 2.76 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12806058755347394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12806058755347394 | validation: 0.1798291988161229]
	TIME [epoch: 2.76 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11861597435615223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11861597435615223 | validation: 0.16031435824091803]
	TIME [epoch: 2.76 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0953321381197667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0953321381197667 | validation: 0.14391559565367357]
	TIME [epoch: 2.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08425914063308039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08425914063308039 | validation: 0.15860946990751904]
	TIME [epoch: 2.75 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10402103877204953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10402103877204953 | validation: 0.22035714267944045]
	TIME [epoch: 2.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11988342823800707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11988342823800707 | validation: 0.14501022783959036]
	TIME [epoch: 2.75 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11691803327823688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11691803327823688 | validation: 0.1883262069822849]
	TIME [epoch: 2.75 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12754075779054552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12754075779054552 | validation: 0.24026936761576612]
	TIME [epoch: 2.75 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1748426062831262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1748426062831262 | validation: 0.26141952112143435]
	TIME [epoch: 2.75 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17808124378592324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17808124378592324 | validation: 0.17564393430663944]
	TIME [epoch: 2.76 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09517100889262224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09517100889262224 | validation: 0.13551748252005105]
	TIME [epoch: 2.75 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06672166029059923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06672166029059923 | validation: 0.1294307367253627]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_676.pth
	Model improved!!!
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06424693795555424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06424693795555424 | validation: 0.14370592418344405]
	TIME [epoch: 2.74 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732664542188865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0732664542188865 | validation: 0.15683960854753398]
	TIME [epoch: 2.75 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08686939969136544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08686939969136544 | validation: 0.17085682200191732]
	TIME [epoch: 2.74 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12425689544340716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12425689544340716 | validation: 0.2981654356248716]
	TIME [epoch: 2.75 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19514695137541563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19514695137541563 | validation: 0.2255311959114477]
	TIME [epoch: 2.75 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17349673046030517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17349673046030517 | validation: 0.21704826448144507]
	TIME [epoch: 2.75 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12710092431208794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12710092431208794 | validation: 0.17474218044721176]
	TIME [epoch: 2.76 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09070313411642995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09070313411642995 | validation: 0.24152135078629514]
	TIME [epoch: 2.75 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1949900479669131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1949900479669131 | validation: 0.39372009767993393]
	TIME [epoch: 2.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783290856052263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2783290856052263 | validation: 0.18310155276347495]
	TIME [epoch: 2.75 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11537770816885329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11537770816885329 | validation: 0.15968289419203688]
	TIME [epoch: 2.75 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07068913567514344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07068913567514344 | validation: 0.1656075324404431]
	TIME [epoch: 2.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08429362752631377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08429362752631377 | validation: 0.1562442211742622]
	TIME [epoch: 2.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08159478021740739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08159478021740739 | validation: 0.14749779210699038]
	TIME [epoch: 2.75 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0782964824990508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0782964824990508 | validation: 0.16731587561597727]
	TIME [epoch: 2.76 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09114093241125175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09114093241125175 | validation: 0.1481256571171377]
	TIME [epoch: 2.76 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11623432592696005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11623432592696005 | validation: 0.18596449190097805]
	TIME [epoch: 2.75 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11590658300035518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11590658300035518 | validation: 0.16904400734410602]
	TIME [epoch: 2.75 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10942033944933663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10942033944933663 | validation: 0.2120625433949942]
	TIME [epoch: 2.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13182140484707458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13182140484707458 | validation: 0.3061413469418728]
	TIME [epoch: 2.75 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15489462411779964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15489462411779964 | validation: 0.1602035824599762]
	TIME [epoch: 2.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09678707328432967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09678707328432967 | validation: 0.15532557325331708]
	TIME [epoch: 2.75 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09126025889000947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09126025889000947 | validation: 0.24031395954349413]
	TIME [epoch: 2.76 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18541742930110483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18541742930110483 | validation: 0.4466172294247088]
	TIME [epoch: 2.74 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30249895060440907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30249895060440907 | validation: 0.17233331503997126]
	TIME [epoch: 2.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13581734102632637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13581734102632637 | validation: 0.18734253675532977]
	TIME [epoch: 2.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115304291054601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08115304291054601 | validation: 0.1597396924622051]
	TIME [epoch: 2.75 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08695401807089509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08695401807089509 | validation: 0.15354166044869386]
	TIME [epoch: 2.75 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07404400521973792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07404400521973792 | validation: 0.14770069407897074]
	TIME [epoch: 2.74 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698804804312902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06698804804312902 | validation: 0.13128227018758049]
	TIME [epoch: 2.75 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06566566327614085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06566566327614085 | validation: 0.15055444426728634]
	TIME [epoch: 2.75 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06571405293216057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06571405293216057 | validation: 0.12014146001660012]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06943553067268997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06943553067268997 | validation: 0.18911663490459366]
	TIME [epoch: 2.75 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10456663587929942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10456663587929942 | validation: 0.3286160752228073]
	TIME [epoch: 2.76 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23731184540742953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23731184540742953 | validation: 0.3809769051779368]
	TIME [epoch: 2.75 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3145287856789187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3145287856789187 | validation: 0.17947069921457454]
	TIME [epoch: 2.75 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08796727808695705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08796727808695705 | validation: 0.16870179011827854]
	TIME [epoch: 2.75 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08126732970942628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08126732970942628 | validation: 0.18387324339107142]
	TIME [epoch: 2.74 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10049725848312126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10049725848312126 | validation: 0.17297230520566276]
	TIME [epoch: 2.75 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10551853591321414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10551853591321414 | validation: 0.18653990750836863]
	TIME [epoch: 2.75 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13439629215318033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13439629215318033 | validation: 0.3495498452405623]
	TIME [epoch: 2.74 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2209086202783393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2209086202783393 | validation: 0.19933842076599106]
	TIME [epoch: 2.75 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.171813267410463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.171813267410463 | validation: 0.16462743460152174]
	TIME [epoch: 2.75 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735039886489865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735039886489865 | validation: 0.12560986552956882]
	TIME [epoch: 2.75 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06959742408852533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06959742408852533 | validation: 0.15787041456987502]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320733894852721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320733894852721 | validation: 0.14893908528164798]
	TIME [epoch: 2.75 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07138005648062466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07138005648062466 | validation: 0.12242087359483761]
	TIME [epoch: 2.75 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06559785279730533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06559785279730533 | validation: 0.13220916382149336]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306916070981433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306916070981433 | validation: 0.15036163645891798]
	TIME [epoch: 2.75 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08233866834020988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08233866834020988 | validation: 0.3184853828067118]
	TIME [epoch: 2.75 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16854809629407175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16854809629407175 | validation: 0.24044532131391694]
	TIME [epoch: 2.74 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19940110241738432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19940110241738432 | validation: 0.2620661582716706]
	TIME [epoch: 2.75 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14022077954125992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14022077954125992 | validation: 0.14005867348072312]
	TIME [epoch: 2.75 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10079409626402473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10079409626402473 | validation: 0.20037910044011187]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09722133199198069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09722133199198069 | validation: 0.1997984412439263]
	TIME [epoch: 2.75 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11675107414673465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11675107414673465 | validation: 0.20239949799003032]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16107338438354746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16107338438354746 | validation: 0.24979402920932317]
	TIME [epoch: 2.74 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17136670927702213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17136670927702213 | validation: 0.18617569450667637]
	TIME [epoch: 2.74 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09953050235493209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09953050235493209 | validation: 0.15412803312358264]
	TIME [epoch: 2.75 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359974629868142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08359974629868142 | validation: 0.15495977542226458]
	TIME [epoch: 2.75 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08407858877891185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08407858877891185 | validation: 0.16780112476198272]
	TIME [epoch: 2.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09549367950788869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09549367950788869 | validation: 0.15062656322242718]
	TIME [epoch: 2.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539131460134696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09539131460134696 | validation: 0.16455640961568915]
	TIME [epoch: 2.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10152424443942643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10152424443942643 | validation: 0.16644611551078597]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09936998643591899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09936998643591899 | validation: 0.16887099640747394]
	TIME [epoch: 2.74 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10510451395717031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10510451395717031 | validation: 0.21655680279880316]
	TIME [epoch: 2.75 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13139983307982472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13139983307982472 | validation: 0.24203974893181754]
	TIME [epoch: 2.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17372517671070392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17372517671070392 | validation: 0.20288664642876736]
	TIME [epoch: 2.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13106589207511973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13106589207511973 | validation: 0.14986166615996463]
	TIME [epoch: 2.75 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08570863335828127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08570863335828127 | validation: 0.1296514712591073]
	TIME [epoch: 2.74 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07437898848089346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07437898848089346 | validation: 0.15661721162479805]
	TIME [epoch: 2.75 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827801221219801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0827801221219801 | validation: 0.1796140937142312]
	TIME [epoch: 2.76 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10984889265220706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10984889265220706 | validation: 0.27539212587551243]
	TIME [epoch: 2.75 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15396642013165335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15396642013165335 | validation: 0.20641799708838937]
	TIME [epoch: 2.75 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1424060790537112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1424060790537112 | validation: 0.14854679398191914]
	TIME [epoch: 2.75 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08622050524138807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08622050524138807 | validation: 0.1397970372023031]
	TIME [epoch: 2.75 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217453099531261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07217453099531261 | validation: 0.1593483665447221]
	TIME [epoch: 2.75 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07807257660393303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07807257660393303 | validation: 0.16420126200386487]
	TIME [epoch: 2.75 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09637482359200743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09637482359200743 | validation: 0.21906577223059187]
	TIME [epoch: 2.76 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15213565309245017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15213565309245017 | validation: 0.20090314408110396]
	TIME [epoch: 2.74 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15876064090270878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15876064090270878 | validation: 0.18835568809695702]
	TIME [epoch: 2.75 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11172667214534626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11172667214534626 | validation: 0.12248458915185244]
	TIME [epoch: 2.75 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06207144258894602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06207144258894602 | validation: 0.11474625283610934]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_759.pth
	Model improved!!!
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05681372290718706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05681372290718706 | validation: 0.11411224344378322]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_760.pth
	Model improved!!!
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07150168459075106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07150168459075106 | validation: 0.14697454222319623]
	TIME [epoch: 2.75 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09439836001217515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09439836001217515 | validation: 0.21496697969717551]
	TIME [epoch: 2.76 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16144962928781995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16144962928781995 | validation: 0.4309889657245307]
	TIME [epoch: 2.76 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611255914881976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2611255914881976 | validation: 0.19169033015888426]
	TIME [epoch: 2.75 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14956178871365175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14956178871365175 | validation: 0.18506212094056435]
	TIME [epoch: 2.76 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10132067829286473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10132067829286473 | validation: 0.19003230468366994]
	TIME [epoch: 2.76 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11206232680318698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11206232680318698 | validation: 0.17009737797850183]
	TIME [epoch: 2.75 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09341429287938922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09341429287938922 | validation: 0.15212670983698848]
	TIME [epoch: 2.75 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07006955639361616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07006955639361616 | validation: 0.12985838383834267]
	TIME [epoch: 2.76 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679017194523744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06679017194523744 | validation: 0.13600325816681186]
	TIME [epoch: 2.75 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06416400070786298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06416400070786298 | validation: 0.13089906697731016]
	TIME [epoch: 2.76 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772865454206159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06772865454206159 | validation: 0.12885705954428583]
	TIME [epoch: 2.76 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08314962329994678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08314962329994678 | validation: 0.1992449902229884]
	TIME [epoch: 2.76 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1116747128238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1116747128238 | validation: 0.19561184336514573]
	TIME [epoch: 2.75 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17622904848766538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17622904848766538 | validation: 0.20753627412098902]
	TIME [epoch: 2.75 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13052267577369883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13052267577369883 | validation: 0.11758653288282786]
	TIME [epoch: 2.76 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07849053492628764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07849053492628764 | validation: 0.1314952883175695]
	TIME [epoch: 2.75 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06062818276059624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06062818276059624 | validation: 0.10611682213073986]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06270450405512465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06270450405512465 | validation: 0.13657311366140626]
	TIME [epoch: 2.74 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435128583126713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07435128583126713 | validation: 0.12171519564920903]
	TIME [epoch: 2.74 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09943612618851937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09943612618851937 | validation: 0.18762414395354382]
	TIME [epoch: 2.74 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11578766515809452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11578766515809452 | validation: 0.17025451141266518]
	TIME [epoch: 2.74 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10864539634688253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10864539634688253 | validation: 0.1340821050848597]
	TIME [epoch: 2.74 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1018142286164277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1018142286164277 | validation: 0.17319431397273016]
	TIME [epoch: 2.74 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09593609390519779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09593609390519779 | validation: 0.16140783666437505]
	TIME [epoch: 2.74 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10929949436046772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10929949436046772 | validation: 0.3587364662638233]
	TIME [epoch: 2.75 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27815243333563117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27815243333563117 | validation: 0.15958480505930162]
	TIME [epoch: 2.74 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09570999991257827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09570999991257827 | validation: 0.2981945011961543]
	TIME [epoch: 2.76 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1480723772327745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1480723772327745 | validation: 0.2730038382637973]
	TIME [epoch: 2.76 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1738450789441309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1738450789441309 | validation: 0.1806365640092471]
	TIME [epoch: 2.75 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12312627972966139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12312627972966139 | validation: 0.16024866707500052]
	TIME [epoch: 2.75 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07863306728028296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07863306728028296 | validation: 0.14095459121094908]
	TIME [epoch: 2.76 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053681951283521476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053681951283521476 | validation: 0.12463794440039587]
	TIME [epoch: 2.75 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05541457876249892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05541457876249892 | validation: 0.1401765176828876]
	TIME [epoch: 2.75 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059714493646861586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059714493646861586 | validation: 0.11262048377058992]
	TIME [epoch: 2.74 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570415819313385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0570415819313385 | validation: 0.10667200572930109]
	TIME [epoch: 2.74 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058995167142849636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058995167142849636 | validation: 0.1253983447123336]
	TIME [epoch: 2.74 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07164745767254278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07164745767254278 | validation: 0.15998930433567093]
	TIME [epoch: 2.74 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10878643401305786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10878643401305786 | validation: 0.30068020977932397]
	TIME [epoch: 2.74 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2044832238908441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2044832238908441 | validation: 0.2303087507887006]
	TIME [epoch: 2.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23437070692110346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23437070692110346 | validation: 0.20404678225167028]
	TIME [epoch: 2.74 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15732561896497052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15732561896497052 | validation: 0.19509430254084512]
	TIME [epoch: 2.75 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12145349969853471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12145349969853471 | validation: 0.17129407462697843]
	TIME [epoch: 2.75 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08232966521775677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08232966521775677 | validation: 0.13968941899626244]
	TIME [epoch: 2.75 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0641717535501649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0641717535501649 | validation: 0.1155709819347409]
	TIME [epoch: 2.75 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057113882870358276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057113882870358276 | validation: 0.12337602789823006]
	TIME [epoch: 2.75 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05524567062555189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05524567062555189 | validation: 0.15299682200458153]
	TIME [epoch: 2.74 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07151681741012704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07151681741012704 | validation: 0.181360294412098]
	TIME [epoch: 2.75 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12199347927437294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12199347927437294 | validation: 0.17847517001268223]
	TIME [epoch: 2.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11814017850673647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11814017850673647 | validation: 0.153398943727237]
	TIME [epoch: 2.74 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07661859943013564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07661859943013564 | validation: 0.10261928743832022]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_811.pth
	Model improved!!!
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06414128829975291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06414128829975291 | validation: 0.1522738298923222]
	TIME [epoch: 2.74 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07891606209661492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07891606209661492 | validation: 0.15711382571792157]
	TIME [epoch: 2.75 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12097774250051352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12097774250051352 | validation: 0.18990657146687356]
	TIME [epoch: 2.74 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16393056334806852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16393056334806852 | validation: 0.19760136173319306]
	TIME [epoch: 2.75 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11591173802717165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11591173802717165 | validation: 0.1098848150979449]
	TIME [epoch: 2.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06885488364372412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06885488364372412 | validation: 0.11365637412860573]
	TIME [epoch: 2.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060198606989392105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060198606989392105 | validation: 0.11083410054209174]
	TIME [epoch: 2.74 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05887320599012453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05887320599012453 | validation: 0.10582062639534982]
	TIME [epoch: 2.74 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06725562941701614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06725562941701614 | validation: 0.15172217679738975]
	TIME [epoch: 2.75 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962487178294341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0962487178294341 | validation: 0.2581481089290848]
	TIME [epoch: 2.74 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.165045502804047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.165045502804047 | validation: 0.19215370056555334]
	TIME [epoch: 2.74 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12800052335569498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12800052335569498 | validation: 0.16062454616300756]
	TIME [epoch: 2.74 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08658146749076016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08658146749076016 | validation: 0.1719519847048619]
	TIME [epoch: 2.74 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093429146192823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10093429146192823 | validation: 0.16962450026302733]
	TIME [epoch: 2.74 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10588568386400901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10588568386400901 | validation: 0.1801718672859821]
	TIME [epoch: 2.75 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13035226835414757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13035226835414757 | validation: 0.1486666849331864]
	TIME [epoch: 2.74 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10478955299297721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10478955299297721 | validation: 0.10980814649800837]
	TIME [epoch: 2.74 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059100123865823743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059100123865823743 | validation: 0.09854375688914924]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_829.pth
	Model improved!!!
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04852774433080843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04852774433080843 | validation: 0.11334386238368249]
	TIME [epoch: 2.75 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915024429329696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04915024429329696 | validation: 0.08108601432966006]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_831.pth
	Model improved!!!
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05383721930834367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05383721930834367 | validation: 0.11836069499653026]
	TIME [epoch: 2.75 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05929650433738376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05929650433738376 | validation: 0.10934208931980703]
	TIME [epoch: 2.75 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08198816275566438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08198816275566438 | validation: 0.1790767693378982]
	TIME [epoch: 2.76 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11166076996294093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11166076996294093 | validation: 0.18002832849794956]
	TIME [epoch: 2.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12264139624401957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12264139624401957 | validation: 0.18265582665150612]
	TIME [epoch: 2.76 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15313163900443064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15313163900443064 | validation: 0.2364050633799769]
	TIME [epoch: 2.76 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1578706899371023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1578706899371023 | validation: 0.16154696937118726]
	TIME [epoch: 2.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11394089667850449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11394089667850449 | validation: 0.12456199713864638]
	TIME [epoch: 2.75 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06932215584813187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06932215584813187 | validation: 0.11534154071283526]
	TIME [epoch: 2.76 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053595727258980055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053595727258980055 | validation: 0.14427656004818876]
	TIME [epoch: 2.75 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07427438734833623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07427438734833623 | validation: 0.18976209725753723]
	TIME [epoch: 2.76 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11250255738020461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11250255738020461 | validation: 0.21997056932917555]
	TIME [epoch: 2.75 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14059295627494225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14059295627494225 | validation: 0.14700414640003134]
	TIME [epoch: 2.76 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09386779289304921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09386779289304921 | validation: 0.10840486566120165]
	TIME [epoch: 2.75 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05658600612974295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05658600612974295 | validation: 0.09117393822281919]
	TIME [epoch: 2.75 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04693583131203011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04693583131203011 | validation: 0.1568606893734386]
	TIME [epoch: 2.76 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08670436261971913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08670436261971913 | validation: 0.21925253717199356]
	TIME [epoch: 2.77 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13623490318578274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13623490318578274 | validation: 0.21168268513725216]
	TIME [epoch: 2.77 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558679715756831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558679715756831 | validation: 0.17964492805019305]
	TIME [epoch: 2.76 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11104490820595815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11104490820595815 | validation: 0.08591368227379818]
	TIME [epoch: 2.75 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06670879224472122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06670879224472122 | validation: 0.12772602968903254]
	TIME [epoch: 2.76 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050610416390723105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050610416390723105 | validation: 0.10578445057630491]
	TIME [epoch: 2.76 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04961209471539355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04961209471539355 | validation: 0.09297965421914323]
	TIME [epoch: 2.76 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053675190346495365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053675190346495365 | validation: 0.10481689028060713]
	TIME [epoch: 2.76 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060941842121736464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060941842121736464 | validation: 0.14005047356339248]
	TIME [epoch: 2.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08104312150098465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08104312150098465 | validation: 0.17963432271240262]
	TIME [epoch: 2.75 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10985236014835344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10985236014835344 | validation: 0.18466170283565486]
	TIME [epoch: 2.75 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12702656430699819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12702656430699819 | validation: 0.1778664324845861]
	TIME [epoch: 2.76 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470524010775589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10470524010775589 | validation: 0.1359855313003134]
	TIME [epoch: 2.75 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06934014088144631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06934014088144631 | validation: 0.1160563513894246]
	TIME [epoch: 2.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06075007870659964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06075007870659964 | validation: 0.14355520868167024]
	TIME [epoch: 2.76 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10223288091083298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10223288091083298 | validation: 0.21216927933749563]
	TIME [epoch: 2.76 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17595628605218622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17595628605218622 | validation: 0.1983729046867643]
	TIME [epoch: 2.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13632956134196336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13632956134196336 | validation: 0.14427855850687896]
	TIME [epoch: 2.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0598749514379992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0598749514379992 | validation: 0.09619828022433201]
	TIME [epoch: 2.75 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335844935993916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04335844935993916 | validation: 0.09674036427219129]
	TIME [epoch: 2.75 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052592606263321545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052592606263321545 | validation: 0.11740288033477293]
	TIME [epoch: 2.75 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055914304524396816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055914304524396816 | validation: 0.10059053975576755]
	TIME [epoch: 2.75 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060006170100169315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060006170100169315 | validation: 0.12066120411565576]
	TIME [epoch: 2.76 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06215305431174725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06215305431174725 | validation: 0.12074380284045204]
	TIME [epoch: 2.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06818714700831774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06818714700831774 | validation: 0.13046496558575807]
	TIME [epoch: 2.75 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08402686460471748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08402686460471748 | validation: 0.16669470785890994]
	TIME [epoch: 2.76 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10342271059001582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10342271059001582 | validation: 0.24808423708917135]
	TIME [epoch: 2.75 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15164856500587026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15164856500587026 | validation: 0.20770160932745713]
	TIME [epoch: 2.75 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2257147275172953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2257147275172953 | validation: 0.17860578776675795]
	TIME [epoch: 2.76 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11081479164508445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11081479164508445 | validation: 0.09115445523858552]
	TIME [epoch: 2.74 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08133374420715964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08133374420715964 | validation: 0.14655073471239177]
	TIME [epoch: 2.79 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06999373223541228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06999373223541228 | validation: 0.10624165485747956]
	TIME [epoch: 2.75 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05904723031292511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05904723031292511 | validation: 0.08044696402287874]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052318737276787104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052318737276787104 | validation: 0.10818539898372836]
	TIME [epoch: 2.74 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05172819663979505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05172819663979505 | validation: 0.08968114787912657]
	TIME [epoch: 2.74 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011419803338431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05011419803338431 | validation: 0.09877069037345082]
	TIME [epoch: 2.74 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06867553613264724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06867553613264724 | validation: 0.18534388946850971]
	TIME [epoch: 2.74 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14342499245935242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14342499245935242 | validation: 0.28409152952724576]
	TIME [epoch: 2.74 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19148593647920023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19148593647920023 | validation: 0.20140736494512215]
	TIME [epoch: 2.75 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12035030254338537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12035030254338537 | validation: 0.13105840452097786]
	TIME [epoch: 2.75 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06323595393515495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06323595393515495 | validation: 0.0910924997010944]
	TIME [epoch: 2.75 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06562816587301767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06562816587301767 | validation: 0.13724976987906262]
	TIME [epoch: 2.75 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05534007212709138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05534007212709138 | validation: 0.10129255088209184]
	TIME [epoch: 2.75 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04834396827691915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04834396827691915 | validation: 0.10552469643818743]
	TIME [epoch: 2.75 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06295429128089816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06295429128089816 | validation: 0.13711600853686193]
	TIME [epoch: 2.75 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10346475858411007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10346475858411007 | validation: 0.17581421090936572]
	TIME [epoch: 2.75 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12584412525596383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12584412525596383 | validation: 0.17897170383986127]
	TIME [epoch: 2.75 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564152405622805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10564152405622805 | validation: 0.09551982286984378]
	TIME [epoch: 2.75 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052092611008157375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052092611008157375 | validation: 0.08502428027893637]
	TIME [epoch: 2.75 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03972707149413669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03972707149413669 | validation: 0.07286986703371578]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_897.pth
	Model improved!!!
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04257358515615112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04257358515615112 | validation: 0.10426835040877641]
	TIME [epoch: 2.74 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055965352414178736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055965352414178736 | validation: 0.19708631040040517]
	TIME [epoch: 2.73 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12164894793512038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12164894793512038 | validation: 0.2906966720671518]
	TIME [epoch: 2.73 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24365127771052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24365127771052 | validation: 0.1639716592798739]
	TIME [epoch: 2.75 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11050120054487363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11050120054487363 | validation: 0.08422106976707633]
	TIME [epoch: 2.75 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040650232098342565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040650232098342565 | validation: 0.1117878520862037]
	TIME [epoch: 2.75 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05399409006169863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05399409006169863 | validation: 0.1116907760896515]
	TIME [epoch: 2.75 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06241698922801248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06241698922801248 | validation: 0.11287078207909632]
	TIME [epoch: 2.75 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359270830267114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06359270830267114 | validation: 0.12086947779567687]
	TIME [epoch: 2.75 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0702207859701098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0702207859701098 | validation: 0.12338498765103925]
	TIME [epoch: 2.75 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08208219510538163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08208219510538163 | validation: 0.1631127099159477]
	TIME [epoch: 2.75 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10683346985451149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10683346985451149 | validation: 0.1464607132052013]
	TIME [epoch: 2.75 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09140509359135873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09140509359135873 | validation: 0.12343957709548598]
	TIME [epoch: 2.75 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07659132272287016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07659132272287016 | validation: 0.12857939240432315]
	TIME [epoch: 2.75 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0622922312649255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0622922312649255 | validation: 0.08194895579783795]
	TIME [epoch: 2.75 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04394572205121176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04394572205121176 | validation: 0.08267043984250444]
	TIME [epoch: 2.75 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040189461489481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040189461489481 | validation: 0.08883933563727948]
	TIME [epoch: 2.75 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042434669243374544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042434669243374544 | validation: 0.07428546077669375]
	TIME [epoch: 2.75 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05605142446593657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05605142446593657 | validation: 0.1428088366751116]
	TIME [epoch: 2.75 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09840920325580843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09840920325580843 | validation: 0.15996956400683635]
	TIME [epoch: 2.75 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11373209748881584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11373209748881584 | validation: 0.18561777499605978]
	TIME [epoch: 2.75 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15694319840518295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15694319840518295 | validation: 0.22906748337758304]
	TIME [epoch: 2.75 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17123846568780401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17123846568780401 | validation: 0.1283011306465033]
	TIME [epoch: 2.75 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07587307877648133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07587307877648133 | validation: 0.08643821438349127]
	TIME [epoch: 2.75 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04304990041489053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04304990041489053 | validation: 0.087905135753072]
	TIME [epoch: 2.75 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038910987651354936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038910987651354936 | validation: 0.07611329156829716]
	TIME [epoch: 2.75 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039070310778739195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039070310778739195 | validation: 0.10113458684175042]
	TIME [epoch: 2.75 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565488362406446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565488362406446 | validation: 0.13713754521521584]
	TIME [epoch: 2.75 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09316813140489912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09316813140489912 | validation: 0.24451922335919662]
	TIME [epoch: 2.75 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15048971754560797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15048971754560797 | validation: 0.14409839736378152]
	TIME [epoch: 2.75 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1058443364421417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1058443364421417 | validation: 0.08660042871819346]
	TIME [epoch: 2.75 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05492861384487476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05492861384487476 | validation: 0.08445549522410348]
	TIME [epoch: 2.75 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068995206966365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05068995206966365 | validation: 0.13430540802676402]
	TIME [epoch: 2.75 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473164580090245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05473164580090245 | validation: 0.13086679715577243]
	TIME [epoch: 2.75 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07431602614755801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07431602614755801 | validation: 0.16105486817635364]
	TIME [epoch: 2.75 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1180944026920856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1180944026920856 | validation: 0.16987437955408904]
	TIME [epoch: 2.75 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13200163424787906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13200163424787906 | validation: 0.11283358661422534]
	TIME [epoch: 2.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07172580315221998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07172580315221998 | validation: 0.06677944229073822]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_935.pth
	Model improved!!!
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055541593777367046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055541593777367046 | validation: 0.1295166085114671]
	TIME [epoch: 2.74 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05579744120501605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05579744120501605 | validation: 0.11617562733780643]
	TIME [epoch: 2.73 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05872486184678872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05872486184678872 | validation: 0.12438216567089536]
	TIME [epoch: 2.74 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08321294843141287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08321294843141287 | validation: 0.15677471427765444]
	TIME [epoch: 2.73 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11207385185740731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11207385185740731 | validation: 0.12374227859172807]
	TIME [epoch: 2.73 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0811267157825678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0811267157825678 | validation: 0.09998347925240292]
	TIME [epoch: 2.73 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05614874130841111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05614874130841111 | validation: 0.082356776995955]
	TIME [epoch: 2.75 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046805871904264204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046805871904264204 | validation: 0.10329835762872785]
	TIME [epoch: 2.73 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06637923635631099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06637923635631099 | validation: 0.17160221304026674]
	TIME [epoch: 2.73 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1044875627753733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1044875627753733 | validation: 0.20238160141683692]
	TIME [epoch: 2.74 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12481170069069684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12481170069069684 | validation: 0.13050338642961132]
	TIME [epoch: 2.73 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07947149912951697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07947149912951697 | validation: 0.07326684737269251]
	TIME [epoch: 2.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04177144480176769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04177144480176769 | validation: 0.07255793446317839]
	TIME [epoch: 2.73 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033550665182263656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033550665182263656 | validation: 0.07243427541504653]
	TIME [epoch: 2.75 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03358413741640211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03358413741640211 | validation: 0.07059123783816716]
	TIME [epoch: 2.75 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03920893241936949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03920893241936949 | validation: 0.09042790410110774]
	TIME [epoch: 2.74 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05139755247603107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05139755247603107 | validation: 0.167647997830747]
	TIME [epoch: 2.74 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09741609907992597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09741609907992597 | validation: 0.24396615353486367]
	TIME [epoch: 2.75 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14771273965189402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14771273965189402 | validation: 0.17343785531928804]
	TIME [epoch: 2.74 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15225450676132682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15225450676132682 | validation: 0.1611888957412253]
	TIME [epoch: 2.75 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13669920770349517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13669920770349517 | validation: 0.11140687865586783]
	TIME [epoch: 2.75 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08004423604991986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08004423604991986 | validation: 0.10354594325716437]
	TIME [epoch: 2.74 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058367514477596645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058367514477596645 | validation: 0.10171069041057515]
	TIME [epoch: 2.74 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067127118050397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.067127118050397 | validation: 0.07664849807488146]
	TIME [epoch: 2.74 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06571841966861461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06571841966861461 | validation: 0.1021802904782744]
	TIME [epoch: 2.75 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05449794101705464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05449794101705464 | validation: 0.09355454655306043]
	TIME [epoch: 2.74 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05721423136384538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05721423136384538 | validation: 0.11385869074636862]
	TIME [epoch: 2.76 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06934475480021121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06934475480021121 | validation: 0.14138076235207464]
	TIME [epoch: 2.76 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09129495431258726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09129495431258726 | validation: 0.1441042126252595]
	TIME [epoch: 2.75 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0977055521469607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0977055521469607 | validation: 0.1301917077257682]
	TIME [epoch: 2.76 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07571706096433978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07571706096433978 | validation: 0.10913135642386336]
	TIME [epoch: 2.76 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05481034591570176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05481034591570176 | validation: 0.08772859624425171]
	TIME [epoch: 2.76 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04715966552470179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04715966552470179 | validation: 0.08300668774456443]
	TIME [epoch: 2.75 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050862988849140825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050862988849140825 | validation: 0.11039842018147748]
	TIME [epoch: 2.75 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06526762202326993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06526762202326993 | validation: 0.13197626008993738]
	TIME [epoch: 2.75 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07669474410515675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07669474410515675 | validation: 0.11188197760556105]
	TIME [epoch: 2.74 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06566533514001499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06566533514001499 | validation: 0.08413057335124421]
	TIME [epoch: 2.74 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057123661092017705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057123661092017705 | validation: 0.08184263472556612]
	TIME [epoch: 2.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067624158048375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.067624158048375 | validation: 0.10052542338261694]
	TIME [epoch: 2.74 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08807619358811372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08807619358811372 | validation: 0.1483958059796375]
	TIME [epoch: 2.73 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11557586150601684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11557586150601684 | validation: 0.27974829058618694]
	TIME [epoch: 2.74 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17588162713094085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17588162713094085 | validation: 0.17789131630451593]
	TIME [epoch: 2.74 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.127992356627243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.127992356627243 | validation: 0.10702812287256235]
	TIME [epoch: 2.74 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059782760136405336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059782760136405336 | validation: 0.06392311249885173]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04141724276322185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04141724276322185 | validation: 0.07814631494651934]
	TIME [epoch: 2.75 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03216253503591283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03216253503591283 | validation: 0.07573130407984657]
	TIME [epoch: 2.75 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03551494917659743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03551494917659743 | validation: 0.07113135602293595]
	TIME [epoch: 2.75 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034902455411400475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034902455411400475 | validation: 0.07651222684304748]
	TIME [epoch: 2.76 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040329176408085636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040329176408085636 | validation: 0.0908998698752548]
	TIME [epoch: 3.58 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053014366882569046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053014366882569046 | validation: 0.1543925753626129]
	TIME [epoch: 2.74 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09052204890513224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09052204890513224 | validation: 0.14494692394842637]
	TIME [epoch: 2.74 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10775737969064164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10775737969064164 | validation: 0.12835030995733895]
	TIME [epoch: 2.74 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09444203529941725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09444203529941725 | validation: 0.10505462149745395]
	TIME [epoch: 2.73 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07473584328603051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07473584328603051 | validation: 0.1318455865713933]
	TIME [epoch: 2.74 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012413652920572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08012413652920572 | validation: 0.14162378940677964]
	TIME [epoch: 2.75 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09228372936307253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09228372936307253 | validation: 0.16198852311982828]
	TIME [epoch: 2.74 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10994330240977424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10994330240977424 | validation: 0.0924110863723063]
	TIME [epoch: 2.74 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08555040583389976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08555040583389976 | validation: 0.07729629417704995]
	TIME [epoch: 2.74 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040537138927013584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040537138927013584 | validation: 0.051221073937930364]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_994.pth
	Model improved!!!
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030203356883780348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030203356883780348 | validation: 0.05542616205053322]
	TIME [epoch: 2.74 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400696071658356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03400696071658356 | validation: 0.06449098455305914]
	TIME [epoch: 2.74 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041569650894721456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041569650894721456 | validation: 0.07419692360064857]
	TIME [epoch: 2.75 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049701862284206796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049701862284206796 | validation: 0.10420836877738462]
	TIME [epoch: 2.75 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07247128250711274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07247128250711274 | validation: 0.15389000235662975]
	TIME [epoch: 2.74 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10156025486257689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10156025486257689 | validation: 0.10128764094911835]
	TIME [epoch: 2.74 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07629071683169385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07629071683169385 | validation: 0.16763975555031896]
	TIME [epoch: 179 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0984988821959653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0984988821959653 | validation: 0.19770799160772973]
	TIME [epoch: 5.88 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18377203797916614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18377203797916614 | validation: 0.2049049339050055]
	TIME [epoch: 5.86 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12808696276240394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12808696276240394 | validation: 0.05861479686496207]
	TIME [epoch: 5.85 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050795616799084925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050795616799084925 | validation: 0.12325176208770722]
	TIME [epoch: 5.86 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0462825150872247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0462825150872247 | validation: 0.07934793404459575]
	TIME [epoch: 5.86 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038647339180461544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038647339180461544 | validation: 0.06373411455250587]
	TIME [epoch: 5.87 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03528881387811216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03528881387811216 | validation: 0.061560187608755125]
	TIME [epoch: 5.87 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03393456233896202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03393456233896202 | validation: 0.06278447974752153]
	TIME [epoch: 5.86 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03615313133883714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03615313133883714 | validation: 0.07527977532340609]
	TIME [epoch: 5.86 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093502916748923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05093502916748923 | validation: 0.12344534415753397]
	TIME [epoch: 5.86 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09577150275000763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09577150275000763 | validation: 0.20639337500112442]
	TIME [epoch: 5.87 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466899825185481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1466899825185481 | validation: 0.15521245495949812]
	TIME [epoch: 5.86 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11727501851050162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11727501851050162 | validation: 0.10510181579012817]
	TIME [epoch: 5.87 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06381637721888758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06381637721888758 | validation: 0.11627532977769071]
	TIME [epoch: 5.87 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591694488779685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0591694488779685 | validation: 0.11083756586762468]
	TIME [epoch: 5.86 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07246958083833464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07246958083833464 | validation: 0.1009058908607185]
	TIME [epoch: 5.87 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06334116521976661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06334116521976661 | validation: 0.07264272287121355]
	TIME [epoch: 5.87 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0474079320179526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0474079320179526 | validation: 0.0666182675494989]
	TIME [epoch: 5.86 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04011079745483876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04011079745483876 | validation: 0.06173291970247542]
	TIME [epoch: 5.86 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03466064722040093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03466064722040093 | validation: 0.05777059410179219]
	TIME [epoch: 5.87 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034444874584123895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034444874584123895 | validation: 0.04718672324115097]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036728615985975294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036728615985975294 | validation: 0.06586515856594649]
	TIME [epoch: 5.85 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045955735220767534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045955735220767534 | validation: 0.10894068241734618]
	TIME [epoch: 5.87 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07509836280061492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07509836280061492 | validation: 0.1738188348302042]
	TIME [epoch: 5.87 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1328211424112042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1328211424112042 | validation: 0.22781831324568894]
	TIME [epoch: 5.87 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560531822254897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1560531822254897 | validation: 0.12058697502501531]
	TIME [epoch: 5.86 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12291031615828925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12291031615828925 | validation: 0.13438238115374254]
	TIME [epoch: 5.87 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250570382365659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07250570382365659 | validation: 0.11581165154232964]
	TIME [epoch: 5.86 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0585674054660376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0585674054660376 | validation: 0.06979018158915112]
	TIME [epoch: 5.87 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04630645306399441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04630645306399441 | validation: 0.061866503581996635]
	TIME [epoch: 5.86 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03893834043672251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03893834043672251 | validation: 0.07127513475987074]
	TIME [epoch: 5.87 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04058146862123583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04058146862123583 | validation: 0.07230217173921648]
	TIME [epoch: 5.86 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04137204150185991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04137204150185991 | validation: 0.08845624518018802]
	TIME [epoch: 5.87 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050781797618375084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050781797618375084 | validation: 0.14327307727744784]
	TIME [epoch: 5.86 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752734433669228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0752734433669228 | validation: 0.18303771995273466]
	TIME [epoch: 5.87 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1266886880864671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1266886880864671 | validation: 0.1752586636463247]
	TIME [epoch: 5.86 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12893338628836884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12893338628836884 | validation: 0.10182285731101676]
	TIME [epoch: 5.88 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05825822310758257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05825822310758257 | validation: 0.057081680596332965]
	TIME [epoch: 5.88 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03255614251780618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03255614251780618 | validation: 0.0721312524306482]
	TIME [epoch: 5.88 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03853814152260819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03853814152260819 | validation: 0.06562686270036394]
	TIME [epoch: 5.88 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04662692394246459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04662692394246459 | validation: 0.10038933511702963]
	TIME [epoch: 5.87 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06325925949559458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06325925949559458 | validation: 0.0984878051980198]
	TIME [epoch: 5.87 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06963365454428029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06963365454428029 | validation: 0.10681740832069103]
	TIME [epoch: 5.87 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862231509463618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06862231509463618 | validation: 0.10294330463252797]
	TIME [epoch: 5.87 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06597490280454096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06597490280454096 | validation: 0.11129986548468818]
	TIME [epoch: 5.87 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06666001434500946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06666001434500946 | validation: 0.08714969805840173]
	TIME [epoch: 5.86 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676416699606396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04676416699606396 | validation: 0.052904585102997864]
	TIME [epoch: 5.86 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033556179747901024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033556179747901024 | validation: 0.06119234876141704]
	TIME [epoch: 5.85 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776327316336495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776327316336495 | validation: 0.06595335350225123]
	TIME [epoch: 5.86 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06172411350781147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06172411350781147 | validation: 0.0953351104464577]
	TIME [epoch: 5.86 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09222253822553449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09222253822553449 | validation: 0.19030881531380858]
	TIME [epoch: 5.86 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1281284767317097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1281284767317097 | validation: 0.21751260536883266]
	TIME [epoch: 5.86 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15159263666574363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15159263666574363 | validation: 0.20397489125758195]
	TIME [epoch: 5.86 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11082782713240873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11082782713240873 | validation: 0.0782575895758194]
	TIME [epoch: 5.85 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05666688401425651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05666688401425651 | validation: 0.09474204696288846]
	TIME [epoch: 5.86 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045479178899521557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045479178899521557 | validation: 0.06877718060813236]
	TIME [epoch: 5.87 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03210493283376207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03210493283376207 | validation: 0.05392130176829249]
	TIME [epoch: 5.86 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03067703059680036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03067703059680036 | validation: 0.06600369094326049]
	TIME [epoch: 5.87 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028832794901894998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028832794901894998 | validation: 0.04109025336089969]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1060.pth
	Model improved!!!
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025989927927089932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025989927927089932 | validation: 0.049542364812970356]
	TIME [epoch: 5.88 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025933940833111777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025933940833111777 | validation: 0.046487119035486295]
	TIME [epoch: 5.86 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029423248259666763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029423248259666763 | validation: 0.07178281382487144]
	TIME [epoch: 5.86 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04390051337044859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04390051337044859 | validation: 0.12263547424923811]
	TIME [epoch: 5.85 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08024653336781473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08024653336781473 | validation: 0.15793699043299517]
	TIME [epoch: 5.86 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11842198590107633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11842198590107633 | validation: 0.18912745947939272]
	TIME [epoch: 5.87 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12862091130119474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12862091130119474 | validation: 0.0755357388628432]
	TIME [epoch: 5.86 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06779284867907671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06779284867907671 | validation: 0.07141805750779513]
	TIME [epoch: 5.86 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035787641561106975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035787641561106975 | validation: 0.07389811847975493]
	TIME [epoch: 5.86 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04378840781604288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04378840781604288 | validation: 0.09133232462314804]
	TIME [epoch: 5.86 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0718715934465472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0718715934465472 | validation: 0.1163493134589297]
	TIME [epoch: 5.86 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10895255223498644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10895255223498644 | validation: 0.1857484110655454]
	TIME [epoch: 5.86 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12763030881911389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12763030881911389 | validation: 0.15813300107916672]
	TIME [epoch: 5.85 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11046865997531413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11046865997531413 | validation: 0.08692874117182404]
	TIME [epoch: 5.87 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042007252364460686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042007252364460686 | validation: 0.04736732433043769]
	TIME [epoch: 5.87 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029108090554122036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029108090554122036 | validation: 0.0715647465968185]
	TIME [epoch: 5.87 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033610679538110005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033610679538110005 | validation: 0.04339175523577798]
	TIME [epoch: 5.86 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0331438498391582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0331438498391582 | validation: 0.06973760867675043]
	TIME [epoch: 5.87 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033522066953393714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033522066953393714 | validation: 0.06239743130136632]
	TIME [epoch: 5.87 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03940542475035409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03940542475035409 | validation: 0.06219199251313078]
	TIME [epoch: 5.87 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058481188648014375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058481188648014375 | validation: 0.10930216911126807]
	TIME [epoch: 5.87 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.088886757458592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.088886757458592 | validation: 0.16146975267000188]
	TIME [epoch: 5.87 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12529152898787868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12529152898787868 | validation: 0.20707072676558622]
	TIME [epoch: 5.86 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12242357489420075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12242357489420075 | validation: 0.08196271369256165]
	TIME [epoch: 5.87 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287900241261419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05287900241261419 | validation: 0.08832838355739636]
	TIME [epoch: 5.86 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04434836987459641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04434836987459641 | validation: 0.06862205543269177]
	TIME [epoch: 5.87 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05178237162806523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05178237162806523 | validation: 0.09504932392981143]
	TIME [epoch: 5.86 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915090247085164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04915090247085164 | validation: 0.08613267592528329]
	TIME [epoch: 5.86 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04532058162040273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04532058162040273 | validation: 0.06598108348444992]
	TIME [epoch: 5.86 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04689453446180258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04689453446180258 | validation: 0.07941265121383945]
	TIME [epoch: 5.87 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04459452561062479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04459452561062479 | validation: 0.05860903443217985]
	TIME [epoch: 5.87 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051905801589498715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051905801589498715 | validation: 0.0813989401591735]
	TIME [epoch: 5.87 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062146081369734166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062146081369734166 | validation: 0.1290071571188515]
	TIME [epoch: 5.86 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08683557344395965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08683557344395965 | validation: 0.14240246133093068]
	TIME [epoch: 5.87 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10818457254300964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10818457254300964 | validation: 0.16269789218034617]
	TIME [epoch: 5.87 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10213534542297266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10213534542297266 | validation: 0.07611872313672688]
	TIME [epoch: 5.86 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05867278488299131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05867278488299131 | validation: 0.058346155851291104]
	TIME [epoch: 5.87 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034316843363463405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034316843363463405 | validation: 0.057911707539216685]
	TIME [epoch: 5.87 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03134480874968234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03134480874968234 | validation: 0.05627905023829194]
	TIME [epoch: 5.88 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030105574342321157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030105574342321157 | validation: 0.0576552333157776]
	TIME [epoch: 5.88 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03238297583787428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03238297583787428 | validation: 0.06108813297319038]
	TIME [epoch: 5.87 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03632650781870831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03632650781870831 | validation: 0.08566118557922625]
	TIME [epoch: 5.89 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050562139756498536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050562139756498536 | validation: 0.10693310574107673]
	TIME [epoch: 5.87 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696779399644109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0696779399644109 | validation: 0.1265728367740219]
	TIME [epoch: 5.87 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08952487564730395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08952487564730395 | validation: 0.1747316644557968]
	TIME [epoch: 5.86 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10649684159962246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10649684159962246 | validation: 0.08884312514437243]
	TIME [epoch: 5.86 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947165420465759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947165420465759 | validation: 0.09377402947360883]
	TIME [epoch: 5.86 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06596059878358959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06596059878358959 | validation: 0.0841264547242918]
	TIME [epoch: 5.87 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06098813050131361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06098813050131361 | validation: 0.0751632693929001]
	TIME [epoch: 5.86 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05613216794195768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05613216794195768 | validation: 0.10484568021161172]
	TIME [epoch: 5.87 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06729385419133603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06729385419133603 | validation: 0.08226170863761824]
	TIME [epoch: 5.86 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804180298072796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06804180298072796 | validation: 0.1028671357928501]
	TIME [epoch: 5.86 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05636734656285084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05636734656285084 | validation: 0.07910154160928842]
	TIME [epoch: 5.86 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795955711487276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04795955711487276 | validation: 0.08085866259175167]
	TIME [epoch: 5.86 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0501057972606466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0501057972606466 | validation: 0.07527753259785308]
	TIME [epoch: 5.86 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056693386797005994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056693386797005994 | validation: 0.10689530663081948]
	TIME [epoch: 5.86 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06063048501928478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06063048501928478 | validation: 0.10040579324476372]
	TIME [epoch: 5.86 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06353182059407014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06353182059407014 | validation: 0.07357769653115258]
	TIME [epoch: 5.87 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05283036734451793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05283036734451793 | validation: 0.06446805209529731]
	TIME [epoch: 5.86 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042454274569429476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042454274569429476 | validation: 0.05393599614172215]
	TIME [epoch: 5.87 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03448338215684884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03448338215684884 | validation: 0.04876481636769125]
	TIME [epoch: 5.87 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03309392637036605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03309392637036605 | validation: 0.06008955172273389]
	TIME [epoch: 5.88 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906726968866053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03906726968866053 | validation: 0.07302325948782543]
	TIME [epoch: 5.87 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05156083424248661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05156083424248661 | validation: 0.14462421477907958]
	TIME [epoch: 5.88 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08925573547074564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08925573547074564 | validation: 0.21916953454810245]
	TIME [epoch: 5.86 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13768534769875027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13768534769875027 | validation: 0.12933710797180306]
	TIME [epoch: 5.86 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09449222616689479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09449222616689479 | validation: 0.06584951602658459]
	TIME [epoch: 5.86 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046026285112512735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046026285112512735 | validation: 0.05202474074663496]
	TIME [epoch: 5.85 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035040178972969786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035040178972969786 | validation: 0.06947683139840186]
	TIME [epoch: 5.86 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0417243892063749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0417243892063749 | validation: 0.07223366801944663]
	TIME [epoch: 5.85 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04878294241376994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04878294241376994 | validation: 0.0737588264463058]
	TIME [epoch: 5.86 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0456766026855532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0456766026855532 | validation: 0.0648785712344987]
	TIME [epoch: 5.86 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04537339523156383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04537339523156383 | validation: 0.08960063770486913]
	TIME [epoch: 5.86 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052555635293087634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052555635293087634 | validation: 0.09612358838044321]
	TIME [epoch: 5.86 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06311382236495425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06311382236495425 | validation: 0.1745357364833087]
	TIME [epoch: 5.87 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1299348665046725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1299348665046725 | validation: 0.0940812768768371]
	TIME [epoch: 5.87 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057431153757546975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057431153757546975 | validation: 0.1110795015735361]
	TIME [epoch: 5.87 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08132015645389401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08132015645389401 | validation: 0.0710152957570675]
	TIME [epoch: 5.87 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057061758038656825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057061758038656825 | validation: 0.0744069398893206]
	TIME [epoch: 5.87 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687636263821104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03687636263821104 | validation: 0.056074924876141144]
	TIME [epoch: 5.88 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029791428452656456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029791428452656456 | validation: 0.03450121050126675]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029496103509529987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029496103509529987 | validation: 0.05065272960331518]
	TIME [epoch: 5.87 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03630075082063174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03630075082063174 | validation: 0.03962799799628776]
	TIME [epoch: 5.88 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04138208260343817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04138208260343817 | validation: 0.07921436490231243]
	TIME [epoch: 5.89 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04855387089948799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04855387089948799 | validation: 0.07450015141871094]
	TIME [epoch: 5.88 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06226319474742413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06226319474742413 | validation: 0.16479864625370444]
	TIME [epoch: 5.86 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10477058465703638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10477058465703638 | validation: 0.14472128422350616]
	TIME [epoch: 5.87 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564012984872681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10564012984872681 | validation: 0.14084883082542424]
	TIME [epoch: 5.86 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11440870817250265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11440870817250265 | validation: 0.08933354015993347]
	TIME [epoch: 5.87 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055496396169414396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055496396169414396 | validation: 0.0720850180971239]
	TIME [epoch: 5.86 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03759966606331343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03759966606331343 | validation: 0.05025146412680434]
	TIME [epoch: 5.87 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285171915933702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03285171915933702 | validation: 0.05305251888567504]
	TIME [epoch: 5.86 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285057918137421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03285057918137421 | validation: 0.05485601566509312]
	TIME [epoch: 5.87 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034620172163062034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034620172163062034 | validation: 0.09248966619592305]
	TIME [epoch: 5.86 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05026366193524836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05026366193524836 | validation: 0.09163920802470277]
	TIME [epoch: 5.87 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525791818745492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06525791818745492 | validation: 0.13108309134933377]
	TIME [epoch: 5.86 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08190027675907953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08190027675907953 | validation: 0.09626569903651588]
	TIME [epoch: 5.87 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06780844062712271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06780844062712271 | validation: 0.06651696402626606]
	TIME [epoch: 5.87 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988797023935833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04988797023935833 | validation: 0.06627723400768129]
	TIME [epoch: 5.86 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049289597203848276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049289597203848276 | validation: 0.05591507139581543]
	TIME [epoch: 5.87 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04830206221658634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04830206221658634 | validation: 0.08888885990898361]
	TIME [epoch: 5.87 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05469865928564138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05469865928564138 | validation: 0.07944503213622045]
	TIME [epoch: 5.87 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04915182973175246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04915182973175246 | validation: 0.07806925138427108]
	TIME [epoch: 5.89 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049596073282409954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049596073282409954 | validation: 0.08063414218817958]
	TIME [epoch: 5.88 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05090901638312006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05090901638312006 | validation: 0.07151920899626649]
	TIME [epoch: 5.89 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047176064901788876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047176064901788876 | validation: 0.0519981876000489]
	TIME [epoch: 5.86 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023050337266359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04023050337266359 | validation: 0.04846311124234237]
	TIME [epoch: 5.86 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030443520254010252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030443520254010252 | validation: 0.03847709832372932]
	TIME [epoch: 5.86 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030019285356156857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030019285356156857 | validation: 0.093578421851181]
	TIME [epoch: 5.86 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06325499360650637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06325499360650637 | validation: 0.22898969100156574]
	TIME [epoch: 5.86 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1542490386287132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1542490386287132 | validation: 0.19486911277541386]
	TIME [epoch: 5.86 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12695998072465536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12695998072465536 | validation: 0.07374290482487536]
	TIME [epoch: 5.87 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03672646639722602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03672646639722602 | validation: 0.055185104897352705]
	TIME [epoch: 5.87 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023693507800283384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023693507800283384 | validation: 0.048655961625712896]
	TIME [epoch: 5.88 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035496808755285734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035496808755285734 | validation: 0.07223360997772142]
	TIME [epoch: 5.87 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037401502634035085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037401502634035085 | validation: 0.07002020746622]
	TIME [epoch: 5.87 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04093799983612859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04093799983612859 | validation: 0.05691912270950936]
	TIME [epoch: 5.86 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06060615596646902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06060615596646902 | validation: 0.09280037983423506]
	TIME [epoch: 5.86 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08210862308231788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08210862308231788 | validation: 0.0918229394842063]
	TIME [epoch: 5.86 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348784473265182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07348784473265182 | validation: 0.1333616555198401]
	TIME [epoch: 5.86 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07499179441649062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07499179441649062 | validation: 0.07875983299517306]
	TIME [epoch: 5.86 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06291903243574883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06291903243574883 | validation: 0.032823210715840635]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1182.pth
	Model improved!!!
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025558269965353774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025558269965353774 | validation: 0.03906754277588359]
	TIME [epoch: 5.86 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02646964278779065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02646964278779065 | validation: 0.06625851248145768]
	TIME [epoch: 5.88 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029268488987511578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029268488987511578 | validation: 0.05479946911549748]
	TIME [epoch: 5.88 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776152120332897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776152120332897 | validation: 0.10385407419770219]
	TIME [epoch: 5.88 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0695255170778446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0695255170778446 | validation: 0.1572922580631727]
	TIME [epoch: 5.89 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12728049989803084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12728049989803084 | validation: 0.130630608984949]
	TIME [epoch: 5.88 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09335941277768164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09335941277768164 | validation: 0.0723165142807804]
	TIME [epoch: 5.88 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043644434450579556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043644434450579556 | validation: 0.059900117480593734]
	TIME [epoch: 5.87 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02703326587786192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02703326587786192 | validation: 0.03379098475904281]
	TIME [epoch: 5.87 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02181180587960272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02181180587960272 | validation: 0.05342718184033379]
	TIME [epoch: 5.87 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02552483312091523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02552483312091523 | validation: 0.059626070317038184]
	TIME [epoch: 5.87 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03643775029846668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03643775029846668 | validation: 0.0978122820184289]
	TIME [epoch: 5.87 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054239309032147805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054239309032147805 | validation: 0.11429328070345031]
	TIME [epoch: 5.87 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05848976520450554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05848976520450554 | validation: 0.08223488116460342]
	TIME [epoch: 5.87 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947485834842211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947485834842211 | validation: 0.12679735653257793]
	TIME [epoch: 5.87 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07933732077679989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07933732077679989 | validation: 0.11587201904927427]
	TIME [epoch: 5.86 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269004037273416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08269004037273416 | validation: 0.11728650151246499]
	TIME [epoch: 5.88 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06801355803532529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06801355803532529 | validation: 0.06313566332965329]
	TIME [epoch: 5.87 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05289073069217611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05289073069217611 | validation: 0.0532197547247664]
	TIME [epoch: 5.88 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05614324913928586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05614324913928586 | validation: 0.04993359873423484]
	TIME [epoch: 5.89 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04074555144955953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04074555144955953 | validation: 0.03576997501976838]
	TIME [epoch: 5.87 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027539813952257155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027539813952257155 | validation: 0.052920590259927174]
	TIME [epoch: 5.87 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02806469101047842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02806469101047842 | validation: 0.08648282740687457]
	TIME [epoch: 5.87 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044084467715941864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044084467715941864 | validation: 0.11043068169547504]
	TIME [epoch: 5.87 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06335022222280433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06335022222280433 | validation: 0.1097643172250285]
	TIME [epoch: 5.86 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06430027278625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06430027278625 | validation: 0.09354529165620402]
	TIME [epoch: 5.86 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07713036492029426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07713036492029426 | validation: 0.08106614247220495]
	TIME [epoch: 5.86 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06331512650136449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06331512650136449 | validation: 0.05386493460006066]
	TIME [epoch: 5.86 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034437647636132815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034437647636132815 | validation: 0.07063860185186165]
	TIME [epoch: 5.85 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03690477744749439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03690477744749439 | validation: 0.06756798613730841]
	TIME [epoch: 5.86 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04104353398863186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04104353398863186 | validation: 0.06697438545932137]
	TIME [epoch: 5.86 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04000724472092356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04000724472092356 | validation: 0.07437487949095006]
	TIME [epoch: 5.86 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04110506134394609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04110506134394609 | validation: 0.06421590045912816]
	TIME [epoch: 5.86 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358631342583619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0358631342583619 | validation: 0.05225310287200566]
	TIME [epoch: 5.86 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03755125475360131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03755125475360131 | validation: 0.06546986864948498]
	TIME [epoch: 5.86 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055270326120822326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055270326120822326 | validation: 0.06053661790292467]
	TIME [epoch: 5.86 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943573054796357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07943573054796357 | validation: 0.07438856205660287]
	TIME [epoch: 5.86 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054532387741985815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054532387741985815 | validation: 0.05550551214836879]
	TIME [epoch: 5.86 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034856205859605464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034856205859605464 | validation: 0.05270248712121376]
	TIME [epoch: 5.86 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02992511750862533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02992511750862533 | validation: 0.06322242317469152]
	TIME [epoch: 5.85 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039274721658993705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039274721658993705 | validation: 0.07337306254559897]
	TIME [epoch: 5.86 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05857888175041293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05857888175041293 | validation: 0.24799837985658782]
	TIME [epoch: 5.86 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16033898393099133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16033898393099133 | validation: 0.1888752197911539]
	TIME [epoch: 5.86 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.155473465378682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.155473465378682 | validation: 0.0541751904473224]
	TIME [epoch: 5.86 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04538028462501448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04538028462501448 | validation: 0.057123926806993824]
	TIME [epoch: 5.85 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032396944725138994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032396944725138994 | validation: 0.079077692243395]
	TIME [epoch: 5.86 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03515872336891935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03515872336891935 | validation: 0.05163895051458354]
	TIME [epoch: 5.86 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03154178030406996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03154178030406996 | validation: 0.03972154088835032]
	TIME [epoch: 5.86 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02834768166852456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02834768166852456 | validation: 0.03342352574575506]
	TIME [epoch: 5.86 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01939360775667204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01939360775667204 | validation: 0.031723354112739]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1232.pth
	Model improved!!!
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018774816656403046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018774816656403046 | validation: 0.03312355661966073]
	TIME [epoch: 5.86 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022175598148283397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022175598148283397 | validation: 0.05007557195881426]
	TIME [epoch: 5.86 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03129460754416712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03129460754416712 | validation: 0.1115258811288149]
	TIME [epoch: 5.86 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069039731294551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.069039731294551 | validation: 0.16269674435901577]
	TIME [epoch: 5.87 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12970054948719756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12970054948719756 | validation: 0.14093714455074555]
	TIME [epoch: 5.87 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09719442273574747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09719442273574747 | validation: 0.048399342413302474]
	TIME [epoch: 5.88 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029843438450861612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029843438450861612 | validation: 0.030289630665577574]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1239.pth
	Model improved!!!
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020066280435468854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020066280435468854 | validation: 0.04752786253566226]
	TIME [epoch: 5.88 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968369404982613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03968369404982613 | validation: 0.050367318407424824]
	TIME [epoch: 5.87 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07242635132971965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07242635132971965 | validation: 0.08449396033666956]
	TIME [epoch: 5.88 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056619255662637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056619255662637 | validation: 0.10622512724819073]
	TIME [epoch: 5.86 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05952858413590972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05952858413590972 | validation: 0.10197949714597415]
	TIME [epoch: 5.87 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06826273120904822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06826273120904822 | validation: 0.08824506777138333]
	TIME [epoch: 5.86 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567511707073603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0567511707073603 | validation: 0.06059660641296451]
	TIME [epoch: 5.86 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04688530940701158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04688530940701158 | validation: 0.11348925361511533]
	TIME [epoch: 5.86 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07371317011115791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07371317011115791 | validation: 0.11325798970907847]
	TIME [epoch: 5.86 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06819428072799961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06819428072799961 | validation: 0.08803149274052625]
	TIME [epoch: 5.85 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04924344505828719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04924344505828719 | validation: 0.05379206603875586]
	TIME [epoch: 5.86 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027323434164868772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027323434164868772 | validation: 0.03769439798957722]
	TIME [epoch: 5.85 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021474073055987447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021474073055987447 | validation: 0.03173880250037885]
	TIME [epoch: 5.86 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019319053927049405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019319053927049405 | validation: 0.040265962512407494]
	TIME [epoch: 5.86 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020142468167748437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020142468167748437 | validation: 0.03970996529844056]
	TIME [epoch: 5.86 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030666681913577753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030666681913577753 | validation: 0.07351249772606432]
	TIME [epoch: 5.86 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060316530773366135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060316530773366135 | validation: 0.10219070835357665]
	TIME [epoch: 5.86 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08379378457316491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08379378457316491 | validation: 0.11105796165365325]
	TIME [epoch: 5.85 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07857624504976292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07857624504976292 | validation: 0.07251506088062117]
	TIME [epoch: 5.88 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04426694665954125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04426694665954125 | validation: 0.06444990424066163]
	TIME [epoch: 5.86 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214605311770852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03214605311770852 | validation: 0.09385989039156875]
	TIME [epoch: 5.87 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295641717577754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07295641717577754 | validation: 0.1787025543884816]
	TIME [epoch: 5.86 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13117602745264526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13117602745264526 | validation: 0.07683883130543032]
	TIME [epoch: 5.87 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06057930535780679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06057930535780679 | validation: 0.05617091541380729]
	TIME [epoch: 5.86 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03605761829726961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03605761829726961 | validation: 0.034818007799328]
	TIME [epoch: 5.87 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030720079516694466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030720079516694466 | validation: 0.03580587985138987]
	TIME [epoch: 5.86 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024873424225060774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024873424225060774 | validation: 0.0424500897454565]
	TIME [epoch: 5.87 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022868281815507766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022868281815507766 | validation: 0.03787114911909509]
	TIME [epoch: 5.86 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025116862119720373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025116862119720373 | validation: 0.060081039958078286]
	TIME [epoch: 5.86 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032588114879593996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032588114879593996 | validation: 0.08229319606608991]
	TIME [epoch: 5.86 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051892998507991804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051892998507991804 | validation: 0.12430661263580817]
	TIME [epoch: 5.86 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09026539830620584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09026539830620584 | validation: 0.16735993103453992]
	TIME [epoch: 5.86 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10985306921759502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10985306921759502 | validation: 0.1044458685408606]
	TIME [epoch: 5.86 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07160486026161858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07160486026161858 | validation: 0.06924143040360067]
	TIME [epoch: 5.85 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043478157276450775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043478157276450775 | validation: 0.0529865126127022]
	TIME [epoch: 5.86 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04126614777992251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04126614777992251 | validation: 0.0679375165232534]
	TIME [epoch: 5.86 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036297170329704684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036297170329704684 | validation: 0.038518255394814176]
	TIME [epoch: 5.86 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02941559750307051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02941559750307051 | validation: 0.03880173667175904]
	TIME [epoch: 5.85 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02536725214393102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02536725214393102 | validation: 0.03968185506937454]
	TIME [epoch: 5.87 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025992659984513704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025992659984513704 | validation: 0.05304025571548091]
	TIME [epoch: 5.87 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036970228877776845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036970228877776845 | validation: 0.09523851755475628]
	TIME [epoch: 5.87 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0664643307636217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0664643307636217 | validation: 0.13293664645607448]
	TIME [epoch: 5.87 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10239703998127238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10239703998127238 | validation: 0.11543553273097969]
	TIME [epoch: 5.88 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710329312405108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07710329312405108 | validation: 0.045223125483153784]
	TIME [epoch: 5.88 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033904697409109036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033904697409109036 | validation: 0.05227383364145321]
	TIME [epoch: 5.87 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024265666320870124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024265666320870124 | validation: 0.030479048696907575]
	TIME [epoch: 5.86 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026768566758326732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026768566758326732 | validation: 0.05928165446851807]
	TIME [epoch: 5.86 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03434610977385558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03434610977385558 | validation: 0.06394115812428235]
	TIME [epoch: 5.86 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05068133154574511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05068133154574511 | validation: 0.07856480968578383]
	TIME [epoch: 5.86 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05714759322757635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05714759322757635 | validation: 0.08368488230142011]
	TIME [epoch: 5.86 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0573626293789084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0573626293789084 | validation: 0.04982549117721696]
	TIME [epoch: 5.85 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031519434277251214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031519434277251214 | validation: 0.035297118222009476]
	TIME [epoch: 5.86 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01932353352350436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01932353352350436 | validation: 0.030441272068889937]
	TIME [epoch: 5.85 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01990744241818097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01990744241818097 | validation: 0.0932535669302729]
	TIME [epoch: 5.86 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053645837456495114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053645837456495114 | validation: 0.16741323606352512]
	TIME [epoch: 5.85 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15120190880914292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15120190880914292 | validation: 0.16808365557078264]
	TIME [epoch: 5.86 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11264558216581401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11264558216581401 | validation: 0.045068770434818266]
	TIME [epoch: 5.87 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06633384438870586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06633384438870586 | validation: 0.044514933329227804]
	TIME [epoch: 5.88 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02269145838956891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02269145838956891 | validation: 0.05278261059739151]
	TIME [epoch: 5.87 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028190391546694366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028190391546694366 | validation: 0.04110775197806467]
	TIME [epoch: 5.86 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027500733073828557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027500733073828557 | validation: 0.03957298729161507]
	TIME [epoch: 5.86 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022648269216189752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022648269216189752 | validation: 0.03832925052658797]
	TIME [epoch: 5.86 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025257082781254398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025257082781254398 | validation: 0.05072221340847143]
	TIME [epoch: 5.86 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03299714832694515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03299714832694515 | validation: 0.0964318895410004]
	TIME [epoch: 5.85 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062280446408586654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062280446408586654 | validation: 0.13397111808747764]
	TIME [epoch: 5.85 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09903420912274825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09903420912274825 | validation: 0.14618432698831116]
	TIME [epoch: 5.85 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09925003143409646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09925003143409646 | validation: 0.078358826564147]
	TIME [epoch: 5.85 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05203565954250122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05203565954250122 | validation: 0.03878295251596723]
	TIME [epoch: 5.85 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02403481597848426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02403481597848426 | validation: 0.037755925277166286]
	TIME [epoch: 5.86 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021471052175593003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021471052175593003 | validation: 0.03269499609483905]
	TIME [epoch: 5.86 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021223794190478023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021223794190478023 | validation: 0.026733567455454844]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1310.pth
	Model improved!!!
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01897314339455391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01897314339455391 | validation: 0.031229776423318492]
	TIME [epoch: 5.86 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021392878925656018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021392878925656018 | validation: 0.04768758660924225]
	TIME [epoch: 5.86 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623584575299206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03623584575299206 | validation: 0.16166147041556989]
	TIME [epoch: 5.86 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09922715464760139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09922715464760139 | validation: 0.14737247458706718]
	TIME [epoch: 5.86 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12146154550888011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12146154550888011 | validation: 0.09676763610206551]
	TIME [epoch: 5.86 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07601337326853669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07601337326853669 | validation: 0.049482203311982824]
	TIME [epoch: 5.86 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579282540282409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04579282540282409 | validation: 0.07716304553779849]
	TIME [epoch: 5.87 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043574307575243755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043574307575243755 | validation: 0.05871586786663178]
	TIME [epoch: 5.86 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034873497110874015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034873497110874015 | validation: 0.044098066443415886]
	TIME [epoch: 5.86 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025827342918810784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025827342918810784 | validation: 0.03377746227211531]
	TIME [epoch: 5.85 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020975885254191598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020975885254191598 | validation: 0.030828327144060988]
	TIME [epoch: 5.86 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02027603283775899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02027603283775899 | validation: 0.04183186485877182]
	TIME [epoch: 5.85 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024655530109638724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024655530109638724 | validation: 0.05234681904863317]
	TIME [epoch: 5.87 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04672756114537847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04672756114537847 | validation: 0.11493203130125047]
	TIME [epoch: 5.87 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10055159856184492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10055159856184492 | validation: 0.13767084624625903]
	TIME [epoch: 5.86 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110168432205405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1110168432205405 | validation: 0.08795852552720289]
	TIME [epoch: 5.86 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05644269616904276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05644269616904276 | validation: 0.10556147924535303]
	TIME [epoch: 5.86 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766496988387959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06766496988387959 | validation: 0.07370319208840886]
	TIME [epoch: 5.85 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05663823997315779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05663823997315779 | validation: 0.048380485165072866]
	TIME [epoch: 5.86 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026709113237777037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026709113237777037 | validation: 0.024189064461151145]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1330.pth
	Model improved!!!
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.017011623056065454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.017011623056065454 | validation: 0.03263200410708721]
	TIME [epoch: 5.86 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01622457095094301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01622457095094301 | validation: 0.033458679788519664]
	TIME [epoch: 5.86 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0188693294520863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0188693294520863 | validation: 0.04942244670882761]
	TIME [epoch: 5.85 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029732784764491153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029732784764491153 | validation: 0.07726762725400324]
	TIME [epoch: 5.86 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051811617756054036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051811617756054036 | validation: 0.09079327261222948]
	TIME [epoch: 5.88 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06698127054109816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06698127054109816 | validation: 0.0986115126578006]
	TIME [epoch: 5.88 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0667988946275565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0667988946275565 | validation: 0.08095893102915912]
	TIME [epoch: 5.88 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787553631870181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787553631870181 | validation: 0.09994794444523941]
	TIME [epoch: 5.88 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07048531445457788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07048531445457788 | validation: 0.07482531226552767]
	TIME [epoch: 5.87 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03729467775967344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03729467775967344 | validation: 0.04856950812514105]
	TIME [epoch: 5.86 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028499254258110146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028499254258110146 | validation: 0.03946590428961835]
	TIME [epoch: 5.86 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02597035379585992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02597035379585992 | validation: 0.027098401112705263]
	TIME [epoch: 5.85 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031206042326469303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031206042326469303 | validation: 0.04353112341115422]
	TIME [epoch: 5.86 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03821953509493261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03821953509493261 | validation: 0.02360057100145301]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1344.pth
	Model improved!!!
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0316119545408975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0316119545408975 | validation: 0.030806855849686357]
	TIME [epoch: 5.87 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024396849193785403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024396849193785403 | validation: 0.06559590284631431]
	TIME [epoch: 5.88 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04244294376575356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04244294376575356 | validation: 0.13683922040838556]
	TIME [epoch: 5.87 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08424721173768134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08424721173768134 | validation: 0.10402210427453187]
	TIME [epoch: 5.86 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07381671544107873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07381671544107873 | validation: 0.06183793709971357]
	TIME [epoch: 5.87 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03769138224445533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03769138224445533 | validation: 0.047925627409358844]
	TIME [epoch: 5.86 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050265511194986935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050265511194986935 | validation: 0.08254013337917426]
	TIME [epoch: 5.86 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05457548947562458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05457548947562458 | validation: 0.09083470229070223]
	TIME [epoch: 5.85 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05641410250631199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05641410250631199 | validation: 0.1020497865306961]
	TIME [epoch: 5.86 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07185149088219897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07185149088219897 | validation: 0.09327710887833501]
	TIME [epoch: 5.86 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06450044712512094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06450044712512094 | validation: 0.040709594723616405]
	TIME [epoch: 5.87 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031072795859424664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031072795859424664 | validation: 0.03603228001658792]
	TIME [epoch: 5.88 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029237577115857838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029237577115857838 | validation: 0.04248117966384675]
	TIME [epoch: 5.87 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03309640114317336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03309640114317336 | validation: 0.06023948805921832]
	TIME [epoch: 5.86 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04768785220769442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04768785220769442 | validation: 0.09102432210519495]
	TIME [epoch: 5.86 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0614788273411072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0614788273411072 | validation: 0.07892636260354262]
	TIME [epoch: 5.85 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05531228064886778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05531228064886778 | validation: 0.051723037057640114]
	TIME [epoch: 5.86 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031985039815528804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031985039815528804 | validation: 0.04684782752488819]
	TIME [epoch: 5.86 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032807517283629746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032807517283629746 | validation: 0.0614121554796367]
	TIME [epoch: 5.88 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049951062258371176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049951062258371176 | validation: 0.06766301189068807]
	TIME [epoch: 5.88 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052509213167450684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052509213167450684 | validation: 0.06618531547834809]
	TIME [epoch: 5.87 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04610672637048966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04610672637048966 | validation: 0.06281243940212888]
	TIME [epoch: 5.86 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03195327197722647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03195327197722647 | validation: 0.03960524504939905]
	TIME [epoch: 5.86 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025273169053219604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025273169053219604 | validation: 0.07189020496659002]
	TIME [epoch: 5.86 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04136575549710445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04136575549710445 | validation: 0.0950998093399133]
	TIME [epoch: 5.86 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06293221013178792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06293221013178792 | validation: 0.09449538579577174]
	TIME [epoch: 5.86 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07065702167910506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07065702167910506 | validation: 0.07571467432893951]
	TIME [epoch: 5.89 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0640423329057418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0640423329057418 | validation: 0.03138459339658739]
	TIME [epoch: 5.87 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032125155223320795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032125155223320795 | validation: 0.02677180498926618]
	TIME [epoch: 5.87 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014246350070661155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014246350070661155 | validation: 0.02449939568227351]
	TIME [epoch: 5.86 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011686073655988918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011686073655988918 | validation: 0.022887311197226114]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1375.pth
	Model improved!!!
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01448441319668788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01448441319668788 | validation: 0.033758118537901165]
	TIME [epoch: 5.86 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01753413913570395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01753413913570395 | validation: 0.05543186101978501]
	TIME [epoch: 5.87 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03025640850484323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03025640850484323 | validation: 0.09150725509382242]
	TIME [epoch: 5.87 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053200462016706904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053200462016706904 | validation: 0.12098111198947806]
	TIME [epoch: 5.88 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07796641171278393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07796641171278393 | validation: 0.10102172384910604]
	TIME [epoch: 5.87 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10175903706923546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10175903706923546 | validation: 0.11769831467838548]
	TIME [epoch: 5.87 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1023707220080809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1023707220080809 | validation: 0.122636497659895]
	TIME [epoch: 5.87 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08380772682927141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08380772682927141 | validation: 0.08367852213527825]
	TIME [epoch: 5.87 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06861453106471352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06861453106471352 | validation: 0.03211923684151843]
	TIME [epoch: 5.86 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05119497719909655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05119497719909655 | validation: 0.038289251541160876]
	TIME [epoch: 5.87 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0170419613877521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0170419613877521 | validation: 0.022162788955182245]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1386.pth
	Model improved!!!
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021064523623134424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021064523623134424 | validation: 0.027803113880704413]
	TIME [epoch: 5.86 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021133335416334803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021133335416334803 | validation: 0.029096653385583218]
	TIME [epoch: 5.86 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016702818398100856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016702818398100856 | validation: 0.024033226046363487]
	TIME [epoch: 5.87 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018619628119687728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018619628119687728 | validation: 0.040994937432572]
	TIME [epoch: 5.86 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030870192711160555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030870192711160555 | validation: 0.08291717752993671]
	TIME [epoch: 5.86 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218139005382445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06218139005382445 | validation: 0.16336240805629765]
	TIME [epoch: 5.86 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11257577615610323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11257577615610323 | validation: 0.141026138131558]
	TIME [epoch: 5.87 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09155941943581329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09155941943581329 | validation: 0.0781684053123063]
	TIME [epoch: 5.86 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04497076866668901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04497076866668901 | validation: 0.03906442251436107]
	TIME [epoch: 5.87 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026280393782637176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026280393782637176 | validation: 0.05424445755486443]
	TIME [epoch: 5.85 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02827605669493086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02827605669493086 | validation: 0.04470367751908644]
	TIME [epoch: 5.87 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0250768671700434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0250768671700434 | validation: 0.03857977717522131]
	TIME [epoch: 5.86 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024277881270522796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024277881270522796 | validation: 0.03384515503148682]
	TIME [epoch: 5.88 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022328564802115234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022328564802115234 | validation: 0.026626841039798312]
	TIME [epoch: 5.86 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021595056828098366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021595056828098366 | validation: 0.03672559845397744]
	TIME [epoch: 5.86 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02967687578337559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02967687578337559 | validation: 0.04387991985059119]
	TIME [epoch: 5.87 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04031990088197764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04031990088197764 | validation: 0.08415372970503655]
	TIME [epoch: 5.88 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05783162459449095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05783162459449095 | validation: 0.13461813834212655]
	TIME [epoch: 5.87 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09326863629064862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09326863629064862 | validation: 0.14602778061169747]
	TIME [epoch: 5.88 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0852659241113539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0852659241113539 | validation: 0.08658341439529532]
	TIME [epoch: 5.88 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873054201893519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03873054201893519 | validation: 0.047109148115744674]
	TIME [epoch: 5.89 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03487263369223939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03487263369223939 | validation: 0.04523411538585274]
	TIME [epoch: 5.87 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055397097226222825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055397097226222825 | validation: 0.05903659013428558]
	TIME [epoch: 5.88 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0490594071806777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0490594071806777 | validation: 0.06897889855137326]
	TIME [epoch: 5.87 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044857724174901234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044857724174901234 | validation: 0.06855181324010695]
	TIME [epoch: 5.88 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041949033587954966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041949033587954966 | validation: 0.03481080530022559]
	TIME [epoch: 5.86 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02096854130026769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02096854130026769 | validation: 0.01566405792795276]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1413.pth
	Model improved!!!
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011287785426434197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011287785426434197 | validation: 0.02237474204936876]
	TIME [epoch: 5.86 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01167682021450258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01167682021450258 | validation: 0.020197286837802277]
	TIME [epoch: 5.87 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.012509221162992537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.012509221162992537 | validation: 0.041745304058265154]
	TIME [epoch: 5.87 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023432825475648515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023432825475648515 | validation: 0.11424973216926976]
	TIME [epoch: 5.87 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06967871873711673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06967871873711673 | validation: 0.17899447748414055]
	TIME [epoch: 5.87 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14215250548444172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14215250548444172 | validation: 0.13460214542661247]
	TIME [epoch: 5.87 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09362715223338736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09362715223338736 | validation: 0.060914548891387504]
	TIME [epoch: 5.87 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037185959060405796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037185959060405796 | validation: 0.043691050960402836]
	TIME [epoch: 5.87 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04362839837288858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04362839837288858 | validation: 0.07271137124324223]
	TIME [epoch: 5.86 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676419708259627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04676419708259627 | validation: 0.04458169708213862]
	TIME [epoch: 5.87 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03168512461841592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03168512461841592 | validation: 0.029002756695594337]
	TIME [epoch: 5.88 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020310527401481534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020310527401481534 | validation: 0.037377496877268455]
	TIME [epoch: 5.87 sec]
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01997709969944675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01997709969944675 | validation: 0.03320547263977013]
	TIME [epoch: 5.88 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021077050947300703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021077050947300703 | validation: 0.04620566067440812]
	TIME [epoch: 5.86 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029363772973375397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029363772973375397 | validation: 0.06498804715168871]
	TIME [epoch: 5.87 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04043924907061971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04043924907061971 | validation: 0.0853050021101162]
	TIME [epoch: 5.87 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06137844443316075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06137844443316075 | validation: 0.07695330693956234]
	TIME [epoch: 5.87 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06073315131672263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06073315131672263 | validation: 0.04455923218542558]
	TIME [epoch: 5.86 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04555156202983809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04555156202983809 | validation: 0.04917794848738836]
	TIME [epoch: 5.87 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03194585639127252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03194585639127252 | validation: 0.052670221144011264]
	TIME [epoch: 5.86 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142670122867774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03142670122867774 | validation: 0.06979316150595453]
	TIME [epoch: 5.86 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04073936342413088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04073936342413088 | validation: 0.07979533777010585]
	TIME [epoch: 5.87 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0477664403272042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0477664403272042 | validation: 0.05578983604066104]
	TIME [epoch: 5.87 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059485855038447735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059485855038447735 | validation: 0.07333984854934832]
	TIME [epoch: 5.87 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05369669813964272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05369669813964272 | validation: 0.03525333699362126]
	TIME [epoch: 5.87 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02947906390953385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02947906390953385 | validation: 0.025895851635356827]
	TIME [epoch: 5.87 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023171489085161468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023171489085161468 | validation: 0.03741304097250724]
	TIME [epoch: 5.87 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03279061004992112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03279061004992112 | validation: 0.06512196203180916]
	TIME [epoch: 5.86 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05368872059571343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05368872059571343 | validation: 0.1381007395826722]
	TIME [epoch: 5.89 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.087941917406129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.087941917406129 | validation: 0.09078075273342119]
	TIME [epoch: 5.88 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059630579261745975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059630579261745975 | validation: 0.03642810689513966]
	TIME [epoch: 5.88 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022706643420724176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022706643420724176 | validation: 0.027934368606039397]
	TIME [epoch: 5.88 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020023490598927746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020023490598927746 | validation: 0.03170211997397412]
	TIME [epoch: 5.86 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025840375999859635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025840375999859635 | validation: 0.04905242600675052]
	TIME [epoch: 5.87 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03903703719025127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03903703719025127 | validation: 0.08878581968226955]
	TIME [epoch: 5.86 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06516083626935534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06516083626935534 | validation: 0.083732879744807]
	TIME [epoch: 5.86 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054709278719143044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054709278719143044 | validation: 0.05947738975625683]
	TIME [epoch: 5.86 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776061278874842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776061278874842 | validation: 0.05869385627758353]
	TIME [epoch: 5.86 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035626997956954724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035626997956954724 | validation: 0.04584850440304185]
	TIME [epoch: 5.86 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037513872017116034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037513872017116034 | validation: 0.06321030151485132]
	TIME [epoch: 5.87 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0432675214891348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0432675214891348 | validation: 0.0615448103310834]
	TIME [epoch: 5.86 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04668734115731562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04668734115731562 | validation: 0.03776372060683334]
	TIME [epoch: 5.87 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04049547845604723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04049547845604723 | validation: 0.04683825948610201]
	TIME [epoch: 5.87 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201547292908489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03201547292908489 | validation: 0.03308738734479614]
	TIME [epoch: 5.87 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023941747765028998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023941747765028998 | validation: 0.04953385248165004]
	TIME [epoch: 5.86 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02921831244664347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02921831244664347 | validation: 0.10158900788654701]
	TIME [epoch: 5.87 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05465548295284073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05465548295284073 | validation: 0.0970537099403429]
	TIME [epoch: 5.86 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06511837330092506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06511837330092506 | validation: 0.09937114083383589]
	TIME [epoch: 5.86 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06319823700459276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06319823700459276 | validation: 0.055506098209145864]
	TIME [epoch: 5.86 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05678209087667391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05678209087667391 | validation: 0.026822880350979572]
	TIME [epoch: 5.87 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02974877556632942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02974877556632942 | validation: 0.03225586765371422]
	TIME [epoch: 5.86 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02196402012944787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02196402012944787 | validation: 0.044836007291192054]
	TIME [epoch: 5.87 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03216285010786768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03216285010786768 | validation: 0.06244456386568327]
	TIME [epoch: 5.86 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049547288368037894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049547288368037894 | validation: 0.09426313385061794]
	TIME [epoch: 5.85 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06027334897708657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06027334897708657 | validation: 0.0634032007653674]
	TIME [epoch: 5.87 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048618906197906465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048618906197906465 | validation: 0.05561908663861845]
	TIME [epoch: 5.86 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028229121248156942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028229121248156942 | validation: 0.03893257930470656]
	TIME [epoch: 5.87 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01903994002702001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01903994002702001 | validation: 0.041358744065537245]
	TIME [epoch: 5.86 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030623619426835114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030623619426835114 | validation: 0.06860989215192653]
	TIME [epoch: 5.87 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05400459572757216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05400459572757216 | validation: 0.07137178014007935]
	TIME [epoch: 5.86 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06668106902837416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06668106902837416 | validation: 0.047457202894990426]
	TIME [epoch: 5.87 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036238147973090815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036238147973090815 | validation: 0.05115646985113297]
	TIME [epoch: 5.86 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030546513965754064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030546513965754064 | validation: 0.08276861308509227]
	TIME [epoch: 5.87 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049225730274780125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049225730274780125 | validation: 0.0835371895496526]
	TIME [epoch: 5.87 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05237713741361333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05237713741361333 | validation: 0.07865747858029346]
	TIME [epoch: 5.87 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0559821500295753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0559821500295753 | validation: 0.06701639548870224]
	TIME [epoch: 5.86 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054355336828646746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054355336828646746 | validation: 0.041486342596951435]
	TIME [epoch: 5.87 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031037358551395063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031037358551395063 | validation: 0.027544755313428572]
	TIME [epoch: 5.86 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01797846451324763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01797846451324763 | validation: 0.03688722029796824]
	TIME [epoch: 5.87 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018919849200952788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018919849200952788 | validation: 0.03714994320149454]
	TIME [epoch: 5.86 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028250659696443876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028250659696443876 | validation: 0.08539340820187183]
	TIME [epoch: 5.88 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04769328612733888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04769328612733888 | validation: 0.07110275269811529]
	TIME [epoch: 5.85 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055774883646057795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055774883646057795 | validation: 0.06814542693461093]
	TIME [epoch: 5.87 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04745258228205139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04745258228205139 | validation: 0.041090916261551536]
	TIME [epoch: 5.86 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031039559715505056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031039559715505056 | validation: 0.027943212977714918]
	TIME [epoch: 5.86 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024578009009658234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024578009009658234 | validation: 0.06950984854109034]
	TIME [epoch: 5.86 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616275856976807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04616275856976807 | validation: 0.1351624682053644]
	TIME [epoch: 5.86 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10071836812855516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10071836812855516 | validation: 0.11013313871383641]
	TIME [epoch: 5.86 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0661035226293939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0661035226293939 | validation: 0.017631097009269205]
	TIME [epoch: 5.86 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023429971184445612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023429971184445612 | validation: 0.030235187827815493]
	TIME [epoch: 5.86 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01949831667866109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01949831667866109 | validation: 0.03256827679522805]
	TIME [epoch: 5.86 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022512885660959485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022512885660959485 | validation: 0.03728877364520531]
	TIME [epoch: 5.86 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023710442104822534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023710442104822534 | validation: 0.03420872810898634]
	TIME [epoch: 5.86 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019268655913888686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019268655913888686 | validation: 0.030513597023681772]
	TIME [epoch: 5.86 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020949758043138395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020949758043138395 | validation: 0.030736676298293065]
	TIME [epoch: 5.86 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03277882809156787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03277882809156787 | validation: 0.06760161251341908]
	TIME [epoch: 5.87 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060178043596539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060178043596539 | validation: 0.1289876466114784]
	TIME [epoch: 5.87 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09914469590796376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09914469590796376 | validation: 0.1460171573353764]
	TIME [epoch: 5.87 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08544740153602491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08544740153602491 | validation: 0.05938923325809742]
	TIME [epoch: 5.87 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03255876709943005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03255876709943005 | validation: 0.025852114764664636]
	TIME [epoch: 5.87 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02553240895553803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02553240895553803 | validation: 0.044922604186333016]
	TIME [epoch: 5.87 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028242753869661127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028242753869661127 | validation: 0.027236556538737935]
	TIME [epoch: 5.87 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021353784718950806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021353784718950806 | validation: 0.03825664870624454]
	TIME [epoch: 5.87 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024207748985685573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024207748985685573 | validation: 0.06221316386643188]
	TIME [epoch: 5.87 sec]
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04307065905362156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04307065905362156 | validation: 0.11122942755057173]
	TIME [epoch: 5.86 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06830550214244782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06830550214244782 | validation: 0.07059004857380846]
	TIME [epoch: 5.87 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055366616058158825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055366616058158825 | validation: 0.03803107493107988]
	TIME [epoch: 5.87 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027679421292161603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027679421292161603 | validation: 0.032733936976377775]
	TIME [epoch: 5.87 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0244563524387017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0244563524387017 | validation: 0.04902707833351832]
	TIME [epoch: 5.86 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03528802345163494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03528802345163494 | validation: 0.059070989378983675]
	TIME [epoch: 5.87 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043033486708618146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043033486708618146 | validation: 0.06899816117346289]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_125506/states/model_phi1_4a_v_mmd1_1514.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5737.419 seconds.
