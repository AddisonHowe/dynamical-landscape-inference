Args:
Namespace(name='model_phi1_4a_v_mmd1', outdir='out/model_training/model_phi1_4a_v_mmd1', training_data='data/training_data/data_phi1_4a/training', validation_data='data/training_data/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 546365229

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.936710119599706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.936710119599706 | validation: 5.381990685448209]
	TIME [epoch: 163 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.144606010960921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.144606010960921 | validation: 6.344442319041638]
	TIME [epoch: 0.787 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.868111389670418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.868111389670418 | validation: 5.886346988664643]
	TIME [epoch: 0.708 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9314504126956478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9314504126956478 | validation: 5.321807333173165]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.030480385855367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.030480385855367 | validation: 4.964128499158391]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8401097450268424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8401097450268424 | validation: 4.82309032093727]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5151118037650075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5151118037650075 | validation: 4.674919819911728]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.335098211992632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.335098211992632 | validation: 4.102221799898797]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0729741339421968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0729741339421968 | validation: 3.9177524835050868]
	TIME [epoch: 0.714 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.695815329053019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.695815329053019 | validation: 2.403543403595039]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3825462713719396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3825462713719396 | validation: 3.2102228002947224]
	TIME [epoch: 0.714 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6933627676215526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6933627676215526 | validation: 1.4762644770584419]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0928710168294793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0928710168294793 | validation: 2.9049414788862644]
	TIME [epoch: 0.714 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9273852375154514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9273852375154514 | validation: 1.9337117774132324]
	TIME [epoch: 0.708 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6903879000929356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6903879000929356 | validation: 2.013085604166723]
	TIME [epoch: 0.707 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.885752609689876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.885752609689876 | validation: 1.7362920358168183]
	TIME [epoch: 0.706 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5621552692027383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5621552692027383 | validation: 1.5212882560828205]
	TIME [epoch: 0.706 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.493057431063908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.493057431063908 | validation: 1.341067270422213]
	TIME [epoch: 0.705 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4605715771220513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4605715771220513 | validation: 1.3060597996083452]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4013883454371603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4013883454371603 | validation: 1.2751107435786224]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3447243755496692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3447243755496692 | validation: 1.1153623401745045]
	TIME [epoch: 0.711 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.313117171738773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.313117171738773 | validation: 1.116179252235545]
	TIME [epoch: 0.713 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.270945718763781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.270945718763781 | validation: 0.9942495056171258]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2449903634586763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2449903634586763 | validation: 1.033637764555919]
	TIME [epoch: 0.711 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2294828314792383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2294828314792383 | validation: 0.9907772905904129]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.237237249153186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.237237249153186 | validation: 1.139517998140746]
	TIME [epoch: 0.712 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3149815343312616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3149815343312616 | validation: 1.0932520010121365]
	TIME [epoch: 0.711 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.323736478513286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.323736478513286 | validation: 1.1346845871864075]
	TIME [epoch: 0.707 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2853866618670091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2853866618670091 | validation: 0.9722833781541653]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1691103266976077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1691103266976077 | validation: 0.8927772524700637]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1370210477108944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1370210477108944 | validation: 0.8374260155875479]
	TIME [epoch: 0.715 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1303228597881236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1303228597881236 | validation: 0.8541184365564214]
	TIME [epoch: 0.715 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1229214392573785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1229214392573785 | validation: 0.9429894522870441]
	TIME [epoch: 0.712 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1181137757844815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1181137757844815 | validation: 0.870218409297295]
	TIME [epoch: 0.71 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1336372571924198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1336372571924198 | validation: 1.0443127755410078]
	TIME [epoch: 0.707 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2054057594143361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2054057594143361 | validation: 1.0566684664906247]
	TIME [epoch: 0.708 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2233730487863612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2233730487863612 | validation: 1.225912437521241]
	TIME [epoch: 0.706 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1916543303773557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1916543303773557 | validation: 0.835557648332896]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0784318904258314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0784318904258314 | validation: 1.11624483326662]
	TIME [epoch: 0.713 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0382424887952202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0382424887952202 | validation: 0.755710726770676]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0411526743969222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0411526743969222 | validation: 1.4939921416423871]
	TIME [epoch: 0.711 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1585406237394325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1585406237394325 | validation: 0.7503520263342787]
	TIME [epoch: 0.71 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4903393277127885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4903393277127885 | validation: 0.7454117702590319]
	TIME [epoch: 0.708 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1142338252309865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1142338252309865 | validation: 1.4146067599444656]
	TIME [epoch: 0.711 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1357446475834256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1357446475834256 | validation: 0.7452855030695379]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0432229265789132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0432229265789132 | validation: 0.7017420059198115]
	TIME [epoch: 0.709 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0268575907847648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0268575907847648 | validation: 0.9340958321691561]
	TIME [epoch: 0.712 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9783570770864333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9783570770864333 | validation: 1.1243139268576472]
	TIME [epoch: 0.707 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9821579162627933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9821579162627933 | validation: 0.785853363850104]
	TIME [epoch: 0.705 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9667974033240531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9667974033240531 | validation: 0.9320872322085632]
	TIME [epoch: 0.704 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9377168184383738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9377168184383738 | validation: 1.0102124890469433]
	TIME [epoch: 0.706 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9480242355959164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9480242355959164 | validation: 0.8506586004981905]
	TIME [epoch: 0.704 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9915078598963888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9915078598963888 | validation: 1.142421281473087]
	TIME [epoch: 0.704 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0357641117030496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0357641117030496 | validation: 0.9474346734385173]
	TIME [epoch: 0.735 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0643848227190966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0643848227190966 | validation: 0.9116416370897445]
	TIME [epoch: 0.705 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9444082075768373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9444082075768373 | validation: 1.096976940397578]
	TIME [epoch: 0.704 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9658271263814622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9658271263814622 | validation: 0.6160697420725267]
	TIME [epoch: 0.704 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.087264701165423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.087264701165423 | validation: 0.9050359473308931]
	TIME [epoch: 0.709 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9243573440501106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9243573440501106 | validation: 1.1493990987554825]
	TIME [epoch: 0.707 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0090043799302366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0090043799302366 | validation: 0.5818601938166852]
	TIME [epoch: 0.706 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0880076901077893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0880076901077893 | validation: 0.6954363426995537]
	TIME [epoch: 0.71 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925690439998134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.925690439998134 | validation: 1.1907621305656206]
	TIME [epoch: 0.708 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0215497068827577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0215497068827577 | validation: 0.6156258603909074]
	TIME [epoch: 0.706 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9802763282812292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9802763282812292 | validation: 0.6492097580602556]
	TIME [epoch: 0.705 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9295099000694518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9295099000694518 | validation: 0.9811217666742668]
	TIME [epoch: 0.705 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9168203175068282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9168203175068282 | validation: 0.805955812864916]
	TIME [epoch: 0.706 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915753591223294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8915753591223294 | validation: 0.8085261105084112]
	TIME [epoch: 0.705 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9020148616306547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9020148616306547 | validation: 0.8494407130596532]
	TIME [epoch: 0.704 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9280378945801067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9280378945801067 | validation: 1.040118443709934]
	TIME [epoch: 0.704 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.010122369592694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.010122369592694 | validation: 0.7933508197102869]
	TIME [epoch: 0.705 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9539371440320545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9539371440320545 | validation: 0.9430923767250665]
	TIME [epoch: 0.704 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9196255532602586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9196255532602586 | validation: 0.7211656904884246]
	TIME [epoch: 0.704 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815934926237675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815934926237675 | validation: 0.8361008116146199]
	TIME [epoch: 0.704 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645375373653726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645375373653726 | validation: 0.7225400977195531]
	TIME [epoch: 0.705 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692640598078063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8692640598078063 | validation: 0.8256524949105177]
	TIME [epoch: 0.703 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862708187315526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.862708187315526 | validation: 0.6659284749476463]
	TIME [epoch: 0.704 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638851632579203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638851632579203 | validation: 0.9214248617349974]
	TIME [epoch: 0.707 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8749822840151475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8749822840151475 | validation: 0.6070377087976135]
	TIME [epoch: 0.708 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9967455037784428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9967455037784428 | validation: 1.0961108998471232]
	TIME [epoch: 0.705 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0218680544733476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0218680544733476 | validation: 0.6483261882224712]
	TIME [epoch: 0.705 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0082047062781274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0082047062781274 | validation: 0.6985273892752482]
	TIME [epoch: 0.705 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752540040271172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752540040271172 | validation: 1.110811969190442]
	TIME [epoch: 0.707 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9690240617877056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9690240617877056 | validation: 0.5989491211793804]
	TIME [epoch: 0.704 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220921817184123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9220921817184123 | validation: 0.7555497428841478]
	TIME [epoch: 0.705 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8748162753195527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8748162753195527 | validation: 0.8873060474214465]
	TIME [epoch: 0.705 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9092944368527092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9092944368527092 | validation: 0.7680950911208237]
	TIME [epoch: 0.707 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9478490668807507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9478490668807507 | validation: 0.712858044040086]
	TIME [epoch: 0.705 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075576700960598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075576700960598 | validation: 0.9549550149104467]
	TIME [epoch: 0.705 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9037053100338657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9037053100338657 | validation: 0.6818846126751619]
	TIME [epoch: 0.704 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8454415556107588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8454415556107588 | validation: 0.7297639787208661]
	TIME [epoch: 0.705 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8394796948349722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8394796948349722 | validation: 0.7710290054204265]
	TIME [epoch: 0.704 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424712913062657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8424712913062657 | validation: 0.7184305623963614]
	TIME [epoch: 0.704 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640122139317599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8640122139317599 | validation: 0.808792328811626]
	TIME [epoch: 0.705 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9118246300236058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9118246300236058 | validation: 1.0603115698003716]
	TIME [epoch: 0.705 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0564463912950561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0564463912950561 | validation: 0.7588584128544519]
	TIME [epoch: 0.704 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8876543351350493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8876543351350493 | validation: 0.7602920056964547]
	TIME [epoch: 0.705 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8289774063091371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8289774063091371 | validation: 0.6581352696078104]
	TIME [epoch: 0.704 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241387086066365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241387086066365 | validation: 0.7621074433835388]
	TIME [epoch: 0.705 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.834742866709733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.834742866709733 | validation: 0.5829529602189755]
	TIME [epoch: 0.704 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8644432464091414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8644432464091414 | validation: 0.9161303681789369]
	TIME [epoch: 0.703 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9012139201583677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9012139201583677 | validation: 0.6031462629814435]
	TIME [epoch: 0.712 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451482448044057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451482448044057 | validation: 0.7738840465261858]
	TIME [epoch: 0.706 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549542690406764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8549542690406764 | validation: 0.8251942176673532]
	TIME [epoch: 0.705 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689516478654088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8689516478654088 | validation: 0.7263057725996842]
	TIME [epoch: 0.704 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9327666769564065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9327666769564065 | validation: 1.0841698394035058]
	TIME [epoch: 0.704 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9906285109502396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9906285109502396 | validation: 0.6847990447472743]
	TIME [epoch: 0.704 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8362061873153749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8362061873153749 | validation: 0.5848324947631752]
	TIME [epoch: 0.703 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8239114437751073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8239114437751073 | validation: 0.781408251613722]
	TIME [epoch: 0.705 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8200469307769609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8200469307769609 | validation: 0.6349242616709431]
	TIME [epoch: 0.705 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.804050974560879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.804050974560879 | validation: 0.6473041412956685]
	TIME [epoch: 0.704 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7859903829454656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7859903829454656 | validation: 0.7169946837854643]
	TIME [epoch: 0.704 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174490640232633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8174490640232633 | validation: 0.7685919298194653]
	TIME [epoch: 0.704 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0132408256020065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0132408256020065 | validation: 1.0850271633575894]
	TIME [epoch: 0.704 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1478280724851282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1478280724851282 | validation: 0.9642571417294903]
	TIME [epoch: 0.706 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9585241066938092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9585241066938092 | validation: 0.6264881523269396]
	TIME [epoch: 0.704 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273190774287252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8273190774287252 | validation: 0.7400422418853803]
	TIME [epoch: 0.703 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8827141412359865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8827141412359865 | validation: 0.7995157229961632]
	TIME [epoch: 0.704 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8654671971988299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8654671971988299 | validation: 0.6271765868950347]
	TIME [epoch: 0.704 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932704849221321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932704849221321 | validation: 0.6994137050115905]
	TIME [epoch: 0.704 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942972353080439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7942972353080439 | validation: 0.6632056966543932]
	TIME [epoch: 0.703 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7974860698506129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7974860698506129 | validation: 0.707740122899029]
	TIME [epoch: 0.704 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062417244328464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062417244328464 | validation: 0.6285887794924276]
	TIME [epoch: 0.705 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471609828987365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8471609828987365 | validation: 0.7829182953148541]
	TIME [epoch: 0.707 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8652771368083875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8652771368083875 | validation: 0.6987290995931869]
	TIME [epoch: 0.704 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9133350454971619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9133350454971619 | validation: 0.7062011382547886]
	TIME [epoch: 0.705 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381878026566661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8381878026566661 | validation: 0.8799221764445456]
	TIME [epoch: 0.705 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8555631141782098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8555631141782098 | validation: 0.6013486228346094]
	TIME [epoch: 0.704 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443755629773523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443755629773523 | validation: 0.8213133870929324]
	TIME [epoch: 0.704 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.830255142102946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.830255142102946 | validation: 0.6038721736003945]
	TIME [epoch: 0.705 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7858690456923482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7858690456923482 | validation: 0.6689007812759544]
	TIME [epoch: 0.705 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7720428307724109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7720428307724109 | validation: 0.6668621889239837]
	TIME [epoch: 0.704 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815109741326469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7815109741326469 | validation: 0.6912887734675959]
	TIME [epoch: 0.704 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8956995420681583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8956995420681583 | validation: 0.9165421241786653]
	TIME [epoch: 0.706 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.045833227777012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.045833227777012 | validation: 0.8302278326732587]
	TIME [epoch: 0.708 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238696905351513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238696905351513 | validation: 0.552010475187369]
	TIME [epoch: 0.707 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778450481520124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778450481520124 | validation: 0.7139315555404429]
	TIME [epoch: 0.711 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.820150947293847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.820150947293847 | validation: 0.6445997684535564]
	TIME [epoch: 0.71 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.813044437424733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.813044437424733 | validation: 0.6315731380075875]
	TIME [epoch: 0.708 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826010014799483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7826010014799483 | validation: 0.7324384396833962]
	TIME [epoch: 0.708 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778307620486195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778307620486195 | validation: 0.5589895798839862]
	TIME [epoch: 0.705 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8061095982831344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8061095982831344 | validation: 0.8575240439804972]
	TIME [epoch: 0.705 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8135401139692462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8135401139692462 | validation: 0.5735062380062129]
	TIME [epoch: 0.703 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7873228664376153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7873228664376153 | validation: 0.8023323733034631]
	TIME [epoch: 0.703 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854284835071198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854284835071198 | validation: 0.674865920384833]
	TIME [epoch: 0.704 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292509236799451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8292509236799451 | validation: 0.895162418906325]
	TIME [epoch: 0.704 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.005869748510553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.005869748510553 | validation: 0.7799857245251124]
	TIME [epoch: 0.704 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759163640062323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8759163640062323 | validation: 0.617778079249423]
	TIME [epoch: 0.703 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7941002314545402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7941002314545402 | validation: 0.6236643907373199]
	TIME [epoch: 0.705 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7569634738302334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7569634738302334 | validation: 0.6241284770617958]
	TIME [epoch: 0.707 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773741597610472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773741597610472 | validation: 0.6545592121327447]
	TIME [epoch: 0.708 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665387063249103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7665387063249103 | validation: 0.6538297466409573]
	TIME [epoch: 0.708 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7559105953636674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7559105953636674 | validation: 0.6429935876462172]
	TIME [epoch: 0.712 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7600599150356044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7600599150356044 | validation: 0.6663519752266692]
	TIME [epoch: 0.709 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8166296790078531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8166296790078531 | validation: 0.9407311607484241]
	TIME [epoch: 0.714 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9490913326357591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9490913326357591 | validation: 0.6836479071090278]
	TIME [epoch: 0.711 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85902866213514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.85902866213514 | validation: 0.6125008530603244]
	TIME [epoch: 0.711 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7821894644920636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7821894644920636 | validation: 0.648031439820782]
	TIME [epoch: 0.71 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7400267961413965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7400267961413965 | validation: 0.5801940095872786]
	TIME [epoch: 0.708 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486475709229382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7486475709229382 | validation: 0.7526228291251844]
	TIME [epoch: 0.748 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586996718931843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7586996718931843 | validation: 0.6034510290553468]
	TIME [epoch: 0.71 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7592706167535039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7592706167535039 | validation: 0.7369118456901524]
	TIME [epoch: 0.71 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7864026403381592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7864026403381592 | validation: 0.73747206397831]
	TIME [epoch: 0.709 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800074981183335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8800074981183335 | validation: 0.7390475379960775]
	TIME [epoch: 0.708 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9098159285547754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9098159285547754 | validation: 0.6330820875189297]
	TIME [epoch: 0.711 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7410012652817036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7410012652817036 | validation: 0.5755685080249674]
	TIME [epoch: 0.713 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7310557860003719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7310557860003719 | validation: 0.6711099102315615]
	TIME [epoch: 0.714 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7505136872813385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7505136872813385 | validation: 0.6492027066202483]
	TIME [epoch: 0.71 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7565830567255081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7565830567255081 | validation: 0.7386852595698965]
	TIME [epoch: 0.714 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803314609552144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.803314609552144 | validation: 0.6839904649347537]
	TIME [epoch: 0.717 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352464393610117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8352464393610117 | validation: 0.6928425787919195]
	TIME [epoch: 0.713 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080253287596384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080253287596384 | validation: 0.6152016487474868]
	TIME [epoch: 0.712 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328915928831401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7328915928831401 | validation: 0.5555778536621694]
	TIME [epoch: 0.713 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7089773716991868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7089773716991868 | validation: 0.6785621503322865]
	TIME [epoch: 0.712 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133522466225458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7133522466225458 | validation: 0.5891064977987089]
	TIME [epoch: 0.714 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7364513858855554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7364513858855554 | validation: 0.8722681957312738]
	TIME [epoch: 0.715 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096627544993404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8096627544993404 | validation: 0.6847521561585176]
	TIME [epoch: 0.714 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8535750605314794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8535750605314794 | validation: 0.5979656253427582]
	TIME [epoch: 0.714 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.822824213866547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.822824213866547 | validation: 0.6733307426204886]
	TIME [epoch: 0.711 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7424627785323715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7424627785323715 | validation: 0.5274198703879555]
	TIME [epoch: 0.713 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.712132138711542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.712132138711542 | validation: 0.656203743372526]
	TIME [epoch: 0.714 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023763033120396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023763033120396 | validation: 0.605034723224279]
	TIME [epoch: 0.715 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7016628825574555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7016628825574555 | validation: 0.6028418540328713]
	TIME [epoch: 0.712 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270536821127801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270536821127801 | validation: 0.6601167375192212]
	TIME [epoch: 0.712 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026365668119486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026365668119486 | validation: 0.7264820512959869]
	TIME [epoch: 0.712 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241276279636438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241276279636438 | validation: 0.5864160172808525]
	TIME [epoch: 0.713 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7430785986380516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7430785986380516 | validation: 0.6803842757476091]
	TIME [epoch: 0.711 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7099376076371431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7099376076371431 | validation: 0.5275940135788283]
	TIME [epoch: 0.711 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916560179806786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916560179806786 | validation: 0.6151749783602073]
	TIME [epoch: 0.711 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.684040957468477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.684040957468477 | validation: 0.5439696889903476]
	TIME [epoch: 0.713 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.674573883073206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.674573883073206 | validation: 0.58188529148036]
	TIME [epoch: 0.712 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6733244670189282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6733244670189282 | validation: 0.5711726104723194]
	TIME [epoch: 0.714 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6906699072224709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6906699072224709 | validation: 0.6068480553025108]
	TIME [epoch: 0.714 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7672569990516885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7672569990516885 | validation: 0.934007623531234]
	TIME [epoch: 0.715 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9271386387343694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9271386387343694 | validation: 0.6321785890516651]
	TIME [epoch: 0.713 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8004540554056928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8004540554056928 | validation: 0.5766063962279183]
	TIME [epoch: 0.716 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7133276515465192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7133276515465192 | validation: 0.5899541460462135]
	TIME [epoch: 0.714 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6767269574721289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6767269574721289 | validation: 0.5778274439309289]
	TIME [epoch: 0.713 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6926772281190051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6926772281190051 | validation: 0.597539163569998]
	TIME [epoch: 0.709 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6772489116918969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6772489116918969 | validation: 0.5412501854514312]
	TIME [epoch: 0.71 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6936171893456233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6936171893456233 | validation: 0.6329866803566379]
	TIME [epoch: 0.711 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900327690184725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6900327690184725 | validation: 0.5906043152432321]
	TIME [epoch: 179 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291812390603807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291812390603807 | validation: 0.691672212727101]
	TIME [epoch: 1.41 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7426810690626364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7426810690626364 | validation: 0.5744577664085535]
	TIME [epoch: 1.39 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.734829068524717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.734829068524717 | validation: 0.6215805854899666]
	TIME [epoch: 1.39 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.678534611493194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.678534611493194 | validation: 0.5505509882676585]
	TIME [epoch: 1.39 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6572310106003424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6572310106003424 | validation: 0.5286930381330477]
	TIME [epoch: 1.39 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6426128465605045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6426128465605045 | validation: 0.580270660667087]
	TIME [epoch: 1.39 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6471503440378089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6471503440378089 | validation: 0.49012625801888704]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_208.pth
	Model improved!!!
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6902059489051363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6902059489051363 | validation: 0.6592262468719154]
	TIME [epoch: 1.39 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6867674376375215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6867674376375215 | validation: 0.5870563477055705]
	TIME [epoch: 1.39 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7469417028320643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7469417028320643 | validation: 0.7503925206751124]
	TIME [epoch: 1.39 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7598028036271501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7598028036271501 | validation: 0.5685875085198993]
	TIME [epoch: 1.39 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7246851216646853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7246851216646853 | validation: 0.4825766798407478]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_213.pth
	Model improved!!!
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7179078762823679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7179078762823679 | validation: 0.5748956384240875]
	TIME [epoch: 1.39 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503185874738564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503185874738564 | validation: 0.6939303971789639]
	TIME [epoch: 1.39 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7277836977481592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7277836977481592 | validation: 0.4573714677479017]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6960362223589514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6960362223589514 | validation: 0.6701462505459302]
	TIME [epoch: 1.39 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7236891962603188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7236891962603188 | validation: 0.46192579394927047]
	TIME [epoch: 1.39 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822667226146544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6822667226146544 | validation: 0.5822361990046543]
	TIME [epoch: 1.38 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6427727075355927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6427727075355927 | validation: 0.5835772610993559]
	TIME [epoch: 1.39 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6504815515802173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6504815515802173 | validation: 0.5367681979450881]
	TIME [epoch: 1.39 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6639907126191679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6639907126191679 | validation: 0.6301728720175741]
	TIME [epoch: 1.39 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7226010574356758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7226010574356758 | validation: 0.5924799590631848]
	TIME [epoch: 1.39 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7167699517964854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7167699517964854 | validation: 0.5415676212962769]
	TIME [epoch: 1.39 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6693722233597765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6693722233597765 | validation: 0.6505047576288068]
	TIME [epoch: 1.39 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6492879940495627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6492879940495627 | validation: 0.4761471920051612]
	TIME [epoch: 1.39 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.64058018770327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.64058018770327 | validation: 0.554332070616452]
	TIME [epoch: 1.39 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.616407424586715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.616407424586715 | validation: 0.5479008622209105]
	TIME [epoch: 1.39 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6174840605314714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6174840605314714 | validation: 0.47009685840920634]
	TIME [epoch: 1.39 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605625174611804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.605625174611804 | validation: 0.5539741566213257]
	TIME [epoch: 1.39 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6069822594086136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6069822594086136 | validation: 0.4625801501317214]
	TIME [epoch: 1.39 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6355344205033838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6355344205033838 | validation: 0.7232115560321006]
	TIME [epoch: 1.4 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6981772125129877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6981772125129877 | validation: 0.6088567086834197]
	TIME [epoch: 1.39 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.756152047345106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.756152047345106 | validation: 0.5069156235839948]
	TIME [epoch: 1.39 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6946543014383019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6946543014383019 | validation: 0.5239372634839871]
	TIME [epoch: 1.39 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5904628686103501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5904628686103501 | validation: 0.5629137287482469]
	TIME [epoch: 1.39 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033081088682002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6033081088682002 | validation: 0.477652925668031]
	TIME [epoch: 1.39 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6094902653690208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6094902653690208 | validation: 0.4690482844151898]
	TIME [epoch: 1.39 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815094334646884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5815094334646884 | validation: 0.5551727186545822]
	TIME [epoch: 1.39 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.587075725926736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.587075725926736 | validation: 0.44608750172018435]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.602067149290263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.602067149290263 | validation: 0.6110767120370459]
	TIME [epoch: 1.39 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5983744648211269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5983744648211269 | validation: 0.4909272824692752]
	TIME [epoch: 1.39 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6616116507784878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6616116507784878 | validation: 0.6577851309476396]
	TIME [epoch: 1.39 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6479228007236325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6479228007236325 | validation: 0.5092666086409026]
	TIME [epoch: 1.39 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033451780373542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6033451780373542 | validation: 0.4652856662581746]
	TIME [epoch: 1.39 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5686661440381576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5686661440381576 | validation: 0.5626033665377556]
	TIME [epoch: 1.4 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5826405442791814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5826405442791814 | validation: 0.5046886793225122]
	TIME [epoch: 1.4 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5555549758892846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5555549758892846 | validation: 0.39154647707968815]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5880291257159206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5880291257159206 | validation: 0.6419335909973718]
	TIME [epoch: 1.39 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5680737559035393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5680737559035393 | validation: 0.44459722096916554]
	TIME [epoch: 1.39 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5786191883539067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5786191883539067 | validation: 0.48769940213483115]
	TIME [epoch: 1.38 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.531883576620686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.531883576620686 | validation: 0.557789713093481]
	TIME [epoch: 1.39 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5401227686270232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5401227686270232 | validation: 0.4877756958990096]
	TIME [epoch: 1.39 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5467884361655311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5467884361655311 | validation: 0.435461275303449]
	TIME [epoch: 1.39 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.603091386658918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.603091386658918 | validation: 0.6014765266394198]
	TIME [epoch: 1.38 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5149181478233931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5149181478233931 | validation: 0.38783781967994213]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4888142335970568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4888142335970568 | validation: 0.49137056353710007]
	TIME [epoch: 1.39 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4750487210763263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4750487210763263 | validation: 0.4340192805889668]
	TIME [epoch: 1.39 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46603023220682965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46603023220682965 | validation: 0.5721351739446682]
	TIME [epoch: 1.39 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5480853166217474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5480853166217474 | validation: 0.6521630977406457]
	TIME [epoch: 1.39 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8448217321022772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8448217321022772 | validation: 0.8600213674351963]
	TIME [epoch: 1.39 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8189113932321611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8189113932321611 | validation: 0.7150472594489226]
	TIME [epoch: 1.39 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.711851428056962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.711851428056962 | validation: 0.4528204472928805]
	TIME [epoch: 1.39 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4959105889344111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4959105889344111 | validation: 0.6034823728250752]
	TIME [epoch: 1.39 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5842331485442672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5842331485442672 | validation: 0.47574885834814484]
	TIME [epoch: 1.39 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936414973697077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5936414973697077 | validation: 0.5503198212209297]
	TIME [epoch: 1.38 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5344763593806274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5344763593806274 | validation: 0.5719801288946607]
	TIME [epoch: 1.39 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5148745039058301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5148745039058301 | validation: 0.4365674571878514]
	TIME [epoch: 1.39 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5286509765565274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5286509765565274 | validation: 0.6239438198716055]
	TIME [epoch: 1.39 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5122331235844634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5122331235844634 | validation: 0.4413956395160523]
	TIME [epoch: 1.38 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43730524810456195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43730524810456195 | validation: 0.5042017629022598]
	TIME [epoch: 1.39 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4288945562297816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4288945562297816 | validation: 0.4481722805903845]
	TIME [epoch: 1.38 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.428699056826165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.428699056826165 | validation: 0.5057302421940121]
	TIME [epoch: 1.38 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5231517227799097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5231517227799097 | validation: 0.6976639310221329]
	TIME [epoch: 1.38 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5898917298322579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5898917298322579 | validation: 0.4250093907909451]
	TIME [epoch: 1.39 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7854690699208376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7854690699208376 | validation: 0.5394425818896095]
	TIME [epoch: 1.39 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6644743963380105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6644743963380105 | validation: 0.595776947777013]
	TIME [epoch: 1.39 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45943595666097525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45943595666097525 | validation: 0.5567182106358537]
	TIME [epoch: 1.39 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336260474662377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336260474662377 | validation: 0.40305768619453064]
	TIME [epoch: 1.39 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5016853415239898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5016853415239898 | validation: 0.48700813524796127]
	TIME [epoch: 1.39 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42372103833772273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42372103833772273 | validation: 0.5538724462455639]
	TIME [epoch: 1.39 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44402361550359376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44402361550359376 | validation: 0.4266638703097753]
	TIME [epoch: 1.39 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45269196574679654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45269196574679654 | validation: 0.4817817487386562]
	TIME [epoch: 1.39 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4045324421568421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4045324421568421 | validation: 0.4457395497317581]
	TIME [epoch: 1.38 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3749298902721704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3749298902721704 | validation: 0.39610303812720865]
	TIME [epoch: 1.39 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3583283491130155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3583283491130155 | validation: 0.438046548456133]
	TIME [epoch: 1.39 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3636494638626846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3636494638626846 | validation: 0.45622681022563766]
	TIME [epoch: 1.39 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3853693597844975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3853693597844975 | validation: 0.4878702384667434]
	TIME [epoch: 1.39 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6110586795335468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6110586795335468 | validation: 0.522192959748578]
	TIME [epoch: 1.39 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3704873198005176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3704873198005176 | validation: 0.3595280904771636]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4592727784396853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4592727784396853 | validation: 0.4261510538638268]
	TIME [epoch: 1.39 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3147687114506719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3147687114506719 | validation: 0.618686434007009]
	TIME [epoch: 1.4 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4228674859101437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4228674859101437 | validation: 0.49822504004456736]
	TIME [epoch: 1.39 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132728009246617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8132728009246617 | validation: 0.4702316712353618]
	TIME [epoch: 1.39 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.722937833526402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.722937833526402 | validation: 0.5799068206301631]
	TIME [epoch: 1.39 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41268572878003396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41268572878003396 | validation: 0.43026154991202825]
	TIME [epoch: 1.4 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31798155754081625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31798155754081625 | validation: 0.35841417472735837]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34331501534896175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34331501534896175 | validation: 0.4624730381246182]
	TIME [epoch: 1.39 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3493172926768126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3493172926768126 | validation: 0.4341202774016362]
	TIME [epoch: 1.39 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41447839700064015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41447839700064015 | validation: 0.462911129936595]
	TIME [epoch: 1.39 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32895565998424225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32895565998424225 | validation: 0.3273354005109817]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_301.pth
	Model improved!!!
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3250970341349511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3250970341349511 | validation: 0.5512114957620092]
	TIME [epoch: 1.39 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38324016004796135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38324016004796135 | validation: 0.4653564581280136]
	TIME [epoch: 1.39 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5594603061790242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5594603061790242 | validation: 0.3524558435389098]
	TIME [epoch: 1.4 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28155719612778457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28155719612778457 | validation: 0.5757217053016183]
	TIME [epoch: 1.39 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47674319608990123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47674319608990123 | validation: 0.4447888850745063]
	TIME [epoch: 1.39 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632382702482484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5632382702482484 | validation: 0.46939178718071944]
	TIME [epoch: 1.39 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39235773890713305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39235773890713305 | validation: 0.500859794247332]
	TIME [epoch: 1.39 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45002905733554716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45002905733554716 | validation: 0.3392847801718375]
	TIME [epoch: 1.39 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33367988830341316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33367988830341316 | validation: 0.44635302384631803]
	TIME [epoch: 1.4 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28325984175028907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28325984175028907 | validation: 0.401475191782907]
	TIME [epoch: 1.39 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29405559651946994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29405559651946994 | validation: 0.38293322979757183]
	TIME [epoch: 1.39 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34715183105770664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34715183105770664 | validation: 0.41635489131324466]
	TIME [epoch: 1.39 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2655159314777984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2655159314777984 | validation: 0.3413270146044914]
	TIME [epoch: 1.39 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2495025622305163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2495025622305163 | validation: 0.37922831541372815]
	TIME [epoch: 1.39 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22559732664133278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22559732664133278 | validation: 0.27842101282795684]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_316.pth
	Model improved!!!
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27802953365116573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27802953365116573 | validation: 0.650713386067493]
	TIME [epoch: 1.39 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4732697569475798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4732697569475798 | validation: 0.545365866213578]
	TIME [epoch: 1.39 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7551832161112151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7551832161112151 | validation: 0.45827542419545875]
	TIME [epoch: 1.39 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4584078416863862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4584078416863862 | validation: 0.6424998731861653]
	TIME [epoch: 1.39 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5788088624872373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5788088624872373 | validation: 0.3283365400529685]
	TIME [epoch: 1.4 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27657991313214775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27657991313214775 | validation: 0.43614949593239327]
	TIME [epoch: 1.39 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353084440485854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3353084440485854 | validation: 0.4307434166756239]
	TIME [epoch: 1.39 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28126803529848604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28126803529848604 | validation: 0.32820178241009285]
	TIME [epoch: 1.4 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2361332088233058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2361332088233058 | validation: 0.3927708538184021]
	TIME [epoch: 1.39 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2266048223391008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2266048223391008 | validation: 0.30272260421928254]
	TIME [epoch: 1.39 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21349256643834238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21349256643834238 | validation: 0.3480309823814809]
	TIME [epoch: 1.39 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058091083902834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2058091083902834 | validation: 0.28906609120728105]
	TIME [epoch: 1.4 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20058966861319108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20058966861319108 | validation: 0.33281715042507715]
	TIME [epoch: 1.4 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25569103438775626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25569103438775626 | validation: 0.6034938263514745]
	TIME [epoch: 1.4 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420485566660235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5420485566660235 | validation: 0.3919963232985866]
	TIME [epoch: 1.4 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5766138269275912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5766138269275912 | validation: 0.44003690122293027]
	TIME [epoch: 1.39 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30168576708214695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30168576708214695 | validation: 0.4706772399769952]
	TIME [epoch: 1.39 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45432585964573563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45432585964573563 | validation: 0.25216684825025903]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_334.pth
	Model improved!!!
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30130132214604805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30130132214604805 | validation: 0.3943779448405729]
	TIME [epoch: 1.39 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.277031594468773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.277031594468773 | validation: 0.3841704497780809]
	TIME [epoch: 1.39 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23549897245946724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23549897245946724 | validation: 0.2780185504758639]
	TIME [epoch: 1.39 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20058478713282285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20058478713282285 | validation: 0.3158299383124468]
	TIME [epoch: 1.39 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18834877403855085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18834877403855085 | validation: 0.31850618819318455]
	TIME [epoch: 1.39 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18068676581403395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18068676581403395 | validation: 0.24943740479394877]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_340.pth
	Model improved!!!
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18498240182231523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18498240182231523 | validation: 0.349257506656341]
	TIME [epoch: 1.39 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2049612874955426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2049612874955426 | validation: 0.2814747076227901]
	TIME [epoch: 1.39 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32608967318641113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32608967318641113 | validation: 0.35492843065847596]
	TIME [epoch: 1.38 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24077428136479007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24077428136479007 | validation: 0.5242889792919864]
	TIME [epoch: 1.39 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4646830616371571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4646830616371571 | validation: 0.5592796396679826]
	TIME [epoch: 1.39 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4360691549621833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4360691549621833 | validation: 0.3476942287263275]
	TIME [epoch: 1.39 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34917475242584173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34917475242584173 | validation: 0.27279636274443975]
	TIME [epoch: 1.39 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21957597236316717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21957597236316717 | validation: 0.3883767515866655]
	TIME [epoch: 1.39 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3059148592918259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3059148592918259 | validation: 0.3405223955551483]
	TIME [epoch: 1.39 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3028395531006937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3028395531006937 | validation: 0.3029358513441275]
	TIME [epoch: 1.39 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1740889162665445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1740889162665445 | validation: 0.266107793963502]
	TIME [epoch: 1.39 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19347899842003422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19347899842003422 | validation: 0.28500927666351644]
	TIME [epoch: 1.39 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20698702687709578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20698702687709578 | validation: 0.29456380043571423]
	TIME [epoch: 1.39 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20073138115564818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20073138115564818 | validation: 0.31967615949822326]
	TIME [epoch: 1.39 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2796661012552819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2796661012552819 | validation: 0.3603255544797477]
	TIME [epoch: 1.39 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19922901118479652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19922901118479652 | validation: 0.19327077243386306]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185045707781309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.185045707781309 | validation: 0.34688143906005164]
	TIME [epoch: 1.39 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16732577663672146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16732577663672146 | validation: 0.17247105953165787]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058202161678754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2058202161678754 | validation: 0.41807528531684285]
	TIME [epoch: 1.4 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23533772867228892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23533772867228892 | validation: 0.24879479752261804]
	TIME [epoch: 1.4 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2932807045817278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2932807045817278 | validation: 0.3837912324181456]
	TIME [epoch: 1.39 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24749681746447613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24749681746447613 | validation: 0.35556230454602955]
	TIME [epoch: 1.39 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29499503245114933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29499503245114933 | validation: 0.42560674052684605]
	TIME [epoch: 1.39 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5322035545774412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5322035545774412 | validation: 0.24878780054970875]
	TIME [epoch: 1.39 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4098239779296587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4098239779296587 | validation: 0.30484039001571683]
	TIME [epoch: 1.39 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16536806082184258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16536806082184258 | validation: 0.2948593443386909]
	TIME [epoch: 1.4 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21917074154919483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21917074154919483 | validation: 0.23644582002975967]
	TIME [epoch: 1.39 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519953173000929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1519953173000929 | validation: 0.23751655510446235]
	TIME [epoch: 1.39 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1347607368157086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1347607368157086 | validation: 0.2407473251603805]
	TIME [epoch: 1.39 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12680218570151092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12680218570151092 | validation: 0.205761751410659]
	TIME [epoch: 1.39 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12327672626371523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12327672626371523 | validation: 0.2122355231755746]
	TIME [epoch: 1.39 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1219311144799714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1219311144799714 | validation: 0.2432641876112784]
	TIME [epoch: 1.39 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16008130348999194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16008130348999194 | validation: 0.547792278982573]
	TIME [epoch: 1.39 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289240918766906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4289240918766906 | validation: 0.3430910845885974]
	TIME [epoch: 1.39 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.530390450135085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.530390450135085 | validation: 0.716785989183112]
	TIME [epoch: 1.39 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5159509722182761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5159509722182761 | validation: 0.8189005188944858]
	TIME [epoch: 1.39 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033113474971391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6033113474971391 | validation: 0.452445493971071]
	TIME [epoch: 1.39 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3131062246143466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3131062246143466 | validation: 0.2129005727472925]
	TIME [epoch: 1.39 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20707225871319743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20707225871319743 | validation: 0.22666247805714346]
	TIME [epoch: 1.4 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18961087741193883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18961087741193883 | validation: 0.2199412176895358]
	TIME [epoch: 1.39 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13559364901692003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13559364901692003 | validation: 0.2267620504017047]
	TIME [epoch: 1.4 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12447433787162389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12447433787162389 | validation: 0.23202107416452977]
	TIME [epoch: 1.39 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13107439326528597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13107439326528597 | validation: 0.18837519717415457]
	TIME [epoch: 1.39 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13634286509914148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13634286509914148 | validation: 0.21511683267477652]
	TIME [epoch: 1.39 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1436323942743296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1436323942743296 | validation: 0.20898120552152127]
	TIME [epoch: 1.39 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22316273446105883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22316273446105883 | validation: 0.38209144830531216]
	TIME [epoch: 1.4 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2370614510031959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2370614510031959 | validation: 0.25539884094811216]
	TIME [epoch: 1.39 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2318689463916107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2318689463916107 | validation: 0.2413929187669743]
	TIME [epoch: 1.42 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14487163825739197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14487163825739197 | validation: 0.24041969002356192]
	TIME [epoch: 1.4 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376179978499805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1376179978499805 | validation: 0.19996224384854566]
	TIME [epoch: 1.4 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13352088774005377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13352088774005377 | validation: 0.18234298968253893]
	TIME [epoch: 1.4 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15469033422337364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15469033422337364 | validation: 0.4239484719381036]
	TIME [epoch: 1.39 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29559219486213906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29559219486213906 | validation: 0.22882629442554947]
	TIME [epoch: 1.4 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25494031540654577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25494031540654577 | validation: 0.19846641212481622]
	TIME [epoch: 1.39 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10975348149468964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10975348149468964 | validation: 0.18185419333095643]
	TIME [epoch: 1.39 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486735914752719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1486735914752719 | validation: 0.21972031457334573]
	TIME [epoch: 1.39 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20010534865942892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20010534865942892 | validation: 0.3889429592345672]
	TIME [epoch: 1.39 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20651479761214275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20651479761214275 | validation: 0.17905340647682366]
	TIME [epoch: 1.39 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2722228085121041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2722228085121041 | validation: 0.3120593717417944]
	TIME [epoch: 1.39 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15500212470357222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15500212470357222 | validation: 0.24660877099966735]
	TIME [epoch: 1.39 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11057162292266703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11057162292266703 | validation: 0.11217416205119948]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14667999772810267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14667999772810267 | validation: 0.28852755760618726]
	TIME [epoch: 1.39 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13435979386732924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13435979386732924 | validation: 0.2577637584697008]
	TIME [epoch: 1.39 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21832360788088812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21832360788088812 | validation: 0.1427717258050322]
	TIME [epoch: 1.39 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28050489117620764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28050489117620764 | validation: 0.2435309758682112]
	TIME [epoch: 1.39 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1219717800319443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1219717800319443 | validation: 0.2164449872428552]
	TIME [epoch: 1.39 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09455970795746342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09455970795746342 | validation: 0.1309696494857364]
	TIME [epoch: 1.39 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11096282236866419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11096282236866419 | validation: 0.30972750197679916]
	TIME [epoch: 1.39 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16702352245124943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16702352245124943 | validation: 0.22482221290953008]
	TIME [epoch: 1.44 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29278962453250446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29278962453250446 | validation: 0.3122363237546116]
	TIME [epoch: 1.38 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1440363069705581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1440363069705581 | validation: 0.28918872094363507]
	TIME [epoch: 1.39 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18171739044061241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18171739044061241 | validation: 0.2534783967869499]
	TIME [epoch: 1.39 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1613174805221828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1613174805221828 | validation: 0.19380434378347466]
	TIME [epoch: 1.39 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09953673524092045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09953673524092045 | validation: 0.10487076407731735]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909895750273505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0909895750273505 | validation: 0.23153986212314237]
	TIME [epoch: 1.39 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13142711228032847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13142711228032847 | validation: 0.16158290565632677]
	TIME [epoch: 1.4 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13953349108384594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13953349108384594 | validation: 0.25656575878985827]
	TIME [epoch: 1.39 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3432608512041251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3432608512041251 | validation: 0.49108845631877984]
	TIME [epoch: 1.39 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2765368708535662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2765368708535662 | validation: 0.1963780794763077]
	TIME [epoch: 1.39 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1349600106042497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1349600106042497 | validation: 0.11900620541952406]
	TIME [epoch: 1.4 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17650601590241327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17650601590241327 | validation: 0.4509314744473052]
	TIME [epoch: 1.39 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26673141461960265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26673141461960265 | validation: 0.2064324266850818]
	TIME [epoch: 1.39 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24044372310150003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24044372310150003 | validation: 0.2400939430172962]
	TIME [epoch: 1.39 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18365465521409793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18365465521409793 | validation: 0.20206067227473168]
	TIME [epoch: 1.39 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11016371395630248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11016371395630248 | validation: 0.17933989624131624]
	TIME [epoch: 1.39 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09477363394554439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09477363394554439 | validation: 0.11062652351624487]
	TIME [epoch: 1.39 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08568649568320097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08568649568320097 | validation: 0.14795640162066]
	TIME [epoch: 1.39 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08202038029970755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08202038029970755 | validation: 0.13408392667322763]
	TIME [epoch: 1.39 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09359069675796955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09359069675796955 | validation: 0.18064044919277011]
	TIME [epoch: 1.39 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356892537373372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1356892537373372 | validation: 0.4620727175690873]
	TIME [epoch: 1.38 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32331171831006905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32331171831006905 | validation: 0.2235700441306329]
	TIME [epoch: 1.39 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3696776768156545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3696776768156545 | validation: 0.17611172638141862]
	TIME [epoch: 1.39 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14230658594417542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14230658594417542 | validation: 0.24114350120164146]
	TIME [epoch: 1.38 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11815368377938482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11815368377938482 | validation: 0.13776204121133553]
	TIME [epoch: 1.39 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0764703617852008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0764703617852008 | validation: 0.16624074325057522]
	TIME [epoch: 1.39 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770274870899263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07770274870899263 | validation: 0.12116738692811656]
	TIME [epoch: 1.39 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13056523998261305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13056523998261305 | validation: 0.34564484060817935]
	TIME [epoch: 1.38 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24164093729667532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24164093729667532 | validation: 0.19063410886823023]
	TIME [epoch: 1.39 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2027460454271801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2027460454271801 | validation: 0.18921225200534064]
	TIME [epoch: 1.39 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11936958935873479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11936958935873479 | validation: 0.1380726521509684]
	TIME [epoch: 1.39 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09645103897252984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09645103897252984 | validation: 0.24567151553263564]
	TIME [epoch: 1.39 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11205332346906778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11205332346906778 | validation: 0.1421247669967904]
	TIME [epoch: 1.38 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10695318428163816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10695318428163816 | validation: 0.24044298658641006]
	TIME [epoch: 1.38 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12186542885824139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12186542885824139 | validation: 0.20037839065693364]
	TIME [epoch: 1.38 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12328008894963036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12328008894963036 | validation: 0.23515313085516448]
	TIME [epoch: 1.38 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17379706383035878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17379706383035878 | validation: 0.19989800996140514]
	TIME [epoch: 1.38 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.297492677716864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.297492677716864 | validation: 0.3873977744374231]
	TIME [epoch: 1.38 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17322686986027988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17322686986027988 | validation: 0.12465848760823689]
	TIME [epoch: 1.39 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09642074904070846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09642074904070846 | validation: 0.22417483272934557]
	TIME [epoch: 1.39 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08197070477705573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08197070477705573 | validation: 0.12362981530951744]
	TIME [epoch: 1.38 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058677210713342504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058677210713342504 | validation: 0.10080396935183929]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056008621589855916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056008621589855916 | validation: 0.16017394658020825]
	TIME [epoch: 1.39 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252214763829265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252214763829265 | validation: 0.10545085255657219]
	TIME [epoch: 1.4 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1310850152264266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1310850152264266 | validation: 0.2587420245611212]
	TIME [epoch: 1.39 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19834483269107736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19834483269107736 | validation: 0.1747990414613463]
	TIME [epoch: 1.39 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12020364410920831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12020364410920831 | validation: 0.14675089487865597]
	TIME [epoch: 1.39 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25661253064407336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25661253064407336 | validation: 0.4269753116160659]
	TIME [epoch: 1.38 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29342242226259424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29342242226259424 | validation: 0.22240778053001709]
	TIME [epoch: 1.38 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17965917998030442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17965917998030442 | validation: 0.32380116995021974]
	TIME [epoch: 1.39 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25192174290776975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25192174290776975 | validation: 0.13783574035052357]
	TIME [epoch: 1.38 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10984046822887499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10984046822887499 | validation: 0.2093391200215244]
	TIME [epoch: 1.38 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0910937942633062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0910937942633062 | validation: 0.12344334859897521]
	TIME [epoch: 1.38 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06214185207325702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06214185207325702 | validation: 0.09971319566497377]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0738187804932557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0738187804932557 | validation: 0.2567908736761079]
	TIME [epoch: 1.39 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14180026681485428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14180026681485428 | validation: 0.1844893709211063]
	TIME [epoch: 1.4 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22032292146691781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22032292146691781 | validation: 0.37980523953106354]
	TIME [epoch: 1.39 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24841036364082839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24841036364082839 | validation: 0.14363307915381984]
	TIME [epoch: 1.39 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08437397092678428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08437397092678428 | validation: 0.08702315966203861]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_468.pth
	Model improved!!!
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061073744931626185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061073744931626185 | validation: 0.21898062386387546]
	TIME [epoch: 1.39 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08418099561312734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08418099561312734 | validation: 0.11037944920453856]
	TIME [epoch: 1.39 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0908357360433676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0908357360433676 | validation: 0.19994143302137532]
	TIME [epoch: 1.39 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15318537420470374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15318537420470374 | validation: 0.16440222912896132]
	TIME [epoch: 1.39 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1744057864427932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1744057864427932 | validation: 0.2915901772200631]
	TIME [epoch: 1.39 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1580711023165022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1580711023165022 | validation: 0.14406636018467864]
	TIME [epoch: 1.39 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1173004662405766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1173004662405766 | validation: 0.22409454896922265]
	TIME [epoch: 1.39 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14306507601611881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14306507601611881 | validation: 0.17210215024800882]
	TIME [epoch: 1.4 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16602852213856423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16602852213856423 | validation: 0.2664631770749253]
	TIME [epoch: 1.39 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12060922465494929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12060922465494929 | validation: 0.13606968839519903]
	TIME [epoch: 1.39 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.068577613777385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.068577613777385 | validation: 0.09572681194373604]
	TIME [epoch: 1.4 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08635642979221965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08635642979221965 | validation: 0.23857802465196204]
	TIME [epoch: 1.39 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15609173485881797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15609173485881797 | validation: 0.1715159374385088]
	TIME [epoch: 1.39 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16507109653577487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16507109653577487 | validation: 0.0866900219520244]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_482.pth
	Model improved!!!
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0945967243746275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0945967243746275 | validation: 0.3581014330927457]
	TIME [epoch: 1.39 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1545333025581146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1545333025581146 | validation: 0.09866509736272323]
	TIME [epoch: 1.39 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11540076211079674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11540076211079674 | validation: 0.21671743279467753]
	TIME [epoch: 1.39 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15095602672435654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15095602672435654 | validation: 0.11551081529858608]
	TIME [epoch: 1.39 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862419423918097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0862419423918097 | validation: 0.16670118077964455]
	TIME [epoch: 1.39 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08974731305735555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08974731305735555 | validation: 0.15676735010154255]
	TIME [epoch: 1.39 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12606356109835665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12606356109835665 | validation: 0.3078540992756467]
	TIME [epoch: 1.39 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18579192418205914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18579192418205914 | validation: 0.1745497082904005]
	TIME [epoch: 1.39 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14194337846075725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14194337846075725 | validation: 0.17563311116135485]
	TIME [epoch: 1.39 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1352390960030919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1352390960030919 | validation: 0.11454755422595275]
	TIME [epoch: 1.39 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09574842315181606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09574842315181606 | validation: 0.20602858457058104]
	TIME [epoch: 1.39 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606979210238279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10606979210238279 | validation: 0.11654068073268005]
	TIME [epoch: 1.39 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1844851936477044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1844851936477044 | validation: 0.30466812383300995]
	TIME [epoch: 1.39 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12852468077612655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12852468077612655 | validation: 0.1346187074232846]
	TIME [epoch: 1.39 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060045287627818376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060045287627818376 | validation: 0.0895311706390258]
	TIME [epoch: 1.39 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565270873971108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07565270873971108 | validation: 0.22204950698733797]
	TIME [epoch: 1.39 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10531007824235583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10531007824235583 | validation: 0.18251049225043647]
	TIME [epoch: 1.39 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1207833476223986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1207833476223986 | validation: 0.13924015212984733]
	TIME [epoch: 1.39 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14966140333735098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14966140333735098 | validation: 0.30473161856063513]
	TIME [epoch: 174 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19230406197979605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19230406197979605 | validation: 0.14827320023254265]
	TIME [epoch: 2.76 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284688082929279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284688082929279 | validation: 0.16612587876942547]
	TIME [epoch: 2.75 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15865012183635346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15865012183635346 | validation: 0.10924174560615905]
	TIME [epoch: 2.75 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07685601142044204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07685601142044204 | validation: 0.17786605997985896]
	TIME [epoch: 2.75 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08370473888636586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08370473888636586 | validation: 0.15908333983288575]
	TIME [epoch: 2.75 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1635593771806981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1635593771806981 | validation: 0.1968274515535401]
	TIME [epoch: 2.75 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1564951489953582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1564951489953582 | validation: 0.10030503733658547]
	TIME [epoch: 2.75 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06121171445871847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06121171445871847 | validation: 0.1549817030192327]
	TIME [epoch: 2.75 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06152758686226669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06152758686226669 | validation: 0.0901281175789572]
	TIME [epoch: 2.75 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1085033260849804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1085033260849804 | validation: 0.15411568181022314]
	TIME [epoch: 2.75 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09013901994432813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09013901994432813 | validation: 0.08581930957283547]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07953366221729632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07953366221729632 | validation: 0.1937809887760132]
	TIME [epoch: 2.75 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865650948364813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0865650948364813 | validation: 0.135714733522781]
	TIME [epoch: 2.75 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07681789743903421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07681789743903421 | validation: 0.14412127859079082]
	TIME [epoch: 2.75 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213326295453697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11213326295453697 | validation: 0.24444147067328084]
	TIME [epoch: 2.75 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18706599989740333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18706599989740333 | validation: 0.31523953767864626]
	TIME [epoch: 2.75 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.255810796301768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.255810796301768 | validation: 0.13419573289475184]
	TIME [epoch: 2.75 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25386311452282667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25386311452282667 | validation: 0.29639293120764487]
	TIME [epoch: 2.75 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13464313087828347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13464313087828347 | validation: 0.10352381762316427]
	TIME [epoch: 2.75 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922393721202232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04922393721202232 | validation: 0.07388354054420754]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_521.pth
	Model improved!!!
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05653286549193629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05653286549193629 | validation: 0.13039404706538735]
	TIME [epoch: 2.75 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05510629093273794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05510629093273794 | validation: 0.05858183318741739]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04289341389093303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04289341389093303 | validation: 0.09747480503479908]
	TIME [epoch: 2.75 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03661937296364422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03661937296364422 | validation: 0.08353348494853507]
	TIME [epoch: 2.76 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034434941909094095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034434941909094095 | validation: 0.06042243649859485]
	TIME [epoch: 2.75 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046939450877968476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046939450877968476 | validation: 0.23661657192110416]
	TIME [epoch: 2.75 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13584393459011856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13584393459011856 | validation: 0.24472017149121217]
	TIME [epoch: 2.76 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38353020488844525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38353020488844525 | validation: 0.4241567698734414]
	TIME [epoch: 2.75 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2809811947886074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2809811947886074 | validation: 0.12757599706065587]
	TIME [epoch: 2.76 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057909680734050395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057909680734050395 | validation: 0.061974026305146694]
	TIME [epoch: 2.75 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351637069924073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08351637069924073 | validation: 0.3240627909430847]
	TIME [epoch: 2.75 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13931654868441873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13931654868441873 | validation: 0.10778802282721363]
	TIME [epoch: 2.75 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06994416216542092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06994416216542092 | validation: 0.08386232271853422]
	TIME [epoch: 2.75 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1279708724948229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1279708724948229 | validation: 0.10380290443188059]
	TIME [epoch: 2.75 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08541865060152563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08541865060152563 | validation: 0.13712534192410444]
	TIME [epoch: 2.75 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08139726969510722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08139726969510722 | validation: 0.10076706190749925]
	TIME [epoch: 2.75 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1125598259458221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1125598259458221 | validation: 0.34207587089036684]
	TIME [epoch: 2.75 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17692489553137825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17692489553137825 | validation: 0.09541250775360796]
	TIME [epoch: 2.75 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09246958585954972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09246958585954972 | validation: 0.10284311521168142]
	TIME [epoch: 2.75 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08137164890388694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08137164890388694 | validation: 0.12925028487776657]
	TIME [epoch: 2.75 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0893751821587448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0893751821587448 | validation: 0.19480391151806714]
	TIME [epoch: 2.75 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1319122236311545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1319122236311545 | validation: 0.15837232525178271]
	TIME [epoch: 2.75 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12724379333523875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12724379333523875 | validation: 0.19521375051102519]
	TIME [epoch: 2.76 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10968065607719377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10968065607719377 | validation: 0.09345320471247198]
	TIME [epoch: 2.75 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07064559444378633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07064559444378633 | validation: 0.1343980620327571]
	TIME [epoch: 2.75 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10206678920761643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10206678920761643 | validation: 0.1075748106505357]
	TIME [epoch: 2.75 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11898126093005469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11898126093005469 | validation: 0.20165270898796595]
	TIME [epoch: 2.75 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12909395714299174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12909395714299174 | validation: 0.0806544885740947]
	TIME [epoch: 2.75 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09723558476802893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09723558476802893 | validation: 0.22208352738619475]
	TIME [epoch: 2.75 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08737231742486752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08737231742486752 | validation: 0.07444923117750467]
	TIME [epoch: 2.75 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06413322726884704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06413322726884704 | validation: 0.13652016515466162]
	TIME [epoch: 2.75 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09108339048300863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09108339048300863 | validation: 0.1447957486051474]
	TIME [epoch: 2.75 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1210277021244992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1210277021244992 | validation: 0.23072926366075294]
	TIME [epoch: 2.75 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16966801260490674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16966801260490674 | validation: 0.11355468931093611]
	TIME [epoch: 2.75 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130747509864769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08130747509864769 | validation: 0.09893316740039429]
	TIME [epoch: 2.75 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046908340671015056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046908340671015056 | validation: 0.08925223629193757]
	TIME [epoch: 2.75 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344507434954765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04344507434954765 | validation: 0.08386702892126749]
	TIME [epoch: 2.75 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0678336791691264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0678336791691264 | validation: 0.2220385227372182]
	TIME [epoch: 2.75 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1512904360443935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1512904360443935 | validation: 0.16173514435281164]
	TIME [epoch: 2.75 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22611327018438135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22611327018438135 | validation: 0.2654157056281483]
	TIME [epoch: 2.75 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11574582754619156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11574582754619156 | validation: 0.06596060068893132]
	TIME [epoch: 2.75 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07606199002402352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07606199002402352 | validation: 0.15597175978300432]
	TIME [epoch: 2.75 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1153093081225424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1153093081225424 | validation: 0.09552109845681507]
	TIME [epoch: 2.75 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10812154120557108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10812154120557108 | validation: 0.10948185667027482]
	TIME [epoch: 2.75 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05962585103555135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05962585103555135 | validation: 0.07402041422811582]
	TIME [epoch: 2.75 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037323997468073786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037323997468073786 | validation: 0.08876007864217228]
	TIME [epoch: 2.75 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04538902540413302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04538902540413302 | validation: 0.12925842273225877]
	TIME [epoch: 2.75 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08131511403876351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08131511403876351 | validation: 0.18698675715955565]
	TIME [epoch: 2.75 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19253220256708153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19253220256708153 | validation: 0.3745091908812408]
	TIME [epoch: 2.75 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32639834675542745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32639834675542745 | validation: 0.0757244097501849]
	TIME [epoch: 2.75 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07288783725310921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07288783725310921 | validation: 0.20348147169811626]
	TIME [epoch: 2.75 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10910536233276498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10910536233276498 | validation: 0.13778607861492018]
	TIME [epoch: 2.75 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08652536242506802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08652536242506802 | validation: 0.08792375180482465]
	TIME [epoch: 2.75 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09470240510466345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09470240510466345 | validation: 0.3089764891939784]
	TIME [epoch: 2.75 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15201579121969827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15201579121969827 | validation: 0.08724178673459487]
	TIME [epoch: 2.75 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09344305896273233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09344305896273233 | validation: 0.07898169667550504]
	TIME [epoch: 2.75 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1106416841657706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1106416841657706 | validation: 0.10470865756816533]
	TIME [epoch: 2.75 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06235605666160144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06235605666160144 | validation: 0.07673847419804686]
	TIME [epoch: 2.75 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03997639199213109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03997639199213109 | validation: 0.05854798854418275]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_580.pth
	Model improved!!!
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03731573541935651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03731573541935651 | validation: 0.10831714601018386]
	TIME [epoch: 2.75 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037139930559947375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037139930559947375 | validation: 0.05378563442771536]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_582.pth
	Model improved!!!
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03771290407635952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03771290407635952 | validation: 0.08931912664622113]
	TIME [epoch: 2.75 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04505553156409853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04505553156409853 | validation: 0.0989676005333714]
	TIME [epoch: 2.76 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0794742685453686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0794742685453686 | validation: 0.18130532122001108]
	TIME [epoch: 2.76 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14316444308188872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14316444308188872 | validation: 0.3044933539328659]
	TIME [epoch: 2.75 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24674835220859664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24674835220859664 | validation: 0.12163081949436028]
	TIME [epoch: 2.75 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18402261307459292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18402261307459292 | validation: 0.4171832247261984]
	TIME [epoch: 2.75 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20862083804915002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20862083804915002 | validation: 0.12909079002780763]
	TIME [epoch: 2.75 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452073837536226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10452073837536226 | validation: 0.05300382655755738]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1162253348155822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1162253348155822 | validation: 0.12901718706563048]
	TIME [epoch: 2.75 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05486677950202625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05486677950202625 | validation: 0.08707680034311602]
	TIME [epoch: 2.75 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058065610018980696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058065610018980696 | validation: 0.09166806731654037]
	TIME [epoch: 2.75 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09536803438164886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09536803438164886 | validation: 0.06927029417934567]
	TIME [epoch: 2.76 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06723812075203202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06723812075203202 | validation: 0.1673242423062581]
	TIME [epoch: 2.76 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08054643299236751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08054643299236751 | validation: 0.07300119639875607]
	TIME [epoch: 2.75 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07405913282399555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07405913282399555 | validation: 0.14910655841760093]
	TIME [epoch: 2.75 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987219555441007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0987219555441007 | validation: 0.12622836021906653]
	TIME [epoch: 2.75 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12161030059719136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12161030059719136 | validation: 0.20933442997337598]
	TIME [epoch: 2.75 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14956545185490885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14956545185490885 | validation: 0.07005832963889913]
	TIME [epoch: 2.75 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08493028368641443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08493028368641443 | validation: 0.15239046135747578]
	TIME [epoch: 2.75 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744762842102242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0744762842102242 | validation: 0.062396722693059015]
	TIME [epoch: 2.75 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513345026029761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04513345026029761 | validation: 0.05959812215288987]
	TIME [epoch: 2.76 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05002229456004824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05002229456004824 | validation: 0.08941125494969776]
	TIME [epoch: 2.75 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06889556269412475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06889556269412475 | validation: 0.13471795495336253]
	TIME [epoch: 2.75 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11920080441045446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11920080441045446 | validation: 0.12285823129399748]
	TIME [epoch: 2.75 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10620234300396478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10620234300396478 | validation: 0.2849495561621957]
	TIME [epoch: 2.75 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997708879454731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11997708879454731 | validation: 0.06782480982831791]
	TIME [epoch: 2.75 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044082421721820586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044082421721820586 | validation: 0.06409439490670277]
	TIME [epoch: 2.75 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05689383073450112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05689383073450112 | validation: 0.1997169597268622]
	TIME [epoch: 2.75 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1235043977302675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1235043977302675 | validation: 0.1317722202153386]
	TIME [epoch: 2.75 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1615520022361276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1615520022361276 | validation: 0.21721585257009488]
	TIME [epoch: 2.75 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19081945309261197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19081945309261197 | validation: 0.07591248343057726]
	TIME [epoch: 2.75 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04854714877787947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04854714877787947 | validation: 0.07750180768019739]
	TIME [epoch: 2.75 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03302296837246191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03302296837246191 | validation: 0.07495541209952507]
	TIME [epoch: 2.75 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04918790381091186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04918790381091186 | validation: 0.0748363164938869]
	TIME [epoch: 2.75 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055382403453759964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055382403453759964 | validation: 0.09879254109223902]
	TIME [epoch: 2.75 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06142449640640093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06142449640640093 | validation: 0.06569163859202055]
	TIME [epoch: 2.76 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06434697895160867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06434697895160867 | validation: 0.17110090492347477]
	TIME [epoch: 2.75 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09341325887157187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09341325887157187 | validation: 0.10167359333953829]
	TIME [epoch: 2.75 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563020340194858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1563020340194858 | validation: 0.36516979120877907]
	TIME [epoch: 2.75 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19304692195823883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19304692195823883 | validation: 0.09494044311407249]
	TIME [epoch: 2.75 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.066831872179102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.066831872179102 | validation: 0.05308616744583447]
	TIME [epoch: 2.75 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09054669260583156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09054669260583156 | validation: 0.16354737016490548]
	TIME [epoch: 2.75 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10934982972631184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10934982972631184 | validation: 0.1182114185916585]
	TIME [epoch: 2.75 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07864331113772455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07864331113772455 | validation: 0.09647529808418022]
	TIME [epoch: 2.75 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076559575147463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.076559575147463 | validation: 0.1833634820593365]
	TIME [epoch: 2.75 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10309224712354638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10309224712354638 | validation: 0.09530985457424695]
	TIME [epoch: 2.75 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08885641793254344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08885641793254344 | validation: 0.091358261237041]
	TIME [epoch: 2.75 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07875695681854292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07875695681854292 | validation: 0.07025077471083245]
	TIME [epoch: 2.75 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056484554097387306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056484554097387306 | validation: 0.10983230235441475]
	TIME [epoch: 2.75 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06778926483478893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06778926483478893 | validation: 0.10549681471071232]
	TIME [epoch: 2.76 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08599561091027098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08599561091027098 | validation: 0.12273916350176091]
	TIME [epoch: 2.75 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09500045012528764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09500045012528764 | validation: 0.09271362935176014]
	TIME [epoch: 2.75 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07406634623135755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07406634623135755 | validation: 0.1363476238818557]
	TIME [epoch: 2.75 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08471346320952078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08471346320952078 | validation: 0.0934043317824601]
	TIME [epoch: 2.76 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10827352915798832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10827352915798832 | validation: 0.25137408322789206]
	TIME [epoch: 2.75 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11869416418951084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11869416418951084 | validation: 0.06832653441341026]
	TIME [epoch: 2.75 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05333459893208272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05333459893208272 | validation: 0.044060287216002525]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_639.pth
	Model improved!!!
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06138343433083749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06138343433083749 | validation: 0.11855310827254546]
	TIME [epoch: 2.75 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004256211856465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07004256211856465 | validation: 0.08211081660425174]
	TIME [epoch: 2.75 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06573546908634262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06573546908634262 | validation: 0.049387108289497876]
	TIME [epoch: 2.75 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045361258028933385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045361258028933385 | validation: 0.12265049912867805]
	TIME [epoch: 2.75 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06857264166852592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06857264166852592 | validation: 0.10021692747342424]
	TIME [epoch: 2.75 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1126583267290694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1126583267290694 | validation: 0.2651836712224224]
	TIME [epoch: 2.75 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19769440478303094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19769440478303094 | validation: 0.09518278398159573]
	TIME [epoch: 2.75 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09071944089423564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09071944089423564 | validation: 0.09979924949100494]
	TIME [epoch: 2.75 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.067819310425694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.067819310425694 | validation: 0.08333036688401735]
	TIME [epoch: 2.75 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06071927405615729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06071927405615729 | validation: 0.13271656600054338]
	TIME [epoch: 2.75 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07790764566480929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07790764566480929 | validation: 0.11867758705239072]
	TIME [epoch: 2.75 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07990760882742386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07990760882742386 | validation: 0.06013178683017817]
	TIME [epoch: 2.75 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0628281479728217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0628281479728217 | validation: 0.09771844995283703]
	TIME [epoch: 2.75 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04815353936471608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04815353936471608 | validation: 0.03662538910769427]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_653.pth
	Model improved!!!
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384382811995799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0384382811995799 | validation: 0.11488873832761332]
	TIME [epoch: 2.75 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04920323978095119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04920323978095119 | validation: 0.04123216521057735]
	TIME [epoch: 2.76 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051489181144397024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051489181144397024 | validation: 0.18297255706320348]
	TIME [epoch: 2.75 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005462614485387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08005462614485387 | validation: 0.09338504802477987]
	TIME [epoch: 2.75 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12060904422876605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12060904422876605 | validation: 0.2337647539085766]
	TIME [epoch: 2.75 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24026328085276985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24026328085276985 | validation: 0.09635066625760773]
	TIME [epoch: 2.75 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06986689256649081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06986689256649081 | validation: 0.08334003494303494]
	TIME [epoch: 2.75 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03915569882918414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03915569882918414 | validation: 0.04254660152345233]
	TIME [epoch: 2.75 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038063039517630584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038063039517630584 | validation: 0.07321196918369796]
	TIME [epoch: 2.76 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049723997705703536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049723997705703536 | validation: 0.09664798322527418]
	TIME [epoch: 2.75 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08067540710790283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08067540710790283 | validation: 0.077707905192937]
	TIME [epoch: 2.75 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09636172220294892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09636172220294892 | validation: 0.2564780262595001]
	TIME [epoch: 2.75 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20728776887508427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20728776887508427 | validation: 0.07071397355892726]
	TIME [epoch: 2.75 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07284228364066603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07284228364066603 | validation: 0.15561265845722394]
	TIME [epoch: 2.75 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07562345335838333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07562345335838333 | validation: 0.07346583026714087]
	TIME [epoch: 2.75 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446113020725685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05446113020725685 | validation: 0.07210183728229883]
	TIME [epoch: 2.75 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046838045903222485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046838045903222485 | validation: 0.12284651605297917]
	TIME [epoch: 2.75 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299548094704394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08299548094704394 | validation: 0.12066621120500276]
	TIME [epoch: 2.77 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11065237583992257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11065237583992257 | validation: 0.11984368768068282]
	TIME [epoch: 2.75 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07688445091946783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07688445091946783 | validation: 0.072743523265656]
	TIME [epoch: 2.75 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041008885854883846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041008885854883846 | validation: 0.045551104976558225]
	TIME [epoch: 2.75 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03203893131973046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03203893131973046 | validation: 0.04437525254503876]
	TIME [epoch: 2.75 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03101272910195546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03101272910195546 | validation: 0.08422320552272242]
	TIME [epoch: 2.75 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06666923402227794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06666923402227794 | validation: 0.12297497906905386]
	TIME [epoch: 2.75 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16154043829858836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16154043829858836 | validation: 0.3166713962203578]
	TIME [epoch: 2.75 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21375374507049918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21375374507049918 | validation: 0.052655259201392315]
	TIME [epoch: 2.75 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05508253131532239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05508253131532239 | validation: 0.0795899043846951]
	TIME [epoch: 2.75 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048980301933115325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048980301933115325 | validation: 0.06149997706028201]
	TIME [epoch: 2.75 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06674181205729818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06674181205729818 | validation: 0.1033839846613764]
	TIME [epoch: 2.75 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06972477540112959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06972477540112959 | validation: 0.051343529170621294]
	TIME [epoch: 2.75 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06442099001401702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06442099001401702 | validation: 0.06756857504689066]
	TIME [epoch: 2.75 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04276524105271095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04276524105271095 | validation: 0.03735097267031225]
	TIME [epoch: 2.75 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029376745732361986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029376745732361986 | validation: 0.04334440991530956]
	TIME [epoch: 2.75 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028456138867981134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028456138867981134 | validation: 0.10116542324825534]
	TIME [epoch: 2.75 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05921487332311458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05921487332311458 | validation: 0.13207502844501673]
	TIME [epoch: 2.75 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14285375599955688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14285375599955688 | validation: 0.33894430380195795]
	TIME [epoch: 2.75 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24727708073103985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24727708073103985 | validation: 0.06515096986840589]
	TIME [epoch: 2.75 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058471128704588565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058471128704588565 | validation: 0.0503371408277008]
	TIME [epoch: 2.75 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727828393319361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06727828393319361 | validation: 0.13821532391344346]
	TIME [epoch: 2.75 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0758910054656279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0758910054656279 | validation: 0.05121084036107705]
	TIME [epoch: 2.75 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03636385653330526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03636385653330526 | validation: 0.054754541914753166]
	TIME [epoch: 2.75 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040574669228325054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040574669228325054 | validation: 0.1742139418668922]
	TIME [epoch: 2.75 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07221566732743268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07221566732743268 | validation: 0.1015286950927586]
	TIME [epoch: 2.75 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08679571774330022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08679571774330022 | validation: 0.15500151091581083]
	TIME [epoch: 2.75 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12575630211876254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12575630211876254 | validation: 0.08476888263483318]
	TIME [epoch: 2.74 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815458127629947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0815458127629947 | validation: 0.10389034709473657]
	TIME [epoch: 2.74 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07405675137751326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07405675137751326 | validation: 0.06518860079789796]
	TIME [epoch: 2.75 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0542888190679113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0542888190679113 | validation: 0.09258447971833353]
	TIME [epoch: 2.74 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662296967621445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06662296967621445 | validation: 0.07817684264733465]
	TIME [epoch: 2.74 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060365344646892964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060365344646892964 | validation: 0.08846643010712932]
	TIME [epoch: 2.74 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05123921506901434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05123921506901434 | validation: 0.0504519294648207]
	TIME [epoch: 2.74 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04984546491985733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04984546491985733 | validation: 0.08140141794860979]
	TIME [epoch: 2.73 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07095413184411326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07095413184411326 | validation: 0.07509432128340986]
	TIME [epoch: 2.74 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638803493367329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07638803493367329 | validation: 0.13926162288938512]
	TIME [epoch: 2.74 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10403471881073334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10403471881073334 | validation: 0.0892407374790648]
	TIME [epoch: 2.74 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08123080747506879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08123080747506879 | validation: 0.12832643714627487]
	TIME [epoch: 2.73 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634111017447742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0634111017447742 | validation: 0.04425028312057853]
	TIME [epoch: 2.74 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03020588694988893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03020588694988893 | validation: 0.06293812191962027]
	TIME [epoch: 2.74 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025964782663433255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025964782663433255 | validation: 0.031839637935913115]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_712.pth
	Model improved!!!
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02931420293132325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02931420293132325 | validation: 0.0797950013708073]
	TIME [epoch: 2.74 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03722590112271442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03722590112271442 | validation: 0.04767162089942273]
	TIME [epoch: 2.74 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857194894031542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03857194894031542 | validation: 0.13806528169430043]
	TIME [epoch: 2.74 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07252207052904427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07252207052904427 | validation: 0.1267897340515792]
	TIME [epoch: 2.74 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.131249334199568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.131249334199568 | validation: 0.15942126295343306]
	TIME [epoch: 2.74 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16800807791406663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16800807791406663 | validation: 0.12046622464829977]
	TIME [epoch: 2.74 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07184582865067067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07184582865067067 | validation: 0.07041580384399625]
	TIME [epoch: 2.74 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055491762222790675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055491762222790675 | validation: 0.09386856564925679]
	TIME [epoch: 2.74 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046172844890314535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046172844890314535 | validation: 0.11894360649051319]
	TIME [epoch: 2.74 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539958134447925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09539958134447925 | validation: 0.13439625101718536]
	TIME [epoch: 2.74 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1445003864293585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1445003864293585 | validation: 0.15975656207949682]
	TIME [epoch: 2.74 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10352177354894618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10352177354894618 | validation: 0.062364306531417606]
	TIME [epoch: 2.74 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483726220374589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0483726220374589 | validation: 0.04669765713622148]
	TIME [epoch: 2.73 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301043845484753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05301043845484753 | validation: 0.06012655851158033]
	TIME [epoch: 2.74 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043687704026858364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043687704026858364 | validation: 0.05189846263034671]
	TIME [epoch: 2.73 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047522668262908394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047522668262908394 | validation: 0.062191459174627234]
	TIME [epoch: 2.74 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051325169369414336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051325169369414336 | validation: 0.12590781368293907]
	TIME [epoch: 2.74 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07797705537974117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07797705537974117 | validation: 0.07133042256230145]
	TIME [epoch: 2.74 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07223288628742464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07223288628742464 | validation: 0.13690666484293387]
	TIME [epoch: 2.74 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07827255466908957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07827255466908957 | validation: 0.07278381919915834]
	TIME [epoch: 2.74 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07025232640225144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07025232640225144 | validation: 0.08547641075717859]
	TIME [epoch: 2.74 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938501624713257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06938501624713257 | validation: 0.09113366822470167]
	TIME [epoch: 2.74 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06522858303629916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06522858303629916 | validation: 0.08643819544485232]
	TIME [epoch: 2.74 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07241695222116225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07241695222116225 | validation: 0.07671853454080524]
	TIME [epoch: 2.74 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04799627154634589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04799627154634589 | validation: 0.05669774417860275]
	TIME [epoch: 2.74 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03608465733017438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03608465733017438 | validation: 0.035074425041528044]
	TIME [epoch: 2.74 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03287837416957655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03287837416957655 | validation: 0.0748062607378015]
	TIME [epoch: 2.74 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045637032829280604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045637032829280604 | validation: 0.042737871234367765]
	TIME [epoch: 2.74 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0686861148971714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0686861148971714 | validation: 0.09941814232579316]
	TIME [epoch: 2.74 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0835576995902423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0835576995902423 | validation: 0.06371521697075402]
	TIME [epoch: 2.74 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07706487341789704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07706487341789704 | validation: 0.10700250601448458]
	TIME [epoch: 2.74 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10287046673270496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10287046673270496 | validation: 0.2936272335897649]
	TIME [epoch: 2.74 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17675470050491243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17675470050491243 | validation: 0.07047041713141698]
	TIME [epoch: 2.74 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0732299565056088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0732299565056088 | validation: 0.08223283806728551]
	TIME [epoch: 2.75 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567294128502734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0567294128502734 | validation: 0.08950473985663308]
	TIME [epoch: 2.75 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046367239893497766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046367239893497766 | validation: 0.05918667785975029]
	TIME [epoch: 2.74 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03477089800259095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03477089800259095 | validation: 0.044995587362635196]
	TIME [epoch: 2.74 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03126066095219028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03126066095219028 | validation: 0.04755855804515278]
	TIME [epoch: 2.74 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02728468787346694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02728468787346694 | validation: 0.04049445115063741]
	TIME [epoch: 2.74 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02962758109453569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02962758109453569 | validation: 0.05986768634430657]
	TIME [epoch: 2.74 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486659456813585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0486659456813585 | validation: 0.0994356899325407]
	TIME [epoch: 2.74 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09514101263596221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09514101263596221 | validation: 0.14852995349683576]
	TIME [epoch: 2.74 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1341813648325698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1341813648325698 | validation: 0.08827874147104255]
	TIME [epoch: 2.74 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08480241686637807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08480241686637807 | validation: 0.1643115087794001]
	TIME [epoch: 2.74 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07361541272359184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07361541272359184 | validation: 0.05539300007910308]
	TIME [epoch: 2.74 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05091584242780058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05091584242780058 | validation: 0.07430523896550868]
	TIME [epoch: 2.74 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03475177875403164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03475177875403164 | validation: 0.05661015717280359]
	TIME [epoch: 2.74 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03176751008470533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03176751008470533 | validation: 0.04508396352815354]
	TIME [epoch: 2.74 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04998583232411665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04998583232411665 | validation: 0.10940290769509989]
	TIME [epoch: 2.74 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07325826083277924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07325826083277924 | validation: 0.047266738896127994]
	TIME [epoch: 2.74 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386745667807542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06386745667807542 | validation: 0.0747505902250472]
	TIME [epoch: 2.74 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0488176118625585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0488176118625585 | validation: 0.07067651019389408]
	TIME [epoch: 2.74 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945087390567744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04945087390567744 | validation: 0.07354793151789506]
	TIME [epoch: 2.74 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.075707673063741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.075707673063741 | validation: 0.17815664879528523]
	TIME [epoch: 2.74 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15919660268581598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15919660268581598 | validation: 0.09584649297635386]
	TIME [epoch: 2.74 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08063395839280318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08063395839280318 | validation: 0.09450601637253243]
	TIME [epoch: 2.74 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05147673585117523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05147673585117523 | validation: 0.027279925939021645]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_769.pth
	Model improved!!!
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03124122091750228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03124122091750228 | validation: 0.046187960376642793]
	TIME [epoch: 2.74 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023275601900406038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023275601900406038 | validation: 0.05201682256275328]
	TIME [epoch: 2.74 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027620246055051593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027620246055051593 | validation: 0.0549033332741007]
	TIME [epoch: 2.74 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0449286888458648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0449286888458648 | validation: 0.116746793811415]
	TIME [epoch: 2.74 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06537926480808484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06537926480808484 | validation: 0.07025451445682458]
	TIME [epoch: 2.74 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06945865026881723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06945865026881723 | validation: 0.1737690077197226]
	TIME [epoch: 2.74 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08491387036947882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08491387036947882 | validation: 0.0859220148640888]
	TIME [epoch: 2.74 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0938296090410201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0938296090410201 | validation: 0.09748470918614291]
	TIME [epoch: 2.74 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1394792247065929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1394792247065929 | validation: 0.13024580454392123]
	TIME [epoch: 2.74 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07324528457909764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07324528457909764 | validation: 0.0993556638655132]
	TIME [epoch: 2.74 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06582367666120204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06582367666120204 | validation: 0.031943354322606934]
	TIME [epoch: 2.74 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038473969461714046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038473969461714046 | validation: 0.07751503712999061]
	TIME [epoch: 2.74 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03890966937241352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03890966937241352 | validation: 0.06070792634314031]
	TIME [epoch: 2.74 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041553197862053554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041553197862053554 | validation: 0.06765720903586218]
	TIME [epoch: 2.74 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04299232116097247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04299232116097247 | validation: 0.0761121815714938]
	TIME [epoch: 2.74 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05630401230564719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05630401230564719 | validation: 0.09352019213110618]
	TIME [epoch: 2.74 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074376016945635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07074376016945635 | validation: 0.07633922240362645]
	TIME [epoch: 2.74 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07381419564689858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07381419564689858 | validation: 0.11063509089334043]
	TIME [epoch: 2.74 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08721446138604673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08721446138604673 | validation: 0.10280451223516024]
	TIME [epoch: 2.74 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253386796412943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1253386796412943 | validation: 0.236626389410527]
	TIME [epoch: 2.74 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1630485518218193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1630485518218193 | validation: 0.048304979839376254]
	TIME [epoch: 2.74 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04336278021277943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04336278021277943 | validation: 0.03788306656835036]
	TIME [epoch: 2.74 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024675735467984565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024675735467984565 | validation: 0.042569926477135756]
	TIME [epoch: 2.74 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036163396487505425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036163396487505425 | validation: 0.05447315613780632]
	TIME [epoch: 2.75 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038912390003423655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038912390003423655 | validation: 0.04069835169140234]
	TIME [epoch: 2.74 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04095722270792591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04095722270792591 | validation: 0.04476505385129699]
	TIME [epoch: 2.74 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033419552657006295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033419552657006295 | validation: 0.04367463632188519]
	TIME [epoch: 2.75 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464632462736865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03464632462736865 | validation: 0.04121158213448507]
	TIME [epoch: 2.74 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030836801736535555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030836801736535555 | validation: 0.07702844436739749]
	TIME [epoch: 2.74 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05258347914074819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05258347914074819 | validation: 0.09686545105743578]
	TIME [epoch: 2.75 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11066980763410592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11066980763410592 | validation: 0.19578281898462102]
	TIME [epoch: 2.74 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11953704762396043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11953704762396043 | validation: 0.04341712662905626]
	TIME [epoch: 2.75 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03352814593138459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03352814593138459 | validation: 0.03363014141983333]
	TIME [epoch: 2.76 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031671410977841784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031671410977841784 | validation: 0.1554280535827513]
	TIME [epoch: 2.75 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07108855453144668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07108855453144668 | validation: 0.09630428787076141]
	TIME [epoch: 2.75 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924314563030746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924314563030746 | validation: 0.1291596502716014]
	TIME [epoch: 2.75 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15734518651426296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15734518651426296 | validation: 0.08271905251658083]
	TIME [epoch: 2.75 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914348061675685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05914348061675685 | validation: 0.04167340835970626]
	TIME [epoch: 2.75 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021599522429247692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021599522429247692 | validation: 0.036160802040861195]
	TIME [epoch: 2.75 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018287997666618237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018287997666618237 | validation: 0.030231070558331276]
	TIME [epoch: 2.75 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019400938201006922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019400938201006922 | validation: 0.03297051556759323]
	TIME [epoch: 2.75 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019601296886938357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019601296886938357 | validation: 0.04722751706952562]
	TIME [epoch: 2.75 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03239162853061688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03239162853061688 | validation: 0.09453142196332484]
	TIME [epoch: 2.75 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06577940975950716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06577940975950716 | validation: 0.12385364841585858]
	TIME [epoch: 2.75 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09774731574217067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09774731574217067 | validation: 0.08600563269701947]
	TIME [epoch: 2.75 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07588985364355437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07588985364355437 | validation: 0.0837207010324856]
	TIME [epoch: 2.75 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06814390037475049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06814390037475049 | validation: 0.07644069802926168]
	TIME [epoch: 2.75 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960280480250314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0960280480250314 | validation: 0.18256415183292055]
	TIME [epoch: 2.75 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582936741726986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1582936741726986 | validation: 0.0980888699830177]
	TIME [epoch: 2.75 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0766874161854515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0766874161854515 | validation: 0.04135091848058047]
	TIME [epoch: 2.75 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03924430125003612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03924430125003612 | validation: 0.061492135741566124]
	TIME [epoch: 2.75 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022389366558334702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022389366558334702 | validation: 0.023621961003574755]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_821.pth
	Model improved!!!
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02238806698392347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02238806698392347 | validation: 0.04964332221109749]
	TIME [epoch: 2.75 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02179950877859202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02179950877859202 | validation: 0.031275874844085]
	TIME [epoch: 2.76 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021924935344438894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021924935344438894 | validation: 0.06491277804718053]
	TIME [epoch: 2.75 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03456102254092261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03456102254092261 | validation: 0.09718095431784313]
	TIME [epoch: 2.74 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09034908886919212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09034908886919212 | validation: 0.18595711256569114]
	TIME [epoch: 2.74 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14691838688645983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14691838688645983 | validation: 0.08006905471421934]
	TIME [epoch: 2.74 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07312845724521722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07312845724521722 | validation: 0.03945901542237882]
	TIME [epoch: 2.75 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03399396644390293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03399396644390293 | validation: 0.058609616788927045]
	TIME [epoch: 2.74 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027559835267924764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027559835267924764 | validation: 0.03277820787105049]
	TIME [epoch: 2.75 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040832516842757606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040832516842757606 | validation: 0.09626161706949538]
	TIME [epoch: 2.74 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679861476106214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06679861476106214 | validation: 0.0941117458790593]
	TIME [epoch: 2.75 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07645168769687205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07645168769687205 | validation: 0.07886662315411044]
	TIME [epoch: 2.75 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06586244206516283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06586244206516283 | validation: 0.09754592371983004]
	TIME [epoch: 2.75 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046756617595531895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046756617595531895 | validation: 0.030925749084946122]
	TIME [epoch: 2.75 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029160710576582895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029160710576582895 | validation: 0.057772900544463705]
	TIME [epoch: 2.75 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021522754867983992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021522754867983992 | validation: 0.019996372308716637]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_837.pth
	Model improved!!!
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016009494588926046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016009494588926046 | validation: 0.03788821463135005]
	TIME [epoch: 2.75 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023914508318169297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023914508318169297 | validation: 0.05909071157123255]
	TIME [epoch: 2.74 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0669861081213943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0669861081213943 | validation: 0.1090629875300432]
	TIME [epoch: 2.75 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12899220234909078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12899220234909078 | validation: 0.25953317565255496]
	TIME [epoch: 2.74 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18476317169956866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18476317169956866 | validation: 0.09668406289518867]
	TIME [epoch: 2.74 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07217040033610628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07217040033610628 | validation: 0.054862689506559904]
	TIME [epoch: 2.74 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03801674930093262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03801674930093262 | validation: 0.05418032393597616]
	TIME [epoch: 2.75 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020563774376543817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020563774376543817 | validation: 0.04865352373810016]
	TIME [epoch: 2.76 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035443839955746556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035443839955746556 | validation: 0.07140120881462185]
	TIME [epoch: 2.74 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03967303929491037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03967303929491037 | validation: 0.06565563210560359]
	TIME [epoch: 2.75 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049610578198313306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049610578198313306 | validation: 0.08218249253280425]
	TIME [epoch: 2.75 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06682318757232766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06682318757232766 | validation: 0.13304990423255258]
	TIME [epoch: 2.75 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09811821348616409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09811821348616409 | validation: 0.08625122893884597]
	TIME [epoch: 2.74 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09201883593131607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09201883593131607 | validation: 0.09454088791754033]
	TIME [epoch: 2.75 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06375937540919513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06375937540919513 | validation: 0.05786900488719759]
	TIME [epoch: 2.75 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504621489421501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04504621489421501 | validation: 0.06306334614405999]
	TIME [epoch: 2.75 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0625716022802813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0625716022802813 | validation: 0.06455152454381247]
	TIME [epoch: 2.75 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058084814584308475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058084814584308475 | validation: 0.04865806875470056]
	TIME [epoch: 2.75 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04746925559170723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04746925559170723 | validation: 0.042567386313882766]
	TIME [epoch: 2.75 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024152093610942244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024152093610942244 | validation: 0.031737958597537745]
	TIME [epoch: 2.75 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016732234675903367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016732234675903367 | validation: 0.023269273735714947]
	TIME [epoch: 2.75 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016371680037307063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016371680037307063 | validation: 0.05536648910247785]
	TIME [epoch: 2.75 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02473178601080825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02473178601080825 | validation: 0.05379847910755967]
	TIME [epoch: 2.75 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05661558932127236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05661558932127236 | validation: 0.2537801705585216]
	TIME [epoch: 2.75 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14759592850855247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14759592850855247 | validation: 0.10499000084564088]
	TIME [epoch: 2.76 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11384470124489601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11384470124489601 | validation: 0.1037821957817592]
	TIME [epoch: 2.75 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204541078365103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1204541078365103 | validation: 0.07910341685115108]
	TIME [epoch: 2.75 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044301133636148314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044301133636148314 | validation: 0.034375393717628655]
	TIME [epoch: 2.75 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02019174092420982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02019174092420982 | validation: 0.034556933656061266]
	TIME [epoch: 2.75 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0197709278669051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0197709278669051 | validation: 0.03231450161025823]
	TIME [epoch: 2.76 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021922931220356167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021922931220356167 | validation: 0.03878792128088269]
	TIME [epoch: 2.74 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02245877924318941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02245877924318941 | validation: 0.07045075822598182]
	TIME [epoch: 2.76 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694939528880199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05694939528880199 | validation: 0.17050856523621843]
	TIME [epoch: 2.74 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11430114128401499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11430114128401499 | validation: 0.10823181718761253]
	TIME [epoch: 2.75 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.100681296164279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.100681296164279 | validation: 0.04624223543444711]
	TIME [epoch: 2.74 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056941422637968274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056941422637968274 | validation: 0.04367779600127073]
	TIME [epoch: 2.74 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025735208142473147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025735208142473147 | validation: 0.026031648314723113]
	TIME [epoch: 2.74 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019891864533044836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019891864533044836 | validation: 0.03932187721886896]
	TIME [epoch: 2.74 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026012688985718135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026012688985718135 | validation: 0.07066612690586992]
	TIME [epoch: 2.74 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04864247046367231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04864247046367231 | validation: 0.11045213502082928]
	TIME [epoch: 2.74 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1056865367326832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1056865367326832 | validation: 0.17614731311308468]
	TIME [epoch: 2.74 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14500473519694146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14500473519694146 | validation: 0.06397588363071348]
	TIME [epoch: 2.75 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05981163128769623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05981163128769623 | validation: 0.04646297814727182]
	TIME [epoch: 2.74 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03862427901794204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03862427901794204 | validation: 0.06381823986916264]
	TIME [epoch: 2.74 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041260765981804254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041260765981804254 | validation: 0.04880921545828695]
	TIME [epoch: 2.74 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0415766383437904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0415766383437904 | validation: 0.052459488417332584]
	TIME [epoch: 2.74 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03749116470219616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03749116470219616 | validation: 0.04965730379972231]
	TIME [epoch: 2.74 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033372903013390534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033372903013390534 | validation: 0.051850163223915315]
	TIME [epoch: 2.74 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04042402171667654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04042402171667654 | validation: 0.062082621447149405]
	TIME [epoch: 2.74 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05905709965088649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05905709965088649 | validation: 0.06279803085781947]
	TIME [epoch: 2.74 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05749244673945771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05749244673945771 | validation: 0.08837092591627463]
	TIME [epoch: 2.74 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05922746434599062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05922746434599062 | validation: 0.0599788380771273]
	TIME [epoch: 2.74 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06207900320599061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06207900320599061 | validation: 0.10347891114951474]
	TIME [epoch: 2.74 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05945520899047194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05945520899047194 | validation: 0.04088465314811022]
	TIME [epoch: 2.74 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03930530725340527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03930530725340527 | validation: 0.051494858081639075]
	TIME [epoch: 2.74 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029962313423546407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029962313423546407 | validation: 0.06346913802818131]
	TIME [epoch: 2.74 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04003298249541494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04003298249541494 | validation: 0.08766554239775348]
	TIME [epoch: 2.74 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06556112377726618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06556112377726618 | validation: 0.12584084042407692]
	TIME [epoch: 2.74 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07597566894040382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07597566894040382 | validation: 0.05926591082008009]
	TIME [epoch: 2.74 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057586951640602846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057586951640602846 | validation: 0.09309796841934445]
	TIME [epoch: 2.74 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04963103160812168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04963103160812168 | validation: 0.06119182132272484]
	TIME [epoch: 2.74 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059722217172540545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059722217172540545 | validation: 0.056689417169715345]
	TIME [epoch: 2.74 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06873553758247278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06873553758247278 | validation: 0.07167475432468327]
	TIME [epoch: 2.74 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05430983393756596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05430983393756596 | validation: 0.05433663012519219]
	TIME [epoch: 2.75 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04252236180631105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04252236180631105 | validation: 0.044225696463046395]
	TIME [epoch: 2.76 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037372661556725285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037372661556725285 | validation: 0.05523715051167134]
	TIME [epoch: 2.74 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0347633610266683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0347633610266683 | validation: 0.044832855876629996]
	TIME [epoch: 2.75 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05074196072995679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05074196072995679 | validation: 0.14286191480166793]
	TIME [epoch: 2.74 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08771714634943387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08771714634943387 | validation: 0.08188412089488808]
	TIME [epoch: 2.74 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0789929490299876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0789929490299876 | validation: 0.06533943779891571]
	TIME [epoch: 2.74 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055122919078536746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055122919078536746 | validation: 0.04979272028083629]
	TIME [epoch: 2.75 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02546806723860095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02546806723860095 | validation: 0.02423148462031919]
	TIME [epoch: 2.74 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023350019284007102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023350019284007102 | validation: 0.035643248856539876]
	TIME [epoch: 2.74 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02381816638076112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02381816638076112 | validation: 0.021020793667272997]
	TIME [epoch: 2.74 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030926745273262234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030926745273262234 | validation: 0.04935660032396107]
	TIME [epoch: 2.74 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03482432989115959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03482432989115959 | validation: 0.07681165188632279]
	TIME [epoch: 2.74 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850112405482102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0850112405482102 | validation: 0.20963970643646226]
	TIME [epoch: 2.74 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17345072115158877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17345072115158877 | validation: 0.1503446486084517]
	TIME [epoch: 2.74 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09807570192417542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09807570192417542 | validation: 0.035836903395527885]
	TIME [epoch: 2.74 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018680948782528472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018680948782528472 | validation: 0.03577946305284004]
	TIME [epoch: 2.74 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029305882643504075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029305882643504075 | validation: 0.10777244122747841]
	TIME [epoch: 2.74 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056115561166708475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056115561166708475 | validation: 0.06888683389167181]
	TIME [epoch: 2.74 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07461704407635142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07461704407635142 | validation: 0.10771602997940205]
	TIME [epoch: 2.74 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965367220798637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07965367220798637 | validation: 0.082205586066173]
	TIME [epoch: 2.75 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05307469729142117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05307469729142117 | validation: 0.04499086334618775]
	TIME [epoch: 2.76 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03592404537379501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03592404537379501 | validation: 0.0430108288089878]
	TIME [epoch: 2.74 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026373992318245172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026373992318245172 | validation: 0.02655577456586902]
	TIME [epoch: 2.74 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.016113684145627322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.016113684145627322 | validation: 0.02475971587009127]
	TIME [epoch: 2.74 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01011863630847101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01011863630847101 | validation: 0.023139476686392036]
	TIME [epoch: 2.74 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.009665321129529477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.009665321129529477 | validation: 0.011044914914393002]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_927.pth
	Model improved!!!
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.013915908408296618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.013915908408296618 | validation: 0.061501257365871066]
	TIME [epoch: 2.76 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027580879278754963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027580879278754963 | validation: 0.028196420944775646]
	TIME [epoch: 2.74 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050110223113381876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050110223113381876 | validation: 0.08783071945747269]
	TIME [epoch: 2.74 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07407171590968002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07407171590968002 | validation: 0.07436080173272784]
	TIME [epoch: 2.74 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07675352197784978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07675352197784978 | validation: 0.16174884153512387]
	TIME [epoch: 2.76 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11432173318931557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11432173318931557 | validation: 0.2013699913039135]
	TIME [epoch: 2.75 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11905298204305355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11905298204305355 | validation: 0.08890554399203637]
	TIME [epoch: 2.75 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10730053420467968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10730053420467968 | validation: 0.10131485632701308]
	TIME [epoch: 2.75 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039947193075848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039947193075848 | validation: 0.056525127775793896]
	TIME [epoch: 2.75 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03898949536618586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03898949536618586 | validation: 0.045606142202919524]
	TIME [epoch: 2.75 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02163955436401679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02163955436401679 | validation: 0.02500284379213188]
	TIME [epoch: 2.74 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.014352224183573079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.014352224183573079 | validation: 0.027351991210560634]
	TIME [epoch: 2.74 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.010585958168131975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.010585958168131975 | validation: 0.025909445821622856]
	TIME [epoch: 2.74 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.00995070107166517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.00995070107166517 | validation: 0.018051167741901766]
	TIME [epoch: 2.74 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01240614715784425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01240614715784425 | validation: 0.03734041956363077]
	TIME [epoch: 2.74 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022533672075825805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022533672075825805 | validation: 0.05029206783336442]
	TIME [epoch: 2.74 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05593825759559634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05593825759559634 | validation: 0.10625383866772134]
	TIME [epoch: 2.74 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11357870547626256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11357870547626256 | validation: 0.24389330653267063]
	TIME [epoch: 2.74 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728476576133162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1728476576133162 | validation: 0.08992828864084895]
	TIME [epoch: 2.74 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07097671896365633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07097671896365633 | validation: 0.052180763012771414]
	TIME [epoch: 2.74 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03131784599215722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03131784599215722 | validation: 0.049758102814499994]
	TIME [epoch: 2.74 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02166939284519586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02166939284519586 | validation: 0.04307025596041974]
	TIME [epoch: 2.74 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030222260387483928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030222260387483928 | validation: 0.05017692836632494]
	TIME [epoch: 2.74 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03240895141612733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03240895141612733 | validation: 0.06375982422544248]
	TIME [epoch: 2.74 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047418729958579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047418729958579 | validation: 0.08034372196421408]
	TIME [epoch: 2.74 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725601352338426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0725601352338426 | validation: 0.12236840596279952]
	TIME [epoch: 2.74 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08938059884184124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08938059884184124 | validation: 0.0668310807570587]
	TIME [epoch: 2.75 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06310214394989258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06310214394989258 | validation: 0.05135940300031547]
	TIME [epoch: 2.75 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05256541237256073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05256541237256073 | validation: 0.053324671525854433]
	TIME [epoch: 2.75 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934233361412337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04934233361412337 | validation: 0.05990139894786098]
	TIME [epoch: 2.75 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058886979615890046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058886979615890046 | validation: 0.07212622552293273]
	TIME [epoch: 2.75 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05944325351387361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05944325351387361 | validation: 0.07771847370621754]
	TIME [epoch: 2.75 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04362719554460446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04362719554460446 | validation: 0.03012305048200722]
	TIME [epoch: 2.76 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02623358279935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02623358279935 | validation: 0.0665627402705148]
	TIME [epoch: 2.75 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025036927445121836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025036927445121836 | validation: 0.04319139330678941]
	TIME [epoch: 2.76 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230643422986386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03230643422986386 | validation: 0.07885124982672811]
	TIME [epoch: 2.74 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04102672248217273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04102672248217273 | validation: 0.04199710430165221]
	TIME [epoch: 2.74 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564510907543784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03564510907543784 | validation: 0.08270560698021007]
	TIME [epoch: 2.74 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06897820304235208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06897820304235208 | validation: 0.11257745880524939]
	TIME [epoch: 2.74 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10076234918650932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10076234918650932 | validation: 0.08069924348524]
	TIME [epoch: 2.75 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07888407631584643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07888407631584643 | validation: 0.04142074129320919]
	TIME [epoch: 2.74 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02606072249755956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02606072249755956 | validation: 0.026669483905830252]
	TIME [epoch: 2.74 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.011572398053207587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.011572398053207587 | validation: 0.027682189066575904]
	TIME [epoch: 2.74 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01398103800862789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01398103800862789 | validation: 0.05454103578337088]
	TIME [epoch: 2.74 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03361401661141231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03361401661141231 | validation: 0.1321811466308167]
	TIME [epoch: 2.74 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10206873367040036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10206873367040036 | validation: 0.13501908361104414]
	TIME [epoch: 2.74 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11490708681920589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11490708681920589 | validation: 0.06391063730634577]
	TIME [epoch: 2.74 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05252862808978266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05252862808978266 | validation: 0.02220063714664171]
	TIME [epoch: 2.74 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03152137234503588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03152137234503588 | validation: 0.04405634331348727]
	TIME [epoch: 2.75 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025686321864147512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025686321864147512 | validation: 0.029707419949071126]
	TIME [epoch: 2.74 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027055021743612885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027055021743612885 | validation: 0.051182456102510245]
	TIME [epoch: 2.74 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028219758259936382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028219758259936382 | validation: 0.05485760453248104]
	TIME [epoch: 2.74 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04181125948337087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04181125948337087 | validation: 0.08648649961988306]
	TIME [epoch: 2.75 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08255285382752003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08255285382752003 | validation: 0.20414128790824504]
	TIME [epoch: 2.74 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12919569097533404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12919569097533404 | validation: 0.06918177177773338]
	TIME [epoch: 2.74 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06343501597007307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06343501597007307 | validation: 0.037169784341342986]
	TIME [epoch: 2.74 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026768841890582547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026768841890582547 | validation: 0.04802168332698347]
	TIME [epoch: 2.74 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027718164598414398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027718164598414398 | validation: 0.03719823957479025]
	TIME [epoch: 2.74 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03007609452313405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03007609452313405 | validation: 0.0589436916342003]
	TIME [epoch: 2.74 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034000173410489706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034000173410489706 | validation: 0.05446758817405437]
	TIME [epoch: 2.75 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03574092980473865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03574092980473865 | validation: 0.05723967311593592]
	TIME [epoch: 2.76 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04244300301576134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04244300301576134 | validation: 0.08303625739904114]
	TIME [epoch: 2.75 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056478123536939505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056478123536939505 | validation: 0.07556897642420855]
	TIME [epoch: 2.75 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07060655843688161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07060655843688161 | validation: 0.06274998197046548]
	TIME [epoch: 2.75 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07562787287538174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07562787287538174 | validation: 0.059108739232316715]
	TIME [epoch: 2.75 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047841619357665406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047841619357665406 | validation: 0.0637450372675104]
	TIME [epoch: 2.75 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044810833902515335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044810833902515335 | validation: 0.060001777065018524]
	TIME [epoch: 2.75 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04961289824275771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04961289824275771 | validation: 0.06737814784436523]
	TIME [epoch: 2.74 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046956301532317654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046956301532317654 | validation: 0.04435295189295914]
	TIME [epoch: 2.74 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03252781879627343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03252781879627343 | validation: 0.035849127893537426]
	TIME [epoch: 2.75 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023830344037107338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023830344037107338 | validation: 0.04113874727947342]
	TIME [epoch: 2.74 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025643077192767604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025643077192767604 | validation: 0.057036296960545664]
	TIME [epoch: 2.74 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042289643971503636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042289643971503636 | validation: 0.10404632869062456]
	TIME [epoch: 2.74 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05814483439993525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05814483439993525 | validation: 0.05293437120604815]
	TIME [epoch: 180 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04626829632116209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04626829632116209 | validation: 0.04569267918693013]
	TIME [epoch: 5.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03566277253914125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03566277253914125 | validation: 0.05593638978361076]
	TIME [epoch: 5.88 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06504269669696612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06504269669696612 | validation: 0.14860780918105512]
	TIME [epoch: 5.88 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13746042309892637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13746042309892637 | validation: 0.06053217313267695]
	TIME [epoch: 5.88 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05233281342820828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05233281342820828 | validation: 0.062232529954113616]
	TIME [epoch: 5.89 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943475707420901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03943475707420901 | validation: 0.04295621120410353]
	TIME [epoch: 5.88 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04468899795438768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04468899795438768 | validation: 0.048368317251047226]
	TIME [epoch: 5.88 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028535657903076314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028535657903076314 | validation: 0.0370560180444359]
	TIME [epoch: 5.88 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028642109101990675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028642109101990675 | validation: 0.05564230795125649]
	TIME [epoch: 5.88 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03699702023319369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03699702023319369 | validation: 0.06968197660879398]
	TIME [epoch: 5.89 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044228816165459824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044228816165459824 | validation: 0.05968580709780105]
	TIME [epoch: 5.88 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04348387474844717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04348387474844717 | validation: 0.033229883564258246]
	TIME [epoch: 5.88 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03021637044672777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03021637044672777 | validation: 0.04206282790858196]
	TIME [epoch: 5.89 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024832177940669626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024832177940669626 | validation: 0.024996871168501023]
	TIME [epoch: 5.89 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02842516251446149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02842516251446149 | validation: 0.04919548476473479]
	TIME [epoch: 5.89 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0396459680560593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0396459680560593 | validation: 0.03033813011352171]
	TIME [epoch: 5.89 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05436504317894292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05436504317894292 | validation: 0.07345613545660885]
	TIME [epoch: 5.89 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07631211532416775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07631211532416775 | validation: 0.20299984869073545]
	TIME [epoch: 5.88 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14618150766882793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14618150766882793 | validation: 0.10179336950171476]
	TIME [epoch: 5.89 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09806100588490123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09806100588490123 | validation: 0.0681230984813964]
	TIME [epoch: 5.88 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036439557352670654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036439557352670654 | validation: 0.041603203578163266]
	TIME [epoch: 5.88 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020629657113076214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020629657113076214 | validation: 0.03677027522946744]
	TIME [epoch: 5.88 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029479550673739065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029479550673739065 | validation: 0.06873867760266639]
	TIME [epoch: 5.88 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601709891019144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03601709891019144 | validation: 0.0330854984888266]
	TIME [epoch: 5.89 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02679157266242308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02679157266242308 | validation: 0.03608485466142704]
	TIME [epoch: 5.88 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02674957038879117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02674957038879117 | validation: 0.036055953008167]
	TIME [epoch: 5.89 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02822343711143433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02822343711143433 | validation: 0.05065422549656942]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd1_20241105_114504/states/model_phi1_4a_v_mmd1_1028.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 2848.551 seconds.
