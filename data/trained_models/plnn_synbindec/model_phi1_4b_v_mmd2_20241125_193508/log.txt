Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/basic/data_phi1_4b/training', validation_data='data/training_data/basic/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1481486630

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.83251401917159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.83251401917159 | validation: 6.261311209946719]
	TIME [epoch: 173 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.731205803274308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.731205803274308 | validation: 4.321186035983134]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.872870731124429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.872870731124429 | validation: 5.977793263939401]
	TIME [epoch: 1.38 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.183657563084573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.183657563084573 | validation: 4.483376453936748]
	TIME [epoch: 1.38 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.626464973494018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.626464973494018 | validation: 6.021785232783554]
	TIME [epoch: 1.38 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.538800251092013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.538800251092013 | validation: 5.758530229784893]
	TIME [epoch: 1.38 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.459204801474145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.459204801474145 | validation: 4.943084588937499]
	TIME [epoch: 1.38 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.6488892510371125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.6488892510371125 | validation: 4.387841924656157]
	TIME [epoch: 1.38 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.571467347606392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.571467347606392 | validation: 4.176292435471971]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.342038593624408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.342038593624408 | validation: 3.8033608989267296]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.347825345848432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.347825345848432 | validation: 3.8413653979514493]
	TIME [epoch: 1.39 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.02615967865378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.02615967865378 | validation: 3.8843402277695165]
	TIME [epoch: 1.38 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.04244765236554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.04244765236554 | validation: 3.652232799688625]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.041817842322603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.041817842322603 | validation: 3.760993395189428]
	TIME [epoch: 1.39 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9489022252211243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9489022252211243 | validation: 3.6675722929358834]
	TIME [epoch: 1.39 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9092664813756715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9092664813756715 | validation: 3.625956459866498]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8881780970389745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8881780970389745 | validation: 3.6424245824086223]
	TIME [epoch: 1.39 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8686539189638887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8686539189638887 | validation: 3.549891952353846]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8565241183928367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8565241183928367 | validation: 3.661397851287626]
	TIME [epoch: 1.39 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.85894086871519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.85894086871519 | validation: 3.503300791356951]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8987358984961675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8987358984961675 | validation: 3.696246344115476]
	TIME [epoch: 1.38 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.880303652112424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.880303652112424 | validation: 3.4724404584452557]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8526223097985817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8526223097985817 | validation: 3.559799069106784]
	TIME [epoch: 1.39 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7859204230534154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7859204230534154 | validation: 3.4386524812228325]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7568417124008704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7568417124008704 | validation: 3.4957068837149237]
	TIME [epoch: 1.38 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.737939174298034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737939174298034 | validation: 3.4082172730990834]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7248580803187257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7248580803187257 | validation: 3.5061759389748435]
	TIME [epoch: 1.39 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7316237885046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7316237885046 | validation: 3.394242509019465]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7647272810290318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7647272810290318 | validation: 3.553046165461976]
	TIME [epoch: 1.39 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7691356148824973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7691356148824973 | validation: 3.3720125749444936]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7522462701748016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7522462701748016 | validation: 3.4364386449909894]
	TIME [epoch: 1.39 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6791988221661303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6791988221661303 | validation: 3.331404595638735]
	TIME [epoch: 1.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6489351125836595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6489351125836595 | validation: 3.365539441830073]
	TIME [epoch: 1.39 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.626098862730396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.626098862730396 | validation: 3.3117770258535675]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.615040177326138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.615040177326138 | validation: 3.341813242565623]
	TIME [epoch: 1.38 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6062541191783577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6062541191783577 | validation: 3.285256439417498]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.60752324843132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.60752324843132 | validation: 3.3903810278829534]
	TIME [epoch: 1.39 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6322683799401667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6322683799401667 | validation: 3.3161065656191298]
	TIME [epoch: 1.38 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.703696246131532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.703696246131532 | validation: 3.379788114029452]
	TIME [epoch: 1.38 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6258674872538723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6258674872538723 | validation: 3.2414521024682887]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.579676992808931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.579676992808931 | validation: 3.268689259622231]
	TIME [epoch: 1.38 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5420661560577797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5420661560577797 | validation: 3.2139757752539513]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.524108059452466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.524108059452466 | validation: 3.2229325961832673]
	TIME [epoch: 1.38 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.508897090356477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.508897090356477 | validation: 3.1959206333543775]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.501220955633414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.501220955633414 | validation: 3.206439084255064]
	TIME [epoch: 1.38 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4960277390713546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4960277390713546 | validation: 3.173100811661056]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4925633894627817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4925633894627817 | validation: 3.2286419594263807]
	TIME [epoch: 1.38 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5009971824109303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5009971824109303 | validation: 3.1865004904063206]
	TIME [epoch: 1.38 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5387979994398187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5387979994398187 | validation: 3.2642385327583328]
	TIME [epoch: 1.38 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.529940458065918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.529940458065918 | validation: 3.161137774629019]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5211367348173486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5211367348173486 | validation: 3.1693212144635363]
	TIME [epoch: 1.39 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4599106269678845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4599106269678845 | validation: 3.116361200071162]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.421639128684293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.421639128684293 | validation: 3.098315133495222]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4091483159499756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4091483159499756 | validation: 3.098259176294146]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.400276488060711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.400276488060711 | validation: 3.0739813546276515]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.393288270180327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.393288270180327 | validation: 3.0813841429134885]
	TIME [epoch: 1.39 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.387204211707809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.387204211707809 | validation: 3.0569861844925956]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3833736091001048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3833736091001048 | validation: 3.092235753850487]
	TIME [epoch: 1.4 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3875577805492867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3875577805492867 | validation: 3.0529626244848616]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3993795021341437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3993795021341437 | validation: 3.0953785191952847]
	TIME [epoch: 1.39 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3889189150072596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3889189150072596 | validation: 3.0277360126776927]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3773264105389216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3773264105389216 | validation: 3.03069123450154]
	TIME [epoch: 1.39 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3368794318376946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3368794318376946 | validation: 2.999864211096592]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3221649591998754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3221649591998754 | validation: 2.994664383277721]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.312630971225403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.312630971225403 | validation: 2.974837151552398]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.301377386422514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.301377386422514 | validation: 2.971372320630647]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2912891048714505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2912891048714505 | validation: 2.956373004442248]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.277272453560864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.277272453560864 | validation: 2.954606742962534]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.268713643980587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.268713643980587 | validation: 2.941914732240491]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2615465596327553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2615465596327553 | validation: 2.939187149725436]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2510116730556566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2510116730556566 | validation: 2.921752780413353]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.243787392782199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.243787392782199 | validation: 2.923231254280081]
	TIME [epoch: 1.39 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2331536621387023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2331536621387023 | validation: 2.912347990616568]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2297928788512738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2297928788512738 | validation: 2.9397209138238893]
	TIME [epoch: 1.39 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.238736373412629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.238736373412629 | validation: 2.933556991687339]
	TIME [epoch: 1.39 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.244983544951807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.244983544951807 | validation: 2.884684900437006]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1700710498516504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1700710498516504 | validation: 2.8534380382468947]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.139189286106031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.139189286106031 | validation: 2.838622149393469]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1066067663299113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1066067663299113 | validation: 2.8102564334303297]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0511316846154797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0511316846154797 | validation: 2.754833142051294]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0050511734906498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0050511734906498 | validation: 2.792266818406438]
	TIME [epoch: 1.38 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0350218233209416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0350218233209416 | validation: 2.7929350639122235]
	TIME [epoch: 1.39 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1018606774908366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1018606774908366 | validation: 2.7122889612028924]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.953539655544672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.953539655544672 | validation: 2.7101092263822624]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_84.pth
	Model improved!!!
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.935160093413602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.935160093413602 | validation: 2.656219233163081]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.952853223359058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.952853223359058 | validation: 2.6822538168947476]
	TIME [epoch: 1.39 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9147440234040465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9147440234040465 | validation: 2.6688181687115153]
	TIME [epoch: 1.39 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902787884106305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.902787884106305 | validation: 2.6538396268381037]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8883135855255966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8883135855255966 | validation: 2.6256209945701063]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_89.pth
	Model improved!!!
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8796810177611234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8796810177611234 | validation: 2.6233848808803475]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8685354360431066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8685354360431066 | validation: 2.6275492248728467]
	TIME [epoch: 1.39 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8552973903095586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8552973903095586 | validation: 2.6102813012049224]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.844280191064631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.844280191064631 | validation: 2.6008758189173418]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8359536662399516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8359536662399516 | validation: 2.6192475526742753]
	TIME [epoch: 1.39 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8311961347321346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8311961347321346 | validation: 2.6398265083190857]
	TIME [epoch: 1.39 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8626276290123878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8626276290123878 | validation: 2.418329886347823]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3543235424542135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3543235424542135 | validation: 4.717660602205638]
	TIME [epoch: 1.39 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.014644658051109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.014644658051109 | validation: 2.482458174057478]
	TIME [epoch: 1.39 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.829605699770523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.829605699770523 | validation: 3.2614366501128096]
	TIME [epoch: 1.39 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9978142366342326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9978142366342326 | validation: 2.342365537234613]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0213908762197472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0213908762197472 | validation: 1.7130343774319414]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5938546640332365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5938546640332365 | validation: 1.612737005961671]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.524021074931834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.524021074931834 | validation: 1.375954911642799]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_103.pth
	Model improved!!!
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3316887899486731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3316887899486731 | validation: 1.2055237927913962]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1892562297093605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1892562297093605 | validation: 1.4309002150281895]
	TIME [epoch: 1.39 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1898014091908231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1898014091908231 | validation: 1.1063648928376908]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1477429362569362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1477429362569362 | validation: 1.326904103550436]
	TIME [epoch: 1.39 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1301022756649934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1301022756649934 | validation: 1.051078606841341]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0246366926910864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0246366926910864 | validation: 1.0445365989564193]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.001086604473685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.001086604473685 | validation: 1.1424053182706924]
	TIME [epoch: 1.39 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9917401341517708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9917401341517708 | validation: 1.0385812731047688]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.981053303116351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.981053303116351 | validation: 1.010367670813349]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604828812261477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9604828812261477 | validation: 1.133190262805636]
	TIME [epoch: 1.39 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9525526660417637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9525526660417637 | validation: 0.9697670857977863]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9454237568790811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9454237568790811 | validation: 1.1540811815108576]
	TIME [epoch: 1.39 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9510574523455675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9510574523455675 | validation: 0.9314466408157345]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.94060494386052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.94060494386052 | validation: 1.1454209407498968]
	TIME [epoch: 1.39 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9268232867047315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9268232867047315 | validation: 0.964767804413349]
	TIME [epoch: 1.39 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8981448101038921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8981448101038921 | validation: 1.0392044785008339]
	TIME [epoch: 1.39 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808685593568698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808685593568698 | validation: 0.9443177523825472]
	TIME [epoch: 1.39 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.87985815858389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.87985815858389 | validation: 1.0940972600693317]
	TIME [epoch: 1.4 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8869900848422834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8869900848422834 | validation: 0.9078635094052943]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8852939892155947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8852939892155947 | validation: 1.1532956396080045]
	TIME [epoch: 1.39 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9152579084235404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9152579084235404 | validation: 0.901198154110973]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9020745618867764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9020745618867764 | validation: 1.16389357954616]
	TIME [epoch: 1.39 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9046259598674568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9046259598674568 | validation: 0.910691449119036]
	TIME [epoch: 1.39 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706075696260743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706075696260743 | validation: 1.0075825332107886]
	TIME [epoch: 1.39 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8672735676063204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8672735676063204 | validation: 0.9614760807919516]
	TIME [epoch: 1.39 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8619059422414697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8619059422414697 | validation: 0.9117828343881383]
	TIME [epoch: 1.39 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320168092612181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320168092612181 | validation: 0.932567985706325]
	TIME [epoch: 1.39 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145268750530166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145268750530166 | validation: 0.8862988881708754]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8243551803412382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8243551803412382 | validation: 0.9857099654076367]
	TIME [epoch: 1.39 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195150988900498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195150988900498 | validation: 0.8855532743797238]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8306780559302382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8306780559302382 | validation: 1.176407728031833]
	TIME [epoch: 1.39 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8952370282744877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8952370282744877 | validation: 0.9838010730173501]
	TIME [epoch: 1.39 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.006588831006688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.006588831006688 | validation: 1.1725809344476537]
	TIME [epoch: 1.39 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8994550534176898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8994550534176898 | validation: 0.9410364635907673]
	TIME [epoch: 1.39 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8371958457774098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8371958457774098 | validation: 0.8897467700004438]
	TIME [epoch: 1.39 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8452858879425756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8452858879425756 | validation: 0.9312659353950106]
	TIME [epoch: 1.39 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887334740799051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7887334740799051 | validation: 0.8461435467696481]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7923469718566928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7923469718566928 | validation: 0.9667003194529858]
	TIME [epoch: 1.39 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110234115882737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110234115882737 | validation: 0.8650498349207645]
	TIME [epoch: 1.39 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8218649903674594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8218649903674594 | validation: 1.0937458520484005]
	TIME [epoch: 1.39 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882878760559764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882878760559764 | validation: 0.8398060886270774]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_144.pth
	Model improved!!!
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7894184684094051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7894184684094051 | validation: 0.8633878715060723]
	TIME [epoch: 1.39 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650950736013468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650950736013468 | validation: 0.9659857133809338]
	TIME [epoch: 1.4 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7855552589991317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7855552589991317 | validation: 0.8074224679466587]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_147.pth
	Model improved!!!
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7796819372578481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7796819372578481 | validation: 1.0195190694208587]
	TIME [epoch: 1.39 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.812673972031509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.812673972031509 | validation: 0.9375696951575831]
	TIME [epoch: 1.39 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9051570309907204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9051570309907204 | validation: 1.1211822045534399]
	TIME [epoch: 1.39 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8829856397400221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8829856397400221 | validation: 0.8496333550918177]
	TIME [epoch: 1.38 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8349222487968249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8349222487968249 | validation: 0.8341070453283183]
	TIME [epoch: 1.38 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673218891291289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673218891291289 | validation: 1.0034158221857592]
	TIME [epoch: 1.39 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8006029170609529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8006029170609529 | validation: 0.8361621284297757]
	TIME [epoch: 1.39 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922299635014267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7922299635014267 | validation: 0.9679445479100679]
	TIME [epoch: 1.39 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7797109914883956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7797109914883956 | validation: 0.827794169231259]
	TIME [epoch: 1.39 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8028736213210428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8028736213210428 | validation: 1.0729030701814952]
	TIME [epoch: 1.39 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8745451983118073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8745451983118073 | validation: 0.850956883206647]
	TIME [epoch: 1.39 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7862024077880679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7862024077880679 | validation: 0.8824649855611351]
	TIME [epoch: 1.39 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7521821500383856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7521821500383856 | validation: 0.8298354919253207]
	TIME [epoch: 1.39 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7459819836754201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7459819836754201 | validation: 0.9037328547102815]
	TIME [epoch: 1.39 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7573103505524498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7573103505524498 | validation: 0.807507838758093]
	TIME [epoch: 1.39 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8441052923918324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8441052923918324 | validation: 0.9429926595671454]
	TIME [epoch: 1.39 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7711057747741866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7711057747741866 | validation: 0.8711436610331513]
	TIME [epoch: 1.39 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7951088130242028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7951088130242028 | validation: 1.0282977227505368]
	TIME [epoch: 1.39 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8029689613350725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8029689613350725 | validation: 0.8288389239966346]
	TIME [epoch: 1.39 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7857557732957453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7857557732957453 | validation: 0.9437722764417518]
	TIME [epoch: 1.39 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8122343538351823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8122343538351823 | validation: 0.9757802806115035]
	TIME [epoch: 1.39 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265629994961762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8265629994961762 | validation: 0.9056971840389034]
	TIME [epoch: 1.39 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8176066825389521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8176066825389521 | validation: 0.8965298504783885]
	TIME [epoch: 1.39 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7444024789829047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7444024789829047 | validation: 0.794833221500751]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7391488649595849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7391488649595849 | validation: 1.0146848923722034]
	TIME [epoch: 1.39 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7872878643091861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7872878643091861 | validation: 0.8214986928842575]
	TIME [epoch: 1.4 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8877008512673864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8877008512673864 | validation: 1.0838055457740663]
	TIME [epoch: 1.39 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8707149699495247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8707149699495247 | validation: 0.9117728456808406]
	TIME [epoch: 1.4 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7594541827978317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7594541827978317 | validation: 0.8740859842225028]
	TIME [epoch: 1.39 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8064156729433112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8064156729433112 | validation: 1.0028627533468037]
	TIME [epoch: 1.39 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754702322240715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7754702322240715 | validation: 0.831598592779017]
	TIME [epoch: 1.39 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7610945812891438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7610945812891438 | validation: 0.8923808103025742]
	TIME [epoch: 1.39 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771689715839229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771689715839229 | validation: 0.9486377601948572]
	TIME [epoch: 1.39 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8234568261022728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8234568261022728 | validation: 0.8708549682054105]
	TIME [epoch: 1.39 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7799301268693382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7799301268693382 | validation: 0.9264041260407421]
	TIME [epoch: 1.39 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7502378771999011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7502378771999011 | validation: 0.7816985130688278]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_183.pth
	Model improved!!!
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7356052373146739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7356052373146739 | validation: 1.0157046622192265]
	TIME [epoch: 1.39 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.791358341483753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.791358341483753 | validation: 0.8199223314070958]
	TIME [epoch: 1.39 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491894386025092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8491894386025092 | validation: 0.9628823878576732]
	TIME [epoch: 1.39 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7761146032372852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7761146032372852 | validation: 0.8514208193758797]
	TIME [epoch: 1.39 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533050863601958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7533050863601958 | validation: 0.9087492550311576]
	TIME [epoch: 1.39 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7495774987124295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7495774987124295 | validation: 0.8574913952421398]
	TIME [epoch: 1.39 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7574309230330389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7574309230330389 | validation: 0.9047009927045423]
	TIME [epoch: 1.39 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8035576593173775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8035576593173775 | validation: 1.0083047765686524]
	TIME [epoch: 1.39 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8691494800052562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8691494800052562 | validation: 0.8631649321611008]
	TIME [epoch: 1.39 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743721801613096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743721801613096 | validation: 0.9136020927685993]
	TIME [epoch: 1.38 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7361012680239196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7361012680239196 | validation: 0.783210871952432]
	TIME [epoch: 1.38 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350055032898232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7350055032898232 | validation: 0.9633492010355443]
	TIME [epoch: 1.39 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7452189215852876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7452189215852876 | validation: 0.7582032683974572]
	TIME [epoch: 1.39 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_196.pth
	Model improved!!!
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7389348978093006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7389348978093006 | validation: 0.9568576354567266]
	TIME [epoch: 1.39 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7457016619339393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7457016619339393 | validation: 0.7614389208912501]
	TIME [epoch: 1.39 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746730193677735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746730193677735 | validation: 1.0210952934879611]
	TIME [epoch: 1.38 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723707871764108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723707871764108 | validation: 0.7565761632125249]
	TIME [epoch: 1.38 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7222881001773815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7222881001773815 | validation: 0.9007154033413974]
	TIME [epoch: 181 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.752690959498068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.752690959498068 | validation: 0.978589120372755]
	TIME [epoch: 2.77 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.881104457717349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.881104457717349 | validation: 0.8611437570287926]
	TIME [epoch: 2.74 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102314816300661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8102314816300661 | validation: 0.9614182407957736]
	TIME [epoch: 2.73 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445544198927155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445544198927155 | validation: 0.7339436936617162]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_205.pth
	Model improved!!!
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7275937179358823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7275937179358823 | validation: 0.9553819569780183]
	TIME [epoch: 2.74 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690712697865246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690712697865246 | validation: 0.8242149469035278]
	TIME [epoch: 2.74 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.823862218395351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.823862218395351 | validation: 0.798971237781928]
	TIME [epoch: 2.73 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7730318600487559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7730318600487559 | validation: 0.8804869638398887]
	TIME [epoch: 3.69 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7439951733375659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7439951733375659 | validation: 0.9685542676851235]
	TIME [epoch: 2.74 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.772752166367033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.772752166367033 | validation: 0.8382657622193674]
	TIME [epoch: 2.74 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.763111900144463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.763111900144463 | validation: 0.9590455457849152]
	TIME [epoch: 2.74 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771337284446428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771337284446428 | validation: 0.7518888969310419]
	TIME [epoch: 2.74 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7123692878512768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7123692878512768 | validation: 0.8576808448408603]
	TIME [epoch: 2.74 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.703900830036999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.703900830036999 | validation: 0.7914656913995288]
	TIME [epoch: 2.73 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.721081269134923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.721081269134923 | validation: 0.9452165021654486]
	TIME [epoch: 2.74 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7983318876552176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7983318876552176 | validation: 0.8421040498832328]
	TIME [epoch: 2.74 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085605869007176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8085605869007176 | validation: 1.0014703956192588]
	TIME [epoch: 2.73 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7699563598762361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7699563598762361 | validation: 0.7312403242970374]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7284352567493423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7284352567493423 | validation: 0.7761593477470243]
	TIME [epoch: 2.74 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6666645642213374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6666645642213374 | validation: 0.875059790805916]
	TIME [epoch: 2.74 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7110165897531266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7110165897531266 | validation: 0.7925509525073862]
	TIME [epoch: 2.74 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7593359770136451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7593359770136451 | validation: 0.9696253234654173]
	TIME [epoch: 2.75 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090036985360983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090036985360983 | validation: 0.8923149585260113]
	TIME [epoch: 2.74 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160924215723211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160924215723211 | validation: 0.8155349228462697]
	TIME [epoch: 2.74 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6550954115885239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6550954115885239 | validation: 0.6977430747268043]
	TIME [epoch: 2.73 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_226.pth
	Model improved!!!
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6361913023977931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6361913023977931 | validation: 0.8208201288262039]
	TIME [epoch: 2.74 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356641576622883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6356641576622883 | validation: 0.6482335179644272]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677036573790617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.677036573790617 | validation: 0.9687102689975614]
	TIME [epoch: 2.73 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259847075214643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259847075214643 | validation: 0.8004012623953376]
	TIME [epoch: 2.74 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9282708422304049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9282708422304049 | validation: 1.199887481508556]
	TIME [epoch: 2.74 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0519672969586658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0519672969586658 | validation: 1.0720338510477998]
	TIME [epoch: 2.74 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8267579345850864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8267579345850864 | validation: 0.7410323599656193]
	TIME [epoch: 2.74 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6500789730871804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6500789730871804 | validation: 0.8617164754368313]
	TIME [epoch: 2.74 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7128269000651758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7128269000651758 | validation: 0.7849189573160841]
	TIME [epoch: 2.74 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6263202249445994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6263202249445994 | validation: 0.75827398479372]
	TIME [epoch: 2.74 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6337834422262245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6337834422262245 | validation: 0.8351863550372252]
	TIME [epoch: 2.74 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6285017811851047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6285017811851047 | validation: 0.6859677371508348]
	TIME [epoch: 2.74 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.679378394407251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.679378394407251 | validation: 0.9373069153028779]
	TIME [epoch: 2.73 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6663220159999651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6663220159999651 | validation: 0.7039059586200992]
	TIME [epoch: 2.74 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027298964251594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7027298964251594 | validation: 0.8673964560895921]
	TIME [epoch: 2.74 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6503608255408423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6503608255408423 | validation: 0.8405501948858591]
	TIME [epoch: 2.74 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713625682196491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713625682196491 | validation: 0.8087740408901543]
	TIME [epoch: 2.74 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6207815540038434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6207815540038434 | validation: 0.6788198727835941]
	TIME [epoch: 2.74 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5916644147688687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5916644147688687 | validation: 0.893184932020014]
	TIME [epoch: 2.75 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6425113192202744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6425113192202744 | validation: 0.7577246769862861]
	TIME [epoch: 2.74 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082207882527424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082207882527424 | validation: 0.8400289139545436]
	TIME [epoch: 2.74 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5867174387096565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5867174387096565 | validation: 0.6932001189144684]
	TIME [epoch: 2.74 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5625844215127345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5625844215127345 | validation: 0.6961254552430906]
	TIME [epoch: 2.75 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5607567800490517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5607567800490517 | validation: 0.8278705176182529]
	TIME [epoch: 2.74 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6354195799188724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6354195799188724 | validation: 0.815887897819858]
	TIME [epoch: 2.74 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6668258979162457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6668258979162457 | validation: 0.8139828108069133]
	TIME [epoch: 2.74 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6658674560729009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6658674560729009 | validation: 0.890825427329546]
	TIME [epoch: 2.75 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6079887007428566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6079887007428566 | validation: 0.6338850334597463]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6748409210579397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6748409210579397 | validation: 0.8499182442392798]
	TIME [epoch: 2.76 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5700528236459603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5700528236459603 | validation: 0.7240533097756574]
	TIME [epoch: 2.75 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6005416141712727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6005416141712727 | validation: 0.937141598503467]
	TIME [epoch: 2.75 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895362919435783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6895362919435783 | validation: 0.802830940296678]
	TIME [epoch: 2.75 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7290590205746952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7290590205746952 | validation: 0.8713912293560604]
	TIME [epoch: 2.76 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5743797811421872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5743797811421872 | validation: 0.6675315032248419]
	TIME [epoch: 2.74 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5649395463954511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5649395463954511 | validation: 0.7191029395131668]
	TIME [epoch: 2.74 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5393075387584075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5393075387584075 | validation: 0.7467357552576463]
	TIME [epoch: 2.74 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5376551419478517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5376551419478517 | validation: 0.7021704284848083]
	TIME [epoch: 2.75 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5726282461987239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5726282461987239 | validation: 0.9274373457304268]
	TIME [epoch: 2.74 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6675732468860305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6675732468860305 | validation: 0.8098546730084504]
	TIME [epoch: 2.74 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8018151664608183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8018151664608183 | validation: 0.8221135281217514]
	TIME [epoch: 2.74 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5345760566380856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5345760566380856 | validation: 0.6633305000623007]
	TIME [epoch: 2.74 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019974952356373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5019974952356373 | validation: 0.7173998587659312]
	TIME [epoch: 2.74 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5084691519882617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5084691519882617 | validation: 0.7596752948011061]
	TIME [epoch: 2.74 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5527508528878918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5527508528878918 | validation: 0.7526804085037493]
	TIME [epoch: 2.74 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6318317696587171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6318317696587171 | validation: 0.8575099003550148]
	TIME [epoch: 2.74 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5846281895405808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5846281895405808 | validation: 0.6420335789162459]
	TIME [epoch: 2.74 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828349574773879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828349574773879 | validation: 0.9027247371103396]
	TIME [epoch: 2.74 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.601219365614852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.601219365614852 | validation: 0.6021736774340021]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5145541102544375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5145541102544375 | validation: 0.682757848070581]
	TIME [epoch: 2.74 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44144386488407816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44144386488407816 | validation: 0.7315272020303499]
	TIME [epoch: 2.74 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270050578354889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5270050578354889 | validation: 0.9462924485869153]
	TIME [epoch: 2.74 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7176247974975156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7176247974975156 | validation: 0.8069981721830541]
	TIME [epoch: 3.01 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838648024293564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6838648024293564 | validation: 0.8563436308856487]
	TIME [epoch: 2.74 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5455389505956729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5455389505956729 | validation: 0.5971894185423291]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5472849396794746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5472849396794746 | validation: 0.806461419147213]
	TIME [epoch: 2.76 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5266262433469275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5266262433469275 | validation: 0.6326291077106263]
	TIME [epoch: 2.75 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47202847540872306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47202847540872306 | validation: 0.6742983243870067]
	TIME [epoch: 2.75 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47902256389247927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47902256389247927 | validation: 0.8724636463285746]
	TIME [epoch: 2.76 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6116742847790966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6116742847790966 | validation: 0.8645306920390752]
	TIME [epoch: 2.76 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8105167216636838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8105167216636838 | validation: 0.8298529771302349]
	TIME [epoch: 2.75 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4989435324066422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4989435324066422 | validation: 0.6232449313416326]
	TIME [epoch: 2.76 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4789270095239266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4789270095239266 | validation: 0.7067398605425483]
	TIME [epoch: 2.75 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48366362114363565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48366362114363565 | validation: 0.7609464157729677]
	TIME [epoch: 2.75 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5130305018605317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5130305018605317 | validation: 0.7107781477859395]
	TIME [epoch: 2.75 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6180133696954524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6180133696954524 | validation: 0.9166696174455471]
	TIME [epoch: 2.75 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5848718641607127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5848718641607127 | validation: 0.6419593661810821]
	TIME [epoch: 2.75 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6543962682085921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6543962682085921 | validation: 0.7370899476629695]
	TIME [epoch: 2.75 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.465899501736735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.465899501736735 | validation: 0.651629067584131]
	TIME [epoch: 2.75 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.442202614405338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.442202614405338 | validation: 0.7869524120598851]
	TIME [epoch: 2.76 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5611665736539763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5611665736539763 | validation: 0.8785201430240257]
	TIME [epoch: 2.76 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6312538003892472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6312538003892472 | validation: 0.6942200636769327]
	TIME [epoch: 2.76 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42097611465911466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42097611465911466 | validation: 0.5401417775805871]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42633587272611023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42633587272611023 | validation: 0.8125067384420817]
	TIME [epoch: 2.76 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5129494071891249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5129494071891249 | validation: 0.6476763515365294]
	TIME [epoch: 2.76 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6117222095086587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6117222095086587 | validation: 0.7984337487614028]
	TIME [epoch: 2.74 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4718959192512898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4718959192512898 | validation: 0.6010256505114321]
	TIME [epoch: 2.74 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39765652604510393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39765652604510393 | validation: 0.6318217636998671]
	TIME [epoch: 2.74 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4258912950489617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4258912950489617 | validation: 1.0184490171971423]
	TIME [epoch: 2.74 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7227948265058194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7227948265058194 | validation: 0.8854416223759074]
	TIME [epoch: 2.75 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8140396878107344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8140396878107344 | validation: 0.6688505498945576]
	TIME [epoch: 2.75 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43895401820957414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43895401820957414 | validation: 0.8462662098642149]
	TIME [epoch: 2.76 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689930902541243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5689930902541243 | validation: 0.6294025700708016]
	TIME [epoch: 2.76 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5555152882071066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5555152882071066 | validation: 0.7784575404758696]
	TIME [epoch: 2.76 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.480917271421187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.480917271421187 | validation: 0.6456952731384935]
	TIME [epoch: 2.75 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4589770571223932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4589770571223932 | validation: 0.6832500751493953]
	TIME [epoch: 2.75 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4545780727696879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4545780727696879 | validation: 0.7925782788604834]
	TIME [epoch: 2.76 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5221988435423365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5221988435423365 | validation: 0.6206748239761111]
	TIME [epoch: 2.76 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4269201888953819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4269201888953819 | validation: 0.6698707627811731]
	TIME [epoch: 2.75 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40641996805210034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40641996805210034 | validation: 0.5989309667908248]
	TIME [epoch: 2.75 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4517990559268098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4517990559268098 | validation: 0.9344846932629132]
	TIME [epoch: 2.75 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5522852073029985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5522852073029985 | validation: 0.5548278343470802]
	TIME [epoch: 2.75 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4955838139884902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4955838139884902 | validation: 0.7808225094059391]
	TIME [epoch: 2.75 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43777879713593826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43777879713593826 | validation: 0.553784582890926]
	TIME [epoch: 2.76 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45355139444875586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45355139444875586 | validation: 0.9926264815914314]
	TIME [epoch: 2.76 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7053374029174831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7053374029174831 | validation: 0.9162648354364832]
	TIME [epoch: 2.76 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6234148267989662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6234148267989662 | validation: 0.7556677466081383]
	TIME [epoch: 2.75 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42150528838751067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42150528838751067 | validation: 0.5881656136814594]
	TIME [epoch: 2.75 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43890877167491255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43890877167491255 | validation: 0.624615796076888]
	TIME [epoch: 2.75 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3870392478210585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3870392478210585 | validation: 0.6971022776263857]
	TIME [epoch: 2.75 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43344013603088294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43344013603088294 | validation: 0.6333950427070589]
	TIME [epoch: 2.75 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5008874090198325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5008874090198325 | validation: 0.8071473345391542]
	TIME [epoch: 2.76 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5160076191223828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5160076191223828 | validation: 0.6653491716468078]
	TIME [epoch: 2.76 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5536999768194792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5536999768194792 | validation: 0.7398532592520453]
	TIME [epoch: 2.76 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42706965413473075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42706965413473075 | validation: 0.47086048771515615]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_330.pth
	Model improved!!!
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36384020719312204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36384020719312204 | validation: 0.744087840407575]
	TIME [epoch: 2.75 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3878645245116912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3878645245116912 | validation: 0.6674942190132596]
	TIME [epoch: 2.74 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4891004100487446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4891004100487446 | validation: 0.7626197052033155]
	TIME [epoch: 2.74 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5265965076420934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5265965076420934 | validation: 0.6679392409971997]
	TIME [epoch: 2.74 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39716467788777865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39716467788777865 | validation: 0.5066089810772952]
	TIME [epoch: 2.76 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33927028603646575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33927028603646575 | validation: 0.7014998565259047]
	TIME [epoch: 2.75 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4463615461463034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4463615461463034 | validation: 0.7413115111219785]
	TIME [epoch: 2.76 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6376708326875697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6376708326875697 | validation: 0.730861269822661]
	TIME [epoch: 2.76 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4049920841879393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4049920841879393 | validation: 0.4878308187119864]
	TIME [epoch: 2.76 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3147642989934523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3147642989934523 | validation: 0.5387852195008994]
	TIME [epoch: 2.76 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3316769078329111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3316769078329111 | validation: 0.8564599327886878]
	TIME [epoch: 2.76 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49292630571695245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49292630571695245 | validation: 0.7169260515495631]
	TIME [epoch: 2.76 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5279513418681423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5279513418681423 | validation: 0.7024144365468741]
	TIME [epoch: 2.75 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44639004251329406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44639004251329406 | validation: 0.524086852386346]
	TIME [epoch: 2.75 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3521478481517374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3521478481517374 | validation: 0.6295965282919715]
	TIME [epoch: 2.76 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3208045283070799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3208045283070799 | validation: 0.48386727862969076]
	TIME [epoch: 2.76 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34863528335518923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34863528335518923 | validation: 0.8020169875890372]
	TIME [epoch: 2.75 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4289585614009268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4289585614009268 | validation: 0.5061647271711166]
	TIME [epoch: 2.76 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5512908940467057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5512908940467057 | validation: 0.8264630764185068]
	TIME [epoch: 2.76 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5150897509291356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5150897509291356 | validation: 0.8694376536287067]
	TIME [epoch: 2.76 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.534722271476503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.534722271476503 | validation: 0.5791512055715657]
	TIME [epoch: 2.75 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.381599122671591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.381599122671591 | validation: 0.5307023872066478]
	TIME [epoch: 2.76 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3538363246831681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3538363246831681 | validation: 0.5617125616001077]
	TIME [epoch: 2.76 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33272327492582304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33272327492582304 | validation: 0.5636246314736976]
	TIME [epoch: 2.75 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3624926638372283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3624926638372283 | validation: 0.6854486316942885]
	TIME [epoch: 2.75 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45295946960313144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45295946960313144 | validation: 0.632293125141261]
	TIME [epoch: 2.76 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.523539904378506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.523539904378506 | validation: 0.6730905099275775]
	TIME [epoch: 2.75 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3912877217549147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3912877217549147 | validation: 0.4277932150394362]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_358.pth
	Model improved!!!
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33744487988080785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33744487988080785 | validation: 0.6273914432708528]
	TIME [epoch: 2.74 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3136073018469459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3136073018469459 | validation: 0.4209337864108009]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3154677717919423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3154677717919423 | validation: 0.639786303194337]
	TIME [epoch: 2.74 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239260466547661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3239260466547661 | validation: 0.41675895390943674]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_362.pth
	Model improved!!!
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3247621620048608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3247621620048608 | validation: 0.6816419589096065]
	TIME [epoch: 2.74 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3554783995152086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3554783995152086 | validation: 0.5000007928142688]
	TIME [epoch: 2.74 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38160806234381567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38160806234381567 | validation: 0.8110329329566746]
	TIME [epoch: 2.74 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5730662917741005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5730662917741005 | validation: 0.9998622337896583]
	TIME [epoch: 2.75 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.579489772962849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.579489772962849 | validation: 0.5937650703681777]
	TIME [epoch: 2.74 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3602882549959259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3602882549959259 | validation: 0.5648253987170044]
	TIME [epoch: 2.74 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35977247851020594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35977247851020594 | validation: 0.5955452903285013]
	TIME [epoch: 2.74 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3742004392333583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3742004392333583 | validation: 0.6673320003239772]
	TIME [epoch: 2.76 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42226698608526236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42226698608526236 | validation: 0.7242364006682906]
	TIME [epoch: 2.74 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4419242741540281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4419242741540281 | validation: 0.46631094404663326]
	TIME [epoch: 2.75 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3317821350718577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3317821350718577 | validation: 0.6385907198864986]
	TIME [epoch: 2.74 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32624133001560557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32624133001560557 | validation: 0.49070849534934924]
	TIME [epoch: 2.74 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45788901118537695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45788901118537695 | validation: 0.7849239983055081]
	TIME [epoch: 2.74 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4414956267598099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4414956267598099 | validation: 0.40899082998464914]
	TIME [epoch: 2.74 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_376.pth
	Model improved!!!
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848414292608868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2848414292608868 | validation: 0.4504678204324417]
	TIME [epoch: 2.75 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2391640830889581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2391640830889581 | validation: 0.48503378981820183]
	TIME [epoch: 2.75 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26635230489838724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26635230489838724 | validation: 0.503460116635085]
	TIME [epoch: 2.75 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3721807466736382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3721807466736382 | validation: 0.778502837475412]
	TIME [epoch: 2.76 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424587875648108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5424587875648108 | validation: 0.8079049032596471]
	TIME [epoch: 2.75 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4950270754747614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4950270754747614 | validation: 0.4447708860213764]
	TIME [epoch: 2.76 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2829481375230171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2829481375230171 | validation: 0.4122674441196815]
	TIME [epoch: 2.76 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2451566512927666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2451566512927666 | validation: 0.6527461824693368]
	TIME [epoch: 2.76 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33567321168208636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33567321168208636 | validation: 0.6538116843334619]
	TIME [epoch: 2.76 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5356331311683442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5356331311683442 | validation: 0.8245378726564848]
	TIME [epoch: 2.75 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4882467547322104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4882467547322104 | validation: 0.46852388544499707]
	TIME [epoch: 2.76 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3014384325367053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3014384325367053 | validation: 0.5129118705550444]
	TIME [epoch: 2.76 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603903591773517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2603903591773517 | validation: 0.49438889083653115]
	TIME [epoch: 2.75 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36352415154676765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36352415154676765 | validation: 0.6882443606219474]
	TIME [epoch: 2.75 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44446973494597247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44446973494597247 | validation: 0.5705777811936601]
	TIME [epoch: 2.76 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39330972846606216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39330972846606216 | validation: 0.5694881455994564]
	TIME [epoch: 2.76 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32402019121137854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32402019121137854 | validation: 0.4337828990179269]
	TIME [epoch: 2.75 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29593251564458933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29593251564458933 | validation: 0.5327074643522568]
	TIME [epoch: 2.76 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29269403275195643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29269403275195643 | validation: 0.4339902757481111]
	TIME [epoch: 2.76 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3061043831925802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3061043831925802 | validation: 0.5516635897524264]
	TIME [epoch: 2.75 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3261827891599451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3261827891599451 | validation: 0.5586261507922856]
	TIME [epoch: 2.75 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3171087680218071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3171087680218071 | validation: 0.4644908873425683]
	TIME [epoch: 2.76 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2799809345697403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2799809345697403 | validation: 0.7203298793584423]
	TIME [epoch: 2.75 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3221561444562305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3221561444562305 | validation: 0.5246183896509763]
	TIME [epoch: 2.75 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3276442637322626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3276442637322626 | validation: 0.7152027535580575]
	TIME [epoch: 2.74 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3887848036526048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3887848036526048 | validation: 0.518501285674235]
	TIME [epoch: 2.75 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39074334920665976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39074334920665976 | validation: 0.6673620802782015]
	TIME [epoch: 2.75 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.326669085338018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.326669085338018 | validation: 0.39960608701141176]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_404.pth
	Model improved!!!
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3088315189422184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3088315189422184 | validation: 0.670776587982977]
	TIME [epoch: 2.76 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38733238501223627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38733238501223627 | validation: 0.5520948391960633]
	TIME [epoch: 2.76 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33323774968421377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33323774968421377 | validation: 0.5189019591153547]
	TIME [epoch: 2.75 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3091782222256007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3091782222256007 | validation: 0.5440973084846192]
	TIME [epoch: 2.75 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30535354509630674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30535354509630674 | validation: 0.46852156486838015]
	TIME [epoch: 2.75 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.255154649974984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.255154649974984 | validation: 0.48053372501575925]
	TIME [epoch: 2.75 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2305242747335683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2305242747335683 | validation: 0.5025072427728658]
	TIME [epoch: 2.75 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2745619087512984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2745619087512984 | validation: 0.6100338788017704]
	TIME [epoch: 2.75 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3545468182762661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3545468182762661 | validation: 0.48499207462267346]
	TIME [epoch: 2.76 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.261320208902638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.261320208902638 | validation: 0.4576013656631814]
	TIME [epoch: 2.76 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2493415256067519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2493415256067519 | validation: 0.6072724341101645]
	TIME [epoch: 2.75 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35980665313571025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35980665313571025 | validation: 0.4569640516608951]
	TIME [epoch: 2.76 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44228370599148376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44228370599148376 | validation: 0.7626658782710316]
	TIME [epoch: 2.75 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3745531903931735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3745531903931735 | validation: 0.40291175809771884]
	TIME [epoch: 2.75 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24852560341489052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24852560341489052 | validation: 0.6772467077451165]
	TIME [epoch: 2.75 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34148546374475075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34148546374475075 | validation: 0.7349914446136547]
	TIME [epoch: 2.76 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5475634008330407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5475634008330407 | validation: 0.6495936816604232]
	TIME [epoch: 2.75 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40480206224083787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40480206224083787 | validation: 0.6636390252360181]
	TIME [epoch: 2.75 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28446020088219465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28446020088219465 | validation: 0.44422606256856323]
	TIME [epoch: 2.75 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25162253769017856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25162253769017856 | validation: 0.4450496122436751]
	TIME [epoch: 2.75 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28868488930985214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28868488930985214 | validation: 0.46181418507676136]
	TIME [epoch: 2.75 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24509034573994826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24509034573994826 | validation: 0.44813089366992476]
	TIME [epoch: 2.75 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2327272497689532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2327272497689532 | validation: 0.5518763529245042]
	TIME [epoch: 2.75 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3374545769393522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3374545769393522 | validation: 0.5383100386770899]
	TIME [epoch: 2.75 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43724690213659545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43724690213659545 | validation: 0.5521037550871016]
	TIME [epoch: 2.75 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2857738193662332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2857738193662332 | validation: 0.3613769008936387]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20824677305071412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20824677305071412 | validation: 0.44336130806763496]
	TIME [epoch: 2.75 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19729446630182845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19729446630182845 | validation: 0.3807525502552762]
	TIME [epoch: 2.74 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2128240557995341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2128240557995341 | validation: 0.5766552929430789]
	TIME [epoch: 2.74 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3287131731862212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3287131731862212 | validation: 0.6815647832080662]
	TIME [epoch: 2.75 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4055790903756523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4055790903756523 | validation: 0.6166147636251803]
	TIME [epoch: 2.75 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4109268299649858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4109268299649858 | validation: 0.6728493793324657]
	TIME [epoch: 2.75 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3178284734034075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3178284734034075 | validation: 0.4827013406425029]
	TIME [epoch: 2.75 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2824640952426752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2824640952426752 | validation: 0.5296166879628678]
	TIME [epoch: 2.75 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25716845340375427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25716845340375427 | validation: 0.40149073783478606]
	TIME [epoch: 2.75 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29728515854756565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29728515854756565 | validation: 0.6476455259509551]
	TIME [epoch: 2.75 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32965756075171665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32965756075171665 | validation: 0.3616656805639555]
	TIME [epoch: 2.76 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2509639440905383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2509639440905383 | validation: 0.5623351610504566]
	TIME [epoch: 2.75 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23641184028973494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23641184028973494 | validation: 0.4350268490957529]
	TIME [epoch: 2.75 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2638015418005535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2638015418005535 | validation: 0.7047550673687373]
	TIME [epoch: 2.75 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3428050961868121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3428050961868121 | validation: 0.5785303490282607]
	TIME [epoch: 2.75 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36298092821616296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36298092821616296 | validation: 0.5598775677855199]
	TIME [epoch: 2.75 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30059634134658714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30059634134658714 | validation: 0.4561312853292565]
	TIME [epoch: 2.75 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20746392807315958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20746392807315958 | validation: 0.4030080935108877]
	TIME [epoch: 2.76 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20532944046454799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20532944046454799 | validation: 0.5647479567953934]
	TIME [epoch: 2.75 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3211292562776076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3211292562776076 | validation: 0.5241363632603177]
	TIME [epoch: 2.75 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.377529107612329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.377529107612329 | validation: 0.5022701841944647]
	TIME [epoch: 2.75 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564947617387206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2564947617387206 | validation: 0.41145843060904097]
	TIME [epoch: 2.75 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2035991183396606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2035991183396606 | validation: 0.4382839412548034]
	TIME [epoch: 2.75 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2097271267286822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2097271267286822 | validation: 0.5833272361569684]
	TIME [epoch: 2.75 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25676560385738223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25676560385738223 | validation: 0.5909195276736146]
	TIME [epoch: 2.75 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3611546475509684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3611546475509684 | validation: 0.7451855039697334]
	TIME [epoch: 2.76 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35072253100835155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35072253100835155 | validation: 0.3977580443891633]
	TIME [epoch: 2.75 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2425455183313266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2425455183313266 | validation: 0.6236551731478731]
	TIME [epoch: 2.75 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3484556535797178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3484556535797178 | validation: 0.4869763429485738]
	TIME [epoch: 2.75 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36574962784891846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36574962784891846 | validation: 0.4677667257625676]
	TIME [epoch: 2.76 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22858272452589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22858272452589 | validation: 0.40794508447785705]
	TIME [epoch: 2.75 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17204352553152444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17204352553152444 | validation: 0.3570852751962485]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_462.pth
	Model improved!!!
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19150847734227466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19150847734227466 | validation: 0.7019763764037901]
	TIME [epoch: 2.75 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28299761973879256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28299761973879256 | validation: 0.6115770730081534]
	TIME [epoch: 2.74 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4854221043613723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4854221043613723 | validation: 0.689290286427892]
	TIME [epoch: 2.74 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36497231887786974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36497231887786974 | validation: 0.5615152538606827]
	TIME [epoch: 2.75 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.256450821910675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.256450821910675 | validation: 0.36178574699753097]
	TIME [epoch: 2.75 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18889788172361616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18889788172361616 | validation: 0.5218267133240652]
	TIME [epoch: 2.75 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25155526116158533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25155526116158533 | validation: 0.5487834567468689]
	TIME [epoch: 2.74 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3436900732602752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3436900732602752 | validation: 0.5655925612079964]
	TIME [epoch: 2.75 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3275607202191048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3275607202191048 | validation: 0.5408102398941825]
	TIME [epoch: 2.75 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25057894143735093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25057894143735093 | validation: 0.41435834637212915]
	TIME [epoch: 2.74 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20750788490350452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20750788490350452 | validation: 0.46670111020690214]
	TIME [epoch: 2.75 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1744564758775906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1744564758775906 | validation: 0.3631876795417224]
	TIME [epoch: 2.74 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1889608458443947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1889608458443947 | validation: 0.6347154141771485]
	TIME [epoch: 2.74 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22869126904874276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22869126904874276 | validation: 0.5624446308224276]
	TIME [epoch: 2.74 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4106731657867476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4106731657867476 | validation: 0.709542382338841]
	TIME [epoch: 2.75 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32875764098038124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32875764098038124 | validation: 0.4670719150548302]
	TIME [epoch: 2.74 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21784952018774653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21784952018774653 | validation: 0.4087601745033683]
	TIME [epoch: 2.74 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15778623392352273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15778623392352273 | validation: 0.4089235940695135]
	TIME [epoch: 2.74 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1921420205498798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1921420205498798 | validation: 0.5722231361271277]
	TIME [epoch: 2.75 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35326193409214257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35326193409214257 | validation: 0.5624659497716895]
	TIME [epoch: 2.75 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31156898514455383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31156898514455383 | validation: 0.45461794773748676]
	TIME [epoch: 2.74 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26520206845240735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26520206845240735 | validation: 0.4633166432194251]
	TIME [epoch: 2.75 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21998120214659203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21998120214659203 | validation: 0.4406929486334199]
	TIME [epoch: 2.75 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2314099116594142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2314099116594142 | validation: 0.5408111828404754]
	TIME [epoch: 2.74 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32093823123121135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32093823123121135 | validation: 0.638898330625909]
	TIME [epoch: 2.75 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26748510875470644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26748510875470644 | validation: 0.38806029555863986]
	TIME [epoch: 2.75 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24636342568555986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24636342568555986 | validation: 0.7428465706074863]
	TIME [epoch: 2.75 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3127291050674004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3127291050674004 | validation: 0.4748712626132209]
	TIME [epoch: 2.75 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31822962854157105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31822962854157105 | validation: 0.5275007965164392]
	TIME [epoch: 2.75 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20173974947711074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20173974947711074 | validation: 0.43012840113601536]
	TIME [epoch: 2.75 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18346214325478108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18346214325478108 | validation: 0.42990490411978366]
	TIME [epoch: 2.75 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21229893348944992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21229893348944992 | validation: 0.4682021123041633]
	TIME [epoch: 2.74 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24000125887286644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24000125887286644 | validation: 0.5209444531124948]
	TIME [epoch: 2.75 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26160069448678436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26160069448678436 | validation: 0.44915757516012533]
	TIME [epoch: 2.75 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24128969552971588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24128969552971588 | validation: 0.4559639187265678]
	TIME [epoch: 2.74 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23406456380122037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23406456380122037 | validation: 0.47529529707993473]
	TIME [epoch: 2.74 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2311928207222031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2311928207222031 | validation: 0.4126648849332353]
	TIME [epoch: 2.75 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2128934277904265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2128934277904265 | validation: 0.43777408736362367]
	TIME [epoch: 2.75 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19988979340434043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19988979340434043 | validation: 0.4628678252516718]
	TIME [epoch: 189 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20050976593516803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20050976593516803 | validation: 0.41290621476093536]
	TIME [epoch: 5.88 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18396425976962036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18396425976962036 | validation: 0.553569475500593]
	TIME [epoch: 5.87 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20215801897066957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20215801897066957 | validation: 0.49972844176442105]
	TIME [epoch: 5.87 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30911723849250117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30911723849250117 | validation: 0.958488981128091]
	TIME [epoch: 5.87 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4876677342150799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4876677342150799 | validation: 0.4736044808834273]
	TIME [epoch: 5.87 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35069742364886813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35069742364886813 | validation: 0.41298656122813276]
	TIME [epoch: 5.86 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19610949133489478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19610949133489478 | validation: 0.5428886733145507]
	TIME [epoch: 5.86 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26627382152978735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26627382152978735 | validation: 0.5248315871950093]
	TIME [epoch: 5.86 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28476276806506273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28476276806506273 | validation: 0.42080160581434306]
	TIME [epoch: 5.87 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2003397140573098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2003397140573098 | validation: 0.4789430339370595]
	TIME [epoch: 5.88 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18795081744913103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18795081744913103 | validation: 0.31467553474697074]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_512.pth
	Model improved!!!
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15846780293786594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15846780293786594 | validation: 0.3501187073225258]
	TIME [epoch: 5.87 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14722911975049757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14722911975049757 | validation: 0.39052062387137576]
	TIME [epoch: 5.86 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20922031194059784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20922031194059784 | validation: 0.547496936235527]
	TIME [epoch: 5.88 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33894738414389947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33894738414389947 | validation: 0.3960182009925685]
	TIME [epoch: 5.87 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3755731667521819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3755731667521819 | validation: 0.4687176815554726]
	TIME [epoch: 5.88 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23104058514948494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23104058514948494 | validation: 0.46789672636564783]
	TIME [epoch: 5.88 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17661966110606878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17661966110606878 | validation: 0.42748986461554345]
	TIME [epoch: 5.89 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2812477041526215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2812477041526215 | validation: 0.8082918894737455]
	TIME [epoch: 5.88 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3558886750917335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3558886750917335 | validation: 0.43023939967282354]
	TIME [epoch: 5.89 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728907172555035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2728907172555035 | validation: 0.4105451682676636]
	TIME [epoch: 5.88 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14489284788028475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14489284788028475 | validation: 0.38303692016990465]
	TIME [epoch: 5.89 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13753134953218374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13753134953218374 | validation: 0.3589107978288752]
	TIME [epoch: 5.87 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1522773072981663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1522773072981663 | validation: 0.47384598600483047]
	TIME [epoch: 5.89 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19522659068339313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19522659068339313 | validation: 0.496818420993886]
	TIME [epoch: 5.88 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2751800726809432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2751800726809432 | validation: 0.5293484463357657]
	TIME [epoch: 5.88 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2988967100898035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2988967100898035 | validation: 0.44015484316084064]
	TIME [epoch: 5.88 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16055313989253134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16055313989253134 | validation: 0.32280685062993436]
	TIME [epoch: 5.89 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12754475549276123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12754475549276123 | validation: 0.4409810728001533]
	TIME [epoch: 5.88 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.157133234869815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.157133234869815 | validation: 0.40693027117269376]
	TIME [epoch: 5.89 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2200819928178401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2200819928178401 | validation: 0.6961459890411558]
	TIME [epoch: 5.88 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3126031061784071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3126031061784071 | validation: 0.4738372341562583]
	TIME [epoch: 5.89 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36689272510474075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36689272510474075 | validation: 0.5987705510550371]
	TIME [epoch: 5.88 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25115259547763435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25115259547763435 | validation: 0.2987062468879761]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_535.pth
	Model improved!!!
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239414598460508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3239414598460508 | validation: 0.43263948672735036]
	TIME [epoch: 5.89 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19021215013901902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19021215013901902 | validation: 0.3638180341249686]
	TIME [epoch: 5.89 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15276194394617443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15276194394617443 | validation: 0.3422835927578971]
	TIME [epoch: 5.89 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17922217850132022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17922217850132022 | validation: 0.567868299051233]
	TIME [epoch: 5.89 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19814080532845432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19814080532845432 | validation: 0.47128867306061634]
	TIME [epoch: 5.89 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26548728942111394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26548728942111394 | validation: 0.5416115726459165]
	TIME [epoch: 5.9 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19995678053740024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19995678053740024 | validation: 0.4261694636706578]
	TIME [epoch: 5.89 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23874871058641808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23874871058641808 | validation: 0.33564945402652424]
	TIME [epoch: 5.9 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19900296606507012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19900296606507012 | validation: 0.4181317474442068]
	TIME [epoch: 5.88 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20001796022966745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20001796022966745 | validation: 0.44040066607351325]
	TIME [epoch: 5.9 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3015421066703396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3015421066703396 | validation: 0.4690427601104936]
	TIME [epoch: 5.89 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23860787551108673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23860787551108673 | validation: 0.32473970682263]
	TIME [epoch: 5.89 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17160667546112934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17160667546112934 | validation: 0.38017702017704685]
	TIME [epoch: 5.88 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13963989977262697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13963989977262697 | validation: 0.3191344806293865]
	TIME [epoch: 5.88 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1395373935618341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1395373935618341 | validation: 0.3673649625545804]
	TIME [epoch: 5.88 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16398238736409085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16398238736409085 | validation: 0.554532043822121]
	TIME [epoch: 5.88 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2333664384932593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2333664384932593 | validation: 0.7426033022824821]
	TIME [epoch: 5.88 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5251199870360729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5251199870360729 | validation: 0.7680895002405439]
	TIME [epoch: 5.87 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2972450829750389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2972450829750389 | validation: 0.3472737890054969]
	TIME [epoch: 5.88 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4101332782199245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4101332782199245 | validation: 0.565857280973299]
	TIME [epoch: 5.88 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2580532033344364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2580532033344364 | validation: 0.47591496694128577]
	TIME [epoch: 5.87 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18604068796362505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18604068796362505 | validation: 0.314293764307652]
	TIME [epoch: 5.88 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16504285043170627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16504285043170627 | validation: 0.4124423450237975]
	TIME [epoch: 5.87 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1467415082488305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1467415082488305 | validation: 0.3444796837622436]
	TIME [epoch: 5.86 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1549802192912542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1549802192912542 | validation: 0.3755655520072041]
	TIME [epoch: 5.87 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.138276976181054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.138276976181054 | validation: 0.34228873782097363]
	TIME [epoch: 5.87 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13158549848165751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13158549848165751 | validation: 0.34351856146226095]
	TIME [epoch: 5.88 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1332298276530947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1332298276530947 | validation: 0.4365402905044379]
	TIME [epoch: 5.88 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1680302295276846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1680302295276846 | validation: 0.44920468649621625]
	TIME [epoch: 5.88 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24546384941837535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24546384941837535 | validation: 0.47571104615794707]
	TIME [epoch: 5.88 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2484414111199286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2484414111199286 | validation: 0.35161295942627335]
	TIME [epoch: 5.88 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15725049422397003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15725049422397003 | validation: 0.3765419159581092]
	TIME [epoch: 5.88 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13196538333354138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13196538333354138 | validation: 0.36396701098502704]
	TIME [epoch: 5.89 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1771672216956854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1771672216956854 | validation: 0.6588840949878316]
	TIME [epoch: 5.88 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2516413565518922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2516413565518922 | validation: 0.4594751493756719]
	TIME [epoch: 5.88 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34309816162950596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34309816162950596 | validation: 0.6133253796172494]
	TIME [epoch: 5.87 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22848152360065974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22848152360065974 | validation: 0.3312896999820863]
	TIME [epoch: 5.88 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48748908391301654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48748908391301654 | validation: 0.4076243383855895]
	TIME [epoch: 5.88 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19182819491741276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19182819491741276 | validation: 0.5079793944854408]
	TIME [epoch: 5.88 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17984748089660005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17984748089660005 | validation: 0.3402885934635199]
	TIME [epoch: 5.88 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1867891627649729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1867891627649729 | validation: 0.43937907235724644]
	TIME [epoch: 5.89 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14954926429952128		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14954926429952128 | validation: 0.31901990841484895]
	TIME [epoch: 5.88 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14289503475080065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14289503475080065 | validation: 0.33412137677607573]
	TIME [epoch: 5.89 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1336378758829381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1336378758829381 | validation: 0.34651435285146137]
	TIME [epoch: 5.88 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1581362562147896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1581362562147896 | validation: 0.4073319883407326]
	TIME [epoch: 5.88 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19513757468115572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19513757468115572 | validation: 0.3822115028253606]
	TIME [epoch: 5.87 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18038961374210913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18038961374210913 | validation: 0.36279888979732516]
	TIME [epoch: 5.88 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13686362070004662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13686362070004662 | validation: 0.3138804240450657]
	TIME [epoch: 5.88 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293530421296551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293530421296551 | validation: 0.45521825827058304]
	TIME [epoch: 5.88 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1600691524595439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1600691524595439 | validation: 0.4844088995521498]
	TIME [epoch: 5.87 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.295834287686474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.295834287686474 | validation: 0.5483099714387183]
	TIME [epoch: 5.89 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18627146219705595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18627146219705595 | validation: 0.40038765708479035]
	TIME [epoch: 5.88 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32002188157520634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32002188157520634 | validation: 0.9278412862738844]
	TIME [epoch: 5.88 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5302031448198239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5302031448198239 | validation: 0.39335781786271906]
	TIME [epoch: 5.87 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20982848039232094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20982848039232094 | validation: 0.3486375541406562]
	TIME [epoch: 5.87 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18328350781253405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18328350781253405 | validation: 0.4392034808248224]
	TIME [epoch: 5.87 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17129122505606376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17129122505606376 | validation: 0.31391767636717915]
	TIME [epoch: 5.87 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1272752601886913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1272752601886913 | validation: 0.3020454440290723]
	TIME [epoch: 5.88 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11419233873540055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11419233873540055 | validation: 0.3733046839588692]
	TIME [epoch: 5.87 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11739539842188357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11739539842188357 | validation: 0.32024721914710913]
	TIME [epoch: 5.87 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1379422804955108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1379422804955108 | validation: 0.5208322195905222]
	TIME [epoch: 5.88 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16194629568706018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16194629568706018 | validation: 0.3988072463057233]
	TIME [epoch: 5.87 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19325027958612978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19325027958612978 | validation: 0.6249696310723962]
	TIME [epoch: 5.87 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19970862305045017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19970862305045017 | validation: 0.40807807145680935]
	TIME [epoch: 5.88 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2318756565162566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2318756565162566 | validation: 0.43625065003721225]
	TIME [epoch: 5.87 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1364091268904096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1364091268904096 | validation: 0.3757876321081379]
	TIME [epoch: 5.89 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23463936157701615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23463936157701615 | validation: 0.5661526896962812]
	TIME [epoch: 5.88 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339736160071593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.339736160071593 | validation: 0.30386303402229686]
	TIME [epoch: 5.89 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16177301571369582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16177301571369582 | validation: 0.2992331271661827]
	TIME [epoch: 5.88 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206975399041005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206975399041005 | validation: 0.3982283733214149]
	TIME [epoch: 5.88 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14500847877486686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14500847877486686 | validation: 0.3449289049962798]
	TIME [epoch: 5.89 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18171841201201439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18171841201201439 | validation: 0.4799155025193949]
	TIME [epoch: 5.88 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800553225039765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1800553225039765 | validation: 0.2828469491684112]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_608.pth
	Model improved!!!
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11927315364393344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11927315364393344 | validation: 0.3795289763187967]
	TIME [epoch: 5.87 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11127076530933763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11127076530933763 | validation: 0.33065851666244694]
	TIME [epoch: 5.88 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17680715240803252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17680715240803252 | validation: 0.73322964455784]
	TIME [epoch: 5.89 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2889438187826156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2889438187826156 | validation: 0.40881716351428043]
	TIME [epoch: 5.88 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33060996431771067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33060996431771067 | validation: 0.44045171688152873]
	TIME [epoch: 5.89 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22919422501335918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22919422501335918 | validation: 0.6082776837648449]
	TIME [epoch: 5.87 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29419024361049473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29419024361049473 | validation: 0.2970379127566226]
	TIME [epoch: 5.88 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13349363544355458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13349363544355458 | validation: 0.35866443170391443]
	TIME [epoch: 5.88 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14914576877297664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14914576877297664 | validation: 0.30928573031597506]
	TIME [epoch: 5.88 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15635162795761548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15635162795761548 | validation: 0.3956485672102624]
	TIME [epoch: 5.88 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12578213775704097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12578213775704097 | validation: 0.3181285431749719]
	TIME [epoch: 5.88 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13924407680898807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13924407680898807 | validation: 0.3012470211807801]
	TIME [epoch: 5.87 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17238877631840346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17238877631840346 | validation: 0.3699603154350825]
	TIME [epoch: 5.88 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19195008333554306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19195008333554306 | validation: 0.3035916065634734]
	TIME [epoch: 5.88 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.135594901165255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.135594901165255 | validation: 0.2940453797439297]
	TIME [epoch: 5.88 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11831055090611858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11831055090611858 | validation: 0.32775531143091574]
	TIME [epoch: 5.88 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1204935296370763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1204935296370763 | validation: 0.340591922233295]
	TIME [epoch: 5.88 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685807980212359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685807980212359 | validation: 0.5712599458011937]
	TIME [epoch: 5.89 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2193431714312073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2193431714312073 | validation: 0.2901002422617521]
	TIME [epoch: 5.88 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1361345007284363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1361345007284363 | validation: 0.5837652432295525]
	TIME [epoch: 5.89 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18210839571530968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18210839571530968 | validation: 0.43220046702546755]
	TIME [epoch: 5.88 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.394844745112759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.394844745112759 | validation: 0.6508568084643974]
	TIME [epoch: 5.88 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31378578767698834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31378578767698834 | validation: 0.2959523107545294]
	TIME [epoch: 5.88 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11329476360291972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11329476360291972 | validation: 0.24854059511940285]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_632.pth
	Model improved!!!
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11868290693886867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11868290693886867 | validation: 0.4099923018652173]
	TIME [epoch: 5.87 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13243532608357847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13243532608357847 | validation: 0.26706511841812475]
	TIME [epoch: 5.88 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1113199897680871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1113199897680871 | validation: 0.3803535228757867]
	TIME [epoch: 5.87 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11324797149571614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11324797149571614 | validation: 0.3379684372274807]
	TIME [epoch: 5.88 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1569768798069575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1569768798069575 | validation: 0.4547632293583302]
	TIME [epoch: 5.88 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21913448169649008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21913448169649008 | validation: 0.32797807513084276]
	TIME [epoch: 5.88 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15690360228108088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15690360228108088 | validation: 0.38138338456815923]
	TIME [epoch: 5.87 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12594187777594495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12594187777594495 | validation: 0.33219834582471847]
	TIME [epoch: 5.87 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14883464858186693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14883464858186693 | validation: 0.3384930025110934]
	TIME [epoch: 5.88 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13006524214099519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13006524214099519 | validation: 0.30782023421847343]
	TIME [epoch: 5.88 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304439581401353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1304439581401353 | validation: 0.41636916556499204]
	TIME [epoch: 5.89 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1427614409103094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1427614409103094 | validation: 0.4354572192085524]
	TIME [epoch: 5.88 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2574099528862621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2574099528862621 | validation: 0.3464371643982135]
	TIME [epoch: 5.88 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2699600645838264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2699600645838264 | validation: 0.6168535313938387]
	TIME [epoch: 5.89 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21279810964877283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21279810964877283 | validation: 0.38189798605675984]
	TIME [epoch: 5.88 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21409274003266518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21409274003266518 | validation: 0.5781177218802361]
	TIME [epoch: 5.89 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1755117408135009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1755117408135009 | validation: 0.24907473360542026]
	TIME [epoch: 5.88 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145502323626287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.145502323626287 | validation: 0.5823387403426317]
	TIME [epoch: 5.88 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25571779425395486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25571779425395486 | validation: 0.22894227417301508]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_651.pth
	Model improved!!!
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13128985333246398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13128985333246398 | validation: 0.3141378995855079]
	TIME [epoch: 5.88 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11099837445712879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11099837445712879 | validation: 0.2628805433316699]
	TIME [epoch: 5.88 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10320586510182762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10320586510182762 | validation: 0.31261562320171177]
	TIME [epoch: 5.88 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11032595790676859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11032595790676859 | validation: 0.2725303729140393]
	TIME [epoch: 5.87 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10921431628717669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10921431628717669 | validation: 0.456079751410684]
	TIME [epoch: 5.87 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1348496334774923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1348496334774923 | validation: 0.41070695270254803]
	TIME [epoch: 5.88 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2249672326732021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2249672326732021 | validation: 0.5953667945319079]
	TIME [epoch: 5.89 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1725773203321538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1725773203321538 | validation: 0.2688729528247471]
	TIME [epoch: 5.88 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12694064744296546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12694064744296546 | validation: 0.36781596215603346]
	TIME [epoch: 5.88 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1432340159715477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1432340159715477 | validation: 0.2610628846157077]
	TIME [epoch: 5.88 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1785986706083917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1785986706083917 | validation: 0.46171521352038813]
	TIME [epoch: 5.88 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22595546693333923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22595546693333923 | validation: 0.31500445866627547]
	TIME [epoch: 5.88 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2449421661106608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2449421661106608 | validation: 0.30121665037401085]
	TIME [epoch: 5.89 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14877989692926882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14877989692926882 | validation: 0.38530781317359647]
	TIME [epoch: 5.89 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1084784201129343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1084784201129343 | validation: 0.26023181840876497]
	TIME [epoch: 5.88 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1380721102896103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1380721102896103 | validation: 0.5369872706805786]
	TIME [epoch: 5.89 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1706291582313326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1706291582313326 | validation: 0.37664781870070235]
	TIME [epoch: 5.89 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23098150068675685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23098150068675685 | validation: 0.5086341547021546]
	TIME [epoch: 5.88 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12880618374800887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12880618374800887 | validation: 0.3064974529744098]
	TIME [epoch: 5.89 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11274503737762231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11274503737762231 | validation: 0.2937192659802364]
	TIME [epoch: 5.87 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10554929828785227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10554929828785227 | validation: 0.25040782945519363]
	TIME [epoch: 5.87 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10667239367416194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10667239367416194 | validation: 0.42488608029875813]
	TIME [epoch: 5.88 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1261516471153877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1261516471153877 | validation: 0.2823861970361036]
	TIME [epoch: 5.87 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14244007999771818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14244007999771818 | validation: 0.5048455153585166]
	TIME [epoch: 5.88 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14409450306909904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14409450306909904 | validation: 0.2823663772156489]
	TIME [epoch: 5.87 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14782283427998366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14782283427998366 | validation: 0.5123883290777176]
	TIME [epoch: 5.87 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1817840177631923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1817840177631923 | validation: 0.23868970591661212]
	TIME [epoch: 5.88 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18695767275719122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18695767275719122 | validation: 0.3500472526203526]
	TIME [epoch: 5.88 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773269348026199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1773269348026199 | validation: 0.43891108660675465]
	TIME [epoch: 5.87 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17059993304507493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17059993304507493 | validation: 0.3198715403444985]
	TIME [epoch: 5.87 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14888601028113957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14888601028113957 | validation: 0.3967790504689741]
	TIME [epoch: 5.88 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1531780203095035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1531780203095035 | validation: 0.3378971203898913]
	TIME [epoch: 5.88 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17263476258280216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17263476258280216 | validation: 0.2808780212772881]
	TIME [epoch: 5.88 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08754339828809438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08754339828809438 | validation: 0.2314224452641506]
	TIME [epoch: 5.88 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11159753619200136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11159753619200136 | validation: 0.37243652354513296]
	TIME [epoch: 5.87 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13198725978380027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13198725978380027 | validation: 0.29864282311932655]
	TIME [epoch: 5.88 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14372480877576038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14372480877576038 | validation: 0.6646244730192513]
	TIME [epoch: 5.88 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2139982021431049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2139982021431049 | validation: 0.46299293227256344]
	TIME [epoch: 5.88 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2859544085649048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2859544085649048 | validation: 0.4630219839681476]
	TIME [epoch: 5.87 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1443216979806601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1443216979806601 | validation: 0.3202935804175635]
	TIME [epoch: 5.88 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1024880245124856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1024880245124856 | validation: 0.2629843171209626]
	TIME [epoch: 5.88 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08959581017347175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08959581017347175 | validation: 0.32371443360824104]
	TIME [epoch: 5.88 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1189283120112822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1189283120112822 | validation: 0.31102959598279595]
	TIME [epoch: 5.88 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13404378387609409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13404378387609409 | validation: 0.33585235598978547]
	TIME [epoch: 5.87 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16659185287673325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16659185287673325 | validation: 0.3838665638767087]
	TIME [epoch: 5.87 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19724727821665164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19724727821665164 | validation: 0.24223788407509514]
	TIME [epoch: 5.88 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1203363372696088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1203363372696088 | validation: 0.272914691140315]
	TIME [epoch: 5.87 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11753517629049957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11753517629049957 | validation: 0.2949944435475739]
	TIME [epoch: 5.88 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11416352273970004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11416352273970004 | validation: 0.26904843784177157]
	TIME [epoch: 5.88 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11577542458788492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11577542458788492 | validation: 0.44009620156659884]
	TIME [epoch: 5.89 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12931129613454856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12931129613454856 | validation: 0.3283524301350783]
	TIME [epoch: 5.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1625952156603507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1625952156603507 | validation: 0.5952860809149165]
	TIME [epoch: 5.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18299380954592387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18299380954592387 | validation: 0.2528637106336161]
	TIME [epoch: 5.91 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13268181776417512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13268181776417512 | validation: 0.43769450051359426]
	TIME [epoch: 5.9 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2001941318832771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2001941318832771 | validation: 0.2557735389091316]
	TIME [epoch: 5.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15395220043212066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15395220043212066 | validation: 0.4877435326874146]
	TIME [epoch: 5.89 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12114566232035318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12114566232035318 | validation: 0.24271617344191918]
	TIME [epoch: 5.89 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08774240666644602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08774240666644602 | validation: 0.2993865995029145]
	TIME [epoch: 5.9 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09419235108253454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09419235108253454 | validation: 0.277773133769409]
	TIME [epoch: 5.89 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13875214775817973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13875214775817973 | validation: 0.31953635252580476]
	TIME [epoch: 5.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12894321204497744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12894321204497744 | validation: 0.252332564873845]
	TIME [epoch: 5.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10169211537336388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10169211537336388 | validation: 0.28624957885014013]
	TIME [epoch: 5.89 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08681253829734008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08681253829734008 | validation: 0.32005000244758397]
	TIME [epoch: 5.89 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16401398016370844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16401398016370844 | validation: 0.5441335570123311]
	TIME [epoch: 5.89 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3423752750687627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3423752750687627 | validation: 0.3431049386955387]
	TIME [epoch: 5.88 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972648009743579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972648009743579 | validation: 0.21847591708894112]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_717.pth
	Model improved!!!
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08087366060981722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08087366060981722 | validation: 0.31904590049843373]
	TIME [epoch: 5.87 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006179069386748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1006179069386748 | validation: 0.2611749497935414]
	TIME [epoch: 5.89 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11013933804190423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11013933804190423 | validation: 0.42034223972858237]
	TIME [epoch: 5.88 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11456590038510729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11456590038510729 | validation: 0.33061883815820226]
	TIME [epoch: 5.88 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.165385880331496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.165385880331496 | validation: 0.7318882041696857]
	TIME [epoch: 5.88 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28751640018825814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28751640018825814 | validation: 0.2695560046376778]
	TIME [epoch: 5.88 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19574525906520757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19574525906520757 | validation: 0.31909180407013854]
	TIME [epoch: 5.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18079041680518196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18079041680518196 | validation: 0.26844830439069894]
	TIME [epoch: 5.89 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09627109428621601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09627109428621601 | validation: 0.225321486757922]
	TIME [epoch: 5.88 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10344903623788607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10344903623788607 | validation: 0.3189447221534537]
	TIME [epoch: 5.89 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09239739812541743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09239739812541743 | validation: 0.20247189104920357]
	TIME [epoch: 5.89 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_728.pth
	Model improved!!!
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0971691506135942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0971691506135942 | validation: 0.4071570434796467]
	TIME [epoch: 5.89 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09849495589094104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09849495589094104 | validation: 0.20884817295721284]
	TIME [epoch: 5.88 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09243454500712579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09243454500712579 | validation: 0.32138238254214]
	TIME [epoch: 5.87 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09549530106033678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09549530106033678 | validation: 0.22040712569091575]
	TIME [epoch: 5.88 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10573478508987366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10573478508987366 | validation: 0.4703134373069651]
	TIME [epoch: 5.88 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13623793928046957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13623793928046957 | validation: 0.28863331707893103]
	TIME [epoch: 5.89 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17353160196558892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17353160196558892 | validation: 0.4710532898606616]
	TIME [epoch: 5.89 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13492378726142193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13492378726142193 | validation: 0.2502557469876585]
	TIME [epoch: 5.88 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09347657061427668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09347657061427668 | validation: 0.29020667724316146]
	TIME [epoch: 5.89 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327953074924407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1327953074924407 | validation: 0.4602058448436214]
	TIME [epoch: 5.89 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3159237318220885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3159237318220885 | validation: 0.35245241977479547]
	TIME [epoch: 5.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1253795690358873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1253795690358873 | validation: 0.2142847772268505]
	TIME [epoch: 5.9 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938368582876528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06938368582876528 | validation: 0.2533078044678236]
	TIME [epoch: 5.9 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07615386752405624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07615386752405624 | validation: 0.1825914404005073]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_742.pth
	Model improved!!!
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10056285949358099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10056285949358099 | validation: 0.7335077635549382]
	TIME [epoch: 5.88 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26342559975456636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26342559975456636 | validation: 0.33122277207379464]
	TIME [epoch: 5.89 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3046292003025483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3046292003025483 | validation: 0.28019358161575114]
	TIME [epoch: 5.88 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17355136215243036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17355136215243036 | validation: 0.6102089087526903]
	TIME [epoch: 5.88 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19461808982975792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19461808982975792 | validation: 0.21714514461752038]
	TIME [epoch: 5.88 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1250389673164357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1250389673164357 | validation: 0.20956619939387805]
	TIME [epoch: 5.88 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07723804338292488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07723804338292488 | validation: 0.3120680241002134]
	TIME [epoch: 5.89 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0949594564578029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949594564578029 | validation: 0.22574971520729697]
	TIME [epoch: 5.88 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08040431796538487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08040431796538487 | validation: 0.21557410959068246]
	TIME [epoch: 5.88 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07627483579074257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07627483579074257 | validation: 0.30974557812164794]
	TIME [epoch: 5.88 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15205155454637528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15205155454637528 | validation: 0.577495786202569]
	TIME [epoch: 5.87 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39927497600136946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39927497600136946 | validation: 0.4905925481767519]
	TIME [epoch: 5.88 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14182583506372412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14182583506372412 | validation: 0.23600333525735195]
	TIME [epoch: 5.87 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13599116151309312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13599116151309312 | validation: 0.23912555552012946]
	TIME [epoch: 5.88 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10597957609359834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10597957609359834 | validation: 0.5469183074700306]
	TIME [epoch: 5.88 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16124527624062887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16124527624062887 | validation: 0.24378320556609015]
	TIME [epoch: 5.88 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13804544060906176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13804544060906176 | validation: 0.28813468847471146]
	TIME [epoch: 5.89 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12448798057184547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12448798057184547 | validation: 0.2625430242593656]
	TIME [epoch: 5.89 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176078780441535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176078780441535 | validation: 0.23613485201625106]
	TIME [epoch: 5.88 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08985095912730114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08985095912730114 | validation: 0.31090929145707835]
	TIME [epoch: 5.88 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06567474036680268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06567474036680268 | validation: 0.213175098990288]
	TIME [epoch: 5.88 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905451059737481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07905451059737481 | validation: 0.4051414920867862]
	TIME [epoch: 5.88 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10040688874152473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10040688874152473 | validation: 0.2981415598945584]
	TIME [epoch: 5.88 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500020761075322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1500020761075322 | validation: 0.5766771238919646]
	TIME [epoch: 5.87 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16216375829502577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16216375829502577 | validation: 0.23278164891393047]
	TIME [epoch: 5.87 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10291289844585098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10291289844585098 | validation: 0.321284982923011]
	TIME [epoch: 5.87 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09015377370542357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09015377370542357 | validation: 0.20372882266244885]
	TIME [epoch: 5.87 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08300996034537718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08300996034537718 | validation: 0.2386157950013444]
	TIME [epoch: 5.87 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306920042742993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07306920042742993 | validation: 0.1942079663508829]
	TIME [epoch: 5.87 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543855935356894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07543855935356894 | validation: 0.21166298462014704]
	TIME [epoch: 5.87 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012573815931569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08012573815931569 | validation: 0.28611191814392944]
	TIME [epoch: 5.88 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10959914305807791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10959914305807791 | validation: 0.2770611623225859]
	TIME [epoch: 5.88 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477851249932554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1477851249932554 | validation: 0.3989590367929996]
	TIME [epoch: 5.89 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41699067877138596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41699067877138596 | validation: 0.5281116652163652]
	TIME [epoch: 5.87 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16539087613770945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16539087613770945 | validation: 0.37519814434952975]
	TIME [epoch: 5.87 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23213937910550247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23213937910550247 | validation: 0.5118125050352847]
	TIME [epoch: 5.88 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21066638880035918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21066638880035918 | validation: 0.4973470226381871]
	TIME [epoch: 5.87 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18751256276869657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18751256276869657 | validation: 0.2623717637066954]
	TIME [epoch: 5.87 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10400552623390474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10400552623390474 | validation: 0.28857992785386377]
	TIME [epoch: 5.87 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22845333161970538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22845333161970538 | validation: 0.2843241657126007]
	TIME [epoch: 5.86 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12541724054932396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12541724054932396 | validation: 0.27810164153773115]
	TIME [epoch: 5.87 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.092656530286074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.092656530286074 | validation: 0.2763653135510437]
	TIME [epoch: 5.87 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08415071539233562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08415071539233562 | validation: 0.21684041454275482]
	TIME [epoch: 5.88 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06668494439032052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06668494439032052 | validation: 0.26620536371123266]
	TIME [epoch: 5.87 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06446067812600183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06446067812600183 | validation: 0.25171554794210493]
	TIME [epoch: 5.87 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11698955619745363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11698955619745363 | validation: 0.7050621275668914]
	TIME [epoch: 5.87 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24881096020914015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24881096020914015 | validation: 0.2373991668604426]
	TIME [epoch: 5.87 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10105083980853817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10105083980853817 | validation: 0.21890636536205293]
	TIME [epoch: 5.89 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06292688144063444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06292688144063444 | validation: 0.2656247154043923]
	TIME [epoch: 5.87 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10255334986613185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10255334986613185 | validation: 0.1803634932536473]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_792.pth
	Model improved!!!
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07409049751048392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07409049751048392 | validation: 0.22811536158107149]
	TIME [epoch: 5.91 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09268109982082968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09268109982082968 | validation: 0.2599165438967683]
	TIME [epoch: 5.9 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20480798365535668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20480798365535668 | validation: 0.3415447919601113]
	TIME [epoch: 5.92 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21318622212527408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21318622212527408 | validation: 0.1902861922005355]
	TIME [epoch: 5.9 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07817845633908801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07817845633908801 | validation: 0.17761923265337756]
	TIME [epoch: 5.9 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570406638320709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0570406638320709 | validation: 0.2564982977826949]
	TIME [epoch: 5.88 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06867046948477638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06867046948477638 | validation: 0.22663383429524708]
	TIME [epoch: 5.88 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12251100621462088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12251100621462088 | validation: 0.6800498690660246]
	TIME [epoch: 5.88 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5518448816535579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5518448816535579 | validation: 0.4619002171971266]
	TIME [epoch: 5.88 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031383091172016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2031383091172016 | validation: 0.32215208598766876]
	TIME [epoch: 5.87 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22509725357595906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22509725357595906 | validation: 0.2629253618859543]
	TIME [epoch: 5.88 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0936064418300527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0936064418300527 | validation: 0.47516998402513466]
	TIME [epoch: 5.88 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13495318870312265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13495318870312265 | validation: 0.23461710643042144]
	TIME [epoch: 5.89 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508527985558303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09508527985558303 | validation: 0.27749354272360555]
	TIME [epoch: 5.87 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07908216757833102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07908216757833102 | validation: 0.23169358662631218]
	TIME [epoch: 5.89 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07936682524014628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07936682524014628 | validation: 0.44499639325222984]
	TIME [epoch: 5.89 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10095838509321167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10095838509321167 | validation: 0.2516113334257697]
	TIME [epoch: 5.89 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11424496866625475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11424496866625475 | validation: 0.3850849601161828]
	TIME [epoch: 5.91 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11271482492909084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11271482492909084 | validation: 0.20487362190396233]
	TIME [epoch: 5.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08892449791899262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08892449791899262 | validation: 0.32896935402110383]
	TIME [epoch: 5.9 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07595057566070224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07595057566070224 | validation: 0.21024665818983845]
	TIME [epoch: 5.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06293300250604963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06293300250604963 | validation: 0.21978535162759627]
	TIME [epoch: 5.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05873247436457248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05873247436457248 | validation: 0.1810479986984137]
	TIME [epoch: 5.91 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06823791711549848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06823791711549848 | validation: 0.2561849484006329]
	TIME [epoch: 5.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14776156340404023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14776156340404023 | validation: 0.4019684788139947]
	TIME [epoch: 5.89 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18345423942852437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18345423942852437 | validation: 0.36250826386754137]
	TIME [epoch: 5.89 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24426715952293293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24426715952293293 | validation: 0.32855522011345906]
	TIME [epoch: 5.89 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09655567633919614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09655567633919614 | validation: 0.19059783243570752]
	TIME [epoch: 5.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13672131452138173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13672131452138173 | validation: 0.3198967464914222]
	TIME [epoch: 5.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.108549476292481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.108549476292481 | validation: 0.21965239090676225]
	TIME [epoch: 5.89 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08085278071714684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08085278071714684 | validation: 0.5003337978702224]
	TIME [epoch: 5.89 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13655051706284252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13655051706284252 | validation: 0.3665387725654996]
	TIME [epoch: 5.88 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1927604455641599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1927604455641599 | validation: 0.5501873209363287]
	TIME [epoch: 5.89 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1462906592886337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1462906592886337 | validation: 0.2366639585260374]
	TIME [epoch: 5.89 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08468683752887919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08468683752887919 | validation: 0.21906606955936647]
	TIME [epoch: 5.88 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05555346643267085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05555346643267085 | validation: 0.21578444148570736]
	TIME [epoch: 5.88 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05171429558344059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05171429558344059 | validation: 0.22086845403454639]
	TIME [epoch: 5.88 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058731009697893494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058731009697893494 | validation: 0.26711270009242705]
	TIME [epoch: 5.89 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06941830629174041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06941830629174041 | validation: 0.25570328613574483]
	TIME [epoch: 5.89 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1039056848391639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1039056848391639 | validation: 0.35053285958461616]
	TIME [epoch: 5.89 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222597458324627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222597458324627 | validation: 0.3189494825170816]
	TIME [epoch: 5.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15165347386645647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15165347386645647 | validation: 0.19827194401453854]
	TIME [epoch: 5.89 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15923311349379207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15923311349379207 | validation: 0.2851159607078311]
	TIME [epoch: 5.89 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21798028136666792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21798028136666792 | validation: 0.45245571544667645]
	TIME [epoch: 5.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041038056319659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1041038056319659 | validation: 0.20083854270806922]
	TIME [epoch: 5.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11399190181622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11399190181622 | validation: 0.29724799532805485]
	TIME [epoch: 5.89 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1021502971413063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1021502971413063 | validation: 0.23728409523402974]
	TIME [epoch: 5.89 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1090945051758773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1090945051758773 | validation: 0.5038118802260948]
	TIME [epoch: 5.89 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1206721161149151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1206721161149151 | validation: 0.3329985265427061]
	TIME [epoch: 5.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20097974125110213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20097974125110213 | validation: 0.36890762430446017]
	TIME [epoch: 5.89 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1086601784153628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1086601784153628 | validation: 0.30930150963008496]
	TIME [epoch: 5.89 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10982810185244056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10982810185244056 | validation: 0.22784957064990008]
	TIME [epoch: 5.88 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058401104173354916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058401104173354916 | validation: 0.17240714164496745]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_845.pth
	Model improved!!!
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07253692402980183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07253692402980183 | validation: 0.23984867937571988]
	TIME [epoch: 5.86 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13070058764297404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13070058764297404 | validation: 0.2934650649157446]
	TIME [epoch: 5.87 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0746670282244978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0746670282244978 | validation: 0.21798838380785157]
	TIME [epoch: 5.87 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08396327460732976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08396327460732976 | validation: 0.27656817713186616]
	TIME [epoch: 5.86 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15361919471781074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15361919471781074 | validation: 0.2487004034737339]
	TIME [epoch: 5.87 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862508987635213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06862508987635213 | validation: 0.2895926611002674]
	TIME [epoch: 5.87 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09234373093419239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09234373093419239 | validation: 0.27313605539397434]
	TIME [epoch: 5.87 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15170737551508137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15170737551508137 | validation: 0.36400389913222253]
	TIME [epoch: 5.86 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1229151678395623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1229151678395623 | validation: 0.2699483988935934]
	TIME [epoch: 5.87 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11438714486187807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11438714486187807 | validation: 0.20753773289035307]
	TIME [epoch: 5.87 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09913234029638543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09913234029638543 | validation: 0.28164694671951035]
	TIME [epoch: 5.87 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08023746642370608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08023746642370608 | validation: 0.16800330861050283]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07073609265483383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07073609265483383 | validation: 0.3243224695510237]
	TIME [epoch: 5.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09106599236826973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09106599236826973 | validation: 0.24358666298594325]
	TIME [epoch: 5.89 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15462630076408398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15462630076408398 | validation: 0.5624326033348644]
	TIME [epoch: 5.91 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369600553558079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1369600553558079 | validation: 0.20394392467906655]
	TIME [epoch: 5.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07454285329564037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07454285329564037 | validation: 0.18465919072881196]
	TIME [epoch: 5.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07107279395556437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07107279395556437 | validation: 0.19717935363715372]
	TIME [epoch: 5.89 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07895158538585655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07895158538585655 | validation: 0.3096999885297542]
	TIME [epoch: 5.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09758062878886678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09758062878886678 | validation: 0.22946312940053648]
	TIME [epoch: 5.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11696997878820177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11696997878820177 | validation: 0.2724798006909299]
	TIME [epoch: 5.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09664860784726421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09664860784726421 | validation: 0.2709464488410328]
	TIME [epoch: 5.9 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15271343690493422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15271343690493422 | validation: 0.21751896129972637]
	TIME [epoch: 5.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0854170813179724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0854170813179724 | validation: 0.2162834770156044]
	TIME [epoch: 5.91 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653496563072463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0653496563072463 | validation: 0.21306458651069057]
	TIME [epoch: 5.91 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10359435921744017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10359435921744017 | validation: 0.2850594467481004]
	TIME [epoch: 5.9 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2491839457077888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2491839457077888 | validation: 0.2632552633801739]
	TIME [epoch: 5.89 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13299489621806065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13299489621806065 | validation: 0.5565279932850941]
	TIME [epoch: 5.89 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19898214532933403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19898214532933403 | validation: 0.4355845242117658]
	TIME [epoch: 5.9 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28731779335554575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28731779335554575 | validation: 0.3467160004174605]
	TIME [epoch: 5.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10924131742770184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10924131742770184 | validation: 0.26809272001597656]
	TIME [epoch: 5.91 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12990486210603083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12990486210603083 | validation: 0.30815684607434674]
	TIME [epoch: 5.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608181007483012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06608181007483012 | validation: 0.20286606710219304]
	TIME [epoch: 5.9 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06714700353902671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06714700353902671 | validation: 0.20989561020217218]
	TIME [epoch: 5.89 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06985268329596707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06985268329596707 | validation: 0.2645603880780078]
	TIME [epoch: 5.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052305770033678554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052305770033678554 | validation: 0.17686911080868103]
	TIME [epoch: 5.91 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05920902848166293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05920902848166293 | validation: 0.2580878840632989]
	TIME [epoch: 5.89 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08106260627859317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08106260627859317 | validation: 0.20766601198929227]
	TIME [epoch: 5.89 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11106424111460711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11106424111460711 | validation: 0.503983299146223]
	TIME [epoch: 5.9 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16547747983442548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16547747983442548 | validation: 0.2738022070978556]
	TIME [epoch: 5.91 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14563603527346683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14563603527346683 | validation: 0.4149035810298469]
	TIME [epoch: 5.92 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09029809335136492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09029809335136492 | validation: 0.24119813362850745]
	TIME [epoch: 5.91 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09089190110408145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09089190110408145 | validation: 0.201364308209112]
	TIME [epoch: 5.9 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07550347466860562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07550347466860562 | validation: 0.19582415520567364]
	TIME [epoch: 5.9 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0813751760160699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0813751760160699 | validation: 0.2785804137468074]
	TIME [epoch: 5.91 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08134312061431538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08134312061431538 | validation: 0.18017818798030297]
	TIME [epoch: 5.91 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07119482142912477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07119482142912477 | validation: 0.2255341719844752]
	TIME [epoch: 5.9 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05394402621402678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05394402621402678 | validation: 0.1746231061306338]
	TIME [epoch: 5.89 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017830576405011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05017830576405011 | validation: 0.22987530784655685]
	TIME [epoch: 5.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06297182800867523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06297182800867523 | validation: 0.17014604658833227]
	TIME [epoch: 5.9 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07462512156507899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07462512156507899 | validation: 0.4315737098087009]
	TIME [epoch: 5.91 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11292122191760134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11292122191760134 | validation: 0.34582636538012973]
	TIME [epoch: 5.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24197079213942566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24197079213942566 | validation: 0.5074715894075567]
	TIME [epoch: 5.9 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12343962651722279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12343962651722279 | validation: 0.35096799722483163]
	TIME [epoch: 5.9 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0663982114604583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0663982114604583 | validation: 0.24186720242829918]
	TIME [epoch: 5.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10522337747257489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10522337747257489 | validation: 0.29277995677728214]
	TIME [epoch: 5.87 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11366015648445181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11366015648445181 | validation: 0.241169603132403]
	TIME [epoch: 5.86 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09617788032533263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09617788032533263 | validation: 0.21730543972341662]
	TIME [epoch: 5.87 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057164021853227436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057164021853227436 | validation: 0.22420595988197128]
	TIME [epoch: 5.86 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06329740172723573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06329740172723573 | validation: 0.21971292738577405]
	TIME [epoch: 5.86 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08338361354025116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08338361354025116 | validation: 0.2597581451514647]
	TIME [epoch: 5.87 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14280039782990303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14280039782990303 | validation: 0.21397281810286228]
	TIME [epoch: 5.87 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07946737309188409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07946737309188409 | validation: 0.16797287532108207]
	TIME [epoch: 5.86 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_908.pth
	Model improved!!!
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07366259290088997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07366259290088997 | validation: 0.24512626397698678]
	TIME [epoch: 5.89 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16045718839008394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16045718839008394 | validation: 0.23340543326545182]
	TIME [epoch: 5.89 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08318928406995678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08318928406995678 | validation: 0.2793211444459071]
	TIME [epoch: 5.88 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08504504996381564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08504504996381564 | validation: 0.23554493736206505]
	TIME [epoch: 5.88 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08180368023578764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08180368023578764 | validation: 0.31870362746127273]
	TIME [epoch: 5.87 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07425782683945151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07425782683945151 | validation: 0.19470479199610674]
	TIME [epoch: 5.87 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07097713269804405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07097713269804405 | validation: 0.17815009649145505]
	TIME [epoch: 5.87 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06220256310796676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06220256310796676 | validation: 0.24529991835440082]
	TIME [epoch: 5.88 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0788989347769905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0788989347769905 | validation: 0.18194264996499993]
	TIME [epoch: 5.87 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11221110626250404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11221110626250404 | validation: 0.30283406432930093]
	TIME [epoch: 5.86 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12087950344557388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12087950344557388 | validation: 0.14120500859566662]
	TIME [epoch: 5.87 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_919.pth
	Model improved!!!
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09356279655578088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09356279655578088 | validation: 0.2728819547453219]
	TIME [epoch: 5.9 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19335561162538614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19335561162538614 | validation: 0.5520922839349839]
	TIME [epoch: 5.91 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12458665134349574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12458665134349574 | validation: 0.3398260294049221]
	TIME [epoch: 5.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2403282169699832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2403282169699832 | validation: 0.48460373900378073]
	TIME [epoch: 5.87 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11866261139978082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11866261139978082 | validation: 0.196872533656685]
	TIME [epoch: 5.87 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07938697582591588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07938697582591588 | validation: 0.18876029879064105]
	TIME [epoch: 5.87 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052765266407624224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052765266407624224 | validation: 0.17908657606644107]
	TIME [epoch: 5.87 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041330081594266536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041330081594266536 | validation: 0.15769380940933342]
	TIME [epoch: 5.88 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039506225451969114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039506225451969114 | validation: 0.3997193800734304]
	TIME [epoch: 5.88 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06721730966620174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06721730966620174 | validation: 0.22214250226659096]
	TIME [epoch: 5.88 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08483865044574503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08483865044574503 | validation: 0.2389549336166762]
	TIME [epoch: 5.88 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07928217363576959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07928217363576959 | validation: 0.16278981421874789]
	TIME [epoch: 5.91 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06873723507520263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06873723507520263 | validation: 0.2386508279480733]
	TIME [epoch: 5.9 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08498880884239593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08498880884239593 | validation: 0.18263967556732832]
	TIME [epoch: 5.89 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10570082884918104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10570082884918104 | validation: 0.27483592725769024]
	TIME [epoch: 5.9 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08769880190967776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08769880190967776 | validation: 0.184815724016145]
	TIME [epoch: 5.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06762676796791067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06762676796791067 | validation: 0.4611405892236848]
	TIME [epoch: 5.88 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10780633043559895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10780633043559895 | validation: 0.282786246079072]
	TIME [epoch: 5.88 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20479511381834037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20479511381834037 | validation: 0.2813523418649074]
	TIME [epoch: 5.88 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09747732349299174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09747732349299174 | validation: 0.1936775663704105]
	TIME [epoch: 5.88 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1493570903513228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1493570903513228 | validation: 0.27186882918630567]
	TIME [epoch: 5.88 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12463320177530975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12463320177530975 | validation: 0.18440549628463532]
	TIME [epoch: 5.88 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05438155655230001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05438155655230001 | validation: 0.1826793714818006]
	TIME [epoch: 5.89 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07275239441914597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07275239441914597 | validation: 0.19074040054753705]
	TIME [epoch: 5.88 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852276637552476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06852276637552476 | validation: 0.34042435946734745]
	TIME [epoch: 5.88 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06996436878466565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06996436878466565 | validation: 0.17424752910277197]
	TIME [epoch: 5.89 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06636231146224855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06636231146224855 | validation: 0.1608277361448046]
	TIME [epoch: 5.89 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06141638092801313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06141638092801313 | validation: 0.23441585059921044]
	TIME [epoch: 5.88 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0809080006825356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0809080006825356 | validation: 0.20193363570744416]
	TIME [epoch: 5.88 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12800134101362115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12800134101362115 | validation: 0.2213282991979907]
	TIME [epoch: 5.88 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09337198254994249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09337198254994249 | validation: 0.15089885864037741]
	TIME [epoch: 5.89 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04813721828070369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04813721828070369 | validation: 0.13829858491503155]
	TIME [epoch: 5.88 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_951.pth
	Model improved!!!
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05783398556952284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05783398556952284 | validation: 0.33274834080579396]
	TIME [epoch: 5.89 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08292508199762288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08292508199762288 | validation: 0.20856035268399023]
	TIME [epoch: 5.88 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10943717550635232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10943717550635232 | validation: 0.521493565021271]
	TIME [epoch: 5.88 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12047619546081711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12047619546081711 | validation: 0.21548915310260833]
	TIME [epoch: 5.87 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08560234773062969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08560234773062969 | validation: 0.28846630432249376]
	TIME [epoch: 5.87 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13483022605028736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13483022605028736 | validation: 0.5111927229212201]
	TIME [epoch: 5.87 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29485169111759896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29485169111759896 | validation: 0.5482085490045714]
	TIME [epoch: 5.88 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1677855564574788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1677855564574788 | validation: 0.3881995211540602]
	TIME [epoch: 5.88 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1120809097077969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1120809097077969 | validation: 0.28521771071003543]
	TIME [epoch: 5.88 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14878854358679502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14878854358679502 | validation: 0.34001509224322746]
	TIME [epoch: 5.89 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09654402968150404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09654402968150404 | validation: 0.22340588438266093]
	TIME [epoch: 5.89 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04934914644022818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04934914644022818 | validation: 0.16485535302526333]
	TIME [epoch: 5.89 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04166935765815678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04166935765815678 | validation: 0.16862080990107398]
	TIME [epoch: 5.89 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053687688386834756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053687688386834756 | validation: 0.18114067186614116]
	TIME [epoch: 5.88 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06217150408271072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06217150408271072 | validation: 0.24797075767179522]
	TIME [epoch: 5.89 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04804511794952482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04804511794952482 | validation: 0.14369185593868208]
	TIME [epoch: 5.87 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059036948941626005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059036948941626005 | validation: 0.6609538330861024]
	TIME [epoch: 5.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5361911615943977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5361911615943977 | validation: 0.4617029420758768]
	TIME [epoch: 5.9 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1405235830385666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1405235830385666 | validation: 0.8984780851403258]
	TIME [epoch: 5.91 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9050769089936119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9050769089936119 | validation: 0.9449752309481589]
	TIME [epoch: 5.9 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9651718121240402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9651718121240402 | validation: 0.8613531935898049]
	TIME [epoch: 5.87 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7757946487158697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7757946487158697 | validation: 0.8224914896175513]
	TIME [epoch: 5.86 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7355962923344685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7355962923344685 | validation: 0.7595764511231484]
	TIME [epoch: 5.86 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7093664690870018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7093664690870018 | validation: 0.5882633193246128]
	TIME [epoch: 5.89 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7306948188072728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7306948188072728 | validation: 0.7423912838992822]
	TIME [epoch: 5.89 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8047767227961967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8047767227961967 | validation: 0.7123731948911991]
	TIME [epoch: 5.9 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8072152809552842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8072152809552842 | validation: 0.7490075684416064]
	TIME [epoch: 5.89 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7650177930170959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7650177930170959 | validation: 0.5452834414481139]
	TIME [epoch: 5.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.806993927254992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.806993927254992 | validation: 0.8396005315191998]
	TIME [epoch: 5.89 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7785683328699662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7785683328699662 | validation: 0.950129800970617]
	TIME [epoch: 5.9 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0986306857056822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0986306857056822 | validation: 1.111237122561385]
	TIME [epoch: 5.91 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.231664962325258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.231664962325258 | validation: 1.1633015377704803]
	TIME [epoch: 5.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2798723792100308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2798723792100308 | validation: 1.2362650263818844]
	TIME [epoch: 5.9 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3153094645510066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3153094645510066 | validation: 1.2392848840748958]
	TIME [epoch: 5.89 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3335688385099982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3335688385099982 | validation: 1.282041849145137]
	TIME [epoch: 5.88 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3604457014971474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3604457014971474 | validation: 1.159339367301589]
	TIME [epoch: 5.9 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2752672439738795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2752672439738795 | validation: 1.1102698637652255]
	TIME [epoch: 5.89 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2132216989078424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2132216989078424 | validation: 1.0571486855496524]
	TIME [epoch: 5.89 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.193884747292408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.193884747292408 | validation: 1.0681793041707595]
	TIME [epoch: 5.9 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1967455211637814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1967455211637814 | validation: 1.110477832677305]
	TIME [epoch: 5.88 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2351834011475586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2351834011475586 | validation: 1.1137194692040546]
	TIME [epoch: 5.89 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2325326420470106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2325326420470106 | validation: 1.5830885665791576]
	TIME [epoch: 5.89 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3154496288183921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3154496288183921 | validation: 2.5935112910791336]
	TIME [epoch: 5.9 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.683383867228842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.683383867228842 | validation: 2.139640500339317]
	TIME [epoch: 5.89 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.272574375724231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.272574375724231 | validation: 2.379027957574291]
	TIME [epoch: 5.88 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5584463784124485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5584463784124485 | validation: 2.230514100367018]
	TIME [epoch: 5.9 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.379538567879153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.379538567879153 | validation: 1.9161648783999703]
	TIME [epoch: 5.89 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0347173966267262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0347173966267262 | validation: 1.878307921721496]
	TIME [epoch: 5.91 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9565833049439483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9565833049439483 | validation: 1.6194729468952873]
	TIME [epoch: 5.9 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2914292687624578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2914292687624578 | validation: 1.6020853925037948]
	TIME [epoch: 195 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.653789521728009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.653789521728009 | validation: 0.8433754393669288]
	TIME [epoch: 12.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491660577098953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6491660577098953 | validation: 1.4012739431559231]
	TIME [epoch: 12.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673177776871007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673177776871007 | validation: 1.188832653438837]
	TIME [epoch: 12.4 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5974115595218625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5974115595218625 | validation: 0.7316345854571283]
	TIME [epoch: 12.4 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3941052005316667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3941052005316667 | validation: 0.7759263286728821]
	TIME [epoch: 12.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3818586725830463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3818586725830463 | validation: 0.7133980095611681]
	TIME [epoch: 12.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28188727660059804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28188727660059804 | validation: 0.43870403357895865]
	TIME [epoch: 12.4 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19274846169744222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19274846169744222 | validation: 0.38294947710738936]
	TIME [epoch: 12.4 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16491459476069348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16491459476069348 | validation: 0.36975002637258436]
	TIME [epoch: 12.4 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15770194052507588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15770194052507588 | validation: 0.3538305374970478]
	TIME [epoch: 12.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14693134191202814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14693134191202814 | validation: 0.39103824334093945]
	TIME [epoch: 12.5 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13933481315326518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13933481315326518 | validation: 0.3483725792411105]
	TIME [epoch: 12.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13337453898171833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13337453898171833 | validation: 0.3641472409597819]
	TIME [epoch: 12.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1254648331656429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1254648331656429 | validation: 0.32921152532079206]
	TIME [epoch: 12.5 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1184897018157687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1184897018157687 | validation: 0.4071168897521368]
	TIME [epoch: 12.5 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1281552118909476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1281552118909476 | validation: 0.3394374050561325]
	TIME [epoch: 12.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1323751522105135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1323751522105135 | validation: 0.5103922251849667]
	TIME [epoch: 12.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12805957188605477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12805957188605477 | validation: 0.33236790060738186]
	TIME [epoch: 12.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13051452164202093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13051452164202093 | validation: 0.5421272610398541]
	TIME [epoch: 12.5 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13671298929122958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13671298929122958 | validation: 0.5757798936457897]
	TIME [epoch: 12.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22963577362180623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22963577362180623 | validation: 0.7201906742520251]
	TIME [epoch: 12.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22928799046071027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22928799046071027 | validation: 0.4002096377187545]
	TIME [epoch: 12.5 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13611424163183145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13611424163183145 | validation: 0.3417136145138835]
	TIME [epoch: 12.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09332045239293671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09332045239293671 | validation: 0.3815598240699958]
	TIME [epoch: 12.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09173534015217649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09173534015217649 | validation: 0.3879748215739646]
	TIME [epoch: 12.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09431914976461006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09431914976461006 | validation: 0.6274364028146673]
	TIME [epoch: 12.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3206943965078139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3206943965078139 | validation: 0.6491127274978279]
	TIME [epoch: 12.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21091499419519438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21091499419519438 | validation: 0.5294562332543796]
	TIME [epoch: 12.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13144705087079517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13144705087079517 | validation: 0.62779923929897]
	TIME [epoch: 12.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33231115583782594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33231115583782594 | validation: 0.4645038663410219]
	TIME [epoch: 12.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12074645270757213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12074645270757213 | validation: 0.6021567064800892]
	TIME [epoch: 12.4 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1451304770496273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1451304770496273 | validation: 0.560983977044666]
	TIME [epoch: 12.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23772190873919272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23772190873919272 | validation: 0.2882226084668517]
	TIME [epoch: 12.4 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08617736578275485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08617736578275485 | validation: 0.3587308558981594]
	TIME [epoch: 12.5 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08737697049499248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08737697049499248 | validation: 0.3828926743228538]
	TIME [epoch: 12.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11866841403964472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11866841403964472 | validation: 0.45114373201515756]
	TIME [epoch: 12.5 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10356917624164023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10356917624164023 | validation: 0.36660747259830306]
	TIME [epoch: 12.4 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11542270129000294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11542270129000294 | validation: 0.30801113450344214]
	TIME [epoch: 12.5 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12403889339877612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12403889339877612 | validation: 0.28574195159639953]
	TIME [epoch: 12.5 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09563249839173082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09563249839173082 | validation: 0.30139552743816883]
	TIME [epoch: 12.5 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08881907508555706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08881907508555706 | validation: 0.28571352402696387]
	TIME [epoch: 12.4 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170031448953914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09170031448953914 | validation: 0.39206982915388205]
	TIME [epoch: 12.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11942867632014074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11942867632014074 | validation: 0.29094318099607386]
	TIME [epoch: 12.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128363868896711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128363868896711 | validation: 0.42052488790604986]
	TIME [epoch: 12.5 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1139107813242758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1139107813242758 | validation: 0.29154120764809305]
	TIME [epoch: 12.5 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1038508825002915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1038508825002915 | validation: 0.43525669985665366]
	TIME [epoch: 12.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10526891179247676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10526891179247676 | validation: 0.2837678222983345]
	TIME [epoch: 12.5 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022780252001036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1022780252001036 | validation: 0.37543046710971906]
	TIME [epoch: 12.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12487948894950357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12487948894950357 | validation: 0.38804506908156594]
	TIME [epoch: 12.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20555777762680014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20555777762680014 | validation: 0.4004300669664662]
	TIME [epoch: 12.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08419906508588472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08419906508588472 | validation: 0.549756405127069]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_193508/states/model_phi1_4b_v_mmd2_1052.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5500.051 seconds.
