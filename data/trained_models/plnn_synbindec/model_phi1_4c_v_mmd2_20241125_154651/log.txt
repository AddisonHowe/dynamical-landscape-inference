Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3733771056

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.632605374749396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.632605374749396 | validation: 4.208175884757265]
	TIME [epoch: 160 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.91262582781909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.91262582781909 | validation: 5.061045248503033]
	TIME [epoch: 2.66 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.113169505977356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.113169505977356 | validation: 4.037792492926829]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.947611738304613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.947611738304613 | validation: 4.052453172480803]
	TIME [epoch: 2.65 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6713299699659094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6713299699659094 | validation: 3.9963924346798763]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.58795502165439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.58795502165439 | validation: 3.600231113262968]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.519791674973287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.519791674973287 | validation: 3.5107565074316827]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.178421963934234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.178421963934234 | validation: 3.199551638733807]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9388082569852303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9388082569852303 | validation: 2.887868680990736]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7311867089908355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7311867089908355 | validation: 2.6469469892247433]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.524921071702191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.524921071702191 | validation: 2.432133883328761]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.223777098529209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.223777098529209 | validation: 2.065936499839773]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1221611391691475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1221611391691475 | validation: 2.1923854124924227]
	TIME [epoch: 2.69 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.443633457360602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.443633457360602 | validation: 3.3009446754408813]
	TIME [epoch: 2.68 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.052367393557879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.052367393557879 | validation: 1.6485829168254744]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5435519805152882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5435519805152882 | validation: 1.651212731190772]
	TIME [epoch: 2.66 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6147927425803121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6147927425803121 | validation: 1.310491174098378]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.442922918795502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.442922918795502 | validation: 1.3464825770804079]
	TIME [epoch: 2.68 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2859749455983671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2859749455983671 | validation: 1.6049478048395374]
	TIME [epoch: 2.67 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7096358679882204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7096358679882204 | validation: 1.3183332093616966]
	TIME [epoch: 2.68 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2205481068522148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2205481068522148 | validation: 1.5309843499652276]
	TIME [epoch: 2.68 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4453907324359807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4453907324359807 | validation: 1.2341888224199646]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3477259670595998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3477259670595998 | validation: 1.2064130998268685]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2475586190599077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2475586190599077 | validation: 1.2757200098644097]
	TIME [epoch: 2.69 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2096734784446015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2096734784446015 | validation: 1.2179477224484028]
	TIME [epoch: 2.69 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1567018268192655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1567018268192655 | validation: 1.2991646767004337]
	TIME [epoch: 2.68 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2235074417569778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2235074417569778 | validation: 1.1014461900497658]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.179233155949568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.179233155949568 | validation: 1.1919738321476803]
	TIME [epoch: 2.69 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1302071690427014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1302071690427014 | validation: 1.063643217573292]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1323420720256572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1323420720256572 | validation: 1.2737516233456156]
	TIME [epoch: 2.7 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1548448643222742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1548448643222742 | validation: 1.037512962681621]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1177744481092444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1177744481092444 | validation: 0.999029717260699]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0589174955938112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0589174955938112 | validation: 1.0876567547387477]
	TIME [epoch: 2.69 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0485888550155664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0485888550155664 | validation: 1.1174701780008818]
	TIME [epoch: 2.69 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3688387330987923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3688387330987923 | validation: 1.61452515686742]
	TIME [epoch: 2.7 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.565002735445432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.565002735445432 | validation: 1.0786842903401261]
	TIME [epoch: 2.69 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.215732128900458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.215732128900458 | validation: 1.0242080306626313]
	TIME [epoch: 2.7 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2090199214417732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2090199214417732 | validation: 1.1112646079312185]
	TIME [epoch: 2.69 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1036184213252012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1036184213252012 | validation: 0.972491225013361]
	TIME [epoch: 2.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0477970866141173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0477970866141173 | validation: 0.9648665299382085]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0230822779650277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0230822779650277 | validation: 0.943147419524977]
	TIME [epoch: 2.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.016959442701301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.016959442701301 | validation: 0.9062543121238896]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0057803441925757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0057803441925757 | validation: 0.8713658662640305]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9957105997896232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9957105997896232 | validation: 0.8816692503740994]
	TIME [epoch: 2.66 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9843861398335423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9843861398335423 | validation: 0.8605245284065989]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9764789514009463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9764789514009463 | validation: 0.9791896013550091]
	TIME [epoch: 2.64 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0113821968598458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0113821968598458 | validation: 0.9135695811981039]
	TIME [epoch: 2.68 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0166882717601544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0166882717601544 | validation: 1.3612139467982258]
	TIME [epoch: 2.66 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.291308136644727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.291308136644727 | validation: 0.8683479936116989]
	TIME [epoch: 2.67 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.027676495378491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.027676495378491 | validation: 0.8489810777946136]
	TIME [epoch: 2.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9791214220165776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9791214220165776 | validation: 0.9709160751495809]
	TIME [epoch: 2.66 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9981270292356942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9981270292356942 | validation: 0.8544234874609762]
	TIME [epoch: 2.66 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9695885249532361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9695885249532361 | validation: 0.8534490143119768]
	TIME [epoch: 2.65 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.946659928587697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.946659928587697 | validation: 0.8597681124416692]
	TIME [epoch: 2.67 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9429965182290977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9429965182290977 | validation: 0.8907643007209931]
	TIME [epoch: 2.66 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0130976168869135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0130976168869135 | validation: 1.1897686231366635]
	TIME [epoch: 2.65 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1755487337495762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1755487337495762 | validation: 0.8163800679179236]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9743576858127244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9743576858127244 | validation: 0.8486479784299554]
	TIME [epoch: 2.67 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9449400017850573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9449400017850573 | validation: 0.9282839834760654]
	TIME [epoch: 2.66 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9683131655921378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9683131655921378 | validation: 0.8699503738424694]
	TIME [epoch: 2.65 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9697145095119777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9697145095119777 | validation: 1.132520879572884]
	TIME [epoch: 2.64 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1102456497120545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1102456497120545 | validation: 0.7953709237351861]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9343559669144188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9343559669144188 | validation: 0.8329838665484912]
	TIME [epoch: 2.66 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9258265775045578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9258265775045578 | validation: 0.811701188437274]
	TIME [epoch: 2.64 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9223973183693903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9223973183693903 | validation: 0.8652555717655231]
	TIME [epoch: 2.65 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9976884899715817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9976884899715817 | validation: 1.0580904501759452]
	TIME [epoch: 2.65 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0879791732161526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0879791732161526 | validation: 0.8368172749607519]
	TIME [epoch: 2.67 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.012903971601724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.012903971601724 | validation: 0.7958306055958155]
	TIME [epoch: 2.66 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960650817251141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960650817251141 | validation: 0.7716764407495091]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8949671024303945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8949671024303945 | validation: 0.7535768112104763]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9006220780627247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9006220780627247 | validation: 0.800295332596145]
	TIME [epoch: 2.67 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038864022293254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038864022293254 | validation: 0.8238916241069559]
	TIME [epoch: 2.65 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9789481644787899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9789481644787899 | validation: 0.8603377067358668]
	TIME [epoch: 2.66 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9648560200547919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9648560200547919 | validation: 0.8019819762650114]
	TIME [epoch: 2.66 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9099421575201367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9099421575201367 | validation: 0.7576171161354731]
	TIME [epoch: 2.65 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920665600709515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920665600709515 | validation: 0.7426311757628637]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779892164185742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8779892164185742 | validation: 0.747464282708868]
	TIME [epoch: 2.65 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8692047112790077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8692047112790077 | validation: 0.7630367998276565]
	TIME [epoch: 2.65 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901881741959292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8901881741959292 | validation: 0.7878043880245125]
	TIME [epoch: 2.65 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.927046697490872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.927046697490872 | validation: 0.8051885892690344]
	TIME [epoch: 2.65 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9682867850608702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9682867850608702 | validation: 0.8192759934784384]
	TIME [epoch: 2.65 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9622704121260744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9622704121260744 | validation: 0.7884276233087422]
	TIME [epoch: 2.65 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9545005488772976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9545005488772976 | validation: 0.8120562663223707]
	TIME [epoch: 2.64 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9323665235927515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9323665235927515 | validation: 0.8111932828815928]
	TIME [epoch: 2.65 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9748420616581973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9748420616581973 | validation: 0.8325120967332819]
	TIME [epoch: 2.64 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.974925789916668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.974925789916668 | validation: 0.7657365866185393]
	TIME [epoch: 2.64 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.884935023749982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.884935023749982 | validation: 0.734124132670882]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882534703713093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882534703713093 | validation: 0.807644691943197]
	TIME [epoch: 2.64 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9286947398670711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9286947398670711 | validation: 0.7740667953087654]
	TIME [epoch: 2.67 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9438737783597138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9438737783597138 | validation: 0.751939668253894]
	TIME [epoch: 2.64 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8759999226015022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8759999226015022 | validation: 0.7328442295419335]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8909300896062664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8909300896062664 | validation: 0.7706219072849936]
	TIME [epoch: 2.65 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8932714793884455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8932714793884455 | validation: 0.7641114057656996]
	TIME [epoch: 2.66 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8963974463475719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8963974463475719 | validation: 0.7720732322357774]
	TIME [epoch: 2.65 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.892097492976178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.892097492976178 | validation: 0.8208841599916483]
	TIME [epoch: 2.65 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9451436496516223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9451436496516223 | validation: 0.8512016384534093]
	TIME [epoch: 2.66 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.01741499886019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.01741499886019 | validation: 0.7459249579061832]
	TIME [epoch: 2.66 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8763781370640515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8763781370640515 | validation: 0.7348502698444899]
	TIME [epoch: 2.64 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943111955091244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8943111955091244 | validation: 0.7715727547510145]
	TIME [epoch: 2.65 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.907056228737681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.907056228737681 | validation: 0.7490754567682335]
	TIME [epoch: 2.65 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.913463668161962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.913463668161962 | validation: 0.7789822671764037]
	TIME [epoch: 2.65 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9318086188988153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9318086188988153 | validation: 0.8190327348478675]
	TIME [epoch: 2.65 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9413304644819125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9413304644819125 | validation: 0.8239802184284424]
	TIME [epoch: 2.65 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9912241636763307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9912241636763307 | validation: 0.9111029335494938]
	TIME [epoch: 2.66 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0237883348413868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0237883348413868 | validation: 0.7427906676735007]
	TIME [epoch: 2.66 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8986220051031404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8986220051031404 | validation: 0.8213780487206175]
	TIME [epoch: 2.66 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9355067468043444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9355067468043444 | validation: 0.7374281917554782]
	TIME [epoch: 2.66 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8717813812308834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8717813812308834 | validation: 0.7322868831395255]
	TIME [epoch: 2.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8712768602018788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8712768602018788 | validation: 0.7590929522848364]
	TIME [epoch: 2.65 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803062983580466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803062983580466 | validation: 0.7681262807718454]
	TIME [epoch: 2.66 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9357424368551064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9357424368551064 | validation: 0.7919551680027315]
	TIME [epoch: 2.66 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9320779375848649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9320779375848649 | validation: 0.751773756938008]
	TIME [epoch: 2.66 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9102412487795813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9102412487795813 | validation: 0.7625706789253393]
	TIME [epoch: 2.67 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9052314450701501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9052314450701501 | validation: 0.7339329625565767]
	TIME [epoch: 2.67 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642550764222187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642550764222187 | validation: 0.7180307769942842]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8493825707876809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8493825707876809 | validation: 0.724710122272954]
	TIME [epoch: 2.66 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620885464320444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8620885464320444 | validation: 0.7792005885979403]
	TIME [epoch: 2.66 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9321500796964373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9321500796964373 | validation: 0.9445926209515051]
	TIME [epoch: 2.64 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0656428477098576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0656428477098576 | validation: 0.7390594149426907]
	TIME [epoch: 2.67 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8962166716774469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8962166716774469 | validation: 0.8241994595143748]
	TIME [epoch: 2.66 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315115178137197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9315115178137197 | validation: 0.7787687292306646]
	TIME [epoch: 2.66 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906388301769141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.906388301769141 | validation: 0.7513493766334162]
	TIME [epoch: 2.66 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8635589131066473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8635589131066473 | validation: 0.7576682112010151]
	TIME [epoch: 2.66 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8806328326000208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8806328326000208 | validation: 0.7441119833407833]
	TIME [epoch: 2.66 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875226655616435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8875226655616435 | validation: 0.7368125133966584]
	TIME [epoch: 2.67 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8599081907911631		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8599081907911631 | validation: 0.7342760556888128]
	TIME [epoch: 2.66 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8706294296421382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8706294296421382 | validation: 0.753079922351076]
	TIME [epoch: 2.67 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9305833405195071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9305833405195071 | validation: 0.7604502471461622]
	TIME [epoch: 2.66 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838695478511579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8838695478511579 | validation: 0.7638723819221632]
	TIME [epoch: 2.66 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9360488666251675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9360488666251675 | validation: 0.7584478529618699]
	TIME [epoch: 2.66 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8845737806074178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8845737806074178 | validation: 0.7425492991145342]
	TIME [epoch: 2.65 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637588424876806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8637588424876806 | validation: 0.7129277086555759]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8700015429933666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8700015429933666 | validation: 0.771398790408532]
	TIME [epoch: 2.67 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872842932581399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872842932581399 | validation: 0.7456528789480554]
	TIME [epoch: 2.64 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8888134776709657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8888134776709657 | validation: 0.7450242908212523]
	TIME [epoch: 2.66 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8768963188913967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8768963188913967 | validation: 0.7443934714085791]
	TIME [epoch: 2.64 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9206235986980381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9206235986980381 | validation: 0.7283047748175351]
	TIME [epoch: 2.65 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85003791786749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.85003791786749 | validation: 0.7929935908829006]
	TIME [epoch: 2.64 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998973409027101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998973409027101 | validation: 1.0271810071585965]
	TIME [epoch: 2.65 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0818739272025846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0818739272025846 | validation: 0.7120320052121019]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8152984312861027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8152984312861027 | validation: 0.719117319011807]
	TIME [epoch: 2.66 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026849849806265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026849849806265 | validation: 5.2206683863016625]
	TIME [epoch: 2.66 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.371731213676266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.371731213676266 | validation: 1.1904817779489711]
	TIME [epoch: 2.66 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1588651594710206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1588651594710206 | validation: 0.8278231565053865]
	TIME [epoch: 2.66 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.908665764907835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.908665764907835 | validation: 0.8637704256338726]
	TIME [epoch: 2.66 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9270068796264789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9270068796264789 | validation: 0.8200066606209492]
	TIME [epoch: 2.65 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9156084972551841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9156084972551841 | validation: 0.7756879272757641]
	TIME [epoch: 2.65 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915197681116426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8915197681116426 | validation: 0.7309218625667491]
	TIME [epoch: 2.66 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8508402242400844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8508402242400844 | validation: 0.7789272168434511]
	TIME [epoch: 2.66 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8588227090428288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8588227090428288 | validation: 0.7260632202498054]
	TIME [epoch: 2.65 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8408175786215497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8408175786215497 | validation: 0.7296643215738391]
	TIME [epoch: 2.66 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332279605500983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8332279605500983 | validation: 0.7416935481280126]
	TIME [epoch: 2.64 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8320685417294736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8320685417294736 | validation: 0.7177347022057212]
	TIME [epoch: 2.65 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8433789847390765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8433789847390765 | validation: 0.7101763660929067]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_154.pth
	Model improved!!!
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7898116308584721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7898116308584721 | validation: 0.6654822915344505]
	TIME [epoch: 2.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7097409854842271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7097409854842271 | validation: 1.2243700594227764]
	TIME [epoch: 2.64 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.253281789171821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.253281789171821 | validation: 2.0011196517824557]
	TIME [epoch: 2.65 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3402623979001826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3402623979001826 | validation: 1.2297933201565105]
	TIME [epoch: 2.65 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4624973821704184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4624973821704184 | validation: 0.9908378568398949]
	TIME [epoch: 2.64 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3210064330133895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3210064330133895 | validation: 1.0849383772370247]
	TIME [epoch: 2.65 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.295713494005356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.295713494005356 | validation: 0.9820752017569943]
	TIME [epoch: 2.65 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1478556514006168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1478556514006168 | validation: 0.7812468644304155]
	TIME [epoch: 2.65 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930483323327464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.930483323327464 | validation: 0.7608891688776196]
	TIME [epoch: 2.65 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049946476474244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049946476474244 | validation: 0.78138165364255]
	TIME [epoch: 2.65 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8837076263281425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8837076263281425 | validation: 0.781079236326419]
	TIME [epoch: 2.65 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.91382683252794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.91382683252794 | validation: 0.7318630139132467]
	TIME [epoch: 2.66 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8733442495602863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8733442495602863 | validation: 0.727404322870215]
	TIME [epoch: 2.65 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8622911889634659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8622911889634659 | validation: 0.7419629789248952]
	TIME [epoch: 2.66 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8631854493207937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8631854493207937 | validation: 0.7449768318691414]
	TIME [epoch: 2.65 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604417793659951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604417793659951 | validation: 0.721343102069633]
	TIME [epoch: 2.65 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8548820531633733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8548820531633733 | validation: 0.7103804717195814]
	TIME [epoch: 2.63 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579680989149389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8579680989149389 | validation: 0.732512368390512]
	TIME [epoch: 2.65 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8744218021968309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8744218021968309 | validation: 0.7322462568370597]
	TIME [epoch: 2.64 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531540666629215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531540666629215 | validation: 0.7407732418665857]
	TIME [epoch: 2.65 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565539201938583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565539201938583 | validation: 0.7269029299367604]
	TIME [epoch: 2.64 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861225416485364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.861225416485364 | validation: 0.7125061449743526]
	TIME [epoch: 2.66 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8413470705006646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8413470705006646 | validation: 0.7179348565548439]
	TIME [epoch: 2.64 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552319045336401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8552319045336401 | validation: 0.7227209940301816]
	TIME [epoch: 2.65 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8480573515842628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8480573515842628 | validation: 0.7120171049769692]
	TIME [epoch: 2.66 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481074565582037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8481074565582037 | validation: 0.7196351524898814]
	TIME [epoch: 2.65 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850297710416385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.850297710416385 | validation: 0.7427277609894869]
	TIME [epoch: 2.65 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703687710955147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8703687710955147 | validation: 0.7135417611693646]
	TIME [epoch: 2.65 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864758574164242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864758574164242 | validation: 0.7314892311933008]
	TIME [epoch: 2.64 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8525212729630383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8525212729630383 | validation: 0.7180063099728599]
	TIME [epoch: 2.65 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8551189389146703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8551189389146703 | validation: 0.7208181613987341]
	TIME [epoch: 2.64 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8369293347824405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8369293347824405 | validation: 0.7315991798941209]
	TIME [epoch: 2.65 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8583283708299442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8583283708299442 | validation: 0.7485201835963411]
	TIME [epoch: 2.63 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9036317977628952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9036317977628952 | validation: 0.7423209522729377]
	TIME [epoch: 2.65 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639814193880381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8639814193880381 | validation: 0.7337167526168886]
	TIME [epoch: 2.65 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363335200223025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8363335200223025 | validation: 0.7311312240695838]
	TIME [epoch: 2.65 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8481205577142421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8481205577142421 | validation: 0.7732837018590661]
	TIME [epoch: 2.65 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132575611341139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9132575611341139 | validation: 0.7348611939873951]
	TIME [epoch: 2.65 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195640341930801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195640341930801 | validation: 0.8008730325620879]
	TIME [epoch: 2.66 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7895193207169758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7895193207169758 | validation: 1.1045280264399338]
	TIME [epoch: 2.66 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1705246517745334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1705246517745334 | validation: 0.7267057416219624]
	TIME [epoch: 2.64 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329383700175587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329383700175587 | validation: 0.7470513215749162]
	TIME [epoch: 2.65 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650141197267202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8650141197267202 | validation: 0.7360278825657195]
	TIME [epoch: 2.65 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420185475919708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420185475919708 | validation: 0.6990693464955191]
	TIME [epoch: 2.66 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8169438715087936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8169438715087936 | validation: 0.7052393251529478]
	TIME [epoch: 2.65 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8195608141151872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8195608141151872 | validation: 0.7437695994569591]
	TIME [epoch: 2.66 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8476479978734672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8476479978734672 | validation: 0.6540724055513745]
	TIME [epoch: 170 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_201.pth
	Model improved!!!
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7563015581389384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7563015581389384 | validation: 1.6302846432395022]
	TIME [epoch: 5.78 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.619165105596108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.619165105596108 | validation: 1.815714505670959]
	TIME [epoch: 5.79 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0765390466998572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0765390466998572 | validation: 1.1886275302935556]
	TIME [epoch: 5.77 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4039201070146061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4039201070146061 | validation: 0.8334450851989083]
	TIME [epoch: 5.78 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9646060969603208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9646060969603208 | validation: 0.839245363473502]
	TIME [epoch: 5.8 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9834920153822674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9834920153822674 | validation: 0.7993632204604209]
	TIME [epoch: 5.81 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9379515532906467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9379515532906467 | validation: 0.7710500639571642]
	TIME [epoch: 5.8 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688451510750644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8688451510750644 | validation: 0.7556658313337131]
	TIME [epoch: 5.79 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761794726549333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8761794726549333 | validation: 0.7488466506087953]
	TIME [epoch: 5.77 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8765773305971405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8765773305971405 | validation: 0.7331251331586058]
	TIME [epoch: 5.79 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8492143395876957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8492143395876957 | validation: 0.7266403011286678]
	TIME [epoch: 5.8 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8664203448127477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8664203448127477 | validation: 0.7383099439342191]
	TIME [epoch: 5.8 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8528185256151535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8528185256151535 | validation: 0.717104684398179]
	TIME [epoch: 5.8 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8423127049448381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8423127049448381 | validation: 0.7174530190984116]
	TIME [epoch: 5.79 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461831902104322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461831902104322 | validation: 0.7205804436079855]
	TIME [epoch: 5.79 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8443862530189952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8443862530189952 | validation: 0.7516814658756059]
	TIME [epoch: 5.78 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8426260029604142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8426260029604142 | validation: 0.7267322355510084]
	TIME [epoch: 5.79 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8497872001605637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8497872001605637 | validation: 0.7293560780384525]
	TIME [epoch: 5.78 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8440196774865922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8440196774865922 | validation: 0.7281485816799128]
	TIME [epoch: 5.8 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8345665341200335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8345665341200335 | validation: 0.7375535505239332]
	TIME [epoch: 5.8 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329503283082826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8329503283082826 | validation: 0.7101128340712548]
	TIME [epoch: 5.8 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8548063656372545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8548063656372545 | validation: 0.717731995520986]
	TIME [epoch: 5.78 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8358996725540989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8358996725540989 | validation: 0.7115608790775796]
	TIME [epoch: 5.79 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8274450600642561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8274450600642561 | validation: 0.708506254521923]
	TIME [epoch: 5.77 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850389071346311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.850389071346311 | validation: 0.7374247835884082]
	TIME [epoch: 5.79 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8469121008475172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8469121008475172 | validation: 0.6915549281548299]
	TIME [epoch: 5.75 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7955448662291835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7955448662291835 | validation: 0.676241631826962]
	TIME [epoch: 5.82 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7437445509958154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7437445509958154 | validation: 1.7910704759835863]
	TIME [epoch: 5.79 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7523332572149621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7523332572149621 | validation: 2.3072402784927557]
	TIME [epoch: 5.79 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.572916915930237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.572916915930237 | validation: 1.66679260594745]
	TIME [epoch: 5.79 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9996935449376616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9996935449376616 | validation: 0.8185697947590722]
	TIME [epoch: 5.78 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.935008252880149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.935008252880149 | validation: 0.9190036465813162]
	TIME [epoch: 5.8 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0792862169813957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0792862169813957 | validation: 0.889865991363317]
	TIME [epoch: 5.76 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0273989102564116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0273989102564116 | validation: 0.7560517311756773]
	TIME [epoch: 5.8 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8964509188796654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8964509188796654 | validation: 0.7413501419200044]
	TIME [epoch: 5.76 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8851240126088163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8851240126088163 | validation: 0.7573884717306005]
	TIME [epoch: 5.8 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.877836966601121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.877836966601121 | validation: 0.7370597016964248]
	TIME [epoch: 5.76 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639301253669333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8639301253669333 | validation: 0.7269454657148627]
	TIME [epoch: 5.76 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8641852161837562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8641852161837562 | validation: 0.7331187985129995]
	TIME [epoch: 5.79 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8542851649449316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8542851649449316 | validation: 0.7115537444176999]
	TIME [epoch: 5.76 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573367349724845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573367349724845 | validation: 0.7335610101240091]
	TIME [epoch: 5.78 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857639628602749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.857639628602749 | validation: 0.7243876253767751]
	TIME [epoch: 5.79 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.869611631126244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.869611631126244 | validation: 0.7227996506593051]
	TIME [epoch: 5.79 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.874287732272314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.874287732272314 | validation: 0.7173661415580792]
	TIME [epoch: 5.81 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8567917299582188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8567917299582188 | validation: 0.7456868453114756]
	TIME [epoch: 5.81 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8903338577822746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8903338577822746 | validation: 0.8454707351941437]
	TIME [epoch: 5.79 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.017966785683032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.017966785683032 | validation: 0.7239699891841229]
	TIME [epoch: 5.8 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.88756502409267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.88756502409267 | validation: 0.7485312851471688]
	TIME [epoch: 5.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.913313763826523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.913313763826523 | validation: 0.7216633263886346]
	TIME [epoch: 5.81 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8442494277762441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8442494277762441 | validation: 0.7265415407934415]
	TIME [epoch: 5.81 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865281143054538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8865281143054538 | validation: 0.7158912772710727]
	TIME [epoch: 5.81 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8424430519173066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8424430519173066 | validation: 0.7190227946855654]
	TIME [epoch: 5.78 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839981222160767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839981222160767 | validation: 0.7138290267678592]
	TIME [epoch: 5.81 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8489064031476966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8489064031476966 | validation: 0.7065970953601619]
	TIME [epoch: 5.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840480337892931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.840480337892931 | validation: 0.7143773649616827]
	TIME [epoch: 5.82 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8411075767236786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8411075767236786 | validation: 0.7117552111735737]
	TIME [epoch: 5.81 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420806798359877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420806798359877 | validation: 0.7332731726400556]
	TIME [epoch: 5.81 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556027311046444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556027311046444 | validation: 0.7053185376160258]
	TIME [epoch: 5.81 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8556327694245909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8556327694245909 | validation: 0.7043066994663524]
	TIME [epoch: 5.79 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322540237071857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8322540237071857 | validation: 0.722537452214254]
	TIME [epoch: 5.81 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490691114757385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8490691114757385 | validation: 0.7325116511247037]
	TIME [epoch: 5.8 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8800691702468231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8800691702468231 | validation: 0.7155205723186508]
	TIME [epoch: 5.79 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8420013038533709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8420013038533709 | validation: 0.685563375626447]
	TIME [epoch: 5.82 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8037346412722238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8037346412722238 | validation: 0.6957213768287391]
	TIME [epoch: 5.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7883948460202796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7883948460202796 | validation: 0.7376502766728317]
	TIME [epoch: 5.78 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7974969110377037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7974969110377037 | validation: 1.7199435173041622]
	TIME [epoch: 5.79 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8015830926203404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8015830926203404 | validation: 2.5668257706631508]
	TIME [epoch: 5.78 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.898438895943002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.898438895943002 | validation: 1.849692638195575]
	TIME [epoch: 5.81 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1941981019594676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1941981019594676 | validation: 0.852873083081298]
	TIME [epoch: 5.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9342275582372486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9342275582372486 | validation: 0.881498768094076]
	TIME [epoch: 5.76 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.036374313193914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.036374313193914 | validation: 0.8833442178953533]
	TIME [epoch: 5.79 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0209757557841537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0209757557841537 | validation: 0.7652282649082707]
	TIME [epoch: 5.77 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.889112660342416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.889112660342416 | validation: 0.7428093752115784]
	TIME [epoch: 5.79 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859788052902645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859788052902645 | validation: 0.7625207501082555]
	TIME [epoch: 5.77 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8686648772927115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8686648772927115 | validation: 0.7372535277404476]
	TIME [epoch: 5.77 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8604817058177013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8604817058177013 | validation: 0.7489377046381787]
	TIME [epoch: 5.77 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8601928958418585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8601928958418585 | validation: 0.7318626635543072]
	TIME [epoch: 5.77 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8552933220935646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8552933220935646 | validation: 0.7333091912719621]
	TIME [epoch: 5.78 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663202892830055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663202892830055 | validation: 0.7306440698056176]
	TIME [epoch: 5.78 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8531345489650187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8531345489650187 | validation: 0.740810294527412]
	TIME [epoch: 5.76 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8612819816484438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8612819816484438 | validation: 0.754140987427739]
	TIME [epoch: 5.77 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533568165998616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533568165998616 | validation: 0.7433641831487964]
	TIME [epoch: 5.77 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8532638795840666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8532638795840666 | validation: 0.737670726872731]
	TIME [epoch: 5.77 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8630663820459634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8630663820459634 | validation: 0.7492542441958145]
	TIME [epoch: 5.76 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640141901229963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8640141901229963 | validation: 0.7682415728338037]
	TIME [epoch: 5.77 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9220007601122358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9220007601122358 | validation: 0.7285407051216839]
	TIME [epoch: 5.78 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8460972777820926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8460972777820926 | validation: 0.7302513758220306]
	TIME [epoch: 5.8 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8560328259192851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8560328259192851 | validation: 0.7303272756454453]
	TIME [epoch: 5.78 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8536171749683209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8536171749683209 | validation: 0.7392775915234673]
	TIME [epoch: 5.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8496366845076107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8496366845076107 | validation: 0.7509117307776656]
	TIME [epoch: 5.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8515353622945853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8515353622945853 | validation: 0.7438627531564301]
	TIME [epoch: 5.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487553668221617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8487553668221617 | validation: 0.7359591471337734]
	TIME [epoch: 5.81 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8842862015664639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8842862015664639 | validation: 0.7763756361707881]
	TIME [epoch: 5.85 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9166639343088155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9166639343088155 | validation: 0.7512812171172194]
	TIME [epoch: 5.79 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9007982301449144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9007982301449144 | validation: 0.7342169913416758]
	TIME [epoch: 5.8 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8427790289059327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8427790289059327 | validation: 0.7234825896921828]
	TIME [epoch: 5.78 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8331547540378911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8331547540378911 | validation: 0.7302722509333808]
	TIME [epoch: 5.8 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346600988585143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346600988585143 | validation: 0.7259959339360897]
	TIME [epoch: 5.79 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8327187871144842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8327187871144842 | validation: 0.7142055315747341]
	TIME [epoch: 5.8 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8363344829957017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8363344829957017 | validation: 0.7069310618870643]
	TIME [epoch: 5.77 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8164171603884021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8164171603884021 | validation: 0.6780294392163011]
	TIME [epoch: 5.78 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962397096734071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962397096734071 | validation: 0.7743044301795146]
	TIME [epoch: 5.77 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829689440199085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.829689440199085 | validation: 2.9920485806779116]
	TIME [epoch: 5.79 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0809413567409143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0809413567409143 | validation: 2.116456949133397]
	TIME [epoch: 5.78 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.595081293038098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.595081293038098 | validation: 1.5969435678348505]
	TIME [epoch: 5.72 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.858550143567695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.858550143567695 | validation: 0.8296193572821443]
	TIME [epoch: 5.79 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9485362763240884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9485362763240884 | validation: 0.9742753001732292]
	TIME [epoch: 5.73 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0685044885784856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0685044885784856 | validation: 0.9512105585191645]
	TIME [epoch: 5.76 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0518538894263005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0518538894263005 | validation: 0.7836788270552995]
	TIME [epoch: 5.75 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074631719823311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074631719823311 | validation: 0.746045455233939]
	TIME [epoch: 5.78 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8579112934466623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8579112934466623 | validation: 0.7666673439932872]
	TIME [epoch: 5.8 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8752084923261617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8752084923261617 | validation: 0.7472364405380209]
	TIME [epoch: 5.82 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8587481505112663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8587481505112663 | validation: 0.7205979775266041]
	TIME [epoch: 5.81 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8500728883995271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8500728883995271 | validation: 0.709581006131315]
	TIME [epoch: 5.81 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8495624236120358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8495624236120358 | validation: 0.7126359878902978]
	TIME [epoch: 5.76 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474792790165249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8474792790165249 | validation: 0.708939224957439]
	TIME [epoch: 5.77 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8397356829737441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8397356829737441 | validation: 0.7275662944330942]
	TIME [epoch: 5.78 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.852909538433881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.852909538433881 | validation: 0.7234961869752605]
	TIME [epoch: 5.79 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.85588845054886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.85588845054886 | validation: 0.7334308325157048]
	TIME [epoch: 5.77 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8256471501826985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8256471501826985 | validation: 0.7264713303691865]
	TIME [epoch: 5.8 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8569302980497523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8569302980497523 | validation: 0.7139792986806819]
	TIME [epoch: 5.79 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8618343363353035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8618343363353035 | validation: 0.7215450719560735]
	TIME [epoch: 5.8 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292957032082647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8292957032082647 | validation: 0.7773540936126327]
	TIME [epoch: 5.78 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930919839304745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.930919839304745 | validation: 0.7085236772733752]
	TIME [epoch: 5.9 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8252590275499171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8252590275499171 | validation: 0.6931260183138997]
	TIME [epoch: 5.8 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062557551949289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062557551949289 | validation: 0.673201086834069]
	TIME [epoch: 5.82 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765803252023896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765803252023896 | validation: 0.745838103689431]
	TIME [epoch: 5.79 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7774213114632346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7774213114632346 | validation: 1.2402394192914032]
	TIME [epoch: 5.82 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4065351855563233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4065351855563233 | validation: 0.8142904857428317]
	TIME [epoch: 5.79 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202520201700659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202520201700659 | validation: 0.8574665301756584]
	TIME [epoch: 5.82 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0371660019902802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0371660019902802 | validation: 0.8104168471027151]
	TIME [epoch: 5.78 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542101094377264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9542101094377264 | validation: 0.7102265937720347]
	TIME [epoch: 5.79 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589253601626891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589253601626891 | validation: 0.7467844183083068]
	TIME [epoch: 5.78 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8746629867789942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8746629867789942 | validation: 0.7169069917354304]
	TIME [epoch: 5.8 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.859784806813322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.859784806813322 | validation: 0.7166095054153228]
	TIME [epoch: 5.77 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868840240477497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868840240477497 | validation: 0.7199112391372525]
	TIME [epoch: 5.78 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8392113644395779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8392113644395779 | validation: 0.7191630666625493]
	TIME [epoch: 5.77 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8470742071549257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8470742071549257 | validation: 0.7336499601245473]
	TIME [epoch: 5.76 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960898820635325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960898820635325 | validation: 0.6985327187119634]
	TIME [epoch: 5.77 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8430650914867965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8430650914867965 | validation: 0.7203946575070191]
	TIME [epoch: 5.79 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8419346763742215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8419346763742215 | validation: 0.7122200032448739]
	TIME [epoch: 5.76 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8308376424542637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8308376424542637 | validation: 0.7246780038099518]
	TIME [epoch: 5.78 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8205358206905211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8205358206905211 | validation: 0.7325278270818694]
	TIME [epoch: 5.77 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8191669639300103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8191669639300103 | validation: 0.7111209498262263]
	TIME [epoch: 5.78 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8447195788440968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8447195788440968 | validation: 0.7255716439676045]
	TIME [epoch: 5.79 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296449011959456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8296449011959456 | validation: 0.797975814288285]
	TIME [epoch: 5.78 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.995540829460129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.995540829460129 | validation: 0.7682684487545433]
	TIME [epoch: 5.79 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663855497161188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663855497161188 | validation: 0.720734384569859]
	TIME [epoch: 5.77 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982076317443689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982076317443689 | validation: 0.6472463297185097]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7177500345544989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7177500345544989 | validation: 0.645968899344982]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667841915013085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.667841915013085 | validation: 3.711710114612497]
	TIME [epoch: 5.79 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.337605683344898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.337605683344898 | validation: 1.1206543918839826]
	TIME [epoch: 5.77 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2872594959680148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2872594959680148 | validation: 0.8531242257051986]
	TIME [epoch: 5.79 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.021618447901147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.021618447901147 | validation: 0.7792186872498187]
	TIME [epoch: 5.96 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9031482555549538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9031482555549538 | validation: 0.7645491871890007]
	TIME [epoch: 5.7 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8996816219071759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8996816219071759 | validation: 0.7463832975605615]
	TIME [epoch: 5.78 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8662901484652191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8662901484652191 | validation: 0.7404868445584581]
	TIME [epoch: 5.75 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8549299050540918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8549299050540918 | validation: 0.711778596118414]
	TIME [epoch: 5.77 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8265928021571683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8265928021571683 | validation: 0.7207823209511077]
	TIME [epoch: 5.73 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8256626547335252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8256626547335252 | validation: 0.7844770050812295]
	TIME [epoch: 5.77 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9067688880207296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9067688880207296 | validation: 0.6948149070337095]
	TIME [epoch: 5.75 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8194586624710067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8194586624710067 | validation: 0.6863076261510942]
	TIME [epoch: 5.77 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7980340536466636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7980340536466636 | validation: 0.6630730733107072]
	TIME [epoch: 5.77 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7830079826590296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7830079826590296 | validation: 0.6487233700937249]
	TIME [epoch: 5.82 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7332644540876533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7332644540876533 | validation: 0.6444430199804508]
	TIME [epoch: 5.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_366.pth
	Model improved!!!
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6791385715135695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6791385715135695 | validation: 1.0260342032899497]
	TIME [epoch: 5.79 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.126225234092181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.126225234092181 | validation: 0.6545598602480811]
	TIME [epoch: 5.73 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7630925820139954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7630925820139954 | validation: 1.107552401528374]
	TIME [epoch: 5.75 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0263761683339359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0263761683339359 | validation: 1.1221753016165337]
	TIME [epoch: 5.75 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2330214485349926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2330214485349926 | validation: 0.8027552092896761]
	TIME [epoch: 5.76 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9009517853276503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9009517853276503 | validation: 0.8275006104260805]
	TIME [epoch: 5.77 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.995896828016602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.995896828016602 | validation: 0.7434967497333083]
	TIME [epoch: 5.76 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9033715954074742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9033715954074742 | validation: 0.7295708010631362]
	TIME [epoch: 5.76 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.898889071478714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.898889071478714 | validation: 0.7179056382128945]
	TIME [epoch: 5.77 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788657812223696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8788657812223696 | validation: 0.7566011913346753]
	TIME [epoch: 5.75 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8899295729403671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8899295729403671 | validation: 0.7003696043489649]
	TIME [epoch: 5.74 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8324122619982742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8324122619982742 | validation: 0.6921462329724801]
	TIME [epoch: 5.74 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8228954151965033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8228954151965033 | validation: 0.7029466549884362]
	TIME [epoch: 5.73 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8322046634210457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8322046634210457 | validation: 0.7049317127269286]
	TIME [epoch: 5.78 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8110966802358246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8110966802358246 | validation: 0.6868433351327543]
	TIME [epoch: 5.78 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7933006670013335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7933006670013335 | validation: 0.6727340073279934]
	TIME [epoch: 5.77 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7846577934030942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7846577934030942 | validation: 0.6841272369577415]
	TIME [epoch: 5.79 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7815823227401495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7815823227401495 | validation: 0.6677918142424961]
	TIME [epoch: 5.75 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694733904079185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7694733904079185 | validation: 0.625389288997114]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231608851327497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7231608851327497 | validation: 0.5995316035335646]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6385259438016165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6385259438016165 | validation: 0.895827833841723]
	TIME [epoch: 5.84 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9138737174330536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9138737174330536 | validation: 3.0289020722451463]
	TIME [epoch: 5.85 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.163760488326084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.163760488326084 | validation: 0.7337744453447064]
	TIME [epoch: 5.82 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518134286779987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518134286779987 | validation: 0.7000189155548489]
	TIME [epoch: 5.86 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7772277954674434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7772277954674434 | validation: 0.7756486091289734]
	TIME [epoch: 5.86 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119480439777114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119480439777114 | validation: 0.6898684000977613]
	TIME [epoch: 5.83 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.727976759174747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.727976759174747 | validation: 0.6563329822056271]
	TIME [epoch: 5.83 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696849127189098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696849127189098 | validation: 0.7514668076681932]
	TIME [epoch: 5.86 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7479042515468066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7479042515468066 | validation: 1.000643158268663]
	TIME [epoch: 5.86 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0464793928636957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0464793928636957 | validation: 0.6032442985087885]
	TIME [epoch: 5.86 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6418735242156671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6418735242156671 | validation: 0.77491149039704]
	TIME [epoch: 5.86 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7982275944464154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7982275944464154 | validation: 0.8001596790877594]
	TIME [epoch: 5.85 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8597378391349487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8597378391349487 | validation: 0.6087555134321637]
	TIME [epoch: 5.86 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6755459280903293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6755459280903293 | validation: 0.7641260231431481]
	TIME [epoch: 5.85 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7678995935875657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7678995935875657 | validation: 0.762897014687856]
	TIME [epoch: 5.8 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8151697263815588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8151697263815588 | validation: 0.5681229372390372]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_402.pth
	Model improved!!!
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6324485008787678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6324485008787678 | validation: 0.7472253467012546]
	TIME [epoch: 5.81 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7557097538104114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7557097538104114 | validation: 0.778984360457552]
	TIME [epoch: 5.8 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305936496126108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305936496126108 | validation: 0.581156559579912]
	TIME [epoch: 5.82 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6356720282179297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6356720282179297 | validation: 0.7507528043058737]
	TIME [epoch: 5.79 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7463198480393513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7463198480393513 | validation: 0.7008436572581553]
	TIME [epoch: 5.81 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7317266238532005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7317266238532005 | validation: 0.5769720873592211]
	TIME [epoch: 5.78 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6013675701468103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6013675701468103 | validation: 0.8184337470870521]
	TIME [epoch: 5.81 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.782471896444381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.782471896444381 | validation: 0.8106728301932643]
	TIME [epoch: 5.83 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8853075668047484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8853075668047484 | validation: 0.6231259681822878]
	TIME [epoch: 5.82 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6452155583468288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6452155583468288 | validation: 0.7456384476424812]
	TIME [epoch: 5.82 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7729406982918988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7729406982918988 | validation: 0.6041162416571937]
	TIME [epoch: 5.83 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6476708385573356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6476708385573356 | validation: 0.5129574637928938]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.536834362774551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.536834362774551 | validation: 0.550323576594809]
	TIME [epoch: 5.84 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5415393463694261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5415393463694261 | validation: 0.6529613303780031]
	TIME [epoch: 5.8 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6747800986644188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6747800986644188 | validation: 0.75348377035469]
	TIME [epoch: 5.82 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7690814471727637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7690814471727637 | validation: 0.7418124647342015]
	TIME [epoch: 5.79 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7890101536771771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7890101536771771 | validation: 0.6213574823364625]
	TIME [epoch: 5.83 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.670511320906445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.670511320906445 | validation: 0.608964253010258]
	TIME [epoch: 5.79 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537366694340738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537366694340738 | validation: 0.5005882429975225]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_421.pth
	Model improved!!!
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5163526225652818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5163526225652818 | validation: 0.5091878061275577]
	TIME [epoch: 5.76 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518262186239064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.518262186239064 | validation: 0.565020612953858]
	TIME [epoch: 5.78 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6033366879460097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6033366879460097 | validation: 0.8014410968982644]
	TIME [epoch: 5.74 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592882247580055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592882247580055 | validation: 0.7779281376406572]
	TIME [epoch: 5.77 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7599139800927044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7599139800927044 | validation: 0.5170182766977627]
	TIME [epoch: 5.79 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6025773129327987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6025773129327987 | validation: 0.4616039439004304]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4728217824144229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4728217824144229 | validation: 0.6239417895952094]
	TIME [epoch: 5.79 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5606145149827517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5606145149827517 | validation: 0.7655748819873708]
	TIME [epoch: 5.81 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649594673440192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649594673440192 | validation: 0.48140213330652293]
	TIME [epoch: 5.8 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5154567724170341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5154567724170341 | validation: 0.5155123694957705]
	TIME [epoch: 5.81 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4917086744742008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4917086744742008 | validation: 0.47469245686201955]
	TIME [epoch: 5.77 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5217474074901253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5217474074901253 | validation: 0.7602176184829765]
	TIME [epoch: 5.8 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6888044514450304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6888044514450304 | validation: 0.8577678549619014]
	TIME [epoch: 5.8 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9811855644295306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9811855644295306 | validation: 0.5865435385489182]
	TIME [epoch: 5.81 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.648068890461495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.648068890461495 | validation: 0.6253147978987998]
	TIME [epoch: 5.8 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270287544632589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270287544632589 | validation: 0.45144218633003635]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4765070382333414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4765070382333414 | validation: 0.44420001273488285]
	TIME [epoch: 5.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_438.pth
	Model improved!!!
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44967673753636617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44967673753636617 | validation: 0.5226257894393239]
	TIME [epoch: 5.83 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5234000615033323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5234000615033323 | validation: 0.6934786464921961]
	TIME [epoch: 5.82 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.758396742289125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.758396742289125 | validation: 0.6999943492903835]
	TIME [epoch: 5.81 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590344129692677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6590344129692677 | validation: 0.4490475245622439]
	TIME [epoch: 5.82 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5558398544976451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5558398544976451 | validation: 0.40646249705662285]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_443.pth
	Model improved!!!
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41234551721482976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41234551721482976 | validation: 0.4851790966063563]
	TIME [epoch: 5.79 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.451657187232957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.451657187232957 | validation: 0.6367736971383993]
	TIME [epoch: 5.8 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7237544047344896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7237544047344896 | validation: 0.7148512131658994]
	TIME [epoch: 5.77 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6744925095927096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6744925095927096 | validation: 0.554496076929981]
	TIME [epoch: 5.8 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.606986275032489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.606986275032489 | validation: 0.4602564645374663]
	TIME [epoch: 5.77 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5621297910239039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5621297910239039 | validation: 0.7650956847218631]
	TIME [epoch: 5.85 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7023463574214494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7023463574214494 | validation: 0.5598863172330356]
	TIME [epoch: 5.84 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6030304632452326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6030304632452326 | validation: 0.39813812207134486]
	TIME [epoch: 5.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_451.pth
	Model improved!!!
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4539709457340952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4539709457340952 | validation: 0.5491727898953714]
	TIME [epoch: 5.77 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4973333032524374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4973333032524374 | validation: 0.5306801784662486]
	TIME [epoch: 5.77 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5583237895392025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5583237895392025 | validation: 0.3921265277925119]
	TIME [epoch: 5.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39493961985588116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39493961985588116 | validation: 0.3305156028094638]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_455.pth
	Model improved!!!
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39015607090322446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39015607090322446 | validation: 0.45562612394058655]
	TIME [epoch: 5.78 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42161464389671366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42161464389671366 | validation: 0.5389706284755339]
	TIME [epoch: 5.79 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.596954533468805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.596954533468805 | validation: 0.3921023655167606]
	TIME [epoch: 5.75 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39305066261981514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39305066261981514 | validation: 0.32180393449540445]
	TIME [epoch: 5.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3657456980449295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3657456980449295 | validation: 0.5714770755429467]
	TIME [epoch: 5.81 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47112406807844875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47112406807844875 | validation: 0.5371884165440057]
	TIME [epoch: 5.81 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.647313828660172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.647313828660172 | validation: 0.3529707574284611]
	TIME [epoch: 5.78 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3753780891118829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3753780891118829 | validation: 0.6138794061078319]
	TIME [epoch: 5.81 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5020775911942491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5020775911942491 | validation: 0.45620289706202466]
	TIME [epoch: 5.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5134586757817285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5134586757817285 | validation: 0.5436366898019229]
	TIME [epoch: 5.81 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42608655503800463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42608655503800463 | validation: 0.28086185423079624]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_466.pth
	Model improved!!!
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32054940444250724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32054940444250724 | validation: 0.2776412407794774]
	TIME [epoch: 5.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2801098817767861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2801098817767861 | validation: 0.3249022418491186]
	TIME [epoch: 5.79 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28206187998577387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28206187998577387 | validation: 0.2822676374505694]
	TIME [epoch: 5.79 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3229463615196083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3229463615196083 | validation: 0.5502057791752095]
	TIME [epoch: 5.75 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4936107343744544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4936107343744544 | validation: 0.3579964845996104]
	TIME [epoch: 5.85 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39783796497092594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39783796497092594 | validation: 0.3202359635933085]
	TIME [epoch: 5.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31363771062820756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31363771062820756 | validation: 0.5183709212881239]
	TIME [epoch: 5.85 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39829754592588784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39829754592588784 | validation: 0.840393353351103]
	TIME [epoch: 5.79 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9616710215691443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9616710215691443 | validation: 0.43727671150128905]
	TIME [epoch: 5.78 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4513720535876036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4513720535876036 | validation: 0.38593639743147673]
	TIME [epoch: 5.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3228496762964423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3228496762964423 | validation: 0.6818142505010782]
	TIME [epoch: 5.76 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7992058583025116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7992058583025116 | validation: 0.3722508765855401]
	TIME [epoch: 5.81 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3309309614769868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3309309614769868 | validation: 0.6758191728524855]
	TIME [epoch: 5.85 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5860481754279777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5860481754279777 | validation: 0.49204869643381566]
	TIME [epoch: 5.81 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5967682202718149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5967682202718149 | validation: 0.3980969780180562]
	TIME [epoch: 5.81 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5270300545894449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5270300545894449 | validation: 0.48302267950184014]
	TIME [epoch: 5.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44563328012808373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44563328012808373 | validation: 0.28153126963276176]
	TIME [epoch: 5.83 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30907316623830733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30907316623830733 | validation: 0.2757663619279415]
	TIME [epoch: 5.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_484.pth
	Model improved!!!
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780539941469055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2780539941469055 | validation: 0.3012626716749828]
	TIME [epoch: 5.76 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30936867290593456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30936867290593456 | validation: 0.5194143788708363]
	TIME [epoch: 5.79 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41888426929084205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41888426929084205 | validation: 0.38927379339075874]
	TIME [epoch: 5.78 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5047257316954087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5047257316954087 | validation: 0.25678609475793013]
	TIME [epoch: 5.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.246732079711444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.246732079711444 | validation: 0.2938497742914961]
	TIME [epoch: 5.79 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2611989556215847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2611989556215847 | validation: 0.39059143716265643]
	TIME [epoch: 5.79 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4255901983810752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4255901983810752 | validation: 0.8904457798055267]
	TIME [epoch: 5.82 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6999531017903368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6999531017903368 | validation: 0.21255917982169176]
	TIME [epoch: 5.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_492.pth
	Model improved!!!
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25075926911755986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25075926911755986 | validation: 0.23627811951634559]
	TIME [epoch: 5.82 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27254399324163764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27254399324163764 | validation: 0.5894812545300726]
	TIME [epoch: 5.83 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4464944244508753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4464944244508753 | validation: 0.3657245110935905]
	TIME [epoch: 5.83 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4473152990089042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4473152990089042 | validation: 0.5524240549751266]
	TIME [epoch: 5.82 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4653333818329115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4653333818329115 | validation: 0.31789592258185123]
	TIME [epoch: 5.84 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3233356164454216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3233356164454216 | validation: 0.25100162537262527]
	TIME [epoch: 5.81 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2673323988481674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2673323988481674 | validation: 0.46228915630297474]
	TIME [epoch: 5.82 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3513257203567099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3513257203567099 | validation: 0.33871049646928053]
	TIME [epoch: 5.81 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4443204676363448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4443204676363448 | validation: 0.37255564574589956]
	TIME [epoch: 178 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2848920113297221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2848920113297221 | validation: 0.19966462402063467]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23762113642783717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23762113642783717 | validation: 0.370214217914471]
	TIME [epoch: 12.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2686685978905275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2686685978905275 | validation: 0.33193431393024175]
	TIME [epoch: 12.5 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3938707570218121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3938707570218121 | validation: 0.4231604164164381]
	TIME [epoch: 12.5 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3051072279121708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3051072279121708 | validation: 0.3113741238197776]
	TIME [epoch: 12.5 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35845574435936156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35845574435936156 | validation: 0.409273128087961]
	TIME [epoch: 12.5 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3176787284997306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3176787284997306 | validation: 0.1796138021161177]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_508.pth
	Model improved!!!
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22699532627405064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22699532627405064 | validation: 0.36358007991114727]
	TIME [epoch: 12.5 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24182118252258428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24182118252258428 | validation: 0.36641355234924416]
	TIME [epoch: 12.4 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46403774861496844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46403774861496844 | validation: 0.48343550434624394]
	TIME [epoch: 12.4 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.340599486872237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.340599486872237 | validation: 0.18868564568541335]
	TIME [epoch: 12.4 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23841201207575807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23841201207575807 | validation: 0.3916017159579052]
	TIME [epoch: 12.5 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28131763840190616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28131763840190616 | validation: 0.35600527190219555]
	TIME [epoch: 12.4 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4088613917825261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4088613917825261 | validation: 0.25746961389339934]
	TIME [epoch: 12.5 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22485380076823375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22485380076823375 | validation: 0.255669054892547]
	TIME [epoch: 12.4 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21911828204907718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21911828204907718 | validation: 0.30799055870639547]
	TIME [epoch: 12.4 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2887678081535326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2887678081535326 | validation: 0.3505737909283906]
	TIME [epoch: 12.4 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26619208128417093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26619208128417093 | validation: 0.347686169552966]
	TIME [epoch: 12.5 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4475093324618185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4475093324618185 | validation: 0.5918276792139981]
	TIME [epoch: 12.4 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4311804777392205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4311804777392205 | validation: 0.22798001660630807]
	TIME [epoch: 12.5 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3005701144856978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3005701144856978 | validation: 0.47634010225351475]
	TIME [epoch: 12.5 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3274589486957548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3274589486957548 | validation: 0.21757788079204726]
	TIME [epoch: 12.5 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.260490681555922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.260490681555922 | validation: 0.3651312291578266]
	TIME [epoch: 12.5 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27949408714206864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27949408714206864 | validation: 0.30296827807816573]
	TIME [epoch: 12.4 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3375327854351795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3375327854351795 | validation: 0.36256350988757347]
	TIME [epoch: 12.5 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25119826978440873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25119826978440873 | validation: 0.19986925313830475]
	TIME [epoch: 12.4 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2521168438611896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2521168438611896 | validation: 0.3817536396092585]
	TIME [epoch: 12.5 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24021132495219885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24021132495219885 | validation: 0.25136285189746344]
	TIME [epoch: 12.5 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30353988771370094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30353988771370094 | validation: 0.37423768571216487]
	TIME [epoch: 12.5 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2567414013731783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2567414013731783 | validation: 0.18992946308710978]
	TIME [epoch: 12.5 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2530151500449647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2530151500449647 | validation: 0.3306896456864045]
	TIME [epoch: 12.5 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21660064973461093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21660064973461093 | validation: 0.25530282827018036]
	TIME [epoch: 12.4 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28140316640946794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28140316640946794 | validation: 0.28687589470825386]
	TIME [epoch: 12.5 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2233738569832155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2233738569832155 | validation: 0.24178421990956395]
	TIME [epoch: 12.4 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2437594458815247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2437594458815247 | validation: 0.2928020769883233]
	TIME [epoch: 12.5 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19242898898758373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19242898898758373 | validation: 0.25362623206516766]
	TIME [epoch: 12.5 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3390515365735993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3390515365735993 | validation: 0.5200792112483862]
	TIME [epoch: 12.5 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3470981212743553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3470981212743553 | validation: 0.23773754563084878]
	TIME [epoch: 12.5 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3116298778613747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3116298778613747 | validation: 0.6504884190539715]
	TIME [epoch: 12.5 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5050821373164104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5050821373164104 | validation: 0.3290734571389946]
	TIME [epoch: 12.5 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727140339422469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2727140339422469 | validation: 0.3058321345613418]
	TIME [epoch: 12.5 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40704386555413324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40704386555413324 | validation: 0.47346421377286935]
	TIME [epoch: 12.5 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3849827028485075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3849827028485075 | validation: 0.1613588344850484]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_544.pth
	Model improved!!!
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2108159910454718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2108159910454718 | validation: 0.5124047299566605]
	TIME [epoch: 12.5 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36415929035238637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36415929035238637 | validation: 0.3628620440608689]
	TIME [epoch: 12.4 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46104573439902646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46104573439902646 | validation: 0.17619019201856193]
	TIME [epoch: 12.5 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1991798537140197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1991798537140197 | validation: 0.4988829872221853]
	TIME [epoch: 12.5 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3555742710381383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3555742710381383 | validation: 0.27699682759365285]
	TIME [epoch: 12.5 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29321295982950113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29321295982950113 | validation: 0.1499593771681936]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_550.pth
	Model improved!!!
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15051628522925206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15051628522925206 | validation: 0.24988203771239756]
	TIME [epoch: 12.5 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1923465353920688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1923465353920688 | validation: 0.2222952963918611]
	TIME [epoch: 12.5 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2673535493768336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2673535493768336 | validation: 0.2583849261368753]
	TIME [epoch: 12.4 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18569646502032255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18569646502032255 | validation: 0.19420322893322328]
	TIME [epoch: 12.5 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18457534240216034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18457534240216034 | validation: 0.3431624240734073]
	TIME [epoch: 12.4 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21949668392077468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21949668392077468 | validation: 0.2772599143062667]
	TIME [epoch: 12.4 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3773916601964761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3773916601964761 | validation: 0.9212176923160933]
	TIME [epoch: 12.5 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7356222404855622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7356222404855622 | validation: 0.47240375987218713]
	TIME [epoch: 12.5 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3024400447093783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3024400447093783 | validation: 0.45215668618388316]
	TIME [epoch: 12.5 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5645992878652217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5645992878652217 | validation: 0.19206406714139745]
	TIME [epoch: 12.5 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1979248086639058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1979248086639058 | validation: 0.5504313184300758]
	TIME [epoch: 12.4 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4101772835474341		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4101772835474341 | validation: 0.16109302811340293]
	TIME [epoch: 12.4 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15924716931063912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15924716931063912 | validation: 0.1791469989607488]
	TIME [epoch: 12.4 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21370074228196298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21370074228196298 | validation: 0.3208973909234094]
	TIME [epoch: 12.4 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20388916540954846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20388916540954846 | validation: 0.20749736536456576]
	TIME [epoch: 12.5 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2249776333577199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2249776333577199 | validation: 0.31954981850173736]
	TIME [epoch: 12.3 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21182417220786176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21182417220786176 | validation: 0.20525742357454135]
	TIME [epoch: 12.4 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22935598820319786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22935598820319786 | validation: 0.21768445566948774]
	TIME [epoch: 12.4 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16078304668022736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16078304668022736 | validation: 0.15821143497868762]
	TIME [epoch: 12.4 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15149983494000174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15149983494000174 | validation: 0.22076280452014352]
	TIME [epoch: 12.4 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15477763971756275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15477763971756275 | validation: 0.2091389903039052]
	TIME [epoch: 12.4 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2915448138507188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2915448138507188 | validation: 0.5124882695262744]
	TIME [epoch: 12.4 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32324707104381234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32324707104381234 | validation: 0.19137167083637205]
	TIME [epoch: 12.4 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22801505987599777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22801505987599777 | validation: 0.24016744835086534]
	TIME [epoch: 12.4 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16388748703303585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16388748703303585 | validation: 0.20627851445853196]
	TIME [epoch: 12.4 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2222536443270321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2222536443270321 | validation: 0.3619227954742712]
	TIME [epoch: 12.5 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2636982583921167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2636982583921167 | validation: 0.13173526488771284]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_577.pth
	Model improved!!!
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14867145798049744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14867145798049744 | validation: 0.3162533565607597]
	TIME [epoch: 12.4 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16799044715453415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16799044715453415 | validation: 0.27715336068819774]
	TIME [epoch: 12.5 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38431348509093466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38431348509093466 | validation: 0.34277893150397765]
	TIME [epoch: 12.5 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23621377471265462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23621377471265462 | validation: 0.21448824240866418]
	TIME [epoch: 12.5 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23099531411479313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23099531411479313 | validation: 0.3190450369761381]
	TIME [epoch: 12.5 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1854990249689274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1854990249689274 | validation: 0.1811561429424042]
	TIME [epoch: 12.5 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23244184851374136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23244184851374136 | validation: 0.461171342937136]
	TIME [epoch: 12.5 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2781761504744981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2781761504744981 | validation: 0.1510810800291373]
	TIME [epoch: 12.5 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18823906771023768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18823906771023768 | validation: 0.15556513552210438]
	TIME [epoch: 12.5 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12183461970552134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12183461970552134 | validation: 0.20340764011079482]
	TIME [epoch: 12.4 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13416504025603843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13416504025603843 | validation: 0.24783377051202538]
	TIME [epoch: 12.4 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2789236976352188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2789236976352188 | validation: 0.7222285840920032]
	TIME [epoch: 12.4 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5885587815510803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5885587815510803 | validation: 0.20511667470536718]
	TIME [epoch: 12.4 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16541734120221377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16541734120221377 | validation: 0.28232109362934715]
	TIME [epoch: 12.4 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3410009537132606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3410009537132606 | validation: 0.20256642216195378]
	TIME [epoch: 12.4 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16972228189027486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16972228189027486 | validation: 0.22677686237345673]
	TIME [epoch: 12.4 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18899554171684624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18899554171684624 | validation: 0.3073057546073158]
	TIME [epoch: 12.4 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058401346455664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2058401346455664 | validation: 0.22577851522353162]
	TIME [epoch: 12.4 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2823518859535867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2823518859535867 | validation: 0.3023396385020631]
	TIME [epoch: 12.4 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19625339277495635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19625339277495635 | validation: 0.15959548710285365]
	TIME [epoch: 12.4 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19872396544864118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19872396544864118 | validation: 0.2770929714659941]
	TIME [epoch: 12.4 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16221032719518205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16221032719518205 | validation: 0.1760526995085354]
	TIME [epoch: 12.4 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2151770410156994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2151770410156994 | validation: 0.3225145773259619]
	TIME [epoch: 12.4 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18573708659175864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18573708659175864 | validation: 0.1549599262132738]
	TIME [epoch: 12.4 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20382285760194394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20382285760194394 | validation: 0.28924041768191605]
	TIME [epoch: 12.5 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1693425281779104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1693425281779104 | validation: 0.18852807972619173]
	TIME [epoch: 12.4 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17817778398165948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17817778398165948 | validation: 0.19740837720038967]
	TIME [epoch: 12.4 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1556215422408876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1556215422408876 | validation: 0.1467848960532476]
	TIME [epoch: 12.4 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13568817055415372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13568817055415372 | validation: 0.32042642437692753]
	TIME [epoch: 12.4 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16381793272697603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16381793272697603 | validation: 0.31023172628148254]
	TIME [epoch: 12.4 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4159589657591378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4159589657591378 | validation: 0.2571956863301335]
	TIME [epoch: 12.5 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741130383793902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1741130383793902 | validation: 0.17658108091700017]
	TIME [epoch: 12.5 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1510480803640162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1510480803640162 | validation: 0.2118590672390587]
	TIME [epoch: 12.5 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15325989812259688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15325989812259688 | validation: 0.606506760508303]
	TIME [epoch: 12.4 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4229875293079908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4229875293079908 | validation: 0.15182748807956392]
	TIME [epoch: 12.4 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19761365606607828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19761365606607828 | validation: 0.24153803833333135]
	TIME [epoch: 12.4 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2207711111710141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2207711111710141 | validation: 0.19234648193361795]
	TIME [epoch: 12.4 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.213234410492935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.213234410492935 | validation: 0.475904049647124]
	TIME [epoch: 12.5 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2856794231965277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2856794231965277 | validation: 0.3612708356014742]
	TIME [epoch: 12.4 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45817028181716263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45817028181716263 | validation: 0.13374509076903676]
	TIME [epoch: 12.5 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13594340747738728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13594340747738728 | validation: 0.31802348747280806]
	TIME [epoch: 12.4 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18321768818700118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18321768818700118 | validation: 0.17721417653083804]
	TIME [epoch: 12.4 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.203670742114449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.203670742114449 | validation: 0.3250169607233227]
	TIME [epoch: 12.4 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19760259596307808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19760259596307808 | validation: 0.15469552206499382]
	TIME [epoch: 12.4 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18613111146290542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18613111146290542 | validation: 0.17581151909733755]
	TIME [epoch: 12.4 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12654336987377535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12654336987377535 | validation: 0.1297054989748508]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_623.pth
	Model improved!!!
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11063334478982637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11063334478982637 | validation: 0.13225633226522412]
	TIME [epoch: 12.1 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12412867045919354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12412867045919354 | validation: 0.17836122861476894]
	TIME [epoch: 12.3 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13478914274087078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13478914274087078 | validation: 0.14545504069252338]
	TIME [epoch: 12.3 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09884185380956502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09884185380956502 | validation: 0.10378022831836842]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_627.pth
	Model improved!!!
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13026460669546527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13026460669546527 | validation: 0.46606382677343094]
	TIME [epoch: 12.4 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2740319242146557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2740319242146557 | validation: 0.42214812582848293]
	TIME [epoch: 12.4 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5431393741232916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5431393741232916 | validation: 0.24188962076257703]
	TIME [epoch: 12.4 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1772073906074435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1772073906074435 | validation: 0.2822955263307175]
	TIME [epoch: 12.4 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2232125977030109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2232125977030109 | validation: 0.23215193160366604]
	TIME [epoch: 12.4 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23803561439537504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23803561439537504 | validation: 0.6321904486119796]
	TIME [epoch: 12.3 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39914098112858065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39914098112858065 | validation: 0.47451532484337644]
	TIME [epoch: 12.1 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5912206621468483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5912206621468483 | validation: 0.18412773106297334]
	TIME [epoch: 12.3 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20233711662530987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20233711662530987 | validation: 0.4931255995916196]
	TIME [epoch: 12.1 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.320996483860484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.320996483860484 | validation: 0.24577715935000974]
	TIME [epoch: 12.2 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23520799646787233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23520799646787233 | validation: 0.1461853764548153]
	TIME [epoch: 12.3 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.142918149579279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.142918149579279 | validation: 0.6739717863780301]
	TIME [epoch: 12.2 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5137342900915185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5137342900915185 | validation: 0.2053452219860335]
	TIME [epoch: 12.4 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2559605318633289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2559605318633289 | validation: 0.19895422362505585]
	TIME [epoch: 12.2 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16157238487028155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16157238487028155 | validation: 0.1650136464632216]
	TIME [epoch: 12.3 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1834545590091445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1834545590091445 | validation: 0.48005010171651774]
	TIME [epoch: 12.2 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2812857826753697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2812857826753697 | validation: 0.16714554938445592]
	TIME [epoch: 12.2 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19764686678690496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19764686678690496 | validation: 0.20171672547152625]
	TIME [epoch: 12.3 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14143796625450772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14143796625450772 | validation: 0.16792611304408434]
	TIME [epoch: 12.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1453361796347954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1453361796347954 | validation: 0.1452688634486081]
	TIME [epoch: 12.3 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14437760701408026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14437760701408026 | validation: 0.15458948650273216]
	TIME [epoch: 12.1 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11795895709897444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11795895709897444 | validation: 0.11946016268285581]
	TIME [epoch: 12.3 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10441082617576601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10441082617576601 | validation: 0.09883853230942106]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_650.pth
	Model improved!!!
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11630685161617789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11630685161617789 | validation: 0.4669536622659931]
	TIME [epoch: 12.4 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681879239656315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2681879239656315 | validation: 0.5062956659615149]
	TIME [epoch: 12.3 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265280568245654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6265280568245654 | validation: 0.3874894671036175]
	TIME [epoch: 12.4 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3996802595302534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3996802595302534 | validation: 0.53073414623133]
	TIME [epoch: 12.3 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4209613085952721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4209613085952721 | validation: 0.2889988473127105]
	TIME [epoch: 12.2 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23822162224993398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23822162224993398 | validation: 0.21223870801428113]
	TIME [epoch: 12.2 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22339873108836567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22339873108836567 | validation: 0.17102592121433524]
	TIME [epoch: 12.3 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19148144515947046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19148144515947046 | validation: 0.28370264892685987]
	TIME [epoch: 12.3 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1972489570756752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1972489570756752 | validation: 0.5862757466387448]
	TIME [epoch: 12.3 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.709711644910306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.709711644910306 | validation: 0.3084798519816123]
	TIME [epoch: 12.4 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36475697551813724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36475697551813724 | validation: 1.4423132291574932]
	TIME [epoch: 12.3 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0598169209556965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0598169209556965 | validation: 0.7857880980812756]
	TIME [epoch: 12.4 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6391184714874686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6391184714874686 | validation: 0.7640131456337215]
	TIME [epoch: 12.3 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.832082475984019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.832082475984019 | validation: 0.6168416192193787]
	TIME [epoch: 12.4 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7629222222108116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7629222222108116 | validation: 0.4110789300798401]
	TIME [epoch: 12.3 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4851425333139638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4851425333139638 | validation: 0.45561180889409186]
	TIME [epoch: 12.1 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3233170334499413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3233170334499413 | validation: 0.27081048400182645]
	TIME [epoch: 12.3 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22560367846559629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22560367846559629 | validation: 0.2302974739847139]
	TIME [epoch: 12.2 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24223389244043333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24223389244043333 | validation: 0.2811503673520333]
	TIME [epoch: 12.4 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21866485995650167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21866485995650167 | validation: 0.18882091716702623]
	TIME [epoch: 12.2 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15181414577351401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15181414577351401 | validation: 0.19616933945899395]
	TIME [epoch: 12.4 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13172719548718742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13172719548718742 | validation: 0.20667442688341559]
	TIME [epoch: 12.5 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.300564899087516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.300564899087516 | validation: 0.42896327039280735]
	TIME [epoch: 12.5 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26992392029680096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26992392029680096 | validation: 0.1950632145786314]
	TIME [epoch: 12.5 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22793871686373826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22793871686373826 | validation: 0.26349470710092365]
	TIME [epoch: 12.5 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16846361119585526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16846361119585526 | validation: 0.15405389357099794]
	TIME [epoch: 12.5 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13890525873000453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13890525873000453 | validation: 0.3311992266834339]
	TIME [epoch: 12.5 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1859907165972132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1859907165972132 | validation: 0.21328053262420887]
	TIME [epoch: 12.4 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2808285889976242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2808285889976242 | validation: 0.4770839220245094]
	TIME [epoch: 12.4 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31999592122389864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31999592122389864 | validation: 0.27521089388878717]
	TIME [epoch: 12.2 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2804042886048159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2804042886048159 | validation: 0.185231142861504]
	TIME [epoch: 12.3 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1905036552627686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1905036552627686 | validation: 1.3692665499063166]
	TIME [epoch: 12.4 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3841206105912685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3841206105912685 | validation: 0.5405207743167618]
	TIME [epoch: 12.4 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5642986364330655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5642986364330655 | validation: 0.1948641753239604]
	TIME [epoch: 12.4 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22192406480106933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22192406480106933 | validation: 0.4139939964450057]
	TIME [epoch: 12.3 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2647730811853286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2647730811853286 | validation: 0.16024357168089132]
	TIME [epoch: 12.3 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1933399459871996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1933399459871996 | validation: 0.2018501335762471]
	TIME [epoch: 12.4 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16013372883794433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16013372883794433 | validation: 0.17209945104530944]
	TIME [epoch: 12.4 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14505892493958109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14505892493958109 | validation: 0.17447718378234472]
	TIME [epoch: 12.3 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22855961137218767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22855961137218767 | validation: 0.36766610054244153]
	TIME [epoch: 12.3 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2365633474035717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2365633474035717 | validation: 0.18311717148418383]
	TIME [epoch: 12.3 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21040479234530207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21040479234530207 | validation: 0.29044802593478386]
	TIME [epoch: 12.2 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15859375604253514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15859375604253514 | validation: 0.1535082543616998]
	TIME [epoch: 12.2 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18726898513768342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18726898513768342 | validation: 0.31252383247291127]
	TIME [epoch: 12.2 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16926665437781935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16926665437781935 | validation: 0.15290777402680866]
	TIME [epoch: 12.3 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20185289077272595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20185289077272595 | validation: 0.22322828637356926]
	TIME [epoch: 12.4 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13478701084703607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13478701084703607 | validation: 0.11187944240606083]
	TIME [epoch: 12.2 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274060668330776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1274060668330776 | validation: 0.18572024033486922]
	TIME [epoch: 12.3 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1197590122899998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1197590122899998 | validation: 0.2639566832083133]
	TIME [epoch: 12.4 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1431205235847583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1431205235847583 | validation: 0.2891509804019875]
	TIME [epoch: 12.4 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4171794475707261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4171794475707261 | validation: 0.29737064802554025]
	TIME [epoch: 12.3 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062433978699116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2062433978699116 | validation: 0.2514386716679078]
	TIME [epoch: 12.3 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20288690791157607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20288690791157607 | validation: 0.499377629795743]
	TIME [epoch: 12.3 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5750563022247459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5750563022247459 | validation: 1.2680250265885693]
	TIME [epoch: 12.3 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.002222156468932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.002222156468932 | validation: 0.3518071872499608]
	TIME [epoch: 12.2 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37369836221560293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37369836221560293 | validation: 0.24762900427900153]
	TIME [epoch: 12.4 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2603286230448783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2603286230448783 | validation: 0.2564327731568542]
	TIME [epoch: 12.2 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.198196838362031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.198196838362031 | validation: 0.21786549071021777]
	TIME [epoch: 12.3 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16623011796184187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16623011796184187 | validation: 0.19378325453124778]
	TIME [epoch: 12.3 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13863906181677849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13863906181677849 | validation: 0.13835781344515397]
	TIME [epoch: 12.3 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14248990772793937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14248990772793937 | validation: 0.26989939682868364]
	TIME [epoch: 12.4 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16256592023985392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16256592023985392 | validation: 0.16571820378343582]
	TIME [epoch: 12.4 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20118264240634054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20118264240634054 | validation: 0.3906919500772541]
	TIME [epoch: 12.2 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22066246041098883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22066246041098883 | validation: 0.1611877600260884]
	TIME [epoch: 12.3 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20083125359535836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20083125359535836 | validation: 0.2195087460329206]
	TIME [epoch: 12.2 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13678422288478423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13678422288478423 | validation: 0.12919777777552263]
	TIME [epoch: 12.3 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10751728411131918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10751728411131918 | validation: 0.11223248892660297]
	TIME [epoch: 12.3 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10751270685872072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10751270685872072 | validation: 0.21621115164679106]
	TIME [epoch: 12.3 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13023440837168274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13023440837168274 | validation: 0.23387133301971508]
	TIME [epoch: 12.3 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3046228729674791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3046228729674791 | validation: 0.6661889405304351]
	TIME [epoch: 12.3 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5153096376039734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5153096376039734 | validation: 0.29163004312918234]
	TIME [epoch: 12.4 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1787964877677178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1787964877677178 | validation: 0.17968344502365016]
	TIME [epoch: 12.4 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26515780601289135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26515780601289135 | validation: 0.2518747797128484]
	TIME [epoch: 12.4 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18780556169979068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18780556169979068 | validation: 0.1759337674743788]
	TIME [epoch: 12.4 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1707155798445857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1707155798445857 | validation: 0.1949411216456847]
	TIME [epoch: 12.4 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11883312466049552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11883312466049552 | validation: 0.1389294532966063]
	TIME [epoch: 12.4 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1369153564298266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1369153564298266 | validation: 0.1942271512575753]
	TIME [epoch: 12.4 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11954860716357675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11954860716357675 | validation: 0.6606857205192931]
	TIME [epoch: 12.4 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5089458191146471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5089458191146471 | validation: 0.22747604581006886]
	TIME [epoch: 12.4 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21352349633596604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21352349633596604 | validation: 0.1288399710635656]
	TIME [epoch: 12.4 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21380282462065098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21380282462065098 | validation: 0.30783235021736266]
	TIME [epoch: 12.4 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21139599421243563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21139599421243563 | validation: 0.09891518291317654]
	TIME [epoch: 12.4 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13746918638895553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13746918638895553 | validation: 0.3408197072833614]
	TIME [epoch: 12.4 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17469044183242755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17469044183242755 | validation: 0.23251837645107343]
	TIME [epoch: 12.4 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30398825978893884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30398825978893884 | validation: 0.17384869069675402]
	TIME [epoch: 12.3 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12729774834945407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12729774834945407 | validation: 0.09533231354805949]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_736.pth
	Model improved!!!
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10852540787586992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10852540787586992 | validation: 0.17193231665843914]
	TIME [epoch: 12.4 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987914495923435		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0987914495923435 | validation: 0.09115607822719772]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12213273083139535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12213273083139535 | validation: 0.28237390325277384]
	TIME [epoch: 12.4 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1478171910917225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1478171910917225 | validation: 0.1712910502184177]
	TIME [epoch: 12.4 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20274332695177846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20274332695177846 | validation: 0.2799635804644218]
	TIME [epoch: 12.4 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17375220828755272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17375220828755272 | validation: 0.10301612650838948]
	TIME [epoch: 12.4 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11013121584754217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11013121584754217 | validation: 0.86140983249328]
	TIME [epoch: 12.4 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6537338679636494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6537338679636494 | validation: 0.0754546670246358]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13765329328665224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13765329328665224 | validation: 0.14223671877880464]
	TIME [epoch: 12.4 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10287581006649027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10287581006649027 | validation: 0.15104519864669563]
	TIME [epoch: 12.5 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10365159410036771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10365159410036771 | validation: 0.589028074909811]
	TIME [epoch: 12.5 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43115547533397314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43115547533397314 | validation: 0.1502803108639925]
	TIME [epoch: 12.5 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2181177920166183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2181177920166183 | validation: 0.24683905730183722]
	TIME [epoch: 12.4 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15066506806458088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15066506806458088 | validation: 0.08693239985105429]
	TIME [epoch: 12.5 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14116222174048385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14116222174048385 | validation: 0.40036700114421686]
	TIME [epoch: 12.3 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23749358946784505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23749358946784505 | validation: 0.1517227537477963]
	TIME [epoch: 12.5 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22943081845925672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22943081845925672 | validation: 0.2875902657326882]
	TIME [epoch: 12.3 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14485020734294998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14485020734294998 | validation: 0.18665006397328604]
	TIME [epoch: 12.5 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23506614041091578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23506614041091578 | validation: 0.17284413843818858]
	TIME [epoch: 12.3 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12618290900184756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12618290900184756 | validation: 0.2274090591305054]
	TIME [epoch: 12.4 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15053096870718732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15053096870718732 | validation: 0.14493438678278286]
	TIME [epoch: 12.3 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14313484718393732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14313484718393732 | validation: 0.18733352073674273]
	TIME [epoch: 12.4 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11554703868399276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11554703868399276 | validation: 0.10741001463038713]
	TIME [epoch: 12.4 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14414573196582414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14414573196582414 | validation: 0.6103483039964239]
	TIME [epoch: 12.5 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45131402893833467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45131402893833467 | validation: 0.12667592451312076]
	TIME [epoch: 12.3 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12642262990398012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12642262990398012 | validation: 0.11141101650105618]
	TIME [epoch: 12.3 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11606795471873171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11606795471873171 | validation: 0.4606242502990514]
	TIME [epoch: 12.3 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3353187082479214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3353187082479214 | validation: 0.12910863010963416]
	TIME [epoch: 12.4 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15728659938683978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15728659938683978 | validation: 0.6404678950762315]
	TIME [epoch: 12.3 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3951899489790364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3951899489790364 | validation: 0.3583872380618826]
	TIME [epoch: 12.5 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46328479620938795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46328479620938795 | validation: 0.17221281975692668]
	TIME [epoch: 12.3 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18791028767489973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18791028767489973 | validation: 0.6223922980092782]
	TIME [epoch: 12.3 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48130841642486943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48130841642486943 | validation: 0.23600632019679768]
	TIME [epoch: 12.5 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.223602302668642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.223602302668642 | validation: 0.4269134986971535]
	TIME [epoch: 12.4 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22488254771356814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22488254771356814 | validation: 0.427236676470449]
	TIME [epoch: 12.4 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6020979016188271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6020979016188271 | validation: 0.23937245244195263]
	TIME [epoch: 12.3 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35908925760316124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35908925760316124 | validation: 1.278838129355894]
	TIME [epoch: 12.4 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.06981023740004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.06981023740004 | validation: 0.20925692641018015]
	TIME [epoch: 12.4 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.199555370687253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.199555370687253 | validation: 0.2043949133756737]
	TIME [epoch: 12.3 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27890755012399454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27890755012399454 | validation: 0.40792076472451094]
	TIME [epoch: 12.4 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26488921795106224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26488921795106224 | validation: 0.26317726505321465]
	TIME [epoch: 12.4 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24326899284863363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24326899284863363 | validation: 0.28166378705688355]
	TIME [epoch: 12.3 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14168204223658568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14168204223658568 | validation: 0.14998148799788497]
	TIME [epoch: 12.4 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1804007228912293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1804007228912293 | validation: 0.23028157085565823]
	TIME [epoch: 12.3 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13563522747484183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13563522747484183 | validation: 0.15829251434273886]
	TIME [epoch: 12.4 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20398605812041748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20398605812041748 | validation: 0.40689790908896567]
	TIME [epoch: 12.3 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21343071021268883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21343071021268883 | validation: 0.20066103220685472]
	TIME [epoch: 12.4 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2548204679447382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2548204679447382 | validation: 0.22279599553730056]
	TIME [epoch: 12.4 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17221862619075792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17221862619075792 | validation: 0.1275454945954987]
	TIME [epoch: 12.4 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13433193248448028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13433193248448028 | validation: 0.2106288110626653]
	TIME [epoch: 12.4 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13949425623642722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13949425623642722 | validation: 0.11500181570640922]
	TIME [epoch: 12.4 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13597487681412507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13597487681412507 | validation: 0.22697544848409168]
	TIME [epoch: 12.4 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12103612951060218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12103612951060218 | validation: 0.1322969639438479]
	TIME [epoch: 12.4 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16547381795840252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16547381795840252 | validation: 0.22708181054916043]
	TIME [epoch: 12.4 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12889078187148978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12889078187148978 | validation: 0.15976371330338646]
	TIME [epoch: 12.3 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2157859424165588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2157859424165588 | validation: 0.43065730767602733]
	TIME [epoch: 12.4 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23148202180501215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23148202180501215 | validation: 0.1723295332237618]
	TIME [epoch: 12.3 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19603570539122395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19603570539122395 | validation: 0.1529356172707481]
	TIME [epoch: 12.4 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15687795787426706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15687795787426706 | validation: 0.6621094860568579]
	TIME [epoch: 12.5 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5019573680001914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5019573680001914 | validation: 0.34079070806571465]
	TIME [epoch: 12.4 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22875375668593528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22875375668593528 | validation: 0.14334517954167122]
	TIME [epoch: 12.4 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2253396568569151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2253396568569151 | validation: 0.24392715588380254]
	TIME [epoch: 12.5 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1825709417272255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1825709417272255 | validation: 0.16518484932929334]
	TIME [epoch: 12.4 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15115197795916752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15115197795916752 | validation: 0.20069962527038424]
	TIME [epoch: 12.5 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13119254941861744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13119254941861744 | validation: 0.08643233077110422]
	TIME [epoch: 12.4 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10128370196987935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10128370196987935 | validation: 0.28671681351292405]
	TIME [epoch: 12.4 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17047319259581628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17047319259581628 | validation: 0.09098972918804822]
	TIME [epoch: 12.5 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11313872347812552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11313872347812552 | validation: 0.20485046343381275]
	TIME [epoch: 12.5 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11357068156259217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11357068156259217 | validation: 0.13714491596405612]
	TIME [epoch: 12.4 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1853881623506792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1853881623506792 | validation: 0.3501845740973498]
	TIME [epoch: 12.5 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18520676850344067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18520676850344067 | validation: 0.12461318261901733]
	TIME [epoch: 12.4 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16381229292999386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16381229292999386 | validation: 0.23980626213520617]
	TIME [epoch: 12.5 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11618975658599984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11618975658599984 | validation: 0.08809394344381234]
	TIME [epoch: 12.3 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10941848193882446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10941848193882446 | validation: 0.24504655290730737]
	TIME [epoch: 12.4 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11183201435668894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11183201435668894 | validation: 0.09479984948881832]
	TIME [epoch: 12.3 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11214123573079483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11214123573079483 | validation: 0.6648594230955828]
	TIME [epoch: 12.4 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4805566404361549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4805566404361549 | validation: 0.18073071206558622]
	TIME [epoch: 12.3 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16648785307823538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16648785307823538 | validation: 0.2041766266253383]
	TIME [epoch: 12.4 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24964635739538155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24964635739538155 | validation: 0.2534271514566056]
	TIME [epoch: 12.3 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20712850231759736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20712850231759736 | validation: 0.1265134397008764]
	TIME [epoch: 12.4 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1900486447254841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1900486447254841 | validation: 0.4646289348791912]
	TIME [epoch: 12.3 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23508614447300324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23508614447300324 | validation: 0.1910810407968731]
	TIME [epoch: 12.4 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23515339158401882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23515339158401882 | validation: 0.2480966998670785]
	TIME [epoch: 12.3 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17542926411167412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17542926411167412 | validation: 0.10747969494621859]
	TIME [epoch: 12.4 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10794490332365644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10794490332365644 | validation: 0.13441133644812844]
	TIME [epoch: 12.4 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08447346736561809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08447346736561809 | validation: 0.07513340905727756]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_822.pth
	Model improved!!!
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369881097648575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08369881097648575 | validation: 0.24751146512994604]
	TIME [epoch: 12.4 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10747486869062772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10747486869062772 | validation: 0.1265019355319795]
	TIME [epoch: 12.4 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16977606876882761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16977606876882761 | validation: 0.38959062305588876]
	TIME [epoch: 12.4 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18634573945718524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18634573945718524 | validation: 0.14520509319556757]
	TIME [epoch: 12.5 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19931016169632737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19931016169632737 | validation: 0.2718401517552219]
	TIME [epoch: 12.5 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13538538730028785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13538538730028785 | validation: 0.09432837024634758]
	TIME [epoch: 12.4 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11511084515745493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11511084515745493 | validation: 0.216483248718359]
	TIME [epoch: 12.5 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09920878109405944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09920878109405944 | validation: 0.08765934575998191]
	TIME [epoch: 12.4 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10807914726510048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10807914726510048 | validation: 0.2735735131508545]
	TIME [epoch: 12.5 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12349726982983023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12349726982983023 | validation: 0.09036245544830787]
	TIME [epoch: 12.4 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13170689262971141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13170689262971141 | validation: 0.25551960701537196]
	TIME [epoch: 12.5 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10798773338732474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10798773338732474 | validation: 0.07459820566138058]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_834.pth
	Model improved!!!
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09454245097010056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09454245097010056 | validation: 0.12004411933204949]
	TIME [epoch: 12.4 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07948132915372258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07948132915372258 | validation: 0.08140357901632643]
	TIME [epoch: 12.4 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07734128521289188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07734128521289188 | validation: 0.11905011660354876]
	TIME [epoch: 12.5 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09694942195840764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09694942195840764 | validation: 0.18990096500195972]
	TIME [epoch: 12.4 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16046120911804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16046120911804 | validation: 0.3410065905753008]
	TIME [epoch: 12.4 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2254652102858048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2254652102858048 | validation: 0.18744522323855028]
	TIME [epoch: 12.4 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2644477434559231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2644477434559231 | validation: 0.7908526796975521]
	TIME [epoch: 12.4 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205429117789157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205429117789157 | validation: 0.32139566904787276]
	TIME [epoch: 12.4 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1581081330976456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1581081330976456 | validation: 0.2257784099230234]
	TIME [epoch: 12.3 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28476893113914997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28476893113914997 | validation: 0.3043596646668325]
	TIME [epoch: 12.4 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15670468567542942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15670468567542942 | validation: 0.15286214087008668]
	TIME [epoch: 12.4 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16488590441489856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16488590441489856 | validation: 0.10206241292993981]
	TIME [epoch: 12.3 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10019184689324384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10019184689324384 | validation: 0.12555597163105278]
	TIME [epoch: 12.4 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08368507895316958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08368507895316958 | validation: 0.49428152472655323]
	TIME [epoch: 12.3 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2573747238110194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2573747238110194 | validation: 0.10459730825269316]
	TIME [epoch: 12.4 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056967980694272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056967980694272 | validation: 0.18721992047550817]
	TIME [epoch: 12.3 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13813682278781414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13813682278781414 | validation: 0.09415557558876374]
	TIME [epoch: 12.3 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10226461113533039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10226461113533039 | validation: 0.25761304496735354]
	TIME [epoch: 12.3 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1116512289388659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1116512289388659 | validation: 0.12334560191703724]
	TIME [epoch: 12.3 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1852237370430065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1852237370430065 | validation: 0.26032805271807025]
	TIME [epoch: 12.3 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11827011056988099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11827011056988099 | validation: 0.06597794627488995]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_855.pth
	Model improved!!!
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08528122951112371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08528122951112371 | validation: 0.16528570747077123]
	TIME [epoch: 12.3 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09219466710499054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09219466710499054 | validation: 0.06186519747615254]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_857.pth
	Model improved!!!
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08562495928224899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08562495928224899 | validation: 0.1789219369760679]
	TIME [epoch: 12.3 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362594432377544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08362594432377544 | validation: 0.053531388145663675]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_859.pth
	Model improved!!!
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07612715289911914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07612715289911914 | validation: 0.13532043108455866]
	TIME [epoch: 12.3 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07262232369352475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07262232369352475 | validation: 0.05383755739241323]
	TIME [epoch: 12.4 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0710247534635245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0710247534635245 | validation: 0.23489479821301418]
	TIME [epoch: 12.4 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10498182332017163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10498182332017163 | validation: 0.23387258015749143]
	TIME [epoch: 12.4 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3081189832324455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3081189832324455 | validation: 0.37077377916545]
	TIME [epoch: 12.4 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17631991079465614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17631991079465614 | validation: 0.10395749851068965]
	TIME [epoch: 12.4 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0894129236647753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0894129236647753 | validation: 0.14973461486573544]
	TIME [epoch: 12.4 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10375346936931149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10375346936931149 | validation: 0.16427589645701599]
	TIME [epoch: 12.4 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1506012577960678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1506012577960678 | validation: 0.5015502688847022]
	TIME [epoch: 12.4 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6598588441014941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6598588441014941 | validation: 0.17030107116930712]
	TIME [epoch: 12.4 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24455269072981772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24455269072981772 | validation: 0.1620004074234677]
	TIME [epoch: 12.4 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20348014959261157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20348014959261157 | validation: 0.16664565599479475]
	TIME [epoch: 12.4 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14189516659838408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14189516659838408 | validation: 0.4280128412622384]
	TIME [epoch: 12.3 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2810693664045476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2810693664045476 | validation: 0.2636816826563602]
	TIME [epoch: 12.4 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27495892241920844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27495892241920844 | validation: 0.16277300478863965]
	TIME [epoch: 12.3 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10295150707299719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10295150707299719 | validation: 0.08692398016215708]
	TIME [epoch: 12.4 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11984178134231722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11984178134231722 | validation: 0.31392385166481673]
	TIME [epoch: 12.3 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15519339054538375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15519339054538375 | validation: 0.0918681888164021]
	TIME [epoch: 12.4 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12594307030706756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12594307030706756 | validation: 0.1571303802216642]
	TIME [epoch: 12.2 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1023806287643112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1023806287643112 | validation: 0.07425907404858449]
	TIME [epoch: 12.4 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10480440653468882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10480440653468882 | validation: 0.15569993216245956]
	TIME [epoch: 12.2 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207859713077158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07207859713077158 | validation: 0.0692905562300386]
	TIME [epoch: 12.4 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07489549186427465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07489549186427465 | validation: 0.15197461294526046]
	TIME [epoch: 12.3 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07818139745559335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07818139745559335 | validation: 0.06446013623426215]
	TIME [epoch: 12.4 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770007640119247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07770007640119247 | validation: 0.17365912724038604]
	TIME [epoch: 12.2 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09298357004352205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09298357004352205 | validation: 0.08723085055215443]
	TIME [epoch: 12.4 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10663011788537047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10663011788537047 | validation: 0.23995298589336367]
	TIME [epoch: 12.3 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11234663886723553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11234663886723553 | validation: 0.08669543935307969]
	TIME [epoch: 12.4 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.121906031930097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.121906031930097 | validation: 0.2650174034596182]
	TIME [epoch: 12.3 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10348641429697032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10348641429697032 | validation: 0.07444652926629072]
	TIME [epoch: 12.4 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09721847738333232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09721847738333232 | validation: 0.12012278443086058]
	TIME [epoch: 12.2 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11827838132500607		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11827838132500607 | validation: 0.19053135505078841]
	TIME [epoch: 12.4 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13056508417408366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13056508417408366 | validation: 0.07925145629139735]
	TIME [epoch: 12.2 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09144642577415116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09144642577415116 | validation: 0.06601134423428498]
	TIME [epoch: 12.4 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05533827558940722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05533827558940722 | validation: 0.05583053083673393]
	TIME [epoch: 12.4 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05812123863451791		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05812123863451791 | validation: 0.07077666787934168]
	TIME [epoch: 12.4 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07202617187782298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07202617187782298 | validation: 0.1757104383323613]
	TIME [epoch: 12.4 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09135238807512178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09135238807512178 | validation: 0.13804773147929428]
	TIME [epoch: 12.4 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780873394732017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780873394732017 | validation: 0.7467007753877335]
	TIME [epoch: 12.4 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.607691756089927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.607691756089927 | validation: 0.45829664905424716]
	TIME [epoch: 12.4 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3030036133238433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3030036133238433 | validation: 0.186852337727795]
	TIME [epoch: 12.4 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21818508065823605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21818508065823605 | validation: 0.17898220882584562]
	TIME [epoch: 12.4 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2178239505377801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2178239505377801 | validation: 0.31476776643846277]
	TIME [epoch: 12.3 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26145396543843563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26145396543843563 | validation: 0.1432903280483486]
	TIME [epoch: 12.4 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10990204293131742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10990204293131742 | validation: 0.16921596946265188]
	TIME [epoch: 12.3 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1013924678188731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1013924678188731 | validation: 0.0879515759680187]
	TIME [epoch: 12.4 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06712400257481789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06712400257481789 | validation: 0.047533526910270296]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_906.pth
	Model improved!!!
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634821989734402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0634821989734402 | validation: 0.058063008628618164]
	TIME [epoch: 12.4 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058637077994381614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058637077994381614 | validation: 0.05610463546847853]
	TIME [epoch: 12.4 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07345007771272828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07345007771272828 | validation: 0.20994703404211812]
	TIME [epoch: 12.4 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10253096782263685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10253096782263685 | validation: 0.06266193388293571]
	TIME [epoch: 12.4 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07872159463386419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07872159463386419 | validation: 0.14980425887934043]
	TIME [epoch: 12.5 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07845089463521812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07845089463521812 | validation: 0.05923241989471543]
	TIME [epoch: 12.4 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06618888560991903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06618888560991903 | validation: 0.05892065248919494]
	TIME [epoch: 12.4 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06715387840557707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06715387840557707 | validation: 0.09676248739539672]
	TIME [epoch: 12.4 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06577156670267569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06577156670267569 | validation: 0.07388640041470065]
	TIME [epoch: 12.4 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06467381696425627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06467381696425627 | validation: 0.19833841103698807]
	TIME [epoch: 12.4 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688214814425613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1688214814425613 | validation: 0.20661904104972417]
	TIME [epoch: 12.4 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132234635253143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1132234635253143 | validation: 0.08307679895166052]
	TIME [epoch: 12.4 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11556639095785913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11556639095785913 | validation: 0.46889586431156477]
	TIME [epoch: 12.4 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3050735561436173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3050735561436173 | validation: 0.17436375769371795]
	TIME [epoch: 12.4 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08626350274998827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08626350274998827 | validation: 0.14408188357330556]
	TIME [epoch: 12.4 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18436962877457214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18436962877457214 | validation: 0.6793887574227689]
	TIME [epoch: 12.4 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4756132087042478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4756132087042478 | validation: 0.15588093427681568]
	TIME [epoch: 12.4 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22239639849009757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22239639849009757 | validation: 0.36674524539526954]
	TIME [epoch: 12.4 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3023197978514245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3023197978514245 | validation: 0.1303211851585089]
	TIME [epoch: 12.4 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14507550002438688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14507550002438688 | validation: 0.15873933299024715]
	TIME [epoch: 12.4 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.121846710689453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.121846710689453 | validation: 0.20191118732758406]
	TIME [epoch: 12.4 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13091213585714911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13091213585714911 | validation: 0.07466924397791726]
	TIME [epoch: 12.4 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07754649119697618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07754649119697618 | validation: 0.07578404005450773]
	TIME [epoch: 12.4 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06745105746589893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06745105746589893 | validation: 0.09068678985327078]
	TIME [epoch: 12.4 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06355723403801124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06355723403801124 | validation: 0.05140682778654962]
	TIME [epoch: 12.4 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06379612088358169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06379612088358169 | validation: 0.33481965930301105]
	TIME [epoch: 12.4 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2071442607049689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2071442607049689 | validation: 0.11816033722450661]
	TIME [epoch: 12.4 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13117930985874549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13117930985874549 | validation: 0.1479297559944394]
	TIME [epoch: 12.4 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08856175804673284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08856175804673284 | validation: 0.07069091695595525]
	TIME [epoch: 12.4 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07291530031607024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07291530031607024 | validation: 0.06424282930841212]
	TIME [epoch: 12.4 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056214500755925316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056214500755925316 | validation: 0.038207811134616125]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_937.pth
	Model improved!!!
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04838422517698554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04838422517698554 | validation: 0.0640417024706754]
	TIME [epoch: 12.4 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05010732597428191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05010732597428191 | validation: 0.04496803937295997]
	TIME [epoch: 12.4 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05438549999965651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05438549999965651 | validation: 0.08553686686207129]
	TIME [epoch: 12.4 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0638912935925615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0638912935925615 | validation: 0.06395522210078801]
	TIME [epoch: 12.4 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0634576963540692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0634576963540692 | validation: 0.08879350131192729]
	TIME [epoch: 12.4 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06506110608943899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06506110608943899 | validation: 0.0752251842068824]
	TIME [epoch: 12.4 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07425785819476273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07425785819476273 | validation: 0.08432523556242932]
	TIME [epoch: 12.4 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08866909563206035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08866909563206035 | validation: 0.10110831957117868]
	TIME [epoch: 12.4 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09049125578995841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09049125578995841 | validation: 0.07476452267256109]
	TIME [epoch: 12.4 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0903056657318864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0903056657318864 | validation: 0.3684322750217761]
	TIME [epoch: 12.4 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15081024548445326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15081024548445326 | validation: 0.08572693427155247]
	TIME [epoch: 12.3 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09600485735150999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09600485735150999 | validation: 0.08753535320943627]
	TIME [epoch: 12.4 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08871040408733027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08871040408733027 | validation: 0.05679470056610528]
	TIME [epoch: 12.4 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06077172979722195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06077172979722195 | validation: 0.05969216171196184]
	TIME [epoch: 12.3 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060287300693973846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060287300693973846 | validation: 0.041469163418834276]
	TIME [epoch: 12.4 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05239871012090107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05239871012090107 | validation: 0.0694160904704643]
	TIME [epoch: 12.4 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611634535336124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05611634535336124 | validation: 0.43797514193108444]
	TIME [epoch: 12.4 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21373437978325516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21373437978325516 | validation: 0.11576868329290463]
	TIME [epoch: 12.4 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12795643965009934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12795643965009934 | validation: 0.12452044536786731]
	TIME [epoch: 12.4 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12190115518552137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12190115518552137 | validation: 0.07280626835141951]
	TIME [epoch: 12.4 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10474442541730743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10474442541730743 | validation: 0.09409036144173304]
	TIME [epoch: 12.3 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07727087010211392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07727087010211392 | validation: 0.07581295333438379]
	TIME [epoch: 12.4 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06073062931925622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06073062931925622 | validation: 0.08801017183541064]
	TIME [epoch: 12.4 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05986153357995651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05986153357995651 | validation: 0.04597521046251055]
	TIME [epoch: 12.3 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059595014675817584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059595014675817584 | validation: 0.06320461920752427]
	TIME [epoch: 12.3 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059732389313088774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059732389313088774 | validation: 0.04488201364394571]
	TIME [epoch: 12.2 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05378458237191589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05378458237191589 | validation: 0.0862336093477816]
	TIME [epoch: 12.3 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06382754369847977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06382754369847977 | validation: 0.07460778547751962]
	TIME [epoch: 12.2 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08681642911919965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08681642911919965 | validation: 0.6055271202277986]
	TIME [epoch: 12.3 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33110108181834463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33110108181834463 | validation: 0.13172533410688195]
	TIME [epoch: 12.3 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14655760078919308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14655760078919308 | validation: 2.0912840300101045]
	TIME [epoch: 12.3 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.706064153007248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.706064153007248 | validation: 0.1445335257953851]
	TIME [epoch: 12.3 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14552207800942654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14552207800942654 | validation: 0.08249268878237566]
	TIME [epoch: 12.3 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08172119040814903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08172119040814903 | validation: 0.11747813723428183]
	TIME [epoch: 12.3 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10483417853527788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10483417853527788 | validation: 0.16920836537533088]
	TIME [epoch: 12.4 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13840617296028843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13840617296028843 | validation: 0.3745780094651641]
	TIME [epoch: 12.3 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18892870287815577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18892870287815577 | validation: 0.11332049445903816]
	TIME [epoch: 12.4 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10848049118742599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10848049118742599 | validation: 0.04765806559187257]
	TIME [epoch: 12.4 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06042879678958413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06042879678958413 | validation: 0.08503347799740332]
	TIME [epoch: 12.4 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06666952488508833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06666952488508833 | validation: 0.04848438783507664]
	TIME [epoch: 12.4 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05225523039852029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05225523039852029 | validation: 0.036517397877212775]
	TIME [epoch: 12.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_978.pth
	Model improved!!!
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04345722009398042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04345722009398042 | validation: 0.034833350130767285]
	TIME [epoch: 12.4 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_979.pth
	Model improved!!!
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038909104172300195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038909104172300195 | validation: 0.04001219830127957]
	TIME [epoch: 12.4 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04270549436135413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04270549436135413 | validation: 0.04865045192499093]
	TIME [epoch: 12.2 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05594014419172655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05594014419172655 | validation: 0.07201983141656104]
	TIME [epoch: 12.4 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0687251898505965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0687251898505965 | validation: 0.05871420035126068]
	TIME [epoch: 12.3 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06540673111611993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06540673111611993 | validation: 0.08169184270513347]
	TIME [epoch: 12.4 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0635850730179724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0635850730179724 | validation: 0.0538759927231047]
	TIME [epoch: 12.4 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05885484029242765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05885484029242765 | validation: 0.13427525479216146]
	TIME [epoch: 12.4 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07048741161878389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07048741161878389 | validation: 0.07060970374328783]
	TIME [epoch: 12.4 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07289825323732733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07289825323732733 | validation: 0.14172632498537915]
	TIME [epoch: 12.4 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07952000712558505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07952000712558505 | validation: 0.11202321296286524]
	TIME [epoch: 12.4 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10584333311546186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10584333311546186 | validation: 0.0901318289460251]
	TIME [epoch: 12.4 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07295222008812906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07295222008812906 | validation: 0.0960488925984749]
	TIME [epoch: 12.4 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10191085279179866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10191085279179866 | validation: 0.07814827976891452]
	TIME [epoch: 12.4 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0630485082712168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0630485082712168 | validation: 0.04017342951489662]
	TIME [epoch: 12.4 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827617095253794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04827617095253794 | validation: 0.10064843314061199]
	TIME [epoch: 12.4 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08130783598395375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08130783598395375 | validation: 0.05994245291609204]
	TIME [epoch: 12.4 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0613962086439332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0613962086439332 | validation: 0.053850565422633]
	TIME [epoch: 12.4 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06483413924423442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06483413924423442 | validation: 0.08556809388512861]
	TIME [epoch: 12.4 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06923126347410337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06923126347410337 | validation: 0.0514556196179491]
	TIME [epoch: 12.4 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05399112488441122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05399112488441122 | validation: 0.06287424116996175]
	TIME [epoch: 12.4 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07262397337496024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07262397337496024 | validation: 0.206573661786645]
	TIME [epoch: 12.4 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09869485321003695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09869485321003695 | validation: 0.07968057014699893]
	TIME [epoch: 193 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10254981549029314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10254981549029314 | validation: 0.10706158988907487]
	TIME [epoch: 25.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05436119554285448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05436119554285448 | validation: 0.04300313088374428]
	TIME [epoch: 25.4 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04202359286122775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04202359286122775 | validation: 0.03425139136074773]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1004.pth
	Model improved!!!
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047960532951366965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047960532951366965 | validation: 0.05612983351100958]
	TIME [epoch: 25.2 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047115992957636244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047115992957636244 | validation: 0.07442062854458578]
	TIME [epoch: 25.4 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772085381628404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06772085381628404 | validation: 0.0638695252395728]
	TIME [epoch: 25.4 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08071451314424972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08071451314424972 | validation: 0.06330623669614233]
	TIME [epoch: 25.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947931109733259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947931109733259 | validation: 0.06578761917922495]
	TIME [epoch: 25.3 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06896111539560433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06896111539560433 | validation: 0.048377418168822656]
	TIME [epoch: 25.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05279532139552552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05279532139552552 | validation: 0.03941488268802085]
	TIME [epoch: 25.4 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03748890127279301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03748890127279301 | validation: 0.04626272802996843]
	TIME [epoch: 25.4 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05596678958907671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05596678958907671 | validation: 0.10123771329307957]
	TIME [epoch: 25.2 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08662603510685626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08662603510685626 | validation: 0.0893511365141467]
	TIME [epoch: 25.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09670632967430384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09670632967430384 | validation: 0.05829192624663098]
	TIME [epoch: 25.2 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05504849035914322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05504849035914322 | validation: 0.027105557744167542]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1016.pth
	Model improved!!!
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03501526239499381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03501526239499381 | validation: 0.032180990509707644]
	TIME [epoch: 25.2 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04125919244738555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04125919244738555 | validation: 0.088420218829113]
	TIME [epoch: 25.3 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07164007288421276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07164007288421276 | validation: 0.149227054051616]
	TIME [epoch: 25.3 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15895197845525486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15895197845525486 | validation: 0.28125648786236573]
	TIME [epoch: 25.3 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13793162179778784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13793162179778784 | validation: 0.050685381526266615]
	TIME [epoch: 25.4 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058582961925717277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058582961925717277 | validation: 0.05576324761511602]
	TIME [epoch: 25.3 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09282935318239399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09282935318239399 | validation: 0.8574448283666736]
	TIME [epoch: 25.6 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5185746368221712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5185746368221712 | validation: 0.398247297370085]
	TIME [epoch: 25.2 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48076549629895765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48076549629895765 | validation: 0.12262615276067756]
	TIME [epoch: 25.3 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12542398205495459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12542398205495459 | validation: 0.14155430480154793]
	TIME [epoch: 25.1 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06962277433196891		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06962277433196891 | validation: 0.06644788935108373]
	TIME [epoch: 25.3 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07047632306618383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07047632306618383 | validation: 0.04415349868792983]
	TIME [epoch: 25.4 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047647911920662964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047647911920662964 | validation: 0.036331164086387306]
	TIME [epoch: 25.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046301488797914674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046301488797914674 | validation: 0.03714615718994046]
	TIME [epoch: 25.3 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042632273178745204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042632273178745204 | validation: 0.05383248479341396]
	TIME [epoch: 25.3 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04563253632819629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04563253632819629 | validation: 0.058799400230381174]
	TIME [epoch: 25.2 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377224706042279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377224706042279 | validation: 0.028501029611018094]
	TIME [epoch: 25.5 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035398049885690376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035398049885690376 | validation: 0.02702101063040646]
	TIME [epoch: 25.3 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1034.pth
	Model improved!!!
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616601397833468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03616601397833468 | validation: 0.0255995862256735]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1035.pth
	Model improved!!!
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03590373157522003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03590373157522003 | validation: 0.039627032186842506]
	TIME [epoch: 25.3 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043928727906866055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043928727906866055 | validation: 0.055354999197952705]
	TIME [epoch: 25.2 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06002736380231889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06002736380231889 | validation: 0.0902391043039991]
	TIME [epoch: 25.3 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0806394386323608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0806394386323608 | validation: 0.08975447239934115]
	TIME [epoch: 25.3 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887136612702238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0887136612702238 | validation: 0.0660293088845731]
	TIME [epoch: 25.3 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06248473215942031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06248473215942031 | validation: 0.03270239700432106]
	TIME [epoch: 25.3 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04513081315724321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04513081315724321 | validation: 0.036626826794792344]
	TIME [epoch: 25.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03553997693970525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03553997693970525 | validation: 0.031093797417612513]
	TIME [epoch: 25.4 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687787755790844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03687787755790844 | validation: 0.049286271641671875]
	TIME [epoch: 25.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945342982880813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04945342982880813 | validation: 0.086817386682525]
	TIME [epoch: 25.4 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08845667174508957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08845667174508957 | validation: 0.20782228782030931]
	TIME [epoch: 25.3 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913810021087105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11913810021087105 | validation: 0.08657456920089808]
	TIME [epoch: 25.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08055216614003133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08055216614003133 | validation: 0.7770655975374887]
	TIME [epoch: 25.4 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590694026111519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.590694026111519 | validation: 0.47216805513137194]
	TIME [epoch: 25.4 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24919398838036544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24919398838036544 | validation: 0.29990980212883933]
	TIME [epoch: 25.2 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3327604243205067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3327604243205067 | validation: 0.106467505984961]
	TIME [epoch: 25.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1121782689978995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1121782689978995 | validation: 0.1945628795416831]
	TIME [epoch: 25.2 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17743021890026484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17743021890026484 | validation: 0.37208453555763765]
	TIME [epoch: 25.5 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2875008144503202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2875008144503202 | validation: 0.22779890346517942]
	TIME [epoch: 25.4 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1978789708957283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1978789708957283 | validation: 0.08695130990060064]
	TIME [epoch: 25.3 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06322932747735459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06322932747735459 | validation: 0.06326783756588558]
	TIME [epoch: 25.4 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06294119043928469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06294119043928469 | validation: 0.051343029114633926]
	TIME [epoch: 25.3 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692060933851935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04692060933851935 | validation: 0.054603112735000014]
	TIME [epoch: 25.4 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652038903060589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06652038903060589 | validation: 0.05131883384181948]
	TIME [epoch: 25.2 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04881012894506866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04881012894506866 | validation: 0.05338871131219049]
	TIME [epoch: 25.4 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05201703368385997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05201703368385997 | validation: 0.03306691042914774]
	TIME [epoch: 25.3 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0428530197667376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0428530197667376 | validation: 0.02833772149325884]
	TIME [epoch: 25.2 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03813261843976764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03813261843976764 | validation: 0.0333018878993749]
	TIME [epoch: 25.4 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033679565724062756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033679565724062756 | validation: 0.03594115898323091]
	TIME [epoch: 25.1 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04081136768802345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04081136768802345 | validation: 0.038302530717700935]
	TIME [epoch: 25.4 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624829818972034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04624829818972034 | validation: 0.046763706009678475]
	TIME [epoch: 25.1 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04893325164648097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04893325164648097 | validation: 0.044112956366843294]
	TIME [epoch: 25.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050976009142717625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050976009142717625 | validation: 0.052616176177090516]
	TIME [epoch: 25.4 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05830415928990619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05830415928990619 | validation: 0.06859067899914101]
	TIME [epoch: 25.4 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06913239088124123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06913239088124123 | validation: 0.055015864061134005]
	TIME [epoch: 25.2 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056067348065296126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056067348065296126 | validation: 0.04380251890898823]
	TIME [epoch: 25.4 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04685501689545606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04685501689545606 | validation: 0.06461032499795738]
	TIME [epoch: 25.4 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760089472374981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04760089472374981 | validation: 0.0443511248287779]
	TIME [epoch: 25.4 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058059826249559854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058059826249559854 | validation: 0.043091083436156113]
	TIME [epoch: 25.4 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05061730800719173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05061730800719173 | validation: 0.05288556787948928]
	TIME [epoch: 25.4 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437467860975858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04437467860975858 | validation: 0.05318980035316299]
	TIME [epoch: 25.5 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0632233832146257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0632233832146257 | validation: 0.04094270315117793]
	TIME [epoch: 25.4 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04971086937620603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04971086937620603 | validation: 0.07751812099217918]
	TIME [epoch: 25.4 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056747010516253873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056747010516253873 | validation: 0.041609708509950376]
	TIME [epoch: 25.4 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04472037839160187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04472037839160187 | validation: 0.04100278368113998]
	TIME [epoch: 25.3 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416906068227197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04416906068227197 | validation: 0.053825845545342955]
	TIME [epoch: 25.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0507398732516498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0507398732516498 | validation: 0.07155989414632559]
	TIME [epoch: 25.5 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08467053628412517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08467053628412517 | validation: 0.17899712201379245]
	TIME [epoch: 25.4 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16708855679369233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16708855679369233 | validation: 0.04449982831760856]
	TIME [epoch: 25.4 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061654301503687864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061654301503687864 | validation: 0.03388550574572141]
	TIME [epoch: 25.4 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05381674783558021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05381674783558021 | validation: 0.062058667953232886]
	TIME [epoch: 25.3 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05290965608477837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05290965608477837 | validation: 0.043941539298057754]
	TIME [epoch: 25.5 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04441562966326838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04441562966326838 | validation: 0.024794960071290173]
	TIME [epoch: 25.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1088.pth
	Model improved!!!
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040942331444844715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040942331444844715 | validation: 0.03798362376192134]
	TIME [epoch: 25.4 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04486617905249605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04486617905249605 | validation: 0.06371109970852866]
	TIME [epoch: 25.2 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05674936452332789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05674936452332789 | validation: 0.072242766905499]
	TIME [epoch: 25.4 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833932087077675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06833932087077675 | validation: 0.08872068971810086]
	TIME [epoch: 25.2 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061653670800268685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061653670800268685 | validation: 0.04628350174809964]
	TIME [epoch: 25.4 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059741962569979865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059741962569979865 | validation: 0.11129203268957127]
	TIME [epoch: 25.2 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0848609564291241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0848609564291241 | validation: 0.038009089550029344]
	TIME [epoch: 25.4 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837391208231361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04837391208231361 | validation: 0.028739943062247565]
	TIME [epoch: 25.3 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03583502955188198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03583502955188198 | validation: 0.016764694092705357]
	TIME [epoch: 25.2 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1097.pth
	Model improved!!!
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024708478591884877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024708478591884877 | validation: 0.021827007339552465]
	TIME [epoch: 25.4 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03203351151017857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03203351151017857 | validation: 0.036892940092833885]
	TIME [epoch: 25.2 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0490355721115613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0490355721115613 | validation: 0.07340991543129499]
	TIME [epoch: 25.3 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958949704715071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07958949704715071 | validation: 0.03540595077346127]
	TIME [epoch: 25.2 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04804744033261981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04804744033261981 | validation: 0.03207059129749016]
	TIME [epoch: 25.2 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03943957171583213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03943957171583213 | validation: 0.04454781579592612]
	TIME [epoch: 25.3 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038484826658250926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038484826658250926 | validation: 0.0655725739697606]
	TIME [epoch: 25.2 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570663496986934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06570663496986934 | validation: 0.11150041275441742]
	TIME [epoch: 25.2 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07914267982036638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07914267982036638 | validation: 0.07786430768248971]
	TIME [epoch: 25.4 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08246608326372452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08246608326372452 | validation: 0.14020969747040798]
	TIME [epoch: 25.3 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06772705771138454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06772705771138454 | validation: 0.05572463993413436]
	TIME [epoch: 25.4 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05731571216148277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05731571216148277 | validation: 0.03772955784328994]
	TIME [epoch: 25.4 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042422521357752284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042422521357752284 | validation: 0.024239018363094125]
	TIME [epoch: 25.4 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03686869362980686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03686869362980686 | validation: 0.034786704450333876]
	TIME [epoch: 25.2 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106033663191825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04106033663191825 | validation: 0.03579804349555632]
	TIME [epoch: 25.4 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05032287891164491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05032287891164491 | validation: 0.26052064992851015]
	TIME [epoch: 25.3 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25800040269138286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25800040269138286 | validation: 0.1746637038233817]
	TIME [epoch: 25.4 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1322838468610163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1322838468610163 | validation: 0.33486430830378666]
	TIME [epoch: 25.3 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1622106810427446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1622106810427446 | validation: 0.12998348589230718]
	TIME [epoch: 25.3 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12323219288356113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12323219288356113 | validation: 0.07125198119048458]
	TIME [epoch: 25.2 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07565053892087968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07565053892087968 | validation: 0.056193861026248376]
	TIME [epoch: 25.4 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07669994731809297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07669994731809297 | validation: 0.18657185464286088]
	TIME [epoch: 25.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10835603504983243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10835603504983243 | validation: 0.05564425200411132]
	TIME [epoch: 25.4 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05598995018290613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05598995018290613 | validation: 0.0621201785015724]
	TIME [epoch: 25.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804627684918204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05804627684918204 | validation: 0.05348519992605995]
	TIME [epoch: 25.4 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611163769598742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05611163769598742 | validation: 0.09053269599663755]
	TIME [epoch: 25.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053642920481269576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053642920481269576 | validation: 0.03319438469296054]
	TIME [epoch: 25.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319892653237059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05319892653237059 | validation: 0.04511941742320024]
	TIME [epoch: 25.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04984804467570673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04984804467570673 | validation: 0.031705989130709565]
	TIME [epoch: 25.4 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0411602540132024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0411602540132024 | validation: 0.032828102416054716]
	TIME [epoch: 25.5 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418258226002169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418258226002169 | validation: 0.03547333532451842]
	TIME [epoch: 25.3 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04160594464336236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04160594464336236 | validation: 0.0569082144883507]
	TIME [epoch: 25.4 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06265326338206939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06265326338206939 | validation: 0.6113070920257808]
	TIME [epoch: 25.4 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5030351269713258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5030351269713258 | validation: 0.28351541835190763]
	TIME [epoch: 25.5 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2701040734770391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2701040734770391 | validation: 0.11589544573485536]
	TIME [epoch: 25.4 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12775235638975188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12775235638975188 | validation: 0.07431192629181965]
	TIME [epoch: 25.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08603304373048225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08603304373048225 | validation: 0.05434426178563844]
	TIME [epoch: 25.4 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06547674030166716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06547674030166716 | validation: 0.051053314929626364]
	TIME [epoch: 25.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0461854410648255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0461854410648255 | validation: 0.03867860601232272]
	TIME [epoch: 25.3 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047821476430168194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047821476430168194 | validation: 0.039881981187730946]
	TIME [epoch: 25.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04036339732611064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04036339732611064 | validation: 0.025323699251167064]
	TIME [epoch: 25.3 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03844424636634858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03844424636634858 | validation: 0.04789123385779201]
	TIME [epoch: 25.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04985974275923942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04985974275923942 | validation: 0.04495192325326741]
	TIME [epoch: 25.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07025050992143911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07025050992143911 | validation: 0.056279655771792995]
	TIME [epoch: 25.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05620850082990894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05620850082990894 | validation: 0.03998412429062609]
	TIME [epoch: 25.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04145162111942103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04145162111942103 | validation: 0.02774231155665127]
	TIME [epoch: 25.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03481366421338157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03481366421338157 | validation: 0.03811201339024211]
	TIME [epoch: 25.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028972016672024096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028972016672024096 | validation: 0.024611159708961397]
	TIME [epoch: 25.4 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027141474793176714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027141474793176714 | validation: 0.023769958022054352]
	TIME [epoch: 25.4 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031395125095539635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031395125095539635 | validation: 0.030278443787394263]
	TIME [epoch: 25.4 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404386490580201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0404386490580201 | validation: 0.042051417151225005]
	TIME [epoch: 25.4 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04816621587270864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04816621587270864 | validation: 0.04980923466228346]
	TIME [epoch: 25.4 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056446870120388966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056446870120388966 | validation: 0.06402242108951937]
	TIME [epoch: 25.3 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06579532350689439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06579532350689439 | validation: 0.07040319214719841]
	TIME [epoch: 25.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09324262459561475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09324262459561475 | validation: 0.05433905123662574]
	TIME [epoch: 25.4 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06285320008295982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06285320008295982 | validation: 0.020362051202753074]
	TIME [epoch: 25.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03226851383571692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03226851383571692 | validation: 0.010836303775484925]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1154.pth
	Model improved!!!
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020980398761887686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020980398761887686 | validation: 0.013641246355962512]
	TIME [epoch: 25.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02058253930918454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02058253930918454 | validation: 0.026647006687443686]
	TIME [epoch: 25.3 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206961875006153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04206961875006153 | validation: 0.041906759102049995]
	TIME [epoch: 25.5 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03803468398367405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03803468398367405 | validation: 0.01878273886053057]
	TIME [epoch: 25.4 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02969896074157349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02969896074157349 | validation: 0.025050737076293052]
	TIME [epoch: 25.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03597108975239507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03597108975239507 | validation: 0.13388982822735135]
	TIME [epoch: 25.3 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693547771264574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0693547771264574 | validation: 0.10861141034328999]
	TIME [epoch: 25.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11766982715760158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11766982715760158 | validation: 0.07051864040418811]
	TIME [epoch: 25.3 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08014003635063205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08014003635063205 | validation: 0.03878046393073392]
	TIME [epoch: 25.3 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753000978178754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03753000978178754 | validation: 0.02982082283857329]
	TIME [epoch: 25.4 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03418586298802849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03418586298802849 | validation: 0.05238006939082023]
	TIME [epoch: 25.3 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05931462895526055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05931462895526055 | validation: 0.08612151798919293]
	TIME [epoch: 25.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09810281985720284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09810281985720284 | validation: 0.03258151824351968]
	TIME [epoch: 25.3 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04161103924095554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04161103924095554 | validation: 0.020779488762808565]
	TIME [epoch: 25.4 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027061542883794438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027061542883794438 | validation: 0.017666380146498895]
	TIME [epoch: 25.3 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02392709772507969		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02392709772507969 | validation: 0.018000378641129158]
	TIME [epoch: 25.4 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028080121189995148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028080121189995148 | validation: 0.026915379143358533]
	TIME [epoch: 25.4 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038235075859393945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038235075859393945 | validation: 0.06261229912030576]
	TIME [epoch: 25.4 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06401200541167967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06401200541167967 | validation: 0.05443924022422356]
	TIME [epoch: 25.4 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06351403278870349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06351403278870349 | validation: 0.256411733892694]
	TIME [epoch: 25.4 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18845060841613734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18845060841613734 | validation: 0.07459558944696541]
	TIME [epoch: 25.5 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07634144265132876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07634144265132876 | validation: 0.08613200352414574]
	TIME [epoch: 25.4 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09616922223231726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09616922223231726 | validation: 0.08867961249758573]
	TIME [epoch: 25.4 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08313790128537099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08313790128537099 | validation: 0.04602812864083418]
	TIME [epoch: 25.4 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112829180955581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05112829180955581 | validation: 0.04596184319370622]
	TIME [epoch: 25.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056762789290789074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056762789290789074 | validation: 0.03173826190758069]
	TIME [epoch: 25.3 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03527484921641499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03527484921641499 | validation: 0.04350224196209438]
	TIME [epoch: 25.4 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04622611837134885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04622611837134885 | validation: 0.04143934573097942]
	TIME [epoch: 25.4 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048261615880020536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048261615880020536 | validation: 0.03628748947373689]
	TIME [epoch: 25.4 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047502950153734645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047502950153734645 | validation: 0.04601663541253882]
	TIME [epoch: 25.3 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03884736870089906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03884736870089906 | validation: 0.07631752512132021]
	TIME [epoch: 25.4 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788853760318155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04788853760318155 | validation: 0.04428504111583675]
	TIME [epoch: 25.4 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0571746846882695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0571746846882695 | validation: 0.06232249559647723]
	TIME [epoch: 25.4 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04363548375582192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04363548375582192 | validation: 0.039569866754118534]
	TIME [epoch: 25.4 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729417699494693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04729417699494693 | validation: 0.07096726998509863]
	TIME [epoch: 25.4 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06528436736521742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06528436736521742 | validation: 0.07122948933295187]
	TIME [epoch: 25.4 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051978589349232676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051978589349232676 | validation: 0.020796681313419288]
	TIME [epoch: 25.4 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02978273306588236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02978273306588236 | validation: 0.0370098947582166]
	TIME [epoch: 25.5 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03510888109782686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03510888109782686 | validation: 0.048184656256856875]
	TIME [epoch: 25.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04490702361295845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04490702361295845 | validation: 0.04530549696481884]
	TIME [epoch: 25.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05383635308586703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05383635308586703 | validation: 0.031355845356919265]
	TIME [epoch: 25.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04085450319552227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04085450319552227 | validation: 0.03541065496543956]
	TIME [epoch: 25.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03833832474417732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03833832474417732 | validation: 0.025042093288696467]
	TIME [epoch: 25.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039227589029309146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039227589029309146 | validation: 0.02999704041340573]
	TIME [epoch: 25.4 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776822157642422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776822157642422 | validation: 0.038992150076609736]
	TIME [epoch: 25.4 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05977017762785335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05977017762785335 | validation: 0.04507839198343133]
	TIME [epoch: 25.4 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049865764835987515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049865764835987515 | validation: 0.029299728861952524]
	TIME [epoch: 25.6 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039131008927568205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039131008927568205 | validation: 0.03395727542522203]
	TIME [epoch: 25.6 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038953637056295606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038953637056295606 | validation: 0.05590356453725581]
	TIME [epoch: 25.6 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05614911564229275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05614911564229275 | validation: 0.043201158271446154]
	TIME [epoch: 25.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052345096869172124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052345096869172124 | validation: 0.03306748260005845]
	TIME [epoch: 25.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032827375453321296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032827375453321296 | validation: 0.028198013160907344]
	TIME [epoch: 25.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416066548197363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03416066548197363 | validation: 0.037674277191493716]
	TIME [epoch: 25.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035631311148950594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035631311148950594 | validation: 0.05343574817482375]
	TIME [epoch: 25.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06303379553306615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06303379553306615 | validation: 0.13891737199948492]
	TIME [epoch: 25.2 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06927580600306099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06927580600306099 | validation: 0.04151471240081877]
	TIME [epoch: 25.3 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041038263151113696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041038263151113696 | validation: 0.041309850467811396]
	TIME [epoch: 25.4 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047667145673174496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047667145673174496 | validation: 0.05480601305206256]
	TIME [epoch: 25.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0595932596714285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0595932596714285 | validation: 0.055335370058273006]
	TIME [epoch: 25.3 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058545771162391576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058545771162391576 | validation: 0.029912116496765617]
	TIME [epoch: 25.1 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564700473165821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03564700473165821 | validation: 0.08647392425041289]
	TIME [epoch: 25.3 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09170137574123159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09170137574123159 | validation: 0.03034024208465468]
	TIME [epoch: 25.2 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04007014658712414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04007014658712414 | validation: 0.07809521900543503]
	TIME [epoch: 25.4 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048644833481181024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048644833481181024 | validation: 0.15789590368120496]
	TIME [epoch: 25.3 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1168789342899235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1168789342899235 | validation: 0.0764438397568053]
	TIME [epoch: 25.3 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09493635317381123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09493635317381123 | validation: 0.03318730904815385]
	TIME [epoch: 25.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03456793404352472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03456793404352472 | validation: 0.09416118074633525]
	TIME [epoch: 25.4 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05287711805462477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05287711805462477 | validation: 0.030094800781016895]
	TIME [epoch: 25.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030434909643920163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030434909643920163 | validation: 0.018095265221260936]
	TIME [epoch: 25.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028580364517552588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028580364517552588 | validation: 0.050130737504539724]
	TIME [epoch: 25.3 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04368492445115561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04368492445115561 | validation: 0.06688932899617551]
	TIME [epoch: 25.4 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06329891425751916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06329891425751916 | validation: 0.052767635119614145]
	TIME [epoch: 25.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05456668165176074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05456668165176074 | validation: 0.06263499004574814]
	TIME [epoch: 25.4 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06986614479903783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06986614479903783 | validation: 0.0571308333387631]
	TIME [epoch: 25.2 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059378689451413544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059378689451413544 | validation: 0.022106520774729046]
	TIME [epoch: 25.4 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03071053601864418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03071053601864418 | validation: 0.02744630873811528]
	TIME [epoch: 25.4 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025191804398190572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025191804398190572 | validation: 0.05008548935028341]
	TIME [epoch: 25.3 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03475181219657393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03475181219657393 | validation: 0.03418758923440721]
	TIME [epoch: 25.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040777566536466046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040777566536466046 | validation: 0.055100845059097917]
	TIME [epoch: 25.4 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057555625211862436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057555625211862436 | validation: 0.0605921643185861]
	TIME [epoch: 25.3 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05791033679851532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05791033679851532 | validation: 0.03096219666431084]
	TIME [epoch: 25.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042570719732510844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042570719732510844 | validation: 0.03229839312931152]
	TIME [epoch: 25.3 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03769202704037784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03769202704037784 | validation: 0.031082271243249582]
	TIME [epoch: 25.4 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03980869894804608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03980869894804608 | validation: 0.04516994421931053]
	TIME [epoch: 25.3 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045477644123510956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045477644123510956 | validation: 0.03442766246979623]
	TIME [epoch: 25.2 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04510676664622918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04510676664622918 | validation: 0.032761095103381925]
	TIME [epoch: 25.4 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034021715284299835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034021715284299835 | validation: 0.01605839346206748]
	TIME [epoch: 25.4 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026432235873830884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026432235873830884 | validation: 0.02502747369180699]
	TIME [epoch: 25.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02936534110281241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02936534110281241 | validation: 0.025798857722407666]
	TIME [epoch: 25.3 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520223802129939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03520223802129939 | validation: 0.05719622925263456]
	TIME [epoch: 25.2 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05922012142090706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05922012142090706 | validation: 0.031545660647970666]
	TIME [epoch: 25.4 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03924347394702096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03924347394702096 | validation: 0.02694661138079211]
	TIME [epoch: 25.2 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03044214976966664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03044214976966664 | validation: 0.054882041030514986]
	TIME [epoch: 25.3 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06536661025924918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06536661025924918 | validation: 0.08990845914710997]
	TIME [epoch: 25.2 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829738158991277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08829738158991277 | validation: 0.08439899852962121]
	TIME [epoch: 25.2 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08433155263173898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08433155263173898 | validation: 0.017712827574434386]
	TIME [epoch: 25 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02402643428623663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02402643428623663 | validation: 0.012623113904731044]
	TIME [epoch: 25.1 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021111251012678346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021111251012678346 | validation: 0.01843497205156627]
	TIME [epoch: 25.2 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02527807613193342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02527807613193342 | validation: 0.03890508227167441]
	TIME [epoch: 25.2 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556770933936383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04556770933936383 | validation: 0.0734431463399675]
	TIME [epoch: 25.3 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06833529328964179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06833529328964179 | validation: 0.05846172652950169]
	TIME [epoch: 25.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_154651/states/model_phi1_4c_v_mmd2_1255.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 15701.813 seconds.
