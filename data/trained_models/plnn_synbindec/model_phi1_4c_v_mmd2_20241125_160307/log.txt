Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 3885340140

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.9448434701002775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.9448434701002775 | validation: 4.8601486956386415]
	TIME [epoch: 168 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.111776082778291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.111776082778291 | validation: 4.900653393840367]
	TIME [epoch: 2.78 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.124270230155007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.124270230155007 | validation: 4.383475693676553]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.602507634638816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.602507634638816 | validation: 4.357780994201199]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.595080894479225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.595080894479225 | validation: 4.203636292850962]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.473404487643547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.473404487643547 | validation: 3.9782387942337993]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.21419845203399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.21419845203399 | validation: 3.9909458455762534]
	TIME [epoch: 2.77 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.279912371189895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.279912371189895 | validation: 3.9348477070330854]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1724950706597905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1724950706597905 | validation: 3.8701423779836315]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.1332347988213325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.1332347988213325 | validation: 3.783518719435451]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.043063161679347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.043063161679347 | validation: 3.76990234104174]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.024249423896315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.024249423896315 | validation: 3.702105324354253]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9814014149687633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9814014149687633 | validation: 3.6850718531566957]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.954625588464495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.954625588464495 | validation: 3.6088921220091166]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.924924509428928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.924924509428928 | validation: 3.606353223181377]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9010436303696516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9010436303696516 | validation: 3.5469989828872346]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8818110043977048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8818110043977048 | validation: 3.546606720351204]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.862090359415806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.862090359415806 | validation: 3.5396341218021456]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9402992780202086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9402992780202086 | validation: 3.5245199784606474]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.861452013412768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.861452013412768 | validation: 3.441738454876616]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8206819622834747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8206819622834747 | validation: 3.425209661743226]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7835301006566437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7835301006566437 | validation: 3.3645501458999423]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.767002084368194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.767002084368194 | validation: 3.364628262841703]
	TIME [epoch: 2.76 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7393755903821275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7393755903821275 | validation: 3.310131853479362]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.724328518932422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.724328518932422 | validation: 3.324856284271347]
	TIME [epoch: 2.75 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.70242852520525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.70242852520525 | validation: 3.2713073487472877]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69275560537868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69275560537868 | validation: 3.298705575919902]
	TIME [epoch: 2.76 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6804682652082246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6804682652082246 | validation: 3.288075015710471]
	TIME [epoch: 2.75 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.705993572803987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.705993572803987 | validation: 3.327059394704645]
	TIME [epoch: 2.75 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.676797662505265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.676797662505265 | validation: 3.3430743160921397]
	TIME [epoch: 2.75 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.733856324926185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.733856324926185 | validation: 3.2276190836462435]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6064453198908524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6064453198908524 | validation: 3.2253643715284954]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.599510652184407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.599510652184407 | validation: 3.1901289162675273]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.602913058759183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.602913058759183 | validation: 3.166454466272732]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5576050165463244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5576050165463244 | validation: 3.1259667776764335]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.538645351422939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.538645351422939 | validation: 3.1229285638423465]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.511566037312674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.511566037312674 | validation: 3.108353847542233]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4916628997083605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4916628997083605 | validation: 3.044227653435484]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.427813065012563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.427813065012563 | validation: 3.076420770003418]
	TIME [epoch: 2.76 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.402964630735518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.402964630735518 | validation: 3.1470204358582308]
	TIME [epoch: 2.76 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.460129959506101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.460129959506101 | validation: 3.3079061494980864]
	TIME [epoch: 2.76 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.701503394952722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.701503394952722 | validation: 2.9905219667077905]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.299248467115566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.299248467115566 | validation: 3.1787407084695367]
	TIME [epoch: 2.75 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4689751495701002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4689751495701002 | validation: 2.837004291872534]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.155521782551452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.155521782551452 | validation: 2.8697494315910337]
	TIME [epoch: 2.75 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.126038635584939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.126038635584939 | validation: 2.5251922780164557]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8016005885703708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8016005885703708 | validation: 2.0046137434041085]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.268754005029821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.268754005029821 | validation: 1.5329099304101252]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0006128729162764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0006128729162764 | validation: 1.1898212684356908]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.486615857578746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.486615857578746 | validation: 0.9167990280057201]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2153451137459412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2153451137459412 | validation: 1.3309056167396596]
	TIME [epoch: 2.76 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7412960701426072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7412960701426072 | validation: 1.6853717500443999]
	TIME [epoch: 2.76 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.167034698672802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.167034698672802 | validation: 1.4073790401061483]
	TIME [epoch: 2.75 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6877297739400745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6877297739400745 | validation: 1.056814834875696]
	TIME [epoch: 2.75 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3105979020547915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3105979020547915 | validation: 0.8222608601135484]
	TIME [epoch: 2.75 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0280166208331412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0280166208331412 | validation: 0.871544227955241]
	TIME [epoch: 2.76 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0353782927261241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0353782927261241 | validation: 0.7850495793107225]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9447299406260976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9447299406260976 | validation: 0.7532378342983571]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.937275858546009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.937275858546009 | validation: 0.7952550771262644]
	TIME [epoch: 2.76 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9142176238366722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9142176238366722 | validation: 0.7711770062371514]
	TIME [epoch: 2.76 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9089604964841463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9089604964841463 | validation: 0.8173280841513764]
	TIME [epoch: 2.76 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9633042947225937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9633042947225937 | validation: 0.7712288102352884]
	TIME [epoch: 2.76 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157326041331331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9157326041331331 | validation: 0.8012062982849122]
	TIME [epoch: 2.76 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9809705326167598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9809705326167598 | validation: 0.760349276988542]
	TIME [epoch: 2.76 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9059570178456031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9059570178456031 | validation: 0.7692437335045348]
	TIME [epoch: 2.76 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8976858497325825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8976858497325825 | validation: 0.7771153191161141]
	TIME [epoch: 2.76 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.917979615577411		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.917979615577411 | validation: 0.7755789250842027]
	TIME [epoch: 2.76 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9002173212331593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9002173212331593 | validation: 0.7520907189328994]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901856259163669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8901856259163669 | validation: 0.7791043532097959]
	TIME [epoch: 2.78 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8789385549253633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8789385549253633 | validation: 0.7757914078808965]
	TIME [epoch: 2.77 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8857804607985412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8857804607985412 | validation: 0.7380630136437106]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901784395090299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8901784395090299 | validation: 0.7866690015730816]
	TIME [epoch: 2.77 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9286605872201676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9286605872201676 | validation: 0.8428571274112685]
	TIME [epoch: 2.77 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0512437305646196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0512437305646196 | validation: 0.8758380610870568]
	TIME [epoch: 2.77 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1593372380020739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1593372380020739 | validation: 0.7392045810774468]
	TIME [epoch: 2.77 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9244266001830377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9244266001830377 | validation: 0.9333653782007403]
	TIME [epoch: 2.77 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1697033859962935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1697033859962935 | validation: 0.9308015355082507]
	TIME [epoch: 2.77 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2677163280524653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2677163280524653 | validation: 0.9410681618364882]
	TIME [epoch: 2.78 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1391957775139137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1391957775139137 | validation: 0.7549821411644335]
	TIME [epoch: 2.78 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9175805489349678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9175805489349678 | validation: 0.8442507075160974]
	TIME [epoch: 2.76 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9938181637072621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9938181637072621 | validation: 0.7626978958384796]
	TIME [epoch: 2.77 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9084098332498691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9084098332498691 | validation: 0.7474847520423801]
	TIME [epoch: 2.77 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8931568038346928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8931568038346928 | validation: 0.7453722291763222]
	TIME [epoch: 2.77 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.882093559169897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.882093559169897 | validation: 0.8356445086679726]
	TIME [epoch: 2.77 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9778085021977725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9778085021977725 | validation: 0.822927321302609]
	TIME [epoch: 2.77 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9935850754317062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9935850754317062 | validation: 0.7477517464896692]
	TIME [epoch: 2.77 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8937765742165238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8937765742165238 | validation: 0.7535345536446831]
	TIME [epoch: 2.77 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998379894841938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998379894841938 | validation: 0.7393137897221926]
	TIME [epoch: 2.77 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8859442368085358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8859442368085358 | validation: 0.7621266490512595]
	TIME [epoch: 2.76 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9038764015040681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9038764015040681 | validation: 0.7334351936500494]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8833407071856146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8833407071856146 | validation: 0.7415976488604656]
	TIME [epoch: 2.77 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8732361142509959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8732361142509959 | validation: 0.7277590629233622]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8911008141157112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8911008141157112 | validation: 0.7177807232599556]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8681194219552958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8681194219552958 | validation: 0.7566670659380725]
	TIME [epoch: 2.77 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8919941116166744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8919941116166744 | validation: 0.7328740947861281]
	TIME [epoch: 2.77 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8836328230024323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8836328230024323 | validation: 0.7903976783608279]
	TIME [epoch: 2.76 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9260860936457526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9260860936457526 | validation: 0.962525379546015]
	TIME [epoch: 2.77 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.184052378768912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.184052378768912 | validation: 0.7911226841299487]
	TIME [epoch: 2.77 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9606816936252784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9606816936252784 | validation: 0.7437765142820963]
	TIME [epoch: 2.77 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8924326974569678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8924326974569678 | validation: 0.7455036989700821]
	TIME [epoch: 2.77 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9213242211720966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9213242211720966 | validation: 0.7927695690919327]
	TIME [epoch: 2.77 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9579375728898242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9579375728898242 | validation: 0.7684596883660908]
	TIME [epoch: 2.77 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9269140215996394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9269140215996394 | validation: 0.7281247174901382]
	TIME [epoch: 2.77 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8709761598555948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8709761598555948 | validation: 0.7226247132031338]
	TIME [epoch: 2.77 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8841088950668357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8841088950668357 | validation: 0.7851687290315787]
	TIME [epoch: 2.77 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9101255058396751		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9101255058396751 | validation: 0.7382984127141452]
	TIME [epoch: 2.77 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873375623787727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873375623787727 | validation: 0.761786477398251]
	TIME [epoch: 2.77 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8920301458352674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8920301458352674 | validation: 0.7580467405696008]
	TIME [epoch: 2.77 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8946343098529836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8946343098529836 | validation: 0.8620744708092842]
	TIME [epoch: 2.77 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.073245700571392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.073245700571392 | validation: 0.7472773381311237]
	TIME [epoch: 2.77 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8838881569248875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8838881569248875 | validation: 0.7179678602711626]
	TIME [epoch: 2.77 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8651061732653227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8651061732653227 | validation: 0.7086890997591075]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_112.pth
	Model improved!!!
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.858353822640525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.858353822640525 | validation: 0.7534029982066758]
	TIME [epoch: 2.77 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893546921811182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893546921811182 | validation: 0.9005300533287146]
	TIME [epoch: 2.79 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0774528397009508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0774528397009508 | validation: 0.7392745942055989]
	TIME [epoch: 2.77 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8723313472892624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8723313472892624 | validation: 0.7381184969418453]
	TIME [epoch: 2.77 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8783046392197932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8783046392197932 | validation: 0.8043316695027833]
	TIME [epoch: 2.77 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9533240026466353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9533240026466353 | validation: 0.9536457462347393]
	TIME [epoch: 2.77 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.103952298914028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.103952298914028 | validation: 0.7067307770426956]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8647744208952627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8647744208952627 | validation: 0.8918958369580162]
	TIME [epoch: 2.77 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9731026600797464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9731026600797464 | validation: 0.854790679580651]
	TIME [epoch: 2.77 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9724774521185158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9724774521185158 | validation: 0.7290735261525287]
	TIME [epoch: 2.77 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8589973547357164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8589973547357164 | validation: 0.8191215234894275]
	TIME [epoch: 2.77 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.900359818769377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.900359818769377 | validation: 0.7310906031813519]
	TIME [epoch: 2.77 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8641076844151594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8641076844151594 | validation: 0.717963487122445]
	TIME [epoch: 2.77 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491227364735806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8491227364735806 | validation: 0.7453871795228938]
	TIME [epoch: 2.77 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8595220309412266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8595220309412266 | validation: 0.7467790994641988]
	TIME [epoch: 2.77 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8737676146514605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8737676146514605 | validation: 0.8210644770023541]
	TIME [epoch: 2.78 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9989885632074866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9989885632074866 | validation: 1.0364741893156253]
	TIME [epoch: 2.77 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.245578433559905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.245578433559905 | validation: 0.823470642779764]
	TIME [epoch: 2.77 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0098259943896708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0098259943896708 | validation: 0.8760563720188319]
	TIME [epoch: 2.77 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0339977961449784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0339977961449784 | validation: 0.710841490090362]
	TIME [epoch: 2.77 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8620582924347943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8620582924347943 | validation: 0.7254484849828865]
	TIME [epoch: 2.77 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871248298566455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871248298566455 | validation: 0.7204659244059343]
	TIME [epoch: 2.78 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8650165291336496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8650165291336496 | validation: 0.7284426450269584]
	TIME [epoch: 2.78 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8761415373502847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8761415373502847 | validation: 0.7893466548309221]
	TIME [epoch: 2.78 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9469418151270164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9469418151270164 | validation: 0.8607505028671913]
	TIME [epoch: 2.78 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9582283060198024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9582283060198024 | validation: 0.7131018332821732]
	TIME [epoch: 2.79 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487555771278562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8487555771278562 | validation: 0.7136597620363905]
	TIME [epoch: 2.78 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8347164363510109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8347164363510109 | validation: 0.7336937137570015]
	TIME [epoch: 2.78 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8407628872373336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8407628872373336 | validation: 0.6945285125368934]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_141.pth
	Model improved!!!
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8168193240114494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8168193240114494 | validation: 0.6881209591162444]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975952731847542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975952731847542 | validation: 0.7291183938135124]
	TIME [epoch: 2.78 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7975697100725316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7975697100725316 | validation: 1.134267451338771]
	TIME [epoch: 2.78 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2734239915933636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2734239915933636 | validation: 1.7193342719421432]
	TIME [epoch: 2.79 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9158481367511644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9158481367511644 | validation: 0.8476487121191543]
	TIME [epoch: 2.78 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9238475954996784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9238475954996784 | validation: 0.9181559525999052]
	TIME [epoch: 2.78 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0386445073673278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0386445073673278 | validation: 0.8055514459604398]
	TIME [epoch: 2.78 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9305306934661851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9305306934661851 | validation: 0.7292476309582846]
	TIME [epoch: 2.79 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.848920396865386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.848920396865386 | validation: 0.7383136646147005]
	TIME [epoch: 2.79 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8566016379155856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8566016379155856 | validation: 0.7230252139033976]
	TIME [epoch: 2.78 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8361602293689657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8361602293689657 | validation: 0.7396702136617135]
	TIME [epoch: 2.78 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8480018029347448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8480018029347448 | validation: 0.7304859506704524]
	TIME [epoch: 2.78 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575500034438397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8575500034438397 | validation: 0.7370783637214869]
	TIME [epoch: 2.78 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305808028729972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305808028729972 | validation: 0.7110467480816771]
	TIME [epoch: 2.78 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8346208891212976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8346208891212976 | validation: 0.7166186419607435]
	TIME [epoch: 2.78 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8030212773685798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8030212773685798 | validation: 0.6794490600087164]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103321569283719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103321569283719 | validation: 0.7635523126568482]
	TIME [epoch: 2.77 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7959677912459313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7959677912459313 | validation: 0.8867181270267723]
	TIME [epoch: 2.77 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9827733071084483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9827733071084483 | validation: 1.3087535340532888]
	TIME [epoch: 2.77 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4132077249179966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4132077249179966 | validation: 0.8242055348657452]
	TIME [epoch: 2.78 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0581322285058121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0581322285058121 | validation: 1.025236585057618]
	TIME [epoch: 2.78 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1052013970905539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1052013970905539 | validation: 0.9406565359032548]
	TIME [epoch: 2.78 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030670493767127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.030670493767127 | validation: 0.7197021443356602]
	TIME [epoch: 2.78 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8175023518607395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8175023518607395 | validation: 0.7369069521253357]
	TIME [epoch: 2.78 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537747088492639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8537747088492639 | validation: 0.7070618583050982]
	TIME [epoch: 2.78 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8183797183096209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8183797183096209 | validation: 0.7171494086610976]
	TIME [epoch: 2.78 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7922893983398919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7922893983398919 | validation: 0.7145017085603675]
	TIME [epoch: 2.77 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683558971134263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683558971134263 | validation: 0.6694160409700435]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.73241721696747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.73241721696747 | validation: 0.7737188766028844]
	TIME [epoch: 2.77 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7849540364437724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7849540364437724 | validation: 1.3095459935932483]
	TIME [epoch: 2.76 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.371115979909925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.371115979909925 | validation: 0.72134104751066]
	TIME [epoch: 2.76 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056562502199281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8056562502199281 | validation: 0.7533996521699102]
	TIME [epoch: 2.76 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893006901503415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893006901503415 | validation: 0.7095929618970148]
	TIME [epoch: 2.76 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7772294482551919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7772294482551919 | validation: 0.6752688012228014]
	TIME [epoch: 2.76 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7429939013837747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7429939013837747 | validation: 0.8062420850741394]
	TIME [epoch: 2.76 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7962580983753047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7962580983753047 | validation: 1.042394270026779]
	TIME [epoch: 2.76 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0939537858925836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0939537858925836 | validation: 0.6548717061123072]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7252919358953992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7252919358953992 | validation: 0.7782844058192381]
	TIME [epoch: 2.76 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906122336104977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7906122336104977 | validation: 0.8336494691587024]
	TIME [epoch: 2.76 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8835511627950838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8835511627950838 | validation: 0.6487454314042704]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7255966956274152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7255966956274152 | validation: 0.8018999195205198]
	TIME [epoch: 2.77 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7567783779078312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7567783779078312 | validation: 0.8546538255470758]
	TIME [epoch: 2.77 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8810935912792454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8810935912792454 | validation: 0.7052060096004549]
	TIME [epoch: 2.77 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8260554104848984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8260554104848984 | validation: 0.6359666387274141]
	TIME [epoch: 2.76 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6656492242422379		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6656492242422379 | validation: 0.7257087410501473]
	TIME [epoch: 2.76 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943652615050004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943652615050004 | validation: 1.0281169303619508]
	TIME [epoch: 2.76 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0825756511702067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0825756511702067 | validation: 1.1791178137483846]
	TIME [epoch: 2.76 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2854281590579084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2854281590579084 | validation: 0.8277445327916677]
	TIME [epoch: 2.75 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0067046571052098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0067046571052098 | validation: 0.8756774168746422]
	TIME [epoch: 2.76 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9763505913887648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9763505913887648 | validation: 0.7919148218915146]
	TIME [epoch: 2.76 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8795285123325196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8795285123325196 | validation: 0.7628770244311117]
	TIME [epoch: 2.76 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8516606888218834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8516606888218834 | validation: 0.7644475902495051]
	TIME [epoch: 2.76 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8406512906954133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8406512906954133 | validation: 0.7466572211992868]
	TIME [epoch: 2.77 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8245744080454804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8245744080454804 | validation: 0.7246254648330935]
	TIME [epoch: 2.76 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8131945796845884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8131945796845884 | validation: 0.6916264637772489]
	TIME [epoch: 2.76 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.795911794008508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.795911794008508 | validation: 0.6766183525914224]
	TIME [epoch: 2.76 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.759865120006987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.759865120006987 | validation: 0.6365340967163832]
	TIME [epoch: 2.77 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6895065852606509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6895065852606509 | validation: 0.7233195716788683]
	TIME [epoch: 2.76 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6955407550262299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6955407550262299 | validation: 1.0616155449205553]
	TIME [epoch: 2.76 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0880689284606684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0880689284606684 | validation: 0.8563292765850701]
	TIME [epoch: 180 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8839290941104971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8839290941104971 | validation: 0.7422100292154376]
	TIME [epoch: 5.94 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8002408564149803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8002408564149803 | validation: 0.7071654102372371]
	TIME [epoch: 5.93 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852413168750377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852413168750377 | validation: 0.6892862481951085]
	TIME [epoch: 5.94 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7673004806123538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7673004806123538 | validation: 0.6620375357381765]
	TIME [epoch: 5.93 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7126019194258854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7126019194258854 | validation: 0.6371112439717628]
	TIME [epoch: 5.92 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6500336461351738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6500336461351738 | validation: 0.6291725888508092]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_207.pth
	Model improved!!!
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6407750703767491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6407750703767491 | validation: 0.6334127659569998]
	TIME [epoch: 5.93 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6353950796065556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6353950796065556 | validation: 0.8077228685602684]
	TIME [epoch: 5.93 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8334352419253026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8334352419253026 | validation: 0.6751727817287786]
	TIME [epoch: 5.93 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7777654967708127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7777654967708127 | validation: 0.6679814447299104]
	TIME [epoch: 5.92 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6685343336778234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6685343336778234 | validation: 0.6167511352891787]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6662062339209399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6662062339209399 | validation: 0.8573246366618051]
	TIME [epoch: 5.93 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9210164790115831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9210164790115831 | validation: 0.6928516479065452]
	TIME [epoch: 5.93 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7588440902687412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7588440902687412 | validation: 0.6827918543586888]
	TIME [epoch: 5.93 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7328762411492578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7328762411492578 | validation: 0.5943775629659109]
	TIME [epoch: 5.92 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_216.pth
	Model improved!!!
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6156793678147652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6156793678147652 | validation: 1.0449823832689316]
	TIME [epoch: 5.96 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9733647220820321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9733647220820321 | validation: 1.2324070206572348]
	TIME [epoch: 5.96 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.39537226477532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.39537226477532 | validation: 0.9442289242242676]
	TIME [epoch: 5.96 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1643039159086892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1643039159086892 | validation: 0.8302669658166982]
	TIME [epoch: 5.97 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9307772018895113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9307772018895113 | validation: 0.8110234293244103]
	TIME [epoch: 5.95 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9380914189734463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9380914189734463 | validation: 0.7214891039938999]
	TIME [epoch: 5.97 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8553852194515676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8553852194515676 | validation: 0.7115059947751351]
	TIME [epoch: 5.98 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886232027750277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7886232027750277 | validation: 0.677096739325615]
	TIME [epoch: 5.98 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501763123825173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7501763123825173 | validation: 0.6254634313289907]
	TIME [epoch: 5.97 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.667591415423656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.667591415423656 | validation: 0.6857666377124512]
	TIME [epoch: 5.96 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6804470862721125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6804470862721125 | validation: 0.8130441563031223]
	TIME [epoch: 5.97 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.825920108143877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.825920108143877 | validation: 0.6218273319042537]
	TIME [epoch: 5.98 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7210862218464706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7210862218464706 | validation: 0.6233183450898349]
	TIME [epoch: 5.98 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6254129774206392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6254129774206392 | validation: 0.6905407686918044]
	TIME [epoch: 5.99 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6451037390776913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6451037390776913 | validation: 0.7425530071599216]
	TIME [epoch: 5.97 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618216561756254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618216561756254 | validation: 0.5906302140022842]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_232.pth
	Model improved!!!
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6730969681056359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6730969681056359 | validation: 0.6162936381920829]
	TIME [epoch: 5.95 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5937941504988263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5937941504988263 | validation: 0.5777010585383542]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5673525248894912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5673525248894912 | validation: 0.5598095231961937]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.597638483609658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.597638483609658 | validation: 0.8150272125260689]
	TIME [epoch: 5.95 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729933639232629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729933639232629 | validation: 0.6019332323910033]
	TIME [epoch: 5.97 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6940196996304303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6940196996304303 | validation: 0.592730141144545]
	TIME [epoch: 5.97 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5973588310382902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5973588310382902 | validation: 0.8634835679078791]
	TIME [epoch: 5.98 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9347167419812354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9347167419812354 | validation: 1.068449990413239]
	TIME [epoch: 5.96 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9162680325479114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9162680325479114 | validation: 0.6765400064238298]
	TIME [epoch: 5.96 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7474147305676228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7474147305676228 | validation: 0.6761245278613252]
	TIME [epoch: 5.96 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7498910184874433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7498910184874433 | validation: 0.6522154324465125]
	TIME [epoch: 5.97 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6916353023201078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6916353023201078 | validation: 0.573898807249316]
	TIME [epoch: 5.96 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.580195955767716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.580195955767716 | validation: 0.816360109971411]
	TIME [epoch: 5.97 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7489449255585789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7489449255585789 | validation: 1.134691156136919]
	TIME [epoch: 5.94 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2365744571443933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2365744571443933 | validation: 0.6927375453138203]
	TIME [epoch: 5.96 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.755151952123783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.755151952123783 | validation: 0.8687503715359652]
	TIME [epoch: 5.96 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9577971005464828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9577971005464828 | validation: 0.6375306697400979]
	TIME [epoch: 5.96 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341336332102444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7341336332102444 | validation: 0.6395174104793595]
	TIME [epoch: 5.97 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.643169306699279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.643169306699279 | validation: 0.6396784118675498]
	TIME [epoch: 5.94 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.609199755775534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.609199755775534 | validation: 0.6527799107251744]
	TIME [epoch: 5.97 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6810717339753286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6810717339753286 | validation: 0.5449400056297643]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_253.pth
	Model improved!!!
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5748582785508454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5748582785508454 | validation: 0.5623921190722879]
	TIME [epoch: 5.96 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5443869798478268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5443869798478268 | validation: 0.5332313798466003]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_255.pth
	Model improved!!!
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5338366124848578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5338366124848578 | validation: 0.5940535202794678]
	TIME [epoch: 5.94 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.58236289707235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.58236289707235 | validation: 0.7775998738407165]
	TIME [epoch: 5.96 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8472848922354257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8472848922354257 | validation: 0.6257346674804508]
	TIME [epoch: 5.96 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6725781992481573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6725781992481573 | validation: 0.5715473475995533]
	TIME [epoch: 5.96 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5906902750073377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5906902750073377 | validation: 0.5846410132369754]
	TIME [epoch: 5.95 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.583440713418431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.583440713418431 | validation: 0.9325515975260779]
	TIME [epoch: 5.95 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991033568693391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8991033568693391 | validation: 0.9972247849565288]
	TIME [epoch: 5.96 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1684435847792123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1684435847792123 | validation: 0.8393967022442507]
	TIME [epoch: 5.96 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.013962240859261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.013962240859261 | validation: 0.9241295818256051]
	TIME [epoch: 5.96 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0120594597678174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0120594597678174 | validation: 0.7946525955874268]
	TIME [epoch: 5.94 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871108315811708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8871108315811708 | validation: 0.7093437294285176]
	TIME [epoch: 5.93 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241238192584968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241238192584968 | validation: 0.6449885244611986]
	TIME [epoch: 5.96 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291587313764809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291587313764809 | validation: 0.5613736008088283]
	TIME [epoch: 5.95 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6348462622615929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6348462622615929 | validation: 0.5869250938750886]
	TIME [epoch: 5.95 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5688067702457762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5688067702457762 | validation: 0.7954718364653811]
	TIME [epoch: 5.94 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8174272350903884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8174272350903884 | validation: 0.6630270898865924]
	TIME [epoch: 5.93 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.657547475555169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.657547475555169 | validation: 0.6045724530844959]
	TIME [epoch: 5.95 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6442545905450032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6442545905450032 | validation: 0.5012626371215685]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_273.pth
	Model improved!!!
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5752678936793243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5752678936793243 | validation: 0.6350320276644235]
	TIME [epoch: 5.98 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5951955936695718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5951955936695718 | validation: 0.815266853439659]
	TIME [epoch: 5.97 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8527839130755627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8527839130755627 | validation: 0.5706135973407582]
	TIME [epoch: 5.96 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6583269316136913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6583269316136913 | validation: 0.5725296177127707]
	TIME [epoch: 5.98 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.585393089915752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.585393089915752 | validation: 0.6912898483260563]
	TIME [epoch: 5.97 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6893822305404784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6893822305404784 | validation: 0.6746508818825044]
	TIME [epoch: 5.98 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6968952167417852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6968952167417852 | validation: 0.742624089010485]
	TIME [epoch: 5.97 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7973800803616996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7973800803616996 | validation: 0.6429145800321321]
	TIME [epoch: 5.95 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7076591255507134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7076591255507134 | validation: 0.5903883329752667]
	TIME [epoch: 5.97 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6439773211836254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6439773211836254 | validation: 0.5336391743146834]
	TIME [epoch: 5.98 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5113215288701108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5113215288701108 | validation: 0.53197649248429]
	TIME [epoch: 5.98 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5462791840780858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5462791840780858 | validation: 0.5008467088699998]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_285.pth
	Model improved!!!
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5178929056889554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5178929056889554 | validation: 0.7099254724223167]
	TIME [epoch: 5.95 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6871816127992727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6871816127992727 | validation: 0.7637780949883037]
	TIME [epoch: 5.97 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8623346178773134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8623346178773134 | validation: 0.6264008169830904]
	TIME [epoch: 5.96 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572591321720212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7572591321720212 | validation: 0.5751952729659614]
	TIME [epoch: 5.97 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6110234278781297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6110234278781297 | validation: 0.5765328268128339]
	TIME [epoch: 5.96 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.520119016427023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.520119016427023 | validation: 0.47357268008950854]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5184637632129617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5184637632129617 | validation: 0.5199492365460324]
	TIME [epoch: 5.95 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441745128719598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5441745128719598 | validation: 0.716613571544087]
	TIME [epoch: 5.95 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967988224213265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967988224213265 | validation: 0.7550024399612745]
	TIME [epoch: 5.95 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8807835763027835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8807835763027835 | validation: 0.6050067443001315]
	TIME [epoch: 5.94 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6608442342484909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6608442342484909 | validation: 0.5365863207706711]
	TIME [epoch: 5.93 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5978315020542843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5978315020542843 | validation: 0.47118156072097117]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_297.pth
	Model improved!!!
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47017878281419917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47017878281419917 | validation: 0.44267149166302266]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4472402029465529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4472402029465529 | validation: 0.6546737340126928]
	TIME [epoch: 5.98 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6213034049949517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6213034049949517 | validation: 0.722353455377141]
	TIME [epoch: 5.96 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8461404994860995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8461404994860995 | validation: 0.5829485651672617]
	TIME [epoch: 5.96 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6493561991160667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6493561991160667 | validation: 0.5056726866543447]
	TIME [epoch: 5.96 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5710732650994443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5710732650994443 | validation: 0.43207216757483285]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42598425132520396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42598425132520396 | validation: 0.48951512296319255]
	TIME [epoch: 5.98 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4913435015331541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4913435015331541 | validation: 0.5744290711540379]
	TIME [epoch: 5.98 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6296907828056848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6296907828056848 | validation: 0.5387426266504436]
	TIME [epoch: 5.98 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5805942761072219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5805942761072219 | validation: 0.4126857069292409]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39405146173238836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39405146173238836 | validation: 0.4370800562658089]
	TIME [epoch: 5.94 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4338705037824265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4338705037824265 | validation: 0.35566852999648735]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_309.pth
	Model improved!!!
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39036427985979855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39036427985979855 | validation: 0.7144791815127195]
	TIME [epoch: 5.94 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271224156156598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6271224156156598 | validation: 0.8558980405393299]
	TIME [epoch: 5.94 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0549070600793409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0549070600793409 | validation: 0.5015050581475798]
	TIME [epoch: 5.97 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4694257520525612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4694257520525612 | validation: 0.45893143251538504]
	TIME [epoch: 5.94 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4346733699804238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4346733699804238 | validation: 0.6900334908336399]
	TIME [epoch: 5.95 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7745968951613059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7745968951613059 | validation: 0.38346257306472475]
	TIME [epoch: 5.93 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45836242147601525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45836242147601525 | validation: 0.6237655082225553]
	TIME [epoch: 5.95 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.570901011505864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.570901011505864 | validation: 0.5142148662453477]
	TIME [epoch: 5.95 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5732748337387459		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5732748337387459 | validation: 0.31926783744405995]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40546324238606896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40546324238606896 | validation: 0.6062798026051794]
	TIME [epoch: 5.95 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5186510840728179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5186510840728179 | validation: 0.6254933612609923]
	TIME [epoch: 5.94 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7218367364925709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7218367364925709 | validation: 0.4035483790615976]
	TIME [epoch: 5.95 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5115601358707171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5115601358707171 | validation: 0.5554550779777204]
	TIME [epoch: 5.96 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4793906725386931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4793906725386931 | validation: 0.4840261347473955]
	TIME [epoch: 5.95 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49992449879462586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49992449879462586 | validation: 0.46259560677936434]
	TIME [epoch: 5.96 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45544014617455103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45544014617455103 | validation: 0.4726894591992496]
	TIME [epoch: 5.92 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5420931017344244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5420931017344244 | validation: 0.49025837728642224]
	TIME [epoch: 5.96 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45924435243595924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45924435243595924 | validation: 0.31838773591782926]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3257011381846726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3257011381846726 | validation: 0.34425424828666734]
	TIME [epoch: 5.95 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3488380343337294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3488380343337294 | validation: 0.3560182135776702]
	TIME [epoch: 5.95 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31724001585671807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31724001585671807 | validation: 0.376887164919686]
	TIME [epoch: 5.93 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47823728446672376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47823728446672376 | validation: 0.8320363716770491]
	TIME [epoch: 5.95 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7208904832503007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7208904832503007 | validation: 0.5877703589745139]
	TIME [epoch: 5.95 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220359337680242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220359337680242 | validation: 0.5476092592003927]
	TIME [epoch: 5.94 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7489752866563143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7489752866563143 | validation: 0.6292292981722007]
	TIME [epoch: 5.94 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092546001003458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092546001003458 | validation: 0.6598740960472915]
	TIME [epoch: 5.93 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7654088542683585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7654088542683585 | validation: 0.47502428514570866]
	TIME [epoch: 5.95 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42223541437412837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42223541437412837 | validation: 0.479211761229987]
	TIME [epoch: 5.95 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49310566324553706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49310566324553706 | validation: 0.5454871504155664]
	TIME [epoch: 5.95 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.671742771081537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.671742771081537 | validation: 0.369036622611586]
	TIME [epoch: 5.95 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44102470559324175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44102470559324175 | validation: 0.6541223662392662]
	TIME [epoch: 5.93 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5720570235406304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5720570235406304 | validation: 0.37703151811273505]
	TIME [epoch: 5.95 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330211788289764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4330211788289764 | validation: 0.28977941552488473]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3387877410147306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3387877410147306 | validation: 0.6152842232835548]
	TIME [epoch: 5.95 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5487286387373348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5487286387373348 | validation: 0.5020393372084504]
	TIME [epoch: 5.95 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.615309134687705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.615309134687705 | validation: 0.4289586365092444]
	TIME [epoch: 5.94 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5403613367271624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5403613367271624 | validation: 0.6602035419996701]
	TIME [epoch: 5.95 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.558276402473103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.558276402473103 | validation: 0.32109562267638814]
	TIME [epoch: 5.95 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3208177874879213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3208177874879213 | validation: 0.28560248037496255]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3407525602017759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3407525602017759 | validation: 0.27892721822261557]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29040957714170423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29040957714170423 | validation: 0.3385880064131448]
	TIME [epoch: 5.93 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30907014575135733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30907014575135733 | validation: 0.3781153430823172]
	TIME [epoch: 5.94 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4645787056087534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4645787056087534 | validation: 0.39345729900225684]
	TIME [epoch: 5.95 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39662185019697505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39662185019697505 | validation: 0.37060708821185084]
	TIME [epoch: 5.95 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4608181277005403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4608181277005403 | validation: 0.33412758656196984]
	TIME [epoch: 5.95 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31573277653444687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31573277653444687 | validation: 0.6383337341553805]
	TIME [epoch: 5.93 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5925825112320079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5925825112320079 | validation: 0.8807282968217733]
	TIME [epoch: 5.95 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9229524893269002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9229524893269002 | validation: 0.9442419690049246]
	TIME [epoch: 5.95 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1561667479826385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1561667479826385 | validation: 0.9213473671806622]
	TIME [epoch: 5.95 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2217240866454815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2217240866454815 | validation: 0.6292669441038404]
	TIME [epoch: 5.95 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8120186618262298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8120186618262298 | validation: 0.40035510353694]
	TIME [epoch: 5.93 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5402142316750261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5402142316750261 | validation: 1.0570586750087299]
	TIME [epoch: 5.95 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8858157700197273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8858157700197273 | validation: 0.8290374114761843]
	TIME [epoch: 5.95 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8720171048741916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8720171048741916 | validation: 0.39303663723975124]
	TIME [epoch: 5.95 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5444860991289547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5444860991289547 | validation: 0.7603813067982682]
	TIME [epoch: 5.96 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056290199003326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7056290199003326 | validation: 0.43998599890435663]
	TIME [epoch: 5.94 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49848343772207315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49848343772207315 | validation: 0.33011773665298394]
	TIME [epoch: 5.96 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32707994850569305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32707994850569305 | validation: 0.433765819914961]
	TIME [epoch: 5.95 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41743351600861134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41743351600861134 | validation: 0.4050981947212403]
	TIME [epoch: 5.96 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5031747801298054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5031747801298054 | validation: 0.3261289662106608]
	TIME [epoch: 5.95 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3190093474365506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3190093474365506 | validation: 0.38542600500988633]
	TIME [epoch: 5.94 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3421315319705186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3421315319705186 | validation: 0.43957317260377504]
	TIME [epoch: 5.94 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5474201148291623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5474201148291623 | validation: 0.30190549269906836]
	TIME [epoch: 5.95 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34915247597509935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34915247597509935 | validation: 0.5722166982321312]
	TIME [epoch: 5.95 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49404934712230286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49404934712230286 | validation: 0.4600245492173051]
	TIME [epoch: 5.94 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5375448483796563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5375448483796563 | validation: 0.2850950998928911]
	TIME [epoch: 5.92 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3199639915965207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3199639915965207 | validation: 0.8574643042213803]
	TIME [epoch: 5.95 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524936228740449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524936228740449 | validation: 0.5273145499405952]
	TIME [epoch: 5.94 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602963629356319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6602963629356319 | validation: 0.6882913270629207]
	TIME [epoch: 5.95 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433299143777318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433299143777318 | validation: 0.4104713184518424]
	TIME [epoch: 5.95 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4692398705516777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4692398705516777 | validation: 0.5321628857237377]
	TIME [epoch: 5.95 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42678883333365814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42678883333365814 | validation: 0.3316503496173219]
	TIME [epoch: 5.96 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4131230851974535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4131230851974535 | validation: 0.6736507758935799]
	TIME [epoch: 5.96 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5657046487341556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5657046487341556 | validation: 0.46675087686812633]
	TIME [epoch: 5.95 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.559657594300148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.559657594300148 | validation: 0.5202874797704674]
	TIME [epoch: 5.96 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5862211593623465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5862211593623465 | validation: 0.2642863916950667]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_385.pth
	Model improved!!!
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34389518778582023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34389518778582023 | validation: 0.4767959599276994]
	TIME [epoch: 5.94 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4260694107530681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4260694107530681 | validation: 0.3122510699643728]
	TIME [epoch: 5.95 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2989661514914206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2989661514914206 | validation: 0.34637977067566883]
	TIME [epoch: 5.94 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41239831155555734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41239831155555734 | validation: 0.3074341735694591]
	TIME [epoch: 5.95 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2864165486924547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2864165486924547 | validation: 0.2483671152366838]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_390.pth
	Model improved!!!
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24850291485316928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24850291485316928 | validation: 0.26001457214473167]
	TIME [epoch: 5.94 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3174286480401432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3174286480401432 | validation: 0.685642148847665]
	TIME [epoch: 5.94 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5815878488514211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5815878488514211 | validation: 0.5421322988057652]
	TIME [epoch: 5.94 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6237766724007797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6237766724007797 | validation: 0.44912834126164985]
	TIME [epoch: 5.95 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441796965144701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5441796965144701 | validation: 0.3166686826999735]
	TIME [epoch: 5.94 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36030742989534803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36030742989534803 | validation: 0.3793519071698839]
	TIME [epoch: 5.94 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3072236666323894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3072236666323894 | validation: 0.46690770287826733]
	TIME [epoch: 5.93 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5359404859284342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5359404859284342 | validation: 0.34992198414132475]
	TIME [epoch: 5.95 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2849582635013845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2849582635013845 | validation: 0.2791417829425576]
	TIME [epoch: 5.95 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3072806907838588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3072806907838588 | validation: 0.3679015019431352]
	TIME [epoch: 5.94 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34966956269756033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34966956269756033 | validation: 0.38948158990634635]
	TIME [epoch: 5.94 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44171918272319716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44171918272319716 | validation: 0.28243500511216174]
	TIME [epoch: 5.95 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24829387541373554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24829387541373554 | validation: 0.5649525335276574]
	TIME [epoch: 5.95 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.478057671616974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.478057671616974 | validation: 0.7686217974841569]
	TIME [epoch: 5.96 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8393101639998838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8393101639998838 | validation: 0.9362788862344481]
	TIME [epoch: 5.94 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.114129464718815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.114129464718815 | validation: 0.9649526599840649]
	TIME [epoch: 5.96 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093906411844143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1093906411844143 | validation: 0.8938968208452013]
	TIME [epoch: 5.96 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9685769675121386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9685769675121386 | validation: 0.7046721458868701]
	TIME [epoch: 5.96 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8264379378005035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8264379378005035 | validation: 0.9656886420672433]
	TIME [epoch: 5.95 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2986385692927735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2986385692927735 | validation: 0.8638224164907364]
	TIME [epoch: 5.95 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1088446225564081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1088446225564081 | validation: 0.3454595808455712]
	TIME [epoch: 5.95 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42125896437617344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42125896437617344 | validation: 0.5615770616713538]
	TIME [epoch: 5.95 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4693754343101224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4693754343101224 | validation: 0.3533851885934808]
	TIME [epoch: 5.94 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41379596076167313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41379596076167313 | validation: 0.30902000163842597]
	TIME [epoch: 5.95 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3432940359486251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3432940359486251 | validation: 0.28712030957318774]
	TIME [epoch: 5.92 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27208690651592216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27208690651592216 | validation: 0.299880291482657]
	TIME [epoch: 5.96 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33142296493046247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33142296493046247 | validation: 0.2847548131581493]
	TIME [epoch: 5.94 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2780644743902386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2780644743902386 | validation: 0.3444308712707378]
	TIME [epoch: 5.95 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3954335049124633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3954335049124633 | validation: 0.43523944836338624]
	TIME [epoch: 5.94 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34248885613770796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34248885613770796 | validation: 0.22864638344795962]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_420.pth
	Model improved!!!
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25206752513238156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25206752513238156 | validation: 0.32292392525373037]
	TIME [epoch: 5.93 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29835702928489904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29835702928489904 | validation: 0.44397327075248666]
	TIME [epoch: 5.95 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4989419483321758		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4989419483321758 | validation: 0.27879386786461247]
	TIME [epoch: 5.94 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2854464655237428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2854464655237428 | validation: 0.4336550140496879]
	TIME [epoch: 5.95 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.336916512943237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.336916512943237 | validation: 0.6255285349613674]
	TIME [epoch: 5.92 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6828234440061871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6828234440061871 | validation: 0.39900342567929814]
	TIME [epoch: 5.96 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44823590973979244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44823590973979244 | validation: 0.6215587550805772]
	TIME [epoch: 5.95 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5231226878712056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5231226878712056 | validation: 0.32160888360685663]
	TIME [epoch: 5.95 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40315450123201635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40315450123201635 | validation: 0.3103980405126605]
	TIME [epoch: 5.96 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3119075175621726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3119075175621726 | validation: 0.43182497403199305]
	TIME [epoch: 5.95 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38403093484440176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38403093484440176 | validation: 0.42413069031126727]
	TIME [epoch: 5.96 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47607298328576975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47607298328576975 | validation: 0.27136383157998595]
	TIME [epoch: 5.95 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2697710487599756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2697710487599756 | validation: 0.559249450976602]
	TIME [epoch: 5.95 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4926707183230532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4926707183230532 | validation: 0.5789577938431311]
	TIME [epoch: 5.95 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6375666129352954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6375666129352954 | validation: 0.48891974085755036]
	TIME [epoch: 5.93 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6125731843416233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6125731843416233 | validation: 0.3107592186346747]
	TIME [epoch: 5.95 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3442372242508269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3442372242508269 | validation: 0.5886803249043947]
	TIME [epoch: 5.95 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49279779938587687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49279779938587687 | validation: 0.490846128811561]
	TIME [epoch: 5.95 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5615643432095463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5615643432095463 | validation: 0.3790780379793803]
	TIME [epoch: 5.95 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36107569461312267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36107569461312267 | validation: 0.4714106713623525]
	TIME [epoch: 5.95 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44010821203034994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44010821203034994 | validation: 0.31623898773194054]
	TIME [epoch: 5.93 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37236565387913323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37236565387913323 | validation: 0.45757869470955514]
	TIME [epoch: 5.94 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3858021400468678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3858021400468678 | validation: 0.24693249351588867]
	TIME [epoch: 5.94 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2423556688151811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2423556688151811 | validation: 0.25395903535333203]
	TIME [epoch: 5.94 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31567745131887004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31567745131887004 | validation: 0.38304174777119027]
	TIME [epoch: 5.93 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30532996607462587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30532996607462587 | validation: 0.3666726499801379]
	TIME [epoch: 5.93 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4359596073257189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4359596073257189 | validation: 0.2555127834476532]
	TIME [epoch: 5.92 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2728160695546206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2728160695546206 | validation: 0.48751301078461096]
	TIME [epoch: 5.93 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40015037116570995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40015037116570995 | validation: 0.44332238983470873]
	TIME [epoch: 5.92 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4991473218727026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4991473218727026 | validation: 0.7420167632020904]
	TIME [epoch: 5.93 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5519725350888127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5519725350888127 | validation: 0.545522497434835]
	TIME [epoch: 5.94 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40723900189076434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40723900189076434 | validation: 0.21292600133458325]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.286080908911205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.286080908911205 | validation: 0.24426072230032914]
	TIME [epoch: 5.94 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282851143758856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2282851143758856 | validation: 0.27030137757704464]
	TIME [epoch: 5.94 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28385093173975384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28385093173975384 | validation: 0.23195207673535548]
	TIME [epoch: 5.94 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21627129083078095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21627129083078095 | validation: 0.1580662637864425]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_456.pth
	Model improved!!!
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20447425145868367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20447425145868367 | validation: 0.24296820195132943]
	TIME [epoch: 5.93 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19978040503374087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19978040503374087 | validation: 0.22129575494808879]
	TIME [epoch: 5.93 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2828043839777753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2828043839777753 | validation: 0.4060482686943752]
	TIME [epoch: 5.94 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3395232996015443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3395232996015443 | validation: 0.34943957969805944]
	TIME [epoch: 5.93 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43848010293741624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43848010293741624 | validation: 0.3184594615238856]
	TIME [epoch: 5.92 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35115502139163085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35115502139163085 | validation: 0.36569037893051787]
	TIME [epoch: 5.95 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27703097986747405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27703097986747405 | validation: 0.17013262992004058]
	TIME [epoch: 5.92 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1987900936855327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1987900936855327 | validation: 0.20146177018373201]
	TIME [epoch: 5.94 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18665702614082696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18665702614082696 | validation: 0.21950179376998508]
	TIME [epoch: 5.93 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24056842376282192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24056842376282192 | validation: 0.40037014327741755]
	TIME [epoch: 5.94 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.313372923598448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.313372923598448 | validation: 0.35329740189627556]
	TIME [epoch: 5.94 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41873218591219835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41873218591219835 | validation: 0.27019200173453994]
	TIME [epoch: 5.94 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2028080266304799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2028080266304799 | validation: 0.272150803940423]
	TIME [epoch: 5.94 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2357033504389726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2357033504389726 | validation: 0.34755408526615594]
	TIME [epoch: 5.94 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4157532801405203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4157532801405203 | validation: 0.25953834948785376]
	TIME [epoch: 5.93 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21927688749507213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21927688749507213 | validation: 0.22826010801554172]
	TIME [epoch: 5.94 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21140435756037998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21140435756037998 | validation: 0.2593075605655799]
	TIME [epoch: 5.93 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29774785840764545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29774785840764545 | validation: 0.3539130036215127]
	TIME [epoch: 5.92 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2376318769259049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2376318769259049 | validation: 0.2272297341695585]
	TIME [epoch: 5.92 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27175730202940274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27175730202940274 | validation: 0.3183764471900678]
	TIME [epoch: 5.92 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22974623945560915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22974623945560915 | validation: 0.2876283721063208]
	TIME [epoch: 5.93 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3390520316164515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3390520316164515 | validation: 0.18906181980971898]
	TIME [epoch: 5.94 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19559038486204613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19559038486204613 | validation: 0.5166134612901947]
	TIME [epoch: 5.93 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3726252187362687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3726252187362687 | validation: 0.4562114744267216]
	TIME [epoch: 5.93 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5152465389164831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5152465389164831 | validation: 0.3819679954803308]
	TIME [epoch: 5.92 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41596205525245067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41596205525245067 | validation: 0.6088296209839363]
	TIME [epoch: 5.94 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4591935612772429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4591935612772429 | validation: 0.14844530280689736]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_483.pth
	Model improved!!!
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17678923761149917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17678923761149917 | validation: 0.20793190436799514]
	TIME [epoch: 5.94 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2039793020422592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2039793020422592 | validation: 0.30691186177373153]
	TIME [epoch: 5.95 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23085128410812253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23085128410812253 | validation: 0.2025759646709358]
	TIME [epoch: 5.95 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21480762749017623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21480762749017623 | validation: 0.17585117958153706]
	TIME [epoch: 5.95 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2024583211390199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2024583211390199 | validation: 0.4182064237216787]
	TIME [epoch: 5.95 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2681025312875528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2681025312875528 | validation: 0.3502887092967837]
	TIME [epoch: 5.94 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41148771174811416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41148771174811416 | validation: 0.5112693318497217]
	TIME [epoch: 5.94 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5563779212113826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5563779212113826 | validation: 0.2830863587919974]
	TIME [epoch: 5.93 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2533530668927216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2533530668927216 | validation: 0.24527050926692354]
	TIME [epoch: 5.94 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22031054586319443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22031054586319443 | validation: 0.14249344513523132]
	TIME [epoch: 5.93 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_493.pth
	Model improved!!!
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14563519082849896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14563519082849896 | validation: 0.1745731661712867]
	TIME [epoch: 5.95 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16013749376216216		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16013749376216216 | validation: 0.44255404577013807]
	TIME [epoch: 5.95 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37537415663678625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37537415663678625 | validation: 0.4240121474600191]
	TIME [epoch: 5.95 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5000679033872827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5000679033872827 | validation: 0.394165194358322]
	TIME [epoch: 5.95 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40322339686341296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40322339686341296 | validation: 0.33939257996024047]
	TIME [epoch: 5.93 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3231241999976597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3231241999976597 | validation: 0.3368700668590032]
	TIME [epoch: 5.93 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27899897101193755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27899897101193755 | validation: 0.3776892543964994]
	TIME [epoch: 5.93 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3782429256868038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3782429256868038 | validation: 0.2540729245849166]
	TIME [epoch: 185 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19944365307542672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19944365307542672 | validation: 0.30774610513067]
	TIME [epoch: 12.6 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23295947959519972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23295947959519972 | validation: 0.3454813084662318]
	TIME [epoch: 12.5 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43550973747283295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43550973747283295 | validation: 0.20382526212549773]
	TIME [epoch: 12.6 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1664388929409018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1664388929409018 | validation: 0.3939319584597744]
	TIME [epoch: 12.6 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26989970124600027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26989970124600027 | validation: 0.37121636206264236]
	TIME [epoch: 12.6 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43273048534036945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43273048534036945 | validation: 0.2541566850384065]
	TIME [epoch: 12.6 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2564002136617213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2564002136617213 | validation: 0.6130118955231334]
	TIME [epoch: 12.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46087041748320373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46087041748320373 | validation: 0.2050609809756926]
	TIME [epoch: 12.6 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.240655967263113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.240655967263113 | validation: 0.14303536548770351]
	TIME [epoch: 12.6 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16494585505684772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16494585505684772 | validation: 0.2977862783380586]
	TIME [epoch: 12.6 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21007062393952802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21007062393952802 | validation: 0.21396078033356677]
	TIME [epoch: 12.6 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22911319853557635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22911319853557635 | validation: 0.17634325772495754]
	TIME [epoch: 12.6 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14923901788312177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14923901788312177 | validation: 0.16910232363163769]
	TIME [epoch: 12.6 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14428444069047444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14428444069047444 | validation: 0.15721471919742716]
	TIME [epoch: 12.6 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18508314073200743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18508314073200743 | validation: 0.3194864517393686]
	TIME [epoch: 12.6 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24108359821110908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24108359821110908 | validation: 0.26569123799329314]
	TIME [epoch: 12.6 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2964778389403691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2964778389403691 | validation: 0.2780099435730446]
	TIME [epoch: 12.6 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20528276005716456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20528276005716456 | validation: 0.19501973182126936]
	TIME [epoch: 12.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17685231742173102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17685231742173102 | validation: 0.17453105362472823]
	TIME [epoch: 12.6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22729526363432823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22729526363432823 | validation: 0.34398355738574327]
	TIME [epoch: 12.6 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2263744988305943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2263744988305943 | validation: 0.2449277935888019]
	TIME [epoch: 12.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26625440110685916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26625440110685916 | validation: 0.19629018051047664]
	TIME [epoch: 12.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15675521188444516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15675521188444516 | validation: 0.3282375502281194]
	TIME [epoch: 12.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22499268787432442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22499268787432442 | validation: 0.2744100024538189]
	TIME [epoch: 12.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34060655101556087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34060655101556087 | validation: 0.8128353823926062]
	TIME [epoch: 12.6 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022586503877939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6022586503877939 | validation: 0.4486627057262938]
	TIME [epoch: 12.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2717974906290157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2717974906290157 | validation: 0.2691084164603924]
	TIME [epoch: 12.6 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30542166517086955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30542166517086955 | validation: 0.19861864133745424]
	TIME [epoch: 12.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16477208684779288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16477208684779288 | validation: 0.2263179514029278]
	TIME [epoch: 12.6 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15056878812143126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15056878812143126 | validation: 0.13972774639198154]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20027049616289383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20027049616289383 | validation: 0.3269976948753461]
	TIME [epoch: 12.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24729446568996272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24729446568996272 | validation: 0.22446351942461318]
	TIME [epoch: 12.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24581073221323174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24581073221323174 | validation: 0.16898179044450787]
	TIME [epoch: 12.6 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12783786472427408		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12783786472427408 | validation: 0.30762607478958337]
	TIME [epoch: 12.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22632866419868095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22632866419868095 | validation: 0.24477650633580952]
	TIME [epoch: 12.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34316207017391187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34316207017391187 | validation: 0.22925438104014742]
	TIME [epoch: 12.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18938998006483168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18938998006483168 | validation: 0.3305547056894438]
	TIME [epoch: 12.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.276781517437523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.276781517437523 | validation: 0.22470913124329406]
	TIME [epoch: 12.6 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22723161777445317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22723161777445317 | validation: 0.17321238636015268]
	TIME [epoch: 12.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367560613906581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367560613906581 | validation: 0.12793209682808715]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_541.pth
	Model improved!!!
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14223229395917236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14223229395917236 | validation: 0.17243350846840763]
	TIME [epoch: 12.6 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12505537064395716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12505537064395716 | validation: 0.12197937274217996]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13724877726573337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13724877726573337 | validation: 0.15914849672960552]
	TIME [epoch: 12.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13784800398278954		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13784800398278954 | validation: 0.12786768816158034]
	TIME [epoch: 12.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11182507170264261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11182507170264261 | validation: 0.13575078352963496]
	TIME [epoch: 12.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11115014166647377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11115014166647377 | validation: 0.14605613501101233]
	TIME [epoch: 12.6 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11605310190375759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11605310190375759 | validation: 0.23041175663058838]
	TIME [epoch: 12.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28022168542278025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28022168542278025 | validation: 0.42166118483278936]
	TIME [epoch: 12.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2492841270458542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2492841270458542 | validation: 0.18164348319804846]
	TIME [epoch: 12.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18495066364916357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18495066364916357 | validation: 0.30909495639097284]
	TIME [epoch: 12.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.230187991954438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.230187991954438 | validation: 0.3185344067852596]
	TIME [epoch: 12.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36170640284234423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36170640284234423 | validation: 0.24017004987045248]
	TIME [epoch: 12.6 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2519944991114419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2519944991114419 | validation: 0.4294892403686366]
	TIME [epoch: 12.6 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25550044974224156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25550044974224156 | validation: 0.23704416323754202]
	TIME [epoch: 12.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2630345698602422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2630345698602422 | validation: 0.20156902955823847]
	TIME [epoch: 12.6 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15771698245085508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15771698245085508 | validation: 0.2296373387197598]
	TIME [epoch: 12.6 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17219101284598265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17219101284598265 | validation: 0.1714084090478144]
	TIME [epoch: 12.6 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21486035564486672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21486035564486672 | validation: 0.24900679295788652]
	TIME [epoch: 12.6 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15379215265264598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15379215265264598 | validation: 0.13490748188038096]
	TIME [epoch: 12.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15336531646479906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15336531646479906 | validation: 0.16675363789227476]
	TIME [epoch: 12.6 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11850577404608867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11850577404608867 | validation: 0.11870849032884681]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_562.pth
	Model improved!!!
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10803334498070498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10803334498070498 | validation: 0.1637475344470939]
	TIME [epoch: 12.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12213249576184591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12213249576184591 | validation: 0.15192941295554127]
	TIME [epoch: 12.6 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15400449618742948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15400449618742948 | validation: 0.2977622228536199]
	TIME [epoch: 12.6 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2038290084719499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2038290084719499 | validation: 0.2552097027019708]
	TIME [epoch: 12.6 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30982457125793583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30982457125793583 | validation: 0.23340024980109036]
	TIME [epoch: 12.6 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16731074951154468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16731074951154468 | validation: 0.24957864298490937]
	TIME [epoch: 12.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1685955187252803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1685955187252803 | validation: 0.22261165571418814]
	TIME [epoch: 12.6 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28304450595410924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28304450595410924 | validation: 0.3297669925892944]
	TIME [epoch: 12.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20781311456847598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20781311456847598 | validation: 0.18386053704149827]
	TIME [epoch: 12.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14385709635728802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14385709635728802 | validation: 0.1423438407067155]
	TIME [epoch: 12.6 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1973957081342741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1973957081342741 | validation: 0.3071828975891664]
	TIME [epoch: 12.6 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16228846474657552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16228846474657552 | validation: 0.15810815129734462]
	TIME [epoch: 12.6 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16655958918430666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16655958918430666 | validation: 0.17794061569732406]
	TIME [epoch: 12.6 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12097786263868393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12097786263868393 | validation: 0.11371343034729918]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_576.pth
	Model improved!!!
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1238143544000594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1238143544000594 | validation: 0.2186662847934477]
	TIME [epoch: 12.6 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1283661708982137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1283661708982137 | validation: 0.14267959903651373]
	TIME [epoch: 12.6 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1648603797102122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1648603797102122 | validation: 0.2979160260116777]
	TIME [epoch: 12.6 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17123275908181224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17123275908181224 | validation: 0.18668985449988723]
	TIME [epoch: 12.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20626047325052838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20626047325052838 | validation: 0.1605901473599915]
	TIME [epoch: 12.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10675656236831219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10675656236831219 | validation: 0.11668126812290076]
	TIME [epoch: 12.6 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511844034553946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09511844034553946 | validation: 0.11035674200249446]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_583.pth
	Model improved!!!
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08721694906400101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08721694906400101 | validation: 0.11699470200309103]
	TIME [epoch: 12.6 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08800480526708224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08800480526708224 | validation: 0.11005779661990492]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_585.pth
	Model improved!!!
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08537496266566315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08537496266566315 | validation: 0.15468585618111869]
	TIME [epoch: 12.6 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09650726106080285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09650726106080285 | validation: 0.15842490732541645]
	TIME [epoch: 12.6 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21526832643583124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21526832643583124 | validation: 0.6413809931174242]
	TIME [epoch: 12.6 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4491292384880602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4491292384880602 | validation: 0.2788906090285904]
	TIME [epoch: 12.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25325133809856193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25325133809856193 | validation: 0.09710816144097226]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_590.pth
	Model improved!!!
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327920879091071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1327920879091071 | validation: 0.457454223006148]
	TIME [epoch: 12.6 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2774251989902407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2774251989902407 | validation: 0.27924446513832574]
	TIME [epoch: 12.6 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32236943753409253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32236943753409253 | validation: 0.2340089376587657]
	TIME [epoch: 12.6 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2109233577215124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2109233577215124 | validation: 0.307858721109193]
	TIME [epoch: 12.6 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1696280675238318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1696280675238318 | validation: 0.20335293793096995]
	TIME [epoch: 12.6 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21861885569592715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21861885569592715 | validation: 0.18161215852691384]
	TIME [epoch: 12.6 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15200063997818608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15200063997818608 | validation: 0.096708297365825]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0962041847106564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0962041847106564 | validation: 0.11248475525488347]
	TIME [epoch: 12.6 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09099859257442759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09099859257442759 | validation: 0.10191897443755189]
	TIME [epoch: 12.6 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08983878780091775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08983878780091775 | validation: 0.09741575887265702]
	TIME [epoch: 12.6 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09421928042972834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09421928042972834 | validation: 0.20772670701012497]
	TIME [epoch: 12.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13374292786134878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13374292786134878 | validation: 0.178632120397151]
	TIME [epoch: 12.6 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21129129292763138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21129129292763138 | validation: 0.3039674912319274]
	TIME [epoch: 12.6 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17564197369105933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17564197369105933 | validation: 0.12599926071137532]
	TIME [epoch: 12.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14428259252170075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14428259252170075 | validation: 0.2301263641674876]
	TIME [epoch: 12.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11880741853716092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11880741853716092 | validation: 0.08209710211305087]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_606.pth
	Model improved!!!
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1106321337574841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1106321337574841 | validation: 0.20462336314737262]
	TIME [epoch: 12.6 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13201028742912874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13201028742912874 | validation: 0.2107011659569203]
	TIME [epoch: 12.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19576483465900754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19576483465900754 | validation: 0.1220439164399593]
	TIME [epoch: 12.6 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10290788100284835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10290788100284835 | validation: 0.1416616012967211]
	TIME [epoch: 12.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11782899577158297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11782899577158297 | validation: 0.09137200242791749]
	TIME [epoch: 12.6 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09004272015406155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09004272015406155 | validation: 0.18323607305546694]
	TIME [epoch: 12.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10322731532015102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10322731532015102 | validation: 0.12316491154194656]
	TIME [epoch: 12.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19171280246975722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19171280246975722 | validation: 0.4918390184190809]
	TIME [epoch: 12.6 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2888858916164848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2888858916164848 | validation: 0.2126938741440641]
	TIME [epoch: 12.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21564572682828098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21564572682828098 | validation: 0.1459313206222396]
	TIME [epoch: 12.6 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10949262403000852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10949262403000852 | validation: 0.16349769824312632]
	TIME [epoch: 12.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10542807126690203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10542807126690203 | validation: 0.11350203974003376]
	TIME [epoch: 12.6 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1454807285724882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1454807285724882 | validation: 0.2310691292659696]
	TIME [epoch: 12.6 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13933750556301494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13933750556301494 | validation: 0.10367937106235187]
	TIME [epoch: 12.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10859711400245231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10859711400245231 | validation: 0.21056607340648192]
	TIME [epoch: 12.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11054560205934372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11054560205934372 | validation: 0.08593194382103715]
	TIME [epoch: 12.6 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10522565684549985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10522565684549985 | validation: 0.19827664949693186]
	TIME [epoch: 12.6 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1184230242609759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1184230242609759 | validation: 0.1331989283067277]
	TIME [epoch: 12.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15076984623975404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15076984623975404 | validation: 0.21651629309007792]
	TIME [epoch: 12.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1630023931421481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1630023931421481 | validation: 0.12360274135548227]
	TIME [epoch: 12.6 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12855858641586765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12855858641586765 | validation: 0.23388392119378143]
	TIME [epoch: 12.6 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12525021975452713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12525021975452713 | validation: 0.11770799625330673]
	TIME [epoch: 12.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1926928613469345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1926928613469345 | validation: 0.31053598316839626]
	TIME [epoch: 12.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16712423133049736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16712423133049736 | validation: 0.11487470097057498]
	TIME [epoch: 12.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1311066646318034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1311066646318034 | validation: 0.1407561808102141]
	TIME [epoch: 12.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09305838951454654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09305838951454654 | validation: 0.11586202743111587]
	TIME [epoch: 12.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08020483535538815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08020483535538815 | validation: 0.10943835317747691]
	TIME [epoch: 12.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192554156480504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192554156480504 | validation: 0.08325427299386154]
	TIME [epoch: 12.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08182801606760222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08182801606760222 | validation: 0.11141955013853361]
	TIME [epoch: 12.6 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07760837469449067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07760837469449067 | validation: 0.07745342058894572]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_636.pth
	Model improved!!!
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09168892355066603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09168892355066603 | validation: 0.2825291690579299]
	TIME [epoch: 12.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170296380915081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.170296380915081 | validation: 0.19454452276175505]
	TIME [epoch: 12.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26639685175121813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26639685175121813 | validation: 0.24300838864705412]
	TIME [epoch: 12.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12250591207537728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12250591207537728 | validation: 0.13834946393374248]
	TIME [epoch: 12.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11188186251150503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11188186251150503 | validation: 0.09164617008570146]
	TIME [epoch: 12.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13410106694264287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13410106694264287 | validation: 0.20405383758035828]
	TIME [epoch: 12.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12994869908914874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12994869908914874 | validation: 0.1180711025553308]
	TIME [epoch: 12.6 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1354151193538441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1354151193538441 | validation: 0.16803460949422]
	TIME [epoch: 12.6 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11391137415538953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11391137415538953 | validation: 0.09977863798013904]
	TIME [epoch: 12.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0924237865135107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0924237865135107 | validation: 0.16651010318818504]
	TIME [epoch: 12.6 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09269788538492625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09269788538492625 | validation: 0.09772254283426608]
	TIME [epoch: 12.6 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13144860340498676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13144860340498676 | validation: 0.3496312560986239]
	TIME [epoch: 12.6 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17602819448927107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17602819448927107 | validation: 0.14217882263500234]
	TIME [epoch: 12.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2217765980435479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2217765980435479 | validation: 0.2144880107144701]
	TIME [epoch: 12.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11966267804746164		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11966267804746164 | validation: 0.16925861873869516]
	TIME [epoch: 12.6 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11404329371803233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11404329371803233 | validation: 0.0767104332387279]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_652.pth
	Model improved!!!
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09743008237653296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09743008237653296 | validation: 0.16137937814057357]
	TIME [epoch: 12.6 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09250722139279552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09250722139279552 | validation: 0.11544112824368705]
	TIME [epoch: 12.6 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1208382018871501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1208382018871501 | validation: 0.19519214930600734]
	TIME [epoch: 12.6 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14365702560424332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14365702560424332 | validation: 0.14174553899613845]
	TIME [epoch: 12.6 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13330516453144042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13330516453144042 | validation: 0.18432842056802856]
	TIME [epoch: 12.6 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11343561452466103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11343561452466103 | validation: 0.0818497176543293]
	TIME [epoch: 12.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10969427773110198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10969427773110198 | validation: 0.3675753541688638]
	TIME [epoch: 12.6 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3503076212902091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3503076212902091 | validation: 0.31995679329570903]
	TIME [epoch: 12.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2765146544596721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2765146544596721 | validation: 0.24517945838108102]
	TIME [epoch: 12.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23594608030463804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23594608030463804 | validation: 0.16725960198971052]
	TIME [epoch: 12.6 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16336453519514002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16336453519514002 | validation: 0.12261533726650403]
	TIME [epoch: 12.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10033296066485538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10033296066485538 | validation: 0.09696780981557077]
	TIME [epoch: 12.6 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08150426737188327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08150426737188327 | validation: 0.13837595503233677]
	TIME [epoch: 12.6 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972957770058099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07972957770058099 | validation: 0.07450830024200016]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_666.pth
	Model improved!!!
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08007041702878871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08007041702878871 | validation: 0.18418354784433877]
	TIME [epoch: 12.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10764051765376569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10764051765376569 | validation: 0.14735361988710322]
	TIME [epoch: 12.6 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18373070473700856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18373070473700856 | validation: 0.24946734752090088]
	TIME [epoch: 12.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18928353832838402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18928353832838402 | validation: 0.08947026556279675]
	TIME [epoch: 12.6 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08952272262432424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08952272262432424 | validation: 0.10035942094304494]
	TIME [epoch: 12.6 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.079844775269518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.079844775269518 | validation: 0.08944864012834695]
	TIME [epoch: 12.6 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954515509093395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07954515509093395 | validation: 0.09465542153752174]
	TIME [epoch: 12.6 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10122904858697734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10122904858697734 | validation: 0.33545695503278067]
	TIME [epoch: 12.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25330200627742694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25330200627742694 | validation: 0.13327137451345372]
	TIME [epoch: 12.6 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09888587993185098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09888587993185098 | validation: 0.0993327166973123]
	TIME [epoch: 12.6 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12593785559153275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12593785559153275 | validation: 0.2136960153764158]
	TIME [epoch: 12.6 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11825887077735153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11825887077735153 | validation: 0.11157872559004334]
	TIME [epoch: 12.6 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1513285751395052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1513285751395052 | validation: 0.3558578096326226]
	TIME [epoch: 12.6 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16761358771208054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16761358771208054 | validation: 0.09280788570016037]
	TIME [epoch: 12.6 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10888607957478595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10888607957478595 | validation: 0.10652884517694283]
	TIME [epoch: 12.6 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0724617605607833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0724617605607833 | validation: 0.10313740432511441]
	TIME [epoch: 12.6 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07612107899072118		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07612107899072118 | validation: 0.07707852810524653]
	TIME [epoch: 12.6 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0711165053466737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0711165053466737 | validation: 0.10084826311469593]
	TIME [epoch: 12.6 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06969883156979577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06969883156979577 | validation: 0.06995984621890657]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_685.pth
	Model improved!!!
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911679316700717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06911679316700717 | validation: 0.12723047388215988]
	TIME [epoch: 12.6 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07320584519107008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07320584519107008 | validation: 0.08019054882899623]
	TIME [epoch: 12.6 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11370002640414896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11370002640414896 | validation: 0.3210395584368395]
	TIME [epoch: 12.6 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16713468454062394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16713468454062394 | validation: 0.12610507111104927]
	TIME [epoch: 12.6 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16176525993509763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16176525993509763 | validation: 0.2925962254305489]
	TIME [epoch: 12.6 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14637958273866233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14637958273866233 | validation: 0.11715194892034142]
	TIME [epoch: 12.6 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546438631139878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11546438631139878 | validation: 0.10982561516556935]
	TIME [epoch: 12.6 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08966073028344783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08966073028344783 | validation: 0.0964085420737566]
	TIME [epoch: 12.6 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08348537249493126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08348537249493126 | validation: 0.08982589592133072]
	TIME [epoch: 12.6 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08670377930900049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08670377930900049 | validation: 0.10134040952995556]
	TIME [epoch: 12.6 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0822899901606058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0822899901606058 | validation: 0.10609048729981424]
	TIME [epoch: 12.6 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07379331233451851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07379331233451851 | validation: 0.07063472758225008]
	TIME [epoch: 12.6 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06899930161946018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06899930161946018 | validation: 0.1541078003561618]
	TIME [epoch: 12.6 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10216088197524553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10216088197524553 | validation: 0.13333385640677428]
	TIME [epoch: 12.6 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18899369949011469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18899369949011469 | validation: 0.3414907226343597]
	TIME [epoch: 12.6 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19716394240415852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19716394240415852 | validation: 0.08634249479004169]
	TIME [epoch: 12.6 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08287372583364949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08287372583364949 | validation: 0.07586571643181336]
	TIME [epoch: 12.6 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08936318130326733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08936318130326733 | validation: 0.09600220173953963]
	TIME [epoch: 12.6 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07710251579030986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07710251579030986 | validation: 0.0838619824262491]
	TIME [epoch: 12.6 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08779583448211593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08779583448211593 | validation: 0.13340773507696027]
	TIME [epoch: 12.6 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11950542534696026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11950542534696026 | validation: 0.08370471039241863]
	TIME [epoch: 12.6 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10435894671791243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10435894671791243 | validation: 0.14444523949203134]
	TIME [epoch: 12.6 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09583896985377585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09583896985377585 | validation: 0.06492253493843615]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_708.pth
	Model improved!!!
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08685987711213024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08685987711213024 | validation: 0.16635503592855427]
	TIME [epoch: 12.6 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10052122179501709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10052122179501709 | validation: 0.07410598197675874]
	TIME [epoch: 12.6 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11340790690524884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11340790690524884 | validation: 0.2001983386334757]
	TIME [epoch: 12.6 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11485495140087332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11485495140087332 | validation: 0.07493486291008421]
	TIME [epoch: 12.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09562482975844112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09562482975844112 | validation: 0.1386285218300853]
	TIME [epoch: 12.6 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269568423786933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08269568423786933 | validation: 0.07851949930303027]
	TIME [epoch: 12.6 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08905365179401788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08905365179401788 | validation: 0.13141770903622502]
	TIME [epoch: 12.6 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10295650647859578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10295650647859578 | validation: 0.07792493747359104]
	TIME [epoch: 12.6 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09983332644722476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09983332644722476 | validation: 0.14363541754361284]
	TIME [epoch: 12.6 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09606219931091142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09606219931091142 | validation: 0.05360839733787312]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_718.pth
	Model improved!!!
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600749402352228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600749402352228 | validation: 0.1286027054901275]
	TIME [epoch: 12.7 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08005934629633091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08005934629633091 | validation: 0.06750848916479066]
	TIME [epoch: 12.7 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10832543961507586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10832543961507586 | validation: 0.2548264460627933]
	TIME [epoch: 12.7 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14250991053862028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14250991053862028 | validation: 0.07646624908095098]
	TIME [epoch: 12.7 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09667311855852326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09667311855852326 | validation: 0.08446811633550144]
	TIME [epoch: 12.7 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07997527298939149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07997527298939149 | validation: 0.10685045786726488]
	TIME [epoch: 12.7 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08781064414366054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08781064414366054 | validation: 0.07749466961221617]
	TIME [epoch: 12.7 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08783183490663994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08783183490663994 | validation: 0.10169270107814153]
	TIME [epoch: 12.7 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09182864677586171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09182864677586171 | validation: 0.07631269322388265]
	TIME [epoch: 12.7 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08351848366384232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08351848366384232 | validation: 0.12536222127243188]
	TIME [epoch: 12.6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08957333645741714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08957333645741714 | validation: 0.06618676646992998]
	TIME [epoch: 12.7 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10335767496879857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10335767496879857 | validation: 0.37089776070981045]
	TIME [epoch: 12.6 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1976794833633198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1976794833633198 | validation: 0.06730362775260858]
	TIME [epoch: 12.6 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06978024242124047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06978024242124047 | validation: 0.054332403807140675]
	TIME [epoch: 12.6 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07690092893955339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07690092893955339 | validation: 0.09621221532537588]
	TIME [epoch: 12.6 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07151978681232515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07151978681232515 | validation: 0.08770170785271827]
	TIME [epoch: 12.6 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07943082777919655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07943082777919655 | validation: 0.11759735621638584]
	TIME [epoch: 12.6 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09204807668687515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09204807668687515 | validation: 0.06299924598121544]
	TIME [epoch: 12.6 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08145728271318095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08145728271318095 | validation: 0.11849623693394194]
	TIME [epoch: 12.7 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308023732971275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09308023732971275 | validation: 0.06580791712049214]
	TIME [epoch: 12.7 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08021343750872766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08021343750872766 | validation: 0.08777365742396195]
	TIME [epoch: 12.7 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06571970388143035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06571970388143035 | validation: 0.055956715567211004]
	TIME [epoch: 12.7 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06263995621786146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06263995621786146 | validation: 0.08842971406948233]
	TIME [epoch: 12.7 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0759265483724072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0759265483724072 | validation: 0.08889571807181687]
	TIME [epoch: 12.7 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09056566252624065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09056566252624065 | validation: 0.11076611281571602]
	TIME [epoch: 12.7 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09645086194370225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09645086194370225 | validation: 0.059554790166259725]
	TIME [epoch: 12.7 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06974804315201806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06974804315201806 | validation: 0.11570860258090453]
	TIME [epoch: 12.7 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792019455179056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07792019455179056 | validation: 0.13226342535440994]
	TIME [epoch: 12.7 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21688431082035492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21688431082035492 | validation: 0.30259582156762777]
	TIME [epoch: 12.6 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15994710144496588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15994710144496588 | validation: 0.0811985496357888]
	TIME [epoch: 12.6 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0601639168877361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0601639168877361 | validation: 0.05918107274262466]
	TIME [epoch: 12.6 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07804229373952297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07804229373952297 | validation: 0.1106714003753828]
	TIME [epoch: 12.7 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09253342684803773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09253342684803773 | validation: 0.06793794986300133]
	TIME [epoch: 12.6 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07113210014003021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07113210014003021 | validation: 0.0695725221169665]
	TIME [epoch: 12.7 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07544229168326241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07544229168326241 | validation: 0.09681998143422071]
	TIME [epoch: 12.7 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07919526343276224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07919526343276224 | validation: 0.0898980428204477]
	TIME [epoch: 12.7 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06883133495746156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06883133495746156 | validation: 0.04925347375699758]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_755.pth
	Model improved!!!
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05779338571023314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05779338571023314 | validation: 0.08019943322320461]
	TIME [epoch: 12.6 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06758051639377191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06758051639377191 | validation: 0.07358599318297433]
	TIME [epoch: 12.6 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09570373573511909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09570373573511909 | validation: 0.27620370076403555]
	TIME [epoch: 12.6 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461660082076304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461660082076304 | validation: 0.0606142424451437]
	TIME [epoch: 12.6 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08849941168420082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08849941168420082 | validation: 0.13925555724900035]
	TIME [epoch: 12.6 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08113166700476543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08113166700476543 | validation: 0.06121584898734103]
	TIME [epoch: 12.6 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056502187340224774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056502187340224774 | validation: 0.05578453517165584]
	TIME [epoch: 12.6 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06155055146640926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06155055146640926 | validation: 0.05798251046853057]
	TIME [epoch: 12.6 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05702298781079734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05702298781079734 | validation: 0.053547876563205955]
	TIME [epoch: 12.6 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058576552554028334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058576552554028334 | validation: 0.06060723624607918]
	TIME [epoch: 12.6 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058791860466370305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058791860466370305 | validation: 0.062439110701704985]
	TIME [epoch: 12.6 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06136754736206729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06136754736206729 | validation: 0.08944732792591768]
	TIME [epoch: 12.6 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08375847941727235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08375847941727235 | validation: 0.08931273512677487]
	TIME [epoch: 12.6 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10030731819878412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10030731819878412 | validation: 0.1929207309122835]
	TIME [epoch: 12.6 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12068886378096777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12068886378096777 | validation: 0.09654942350594391]
	TIME [epoch: 12.6 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14271364318491694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14271364318491694 | validation: 0.16357094463567162]
	TIME [epoch: 12.6 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09362960551839343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09362960551839343 | validation: 0.06318450468199194]
	TIME [epoch: 12.5 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05725165166445681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05725165166445681 | validation: 0.0473546508178198]
	TIME [epoch: 12.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_773.pth
	Model improved!!!
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05346650258876553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05346650258876553 | validation: 0.06250151691316794]
	TIME [epoch: 12.6 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060347640381274915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060347640381274915 | validation: 0.07628584911655566]
	TIME [epoch: 12.6 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07369375893560491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07369375893560491 | validation: 0.09464974556147052]
	TIME [epoch: 12.6 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584021701364627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09584021701364627 | validation: 0.08625120611326258]
	TIME [epoch: 12.6 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09834076526961721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09834076526961721 | validation: 0.2092720370596336]
	TIME [epoch: 12.6 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14442575108864347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14442575108864347 | validation: 0.10125247075245102]
	TIME [epoch: 12.6 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272591947133948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12272591947133948 | validation: 0.1683661993672475]
	TIME [epoch: 12.6 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13406574331934137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13406574331934137 | validation: 0.04824087916541131]
	TIME [epoch: 12.6 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050652534481408634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050652534481408634 | validation: 0.04828291310361319]
	TIME [epoch: 12.6 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017556626213925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05017556626213925 | validation: 0.07456676219957485]
	TIME [epoch: 12.6 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06046895450583902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06046895450583902 | validation: 0.0419373691085527]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_784.pth
	Model improved!!!
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0492752988789134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0492752988789134 | validation: 0.055656618860092245]
	TIME [epoch: 12.6 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05964229692244111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05964229692244111 | validation: 0.11225450894710319]
	TIME [epoch: 12.6 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09285686997032176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09285686997032176 | validation: 0.08852106816917973]
	TIME [epoch: 12.6 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0949141034739179		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0949141034739179 | validation: 0.09099755172114335]
	TIME [epoch: 12.6 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08274030714152925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08274030714152925 | validation: 0.058607530940337195]
	TIME [epoch: 12.6 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0715995381602827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0715995381602827 | validation: 0.06275755798419383]
	TIME [epoch: 12.6 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06906828473760458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06906828473760458 | validation: 0.07453984321388717]
	TIME [epoch: 12.6 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09169564687721961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09169564687721961 | validation: 0.08906547955258248]
	TIME [epoch: 12.6 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08743123392189471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08743123392189471 | validation: 0.06004529907864597]
	TIME [epoch: 12.6 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05747010156948035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05747010156948035 | validation: 0.050122621584658904]
	TIME [epoch: 12.6 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046494632002692694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046494632002692694 | validation: 0.04104354402736519]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_795.pth
	Model improved!!!
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04786364442454673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04786364442454673 | validation: 0.08705710835935246]
	TIME [epoch: 12.6 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059103162309314075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059103162309314075 | validation: 0.06747540537902592]
	TIME [epoch: 12.6 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09112132453852638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09112132453852638 | validation: 0.2947514328126478]
	TIME [epoch: 12.6 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18639258576792564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18639258576792564 | validation: 0.07438600792573473]
	TIME [epoch: 12.6 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10334509714058339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10334509714058339 | validation: 0.07489851254787773]
	TIME [epoch: 12.6 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443769059848105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06443769059848105 | validation: 0.058364010736608234]
	TIME [epoch: 12.6 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047305134479486526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047305134479486526 | validation: 0.03963338389402801]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_802.pth
	Model improved!!!
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04543256308722055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04543256308722055 | validation: 0.053634882026496015]
	TIME [epoch: 12.6 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05226115031784701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05226115031784701 | validation: 0.048557961646608416]
	TIME [epoch: 12.6 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06027140768744337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06027140768744337 | validation: 0.08729101609605211]
	TIME [epoch: 12.6 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08986684489190026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08986684489190026 | validation: 0.09982128059230398]
	TIME [epoch: 12.6 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09291371765718456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09291371765718456 | validation: 0.06801482689794774]
	TIME [epoch: 12.6 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07255116088038137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07255116088038137 | validation: 0.05461664324311773]
	TIME [epoch: 12.6 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05679733228536514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05679733228536514 | validation: 0.07132673191049056]
	TIME [epoch: 12.6 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05785364720247049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05785364720247049 | validation: 0.052075899342935744]
	TIME [epoch: 12.6 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060140292583507955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060140292583507955 | validation: 0.11778113625488787]
	TIME [epoch: 12.6 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0832546523850985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0832546523850985 | validation: 0.11585761473662559]
	TIME [epoch: 12.6 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16264749547811627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16264749547811627 | validation: 0.2714589225930161]
	TIME [epoch: 12.6 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367297381294145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367297381294145 | validation: 0.059475352887447736]
	TIME [epoch: 12.6 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05647101802099835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05647101802099835 | validation: 0.06720076985568521]
	TIME [epoch: 12.6 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05859744753996318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05859744753996318 | validation: 0.04293626547025924]
	TIME [epoch: 12.6 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0601100589759835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0601100589759835 | validation: 0.07388995179706591]
	TIME [epoch: 12.6 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815599327834826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05815599327834826 | validation: 0.04919626893734358]
	TIME [epoch: 12.6 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05495617581573636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05495617581573636 | validation: 0.06087921004740937]
	TIME [epoch: 12.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06481662256643395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06481662256643395 | validation: 0.07136167821714495]
	TIME [epoch: 12.6 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07202154233549699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07202154233549699 | validation: 0.07443512573800394]
	TIME [epoch: 12.6 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08078613623812197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08078613623812197 | validation: 0.08148148534939974]
	TIME [epoch: 12.6 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462103951951692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08462103951951692 | validation: 0.08565518293878802]
	TIME [epoch: 12.6 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07372040081419222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07372040081419222 | validation: 0.057751380058186454]
	TIME [epoch: 12.6 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07122113433097739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07122113433097739 | validation: 0.09963715807740688]
	TIME [epoch: 12.6 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741088182847483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0741088182847483 | validation: 0.06288769753870416]
	TIME [epoch: 12.6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09421599183538942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09421599183538942 | validation: 0.20345917635880678]
	TIME [epoch: 12.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11759634408447028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11759634408447028 | validation: 0.03162956502294254]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04489228257832177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04489228257832177 | validation: 0.026717720204594998]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_829.pth
	Model improved!!!
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03612495098685313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03612495098685313 | validation: 0.051125432367697454]
	TIME [epoch: 12.6 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044080334429820525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044080334429820525 | validation: 0.08389560709585263]
	TIME [epoch: 12.6 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08745191160785744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08745191160785744 | validation: 0.10381525990637952]
	TIME [epoch: 12.6 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435734916745279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07435734916745279 | validation: 0.060588919447406575]
	TIME [epoch: 12.6 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06368305641042736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06368305641042736 | validation: 0.0570692901599672]
	TIME [epoch: 12.6 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05665048735423461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05665048735423461 | validation: 0.09780150553314682]
	TIME [epoch: 12.6 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369139696442869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08369139696442869 | validation: 0.0641755575866683]
	TIME [epoch: 12.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06852052621367859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06852052621367859 | validation: 0.08070689985362045]
	TIME [epoch: 12.6 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09359328165738533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09359328165738533 | validation: 0.07994938242292082]
	TIME [epoch: 12.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970890510189749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970890510189749 | validation: 0.08909197060246751]
	TIME [epoch: 12.6 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10093849552225645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10093849552225645 | validation: 0.18913532163671065]
	TIME [epoch: 12.6 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11634916124297863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11634916124297863 | validation: 0.10042379475483354]
	TIME [epoch: 12.6 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1250495698066702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1250495698066702 | validation: 0.12867516944803797]
	TIME [epoch: 12.6 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0734508980272384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0734508980272384 | validation: 0.05335908159492258]
	TIME [epoch: 12.6 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048080414640741774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048080414640741774 | validation: 0.0680524926816674]
	TIME [epoch: 12.6 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05804863834047204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05804863834047204 | validation: 0.09672434717344833]
	TIME [epoch: 12.6 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438587626863024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09438587626863024 | validation: 0.09516847828278184]
	TIME [epoch: 12.6 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08256832380783634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08256832380783634 | validation: 0.0574737219677522]
	TIME [epoch: 12.6 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06464863054065384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06464863054065384 | validation: 0.08254815202742588]
	TIME [epoch: 12.6 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06236918446792067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06236918446792067 | validation: 0.06360689301394178]
	TIME [epoch: 12.6 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07311385491180543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07311385491180543 | validation: 0.06915299871405066]
	TIME [epoch: 12.6 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0680613978703493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0680613978703493 | validation: 0.04321485473052799]
	TIME [epoch: 12.6 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05012478621089152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05012478621089152 | validation: 0.04202125897992536]
	TIME [epoch: 12.6 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04347454275989367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04347454275989367 | validation: 0.048034524291928206]
	TIME [epoch: 12.6 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046243201187394924		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046243201187394924 | validation: 0.04375716511586036]
	TIME [epoch: 12.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052563234788480184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052563234788480184 | validation: 0.0634610838989552]
	TIME [epoch: 12.6 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05953484754034524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05953484754034524 | validation: 0.05116903689639109]
	TIME [epoch: 12.6 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0682591729985332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0682591729985332 | validation: 0.08344130001218542]
	TIME [epoch: 12.6 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07058904676626172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07058904676626172 | validation: 0.043590891401716625]
	TIME [epoch: 12.6 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055574257585536524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055574257585536524 | validation: 0.09295492111250477]
	TIME [epoch: 12.6 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060430794030706995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060430794030706995 | validation: 0.06872338261720577]
	TIME [epoch: 12.6 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11546421309339387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11546421309339387 | validation: 0.21162769414378416]
	TIME [epoch: 12.6 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13107388152976227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13107388152976227 | validation: 0.06745593514766324]
	TIME [epoch: 12.6 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06716705107314193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06716705107314193 | validation: 0.04091987373526762]
	TIME [epoch: 12.6 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04924998548747115		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04924998548747115 | validation: 0.06463151167058832]
	TIME [epoch: 12.6 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05285195048242029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05285195048242029 | validation: 0.057424930713113866]
	TIME [epoch: 12.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05963412698277749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05963412698277749 | validation: 0.060810935952294747]
	TIME [epoch: 12.6 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06407204699375804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06407204699375804 | validation: 0.0993782025299374]
	TIME [epoch: 12.6 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242965225753718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08242965225753718 | validation: 0.07484901683092497]
	TIME [epoch: 12.6 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895982469594437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0895982469594437 | validation: 0.07250296272447225]
	TIME [epoch: 12.6 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186583523169925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07186583523169925 | validation: 0.05520150620401179]
	TIME [epoch: 12.6 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06317350059048456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06317350059048456 | validation: 0.07769339993631848]
	TIME [epoch: 12.6 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057017819913895124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057017819913895124 | validation: 0.036190788313030485]
	TIME [epoch: 12.6 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05549529269269733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05549529269269733 | validation: 0.05550725023120854]
	TIME [epoch: 12.6 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05139196132487913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05139196132487913 | validation: 0.061294354170744185]
	TIME [epoch: 12.6 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057624480334436845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057624480334436845 | validation: 0.052414824121933046]
	TIME [epoch: 12.6 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056732146124634716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056732146124634716 | validation: 0.04223852959594844]
	TIME [epoch: 12.6 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04514966385526169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04514966385526169 | validation: 0.05492261858671288]
	TIME [epoch: 12.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05739954788489927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05739954788489927 | validation: 0.07853176165622303]
	TIME [epoch: 12.6 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08023727669775063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08023727669775063 | validation: 0.11541790389003173]
	TIME [epoch: 12.6 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10267185589172333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10267185589172333 | validation: 0.06628138297636621]
	TIME [epoch: 12.6 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08969413384061369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08969413384061369 | validation: 0.06241316390481164]
	TIME [epoch: 12.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06400900203928657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06400900203928657 | validation: 0.03787896782942987]
	TIME [epoch: 12.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034248977018451074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034248977018451074 | validation: 0.048897252153814776]
	TIME [epoch: 12.6 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046974610069774075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046974610069774075 | validation: 0.09652352654814733]
	TIME [epoch: 12.6 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07554249872922589		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07554249872922589 | validation: 0.06896215036126568]
	TIME [epoch: 12.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07919265551483512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07919265551483512 | validation: 0.06725745600001863]
	TIME [epoch: 12.6 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061520857674605266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061520857674605266 | validation: 0.05675356387835726]
	TIME [epoch: 12.6 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0504204319004392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0504204319004392 | validation: 0.03461035686004599]
	TIME [epoch: 12.6 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04002098480459518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04002098480459518 | validation: 0.0572412493022858]
	TIME [epoch: 12.6 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04387795052159064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04387795052159064 | validation: 0.031093148302603058]
	TIME [epoch: 12.6 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045462062676897064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045462062676897064 | validation: 0.05642908977222275]
	TIME [epoch: 12.6 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05128224077121612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05128224077121612 | validation: 0.05278369501530309]
	TIME [epoch: 12.6 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06908670462525718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06908670462525718 | validation: 0.18659257333293372]
	TIME [epoch: 12.6 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13041931277963986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13041931277963986 | validation: 0.10818113267040337]
	TIME [epoch: 12.6 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10696107603637867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10696107603637867 | validation: 0.0831713510461603]
	TIME [epoch: 12.6 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08559452359665659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08559452359665659 | validation: 0.06985388806395827]
	TIME [epoch: 12.6 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0664638318412323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0664638318412323 | validation: 0.046439048148767564]
	TIME [epoch: 12.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05244657031686578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05244657031686578 | validation: 0.03970043067844056]
	TIME [epoch: 12.6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392223463071965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0392223463071965 | validation: 0.029512197221401304]
	TIME [epoch: 12.6 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023886748857786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04023886748857786 | validation: 0.05736634024020132]
	TIME [epoch: 12.6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053639822069967716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053639822069967716 | validation: 0.03962716153784745]
	TIME [epoch: 12.6 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060155634465827196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060155634465827196 | validation: 0.06394260252382156]
	TIME [epoch: 12.6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058042052125099886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058042052125099886 | validation: 0.03642237118122016]
	TIME [epoch: 12.6 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219132028380779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05219132028380779 | validation: 0.053348775890963686]
	TIME [epoch: 12.6 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04803378649977304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04803378649977304 | validation: 0.037984639391828626]
	TIME [epoch: 12.6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04612966985034387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04612966985034387 | validation: 0.05700796341712353]
	TIME [epoch: 12.6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04955176943864602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04955176943864602 | validation: 0.046721162611849956]
	TIME [epoch: 12.6 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05470338421820059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05470338421820059 | validation: 0.057206063925361844]
	TIME [epoch: 12.6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06759282998589199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06759282998589199 | validation: 0.1404145569889399]
	TIME [epoch: 12.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10836371381645213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10836371381645213 | validation: 0.10127392671791952]
	TIME [epoch: 12.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18672762241567312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18672762241567312 | validation: 0.09372938257677074]
	TIME [epoch: 12.6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06409503501985837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06409503501985837 | validation: 0.07754747277678598]
	TIME [epoch: 12.6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05958547930380213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05958547930380213 | validation: 0.05303142084844638]
	TIME [epoch: 12.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06923395250348727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06923395250348727 | validation: 0.03583457878951667]
	TIME [epoch: 12.6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03977945215969239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03977945215969239 | validation: 0.026643963121436555]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_915.pth
	Model improved!!!
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029251099180665055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029251099180665055 | validation: 0.018371267104716395]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_916.pth
	Model improved!!!
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027793906137862274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027793906137862274 | validation: 0.03295411412603882]
	TIME [epoch: 12.6 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520657330616225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03520657330616225 | validation: 0.050290969980716076]
	TIME [epoch: 12.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056204156803341336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056204156803341336 | validation: 0.06480655312070036]
	TIME [epoch: 12.6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05926263334694312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05926263334694312 | validation: 0.08367617954241113]
	TIME [epoch: 12.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09083487910727689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09083487910727689 | validation: 0.1012347806311001]
	TIME [epoch: 12.6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10842782088069176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10842782088069176 | validation: 0.05037149030988955]
	TIME [epoch: 12.6 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06664444725182465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06664444725182465 | validation: 0.05411807919092341]
	TIME [epoch: 12.6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035979011322856795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035979011322856795 | validation: 0.02665604941019342]
	TIME [epoch: 12.6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03019958132128082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03019958132128082 | validation: 0.033747348273315006]
	TIME [epoch: 12.6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621160020680347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03621160020680347 | validation: 0.039233560561188666]
	TIME [epoch: 12.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0427384797393701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0427384797393701 | validation: 0.05074993791672832]
	TIME [epoch: 12.6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055149380083391354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055149380083391354 | validation: 0.05965573224209511]
	TIME [epoch: 12.6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07660871871536525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07660871871536525 | validation: 0.10747582859403322]
	TIME [epoch: 12.6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08266069382916051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08266069382916051 | validation: 0.05457328330231079]
	TIME [epoch: 12.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06768930976246434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06768930976246434 | validation: 0.06203137762459835]
	TIME [epoch: 12.6 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06888834931496594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06888834931496594 | validation: 0.032913525912249546]
	TIME [epoch: 12.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044718734261337635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044718734261337635 | validation: 0.03781532935021488]
	TIME [epoch: 12.6 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776736521695991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776736521695991 | validation: 0.5973657585492829]
	TIME [epoch: 12.6 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5905648020255712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5905648020255712 | validation: 0.79193026099169]
	TIME [epoch: 12.6 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7738679945316593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7738679945316593 | validation: 0.45739552731676714]
	TIME [epoch: 12.6 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49275887804680735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49275887804680735 | validation: 0.3728101127810091]
	TIME [epoch: 12.6 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43341322676752647		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43341322676752647 | validation: 0.1660779499440997]
	TIME [epoch: 12.6 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15108496992954215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15108496992954215 | validation: 0.051008374521010647]
	TIME [epoch: 12.6 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05725518614007706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05725518614007706 | validation: 0.050015783303629026]
	TIME [epoch: 12.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05194221446940124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05194221446940124 | validation: 0.04959180389075062]
	TIME [epoch: 12.6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054586712898737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04054586712898737 | validation: 0.04662736789604496]
	TIME [epoch: 12.6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036849410973595835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036849410973595835 | validation: 0.02913855586363543]
	TIME [epoch: 12.6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03425333592297461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03425333592297461 | validation: 0.029447377610183203]
	TIME [epoch: 12.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04534295479950687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04534295479950687 | validation: 0.03732869704717003]
	TIME [epoch: 12.6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03983450693882315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03983450693882315 | validation: 0.033223765565298956]
	TIME [epoch: 12.6 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038351051525937756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038351051525937756 | validation: 0.06600321456075674]
	TIME [epoch: 12.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05925150348994659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05925150348994659 | validation: 0.11915814066234842]
	TIME [epoch: 12.6 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09442910505756288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09442910505756288 | validation: 0.0983014373325816]
	TIME [epoch: 12.6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09788954976653996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09788954976653996 | validation: 0.07664233104663694]
	TIME [epoch: 12.6 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057698915010503934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057698915010503934 | validation: 0.029424859780346792]
	TIME [epoch: 12.6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02753006124242794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02753006124242794 | validation: 0.017958184018252745]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_952.pth
	Model improved!!!
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024659640667104982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024659640667104982 | validation: 0.0664296003900834]
	TIME [epoch: 12.6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052736049826084444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052736049826084444 | validation: 0.0409275151469806]
	TIME [epoch: 12.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05557430702409882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05557430702409882 | validation: 0.06213238611760692]
	TIME [epoch: 12.6 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05638335780091445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05638335780091445 | validation: 0.09995471529163491]
	TIME [epoch: 12.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08509017053719052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08509017053719052 | validation: 0.06489450751306743]
	TIME [epoch: 12.6 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07140317548839184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07140317548839184 | validation: 0.05255824858681164]
	TIME [epoch: 12.6 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05308846901159497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05308846901159497 | validation: 0.045425702816112704]
	TIME [epoch: 12.6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04407736645772754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04407736645772754 | validation: 0.045935624066281766]
	TIME [epoch: 12.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04057038516231994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04057038516231994 | validation: 0.029884290616479594]
	TIME [epoch: 12.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032154606123204234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032154606123204234 | validation: 0.034310932448492815]
	TIME [epoch: 12.6 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029290586274337727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029290586274337727 | validation: 0.02955183067252707]
	TIME [epoch: 12.6 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03830531057987529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03830531057987529 | validation: 0.08572481409132952]
	TIME [epoch: 12.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0636225440781575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0636225440781575 | validation: 0.06904310717221585]
	TIME [epoch: 12.6 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09473027759215646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09473027759215646 | validation: 0.11900380728963267]
	TIME [epoch: 12.6 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07693489857692072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07693489857692072 | validation: 0.04150441033180207]
	TIME [epoch: 12.6 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929879268222137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03929879268222137 | validation: 0.03921122977068878]
	TIME [epoch: 12.6 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05272081015357781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05272081015357781 | validation: 0.08940559393180797]
	TIME [epoch: 12.6 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07721645997737606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07721645997737606 | validation: 0.07826012880163337]
	TIME [epoch: 12.6 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08124918252017391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08124918252017391 | validation: 0.07351876976621595]
	TIME [epoch: 12.6 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05745923279829519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05745923279829519 | validation: 0.035189074498360166]
	TIME [epoch: 12.6 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038034020632234364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038034020632234364 | validation: 0.03026466183575982]
	TIME [epoch: 12.6 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032622400605014715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032622400605014715 | validation: 0.030234114127293266]
	TIME [epoch: 12.6 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03308113775333685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03308113775333685 | validation: 0.029638732251640123]
	TIME [epoch: 12.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036530746213475934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036530746213475934 | validation: 0.0499041644915007]
	TIME [epoch: 12.6 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04438023943367176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04438023943367176 | validation: 0.03344527470813272]
	TIME [epoch: 12.6 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04586162425894909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04586162425894909 | validation: 0.05363345199699321]
	TIME [epoch: 12.6 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04411166933022818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04411166933022818 | validation: 0.03037548328972496]
	TIME [epoch: 12.6 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04648973326397975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04648973326397975 | validation: 0.0586622329902971]
	TIME [epoch: 12.6 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05017083377192824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05017083377192824 | validation: 0.05852892439045168]
	TIME [epoch: 12.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07805519722033634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07805519722033634 | validation: 0.06970289508355497]
	TIME [epoch: 12.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05730983108980764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05730983108980764 | validation: 0.02237056177159983]
	TIME [epoch: 12.6 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032129578399650735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032129578399650735 | validation: 0.019875259668935497]
	TIME [epoch: 12.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02492514165893224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02492514165893224 | validation: 0.05158922186759948]
	TIME [epoch: 12.6 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03946274014528105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03946274014528105 | validation: 0.07109142633145449]
	TIME [epoch: 12.6 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07312247240766569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07312247240766569 | validation: 0.07354546932192223]
	TIME [epoch: 12.6 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07970713882942361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07970713882942361 | validation: 0.05158188273148028]
	TIME [epoch: 12.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05523083145263895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05523083145263895 | validation: 0.061060420211232695]
	TIME [epoch: 12.6 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662934696422858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06662934696422858 | validation: 0.06919623368263522]
	TIME [epoch: 12.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0674224489991369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0674224489991369 | validation: 0.07274153106910805]
	TIME [epoch: 12.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04922503389319188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04922503389319188 | validation: 0.03776969972985263]
	TIME [epoch: 12.6 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04976763139447288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04976763139447288 | validation: 0.07522340451618469]
	TIME [epoch: 12.6 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06056223171400437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06056223171400437 | validation: 0.049135716496084905]
	TIME [epoch: 12.6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05726987683347466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05726987683347466 | validation: 0.047381397598527386]
	TIME [epoch: 12.6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04605914606936571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04605914606936571 | validation: 0.03495253949686177]
	TIME [epoch: 12.6 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040337006174892324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040337006174892324 | validation: 0.03910527855124679]
	TIME [epoch: 12.6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041661442690761046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041661442690761046 | validation: 0.041035527907527306]
	TIME [epoch: 12.6 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043640681482711906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043640681482711906 | validation: 0.04091036951548052]
	TIME [epoch: 12.6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0449135640737002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0449135640737002 | validation: 0.04056282953730614]
	TIME [epoch: 12.6 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377890634133556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04377890634133556 | validation: 0.02878298423290896]
	TIME [epoch: 208 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035622386020582035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035622386020582035 | validation: 0.036366629522520344]
	TIME [epoch: 25.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03049782568933233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03049782568933233 | validation: 0.022381313649038684]
	TIME [epoch: 25.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03171434092242596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03171434092242596 | validation: 0.038426122617523]
	TIME [epoch: 25.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03417643890658308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03417643890658308 | validation: 0.03930090962562505]
	TIME [epoch: 25.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044817745728352225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044817745728352225 | validation: 0.14771254168740075]
	TIME [epoch: 25.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09786640583897672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09786640583897672 | validation: 0.08628390052390206]
	TIME [epoch: 25.9 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1497970794128697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1497970794128697 | validation: 0.15742250025787527]
	TIME [epoch: 25.9 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08354165137207034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08354165137207034 | validation: 0.11710945679270578]
	TIME [epoch: 25.9 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06443144431872601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06443144431872601 | validation: 0.04841596877799849]
	TIME [epoch: 25.9 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.066199637022402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.066199637022402 | validation: 0.029126739570412265]
	TIME [epoch: 25.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035353566753700966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035353566753700966 | validation: 0.039056975825217215]
	TIME [epoch: 25.9 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03587800022253311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03587800022253311 | validation: 0.032531855944350645]
	TIME [epoch: 25.9 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03049668620283982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03049668620283982 | validation: 0.035202832709818846]
	TIME [epoch: 25.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588560418085795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03588560418085795 | validation: 0.04540435092124387]
	TIME [epoch: 25.9 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04594713245719936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04594713245719936 | validation: 0.06874895393321874]
	TIME [epoch: 25.9 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06689360990857031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06689360990857031 | validation: 0.061640979968910825]
	TIME [epoch: 25.9 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07809254264318381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07809254264318381 | validation: 0.0670016427270216]
	TIME [epoch: 25.9 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06441582381714404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06441582381714404 | validation: 0.029252782292361626]
	TIME [epoch: 25.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0373462422173208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0373462422173208 | validation: 0.025492664585739545]
	TIME [epoch: 25.9 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026289992672627653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026289992672627653 | validation: 0.017674070160140356]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_1021.pth
	Model improved!!!
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02164116700804012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02164116700804012 | validation: 0.014842192849781832]
	TIME [epoch: 25.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_1022.pth
	Model improved!!!
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019862201545104572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019862201545104572 | validation: 0.01742504906250141]
	TIME [epoch: 25.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022431488306181455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022431488306181455 | validation: 0.05122230022996355]
	TIME [epoch: 25.9 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049284495893262274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049284495893262274 | validation: 0.05618995396585994]
	TIME [epoch: 25.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618590595579383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05618590595579383 | validation: 0.07657639304324981]
	TIME [epoch: 25.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06879563163728163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06879563163728163 | validation: 0.061567978504306876]
	TIME [epoch: 26 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061583085168871385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061583085168871385 | validation: 0.04897735825951419]
	TIME [epoch: 25.9 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0503433373499666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0503433373499666 | validation: 0.06330552547198208]
	TIME [epoch: 26 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06422138221931148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06422138221931148 | validation: 0.05665439463771719]
	TIME [epoch: 25.9 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04387044430426622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04387044430426622 | validation: 0.03348170440562367]
	TIME [epoch: 25.9 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027591572023742926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027591572023742926 | validation: 0.02759500403574913]
	TIME [epoch: 25.9 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024441077527576356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024441077527576356 | validation: 0.0329341136484555]
	TIME [epoch: 25.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03008523719948827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03008523719948827 | validation: 0.052761927069734385]
	TIME [epoch: 25.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048896639270614106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048896639270614106 | validation: 0.06733411968770317]
	TIME [epoch: 25.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06168651859170462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06168651859170462 | validation: 0.06294945668783523]
	TIME [epoch: 26 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05866818071720326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05866818071720326 | validation: 0.070874283522731]
	TIME [epoch: 25.9 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049204916502115725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049204916502115725 | validation: 0.17806152804516673]
	TIME [epoch: 25.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145995873339486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.145995873339486 | validation: 0.11089472848069067]
	TIME [epoch: 25.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09410621250002699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09410621250002699 | validation: 0.04783732010047795]
	TIME [epoch: 25.9 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062056476740328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062056476740328 | validation: 0.055494664775067884]
	TIME [epoch: 25.9 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049893258893098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049893258893098 | validation: 0.04533924691771527]
	TIME [epoch: 26 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710570354822269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04710570354822269 | validation: 0.049016775693760985]
	TIME [epoch: 25.9 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043961096565920704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043961096565920704 | validation: 0.04021371081023944]
	TIME [epoch: 25.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038112355814127225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038112355814127225 | validation: 0.04225468370679519]
	TIME [epoch: 25.9 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04151836338821676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04151836338821676 | validation: 0.04572836697414445]
	TIME [epoch: 25.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04372150086684414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04372150086684414 | validation: 0.05587723666705213]
	TIME [epoch: 25.9 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0555776181704544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0555776181704544 | validation: 0.06604019276743787]
	TIME [epoch: 25.9 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314481716178104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06314481716178104 | validation: 0.05878884621862941]
	TIME [epoch: 25.9 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05425578127166406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05425578127166406 | validation: 0.03290654961381251]
	TIME [epoch: 25.9 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03641420398338527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03641420398338527 | validation: 0.024864552131664344]
	TIME [epoch: 26 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025000618008478186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025000618008478186 | validation: 0.017657410552502008]
	TIME [epoch: 25.9 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019848247110613323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019848247110613323 | validation: 0.016028981246095263]
	TIME [epoch: 25.9 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018990850567411316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018990850567411316 | validation: 0.018553003439975038]
	TIME [epoch: 26 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020311678660687366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020311678660687366 | validation: 0.03585300683504775]
	TIME [epoch: 26 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04123365078917821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04123365078917821 | validation: 0.06487061613631313]
	TIME [epoch: 25.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074611748825002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07074611748825002 | validation: 0.07751548949620392]
	TIME [epoch: 25.9 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07175980830325822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07175980830325822 | validation: 0.04041179667356605]
	TIME [epoch: 26 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050125034737760914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050125034737760914 | validation: 0.0703101112443101]
	TIME [epoch: 26 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05405694515121383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05405694515121383 | validation: 0.1263352879490321]
	TIME [epoch: 25.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15168525503178093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15168525503178093 | validation: 0.06614066398458886]
	TIME [epoch: 25.9 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07088054456222671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07088054456222671 | validation: 0.024981744386592064]
	TIME [epoch: 25.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375528826065784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0375528826065784 | validation: 0.030955419768423687]
	TIME [epoch: 26 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036568143910849636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036568143910849636 | validation: 0.06812094490587255]
	TIME [epoch: 26 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058734076230590376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058734076230590376 | validation: 0.07785561097246015]
	TIME [epoch: 25.9 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0729121550811339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0729121550811339 | validation: 0.05008606931695282]
	TIME [epoch: 25.9 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05229827949176745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05229827949176745 | validation: 0.031203065867965743]
	TIME [epoch: 25.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03500659157696521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03500659157696521 | validation: 0.03687139755585406]
	TIME [epoch: 25.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039248403406753306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039248403406753306 | validation: 0.03205093961351412]
	TIME [epoch: 25.9 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03655295019505807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03655295019505807 | validation: 0.03126802582933471]
	TIME [epoch: 25.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027451457561979645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027451457561979645 | validation: 0.02533186395324302]
	TIME [epoch: 26 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028101188249799673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028101188249799673 | validation: 0.035405473515295716]
	TIME [epoch: 25.9 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03583043740612123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03583043740612123 | validation: 0.03299924730391426]
	TIME [epoch: 26 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039336607392517355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039336607392517355 | validation: 0.04244012578951653]
	TIME [epoch: 25.9 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03934282022716316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03934282022716316 | validation: 0.05283923585461128]
	TIME [epoch: 26 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06570597101898945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06570597101898945 | validation: 0.13688108309625524]
	TIME [epoch: 26 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08046533578235344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08046533578235344 | validation: 0.03747800184648311]
	TIME [epoch: 26 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05123164463170752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05123164463170752 | validation: 0.8176410564772433]
	TIME [epoch: 25.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.815814982976526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.815814982976526 | validation: 1.1898793904713116]
	TIME [epoch: 25.9 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1428258567442429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1428258567442429 | validation: 1.2914339076617016]
	TIME [epoch: 26 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2043806501483005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2043806501483005 | validation: 1.5957388373958463]
	TIME [epoch: 25.9 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4105275149160688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4105275149160688 | validation: 0.7402103931147226]
	TIME [epoch: 25.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8241864738247263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8241864738247263 | validation: 0.4699901317733047]
	TIME [epoch: 26 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592126532020447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592126532020447 | validation: 0.2279111484671118]
	TIME [epoch: 26 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3002938017824293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3002938017824293 | validation: 0.4460866755802837]
	TIME [epoch: 26 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20239408074085163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20239408074085163 | validation: 0.08589578004658112]
	TIME [epoch: 25.9 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1104481499602904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1104481499602904 | validation: 0.10177275901400072]
	TIME [epoch: 25.9 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229787986914284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10229787986914284 | validation: 0.04758068217193252]
	TIME [epoch: 26 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0762412067222977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0762412067222977 | validation: 0.05003268702456413]
	TIME [epoch: 25.9 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07307801988442539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07307801988442539 | validation: 0.05040903341223614]
	TIME [epoch: 25.9 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06851121024253264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06851121024253264 | validation: 0.04094637774834797]
	TIME [epoch: 25.9 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04784152513209123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04784152513209123 | validation: 0.03556821182563557]
	TIME [epoch: 25.9 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039200640216869136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039200640216869136 | validation: 0.031860276399000624]
	TIME [epoch: 25.9 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03863376357637005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03863376357637005 | validation: 0.039002371345078946]
	TIME [epoch: 25.9 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038636818515898404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038636818515898404 | validation: 0.03316448051675069]
	TIME [epoch: 26 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039299550291773525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039299550291773525 | validation: 0.03168531037881063]
	TIME [epoch: 25.9 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039866734728234134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039866734728234134 | validation: 0.039374204342776255]
	TIME [epoch: 25.9 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04675322375480084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04675322375480084 | validation: 0.06019214366702538]
	TIME [epoch: 26 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06744186137226223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06744186137226223 | validation: 0.08018213678953916]
	TIME [epoch: 25.9 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0801900660080087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0801900660080087 | validation: 0.07718965587906992]
	TIME [epoch: 26 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07486593825776824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07486593825776824 | validation: 0.03738597171804037]
	TIME [epoch: 25.9 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04728674668836161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04728674668836161 | validation: 0.037319688647194704]
	TIME [epoch: 25.9 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043307189869591596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043307189869591596 | validation: 0.026201573742194575]
	TIME [epoch: 25.9 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03185091359458089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03185091359458089 | validation: 0.027556124626705314]
	TIME [epoch: 25.9 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031642422975386185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031642422975386185 | validation: 0.038243570024989605]
	TIME [epoch: 25.9 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564544256441858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03564544256441858 | validation: 0.05504204806099902]
	TIME [epoch: 25.9 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591966800711897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0591966800711897 | validation: 0.3852696360664969]
	TIME [epoch: 25.9 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15356199271897253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15356199271897253 | validation: 0.11112676965429405]
	TIME [epoch: 25.9 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040859275959614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1040859275959614 | validation: 0.11288247867145913]
	TIME [epoch: 25.9 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06143595911625283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06143595911625283 | validation: 0.04981508130928979]
	TIME [epoch: 25.9 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04654543603127136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04654543603127136 | validation: 0.04002122634782288]
	TIME [epoch: 25.9 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046537682503121146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046537682503121146 | validation: 0.06303211350332907]
	TIME [epoch: 26 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05860976309238849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05860976309238849 | validation: 0.056109930087424514]
	TIME [epoch: 26 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0570375164016695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0570375164016695 | validation: 0.035041676596725184]
	TIME [epoch: 25.9 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04348804747728235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04348804747728235 | validation: 0.028159953437107643]
	TIME [epoch: 25.9 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03364044840144761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03364044840144761 | validation: 0.028765659352835073]
	TIME [epoch: 25.9 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03160539948962355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03160539948962355 | validation: 0.0347185420660453]
	TIME [epoch: 25.9 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03788083193800146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03788083193800146 | validation: 0.03889241563856673]
	TIME [epoch: 26 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04383855461095129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04383855461095129 | validation: 0.03690968758459891]
	TIME [epoch: 25.9 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047444351493949655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047444351493949655 | validation: 0.04101085972543535]
	TIME [epoch: 26 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04071389567611575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04071389567611575 | validation: 0.037109813626117615]
	TIME [epoch: 25.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032416896450491235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032416896450491235 | validation: 0.03923306054112405]
	TIME [epoch: 26 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0367650689324846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0367650689324846 | validation: 0.07779474794831688]
	TIME [epoch: 26 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_160307/states/model_phi1_4c_v_mmd2_1123.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 12629.692 seconds.
