Args:
Namespace(name='model_phi1_4b_v_mmd2', outdir='out/model_training/model_phi1_4b_v_mmd2', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 505999611

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.969055304789627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.969055304789627 | validation: 4.699311677584322]
	TIME [epoch: 166 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.145262109226161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.145262109226161 | validation: 4.9257296302192515]
	TIME [epoch: 1.33 sec]
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.232529541890212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.232529541890212 | validation: 6.083627940597451]
	TIME [epoch: 1.32 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.5015651009359186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.5015651009359186 | validation: 4.82167212341383]
	TIME [epoch: 1.31 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.7164347969274365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.7164347969274365 | validation: 4.184363424115412]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.5529861623863175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.5529861623863175 | validation: 4.364562382428171]
	TIME [epoch: 1.33 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.740627886405346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.740627886405346 | validation: 4.135528585244259]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.445106849964795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.445106849964795 | validation: 3.917333762791686]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.239222833018424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.239222833018424 | validation: 3.9455314875707135]
	TIME [epoch: 1.32 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.976748244594455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.976748244594455 | validation: 4.014030608376701]
	TIME [epoch: 1.32 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9555310263304975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9555310263304975 | validation: 3.8991381189521714]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8832474880541112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8832474880541112 | validation: 3.7881169004724002]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.840168166593236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.840168166593236 | validation: 3.7476811292607426]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7881185985094574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7881185985094574 | validation: 3.73553281347089]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.739815882803134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.739815882803134 | validation: 3.690081596289872]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.692635254776742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.692635254776742 | validation: 3.6012413687156215]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6350704080993497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6350704080993497 | validation: 3.4976501171544783]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5572491731401112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5572491731401112 | validation: 3.4272562305588625]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4885220052657906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4885220052657906 | validation: 3.340911024788133]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.466204906427918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.466204906427918 | validation: 3.811948719702774]
	TIME [epoch: 1.33 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9911606591307147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9911606591307147 | validation: 3.4388048067623505]
	TIME [epoch: 1.33 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6662267670687503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6662267670687503 | validation: 3.235090348965242]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3319946065710675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3319946065710675 | validation: 3.3542753985730744]
	TIME [epoch: 1.33 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4643763197971693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4643763197971693 | validation: 3.2660430493136867]
	TIME [epoch: 1.32 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2855417988045854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2855417988045854 | validation: 3.1771671473735896]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.221296483070792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.221296483070792 | validation: 3.180736726343687]
	TIME [epoch: 1.32 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1890184499582346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1890184499582346 | validation: 3.1688409328388105]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1295341268752233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1295341268752233 | validation: 3.112608821883553]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1044339511193595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1044339511193595 | validation: 3.140161125851884]
	TIME [epoch: 1.32 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0683259036957775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.0683259036957775 | validation: 3.0523013235401337]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.033496390798853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.033496390798853 | validation: 3.108278764405414]
	TIME [epoch: 1.33 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9877840940165936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9877840940165936 | validation: 2.974703482110647]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9691227561703375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9691227561703375 | validation: 3.071074400235472]
	TIME [epoch: 1.32 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.923383881373196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.923383881373196 | validation: 2.8720197947925326]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.959226560593994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.959226560593994 | validation: 2.98038670152118]
	TIME [epoch: 1.32 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5999116981793846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.5999116981793846 | validation: 2.754785131528172]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.298040631233884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.298040631233884 | validation: 2.4013292691664203]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.084151177937672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.084151177937672 | validation: 3.571352055471067]
	TIME [epoch: 1.32 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2243764293654023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2243764293654023 | validation: 2.039650090708581]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7103651632121484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7103651632121484 | validation: 2.099680529428159]
	TIME [epoch: 1.32 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.604565683626775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.604565683626775 | validation: 1.6810790543512604]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3128492568285501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3128492568285501 | validation: 1.5566336527487632]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1619723843321461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1619723843321461 | validation: 2.0522527939829036]
	TIME [epoch: 1.32 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8875618117969288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8875618117969288 | validation: 2.3014394806869114]
	TIME [epoch: 1.32 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8147365313579509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8147365313579509 | validation: 1.518136664630875]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1970187574799847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1970187574799847 | validation: 1.579218278922333]
	TIME [epoch: 1.33 sec]
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3105657281477208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3105657281477208 | validation: 1.2694199017095262]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9714577291647237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9714577291647237 | validation: 1.1571804939675139]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8881789665358187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8881789665358187 | validation: 1.0928592072284367]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9284776391186658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9284776391186658 | validation: 1.4119674331868834]
	TIME [epoch: 1.32 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0234928078385697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0234928078385697 | validation: 1.0287882861015152]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_51.pth
	Model improved!!!
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8166908894584266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8166908894584266 | validation: 1.0103543782622784]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8296225964724746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8296225964724746 | validation: 1.079504496651868]
	TIME [epoch: 1.31 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.840463835114384		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.840463835114384 | validation: 0.9531909379583428]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8202656905897752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8202656905897752 | validation: 1.0966289396937576]
	TIME [epoch: 1.31 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432795086084913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8432795086084913 | validation: 0.9308245683154827]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8069191066995792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8069191066995792 | validation: 1.0390619480065113]
	TIME [epoch: 1.32 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8162694432745454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8162694432745454 | validation: 0.9183299062559727]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990020064823137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7990020064823137 | validation: 1.0938482010773107]
	TIME [epoch: 1.31 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8323911770918457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8323911770918457 | validation: 0.9090175219802226]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8198905979389729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8198905979389729 | validation: 1.1595778805120645]
	TIME [epoch: 1.31 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754702472997679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8754702472997679 | validation: 0.9349201323477113]
	TIME [epoch: 1.3 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7779072219734623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7779072219734623 | validation: 0.9287943731510326]
	TIME [epoch: 1.31 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7723759158338055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7723759158338055 | validation: 1.0086139540859183]
	TIME [epoch: 1.31 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850084515980663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7850084515980663 | validation: 0.873025622790001]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8103719998245742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8103719998245742 | validation: 1.1416124336123326]
	TIME [epoch: 1.31 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8266108994072253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8266108994072253 | validation: 0.880755112897543]
	TIME [epoch: 1.32 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7887695784847804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7887695784847804 | validation: 1.109707530812751]
	TIME [epoch: 1.32 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8223437917417057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8223437917417057 | validation: 0.8873964588905725]
	TIME [epoch: 1.31 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8197416351475102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8197416351475102 | validation: 1.1686976389194548]
	TIME [epoch: 1.31 sec]
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8993439581522377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8993439581522377 | validation: 0.9376907515281667]
	TIME [epoch: 1.31 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7911370893749091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7911370893749091 | validation: 0.8958474593062787]
	TIME [epoch: 1.31 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7713275699249446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7713275699249446 | validation: 1.0100011220749356]
	TIME [epoch: 1.31 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7688716220333457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7688716220333457 | validation: 0.8594034468910263]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7904216661391604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7904216661391604 | validation: 1.193222709152075]
	TIME [epoch: 1.31 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487358215416825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8487358215416825 | validation: 0.8850149080474135]
	TIME [epoch: 1.31 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8125749910566481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8125749910566481 | validation: 1.1862526779219211]
	TIME [epoch: 1.31 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.856174108903406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.856174108903406 | validation: 0.8702349664681148]
	TIME [epoch: 1.31 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591357260540332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591357260540332 | validation: 0.9236322544062852]
	TIME [epoch: 1.31 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7473022291715833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7473022291715833 | validation: 0.851046433740091]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7407781615034292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7407781615034292 | validation: 0.889079490923593]
	TIME [epoch: 1.31 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7449802618407033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7449802618407033 | validation: 0.8672486303463991]
	TIME [epoch: 1.32 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.760178110800248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.760178110800248 | validation: 1.098154297008133]
	TIME [epoch: 1.31 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051923218630707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8051923218630707 | validation: 0.9510348127347954]
	TIME [epoch: 1.31 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873277028190548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.873277028190548 | validation: 1.093061281536707]
	TIME [epoch: 1.32 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033648197436218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8033648197436218 | validation: 0.843782997143125]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863787611257393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7863787611257393 | validation: 1.1698106654096105]
	TIME [epoch: 1.3 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8799138922002467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8799138922002467 | validation: 0.8905850746177955]
	TIME [epoch: 1.31 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8364525195422181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8364525195422181 | validation: 1.0350793127298514]
	TIME [epoch: 1.31 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8339486393662293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8339486393662293 | validation: 0.8571932331638714]
	TIME [epoch: 1.32 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.745111893063914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.745111893063914 | validation: 0.8838807245678755]
	TIME [epoch: 1.31 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7414746226979178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7414746226979178 | validation: 0.8612065377302033]
	TIME [epoch: 1.32 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.746626640478863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.746626640478863 | validation: 0.9535798306478303]
	TIME [epoch: 1.31 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7627560400529549		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7627560400529549 | validation: 0.8842347627103998]
	TIME [epoch: 1.32 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.794981397770473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.794981397770473 | validation: 1.1461815720544342]
	TIME [epoch: 1.31 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8281403740710854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8281403740710854 | validation: 0.8882849236468128]
	TIME [epoch: 1.31 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8537665824191377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8537665824191377 | validation: 1.2274805320767592]
	TIME [epoch: 1.31 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9181558521830685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9181558521830685 | validation: 0.9086472042948588]
	TIME [epoch: 1.31 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7520552021452571		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7520552021452571 | validation: 0.8479126597208109]
	TIME [epoch: 1.31 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7985588102547077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7985588102547077 | validation: 1.1659433458251238]
	TIME [epoch: 1.31 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471710481276616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8471710481276616 | validation: 0.8525855244338754]
	TIME [epoch: 1.31 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7540890706670262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7540890706670262 | validation: 0.8342540623530621]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7595754649179074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7595754649179074 | validation: 0.9906340153817349]
	TIME [epoch: 1.31 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7852032007227331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7852032007227331 | validation: 0.8358322114749005]
	TIME [epoch: 1.32 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7657573773390098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7657573773390098 | validation: 0.9452124849845114]
	TIME [epoch: 1.31 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7726506240627933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7726506240627933 | validation: 0.8841771117269485]
	TIME [epoch: 1.31 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7698296698206918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7698296698206918 | validation: 0.9204072831420721]
	TIME [epoch: 1.3 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7900573280786742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7900573280786742 | validation: 1.0116299114834593]
	TIME [epoch: 1.32 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7907887197768446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7907887197768446 | validation: 0.9002753418451764]
	TIME [epoch: 1.3 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7969459742438949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7969459742438949 | validation: 1.0096254278382266]
	TIME [epoch: 1.32 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7666630375814789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7666630375814789 | validation: 0.8387448940417124]
	TIME [epoch: 1.31 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7616895631702826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7616895631702826 | validation: 1.0837430020972363]
	TIME [epoch: 1.31 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.792490129266188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.792490129266188 | validation: 0.8492144646938418]
	TIME [epoch: 1.31 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8063744896210885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8063744896210885 | validation: 1.174971696000425]
	TIME [epoch: 1.31 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617405321602928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8617405321602928 | validation: 0.8659633131841323]
	TIME [epoch: 1.3 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7323300743120992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7323300743120992 | validation: 0.8208226868798889]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750795367212277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750795367212277 | validation: 1.0177354783351855]
	TIME [epoch: 1.33 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7871185332986153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7871185332986153 | validation: 0.845272904761626]
	TIME [epoch: 1.32 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7481744834024366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7481744834024366 | validation: 0.8878551536111652]
	TIME [epoch: 1.32 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7562192619755286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7562192619755286 | validation: 0.9646665274891689]
	TIME [epoch: 1.32 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7825934023791443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7825934023791443 | validation: 0.9677181893670372]
	TIME [epoch: 1.33 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8749491884056937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8749491884056937 | validation: 1.0580235888179292]
	TIME [epoch: 1.32 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7841100829424313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7841100829424313 | validation: 0.8291489874384212]
	TIME [epoch: 1.32 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7429363016442804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7429363016442804 | validation: 0.9739955059023884]
	TIME [epoch: 1.32 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7728275478047121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7728275478047121 | validation: 0.844357295301151]
	TIME [epoch: 1.32 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7778176968502291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7778176968502291 | validation: 0.9931280764676567]
	TIME [epoch: 1.32 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097012854731608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8097012854731608 | validation: 0.8335285791222827]
	TIME [epoch: 1.32 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7591024138066726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7591024138066726 | validation: 0.9389892414077379]
	TIME [epoch: 1.32 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7409891049619388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7409891049619388 | validation: 0.8323346879312027]
	TIME [epoch: 1.32 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7594005006456113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7594005006456113 | validation: 1.1174985633123031]
	TIME [epoch: 1.32 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8068748980648774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8068748980648774 | validation: 0.8899856852159513]
	TIME [epoch: 1.32 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.829203852398513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.829203852398513 | validation: 0.9946940939033582]
	TIME [epoch: 1.32 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518005931605126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518005931605126 | validation: 0.8188901680712075]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_133.pth
	Model improved!!!
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7191073132310918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7191073132310918 | validation: 0.8569356821777041]
	TIME [epoch: 1.32 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7309161599730223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7309161599730223 | validation: 0.8820738282125424]
	TIME [epoch: 1.32 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7325630385058147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7325630385058147 | validation: 0.8995768629697724]
	TIME [epoch: 1.32 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7581703739265683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7581703739265683 | validation: 0.889105591759514]
	TIME [epoch: 1.32 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8102216203705461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8102216203705461 | validation: 1.1429872700819304]
	TIME [epoch: 1.32 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8989394198269676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8989394198269676 | validation: 0.8477462544994133]
	TIME [epoch: 1.33 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7152308051141855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7152308051141855 | validation: 0.8286056294110626]
	TIME [epoch: 1.32 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7450214745328827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7450214745328827 | validation: 0.9808775003292264]
	TIME [epoch: 1.32 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7924227105054689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7924227105054689 | validation: 0.8041339139152175]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_142.pth
	Model improved!!!
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7338316169450303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7338316169450303 | validation: 0.8046228704337719]
	TIME [epoch: 1.32 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173991682507009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173991682507009 | validation: 1.0397942987276758]
	TIME [epoch: 1.32 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7928241083648158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7928241083648158 | validation: 0.9685310007021125]
	TIME [epoch: 1.32 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9686829274703178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9686829274703178 | validation: 1.0962626572440368]
	TIME [epoch: 1.32 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8020365563187563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8020365563187563 | validation: 0.9091865358102063]
	TIME [epoch: 1.32 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291628798480191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7291628798480191 | validation: 0.8174314875515676]
	TIME [epoch: 1.32 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7518797296954991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7518797296954991 | validation: 0.9446600409717837]
	TIME [epoch: 1.32 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7253357693886694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7253357693886694 | validation: 0.785769165741963]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_150.pth
	Model improved!!!
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7119129201677157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7119129201677157 | validation: 0.9061880349423781]
	TIME [epoch: 1.32 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7634049183043559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7634049183043559 | validation: 0.8426370432340404]
	TIME [epoch: 1.32 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7567981034044019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7567981034044019 | validation: 0.8886438786174289]
	TIME [epoch: 1.32 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7683252688376677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7683252688376677 | validation: 0.8422324282969049]
	TIME [epoch: 1.31 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232731379047133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7232731379047133 | validation: 0.841030818333365]
	TIME [epoch: 1.32 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6988087424826516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6988087424826516 | validation: 0.7377946781708351]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7040701930260557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7040701930260557 | validation: 0.9345670330639821]
	TIME [epoch: 1.32 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7475211415095409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7475211415095409 | validation: 0.7437402022272824]
	TIME [epoch: 1.32 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771160239733467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.771160239733467 | validation: 1.0372004231482401]
	TIME [epoch: 1.32 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8222653883974889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8222653883974889 | validation: 0.7611888014776932]
	TIME [epoch: 1.32 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7032656720040259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7032656720040259 | validation: 0.9900650176010152]
	TIME [epoch: 1.32 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8271637976713789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8271637976713789 | validation: 0.9381871708989472]
	TIME [epoch: 1.32 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8332864066466661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8332864066466661 | validation: 0.866729175452665]
	TIME [epoch: 1.32 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6825804687465451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6825804687465451 | validation: 0.8005220838426492]
	TIME [epoch: 1.34 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6793632853802487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6793632853802487 | validation: 0.8405240994861775]
	TIME [epoch: 1.32 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6900019905879501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6900019905879501 | validation: 0.7372072906134709]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_166.pth
	Model improved!!!
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6766860019593548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6766860019593548 | validation: 0.8136789749464164]
	TIME [epoch: 1.32 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.696912971197788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.696912971197788 | validation: 0.7787557209545981]
	TIME [epoch: 1.32 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869068752766364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7869068752766364 | validation: 0.9334432228259797]
	TIME [epoch: 1.32 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8095533909431172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8095533909431172 | validation: 0.9243306402875764]
	TIME [epoch: 1.32 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7007711201001487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7007711201001487 | validation: 0.7325061976343268]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7001285837017056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7001285837017056 | validation: 0.9273946999815146]
	TIME [epoch: 1.32 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6878423358573728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6878423358573728 | validation: 0.723017302074563]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_173.pth
	Model improved!!!
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6405274092641813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6405274092641813 | validation: 0.7155569729180661]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6434386597454059		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6434386597454059 | validation: 0.9515879936974138]
	TIME [epoch: 1.32 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7524404691385448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7524404691385448 | validation: 0.8779532893203807]
	TIME [epoch: 1.32 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86938317036131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.86938317036131 | validation: 0.981345508860942]
	TIME [epoch: 1.32 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.750084886112995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.750084886112995 | validation: 1.2616189590639648]
	TIME [epoch: 1.32 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.809570038102579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.809570038102579 | validation: 0.8396746274845666]
	TIME [epoch: 1.32 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7270744284293983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7270744284293983 | validation: 0.8893828271754359]
	TIME [epoch: 1.32 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6943021927949687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6943021927949687 | validation: 0.9894469614801569]
	TIME [epoch: 1.32 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6824545491310622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6824545491310622 | validation: 0.9264616312958428]
	TIME [epoch: 1.33 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6667808880387839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6667808880387839 | validation: 0.8242732951812815]
	TIME [epoch: 1.32 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6595513536787223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6595513536787223 | validation: 0.7391695403949914]
	TIME [epoch: 1.33 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.636095054871878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.636095054871878 | validation: 0.7886617075298185]
	TIME [epoch: 1.32 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.645125493883724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.645125493883724 | validation: 0.6753735043001321]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_186.pth
	Model improved!!!
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7304897116877257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7304897116877257 | validation: 1.0356474023061992]
	TIME [epoch: 1.32 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8987053065068511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8987053065068511 | validation: 0.69877548364176]
	TIME [epoch: 1.33 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205814367728278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6205814367728278 | validation: 0.6858047541804135]
	TIME [epoch: 1.32 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6689354442033989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6689354442033989 | validation: 0.9280002949556979]
	TIME [epoch: 1.32 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6933548820684644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6933548820684644 | validation: 0.6940306074911125]
	TIME [epoch: 1.32 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6163427364091721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6163427364091721 | validation: 0.699373058808853]
	TIME [epoch: 1.32 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6001835035371649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6001835035371649 | validation: 0.7701948591257098]
	TIME [epoch: 1.32 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6652063434302139		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6652063434302139 | validation: 0.9110556533002989]
	TIME [epoch: 1.32 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8117893194239122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8117893194239122 | validation: 0.8690049424854177]
	TIME [epoch: 1.32 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761690748881951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6761690748881951 | validation: 0.7224094263840802]
	TIME [epoch: 1.32 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5938208455063483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5938208455063483 | validation: 0.660861523016971]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_197.pth
	Model improved!!!
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5819790281954681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5819790281954681 | validation: 0.7557413978036327]
	TIME [epoch: 1.32 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6860986790707975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6860986790707975 | validation: 1.1650919772496937]
	TIME [epoch: 1.32 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8052575451616514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8052575451616514 | validation: 0.8915638423005916]
	TIME [epoch: 1.32 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6657751345537188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6657751345537188 | validation: 0.7483340887883901]
	TIME [epoch: 164 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6358795434426219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6358795434426219 | validation: 0.7377387155920432]
	TIME [epoch: 2.63 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6161025984914928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6161025984914928 | validation: 0.7177652945243707]
	TIME [epoch: 2.6 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5728023416707009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5728023416707009 | validation: 0.6444604800846734]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5609088060073486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5609088060073486 | validation: 0.7373032299549341]
	TIME [epoch: 2.61 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6218879504225258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6218879504225258 | validation: 0.6381331787882094]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993459389071296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993459389071296 | validation: 0.723511705226258]
	TIME [epoch: 2.59 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6041996273755281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6041996273755281 | validation: 0.8423283771236655]
	TIME [epoch: 2.6 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6129735639574188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6129735639574188 | validation: 0.6495864159450575]
	TIME [epoch: 2.58 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6140415627408333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6140415627408333 | validation: 0.9437131809865907]
	TIME [epoch: 2.58 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6028926380833407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6028926380833407 | validation: 0.6368332336614437]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5088336851257996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5088336851257996 | validation: 0.5583466960014464]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5328599404474081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5328599404474081 | validation: 0.9871721353658636]
	TIME [epoch: 2.6 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6025658458467595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6025658458467595 | validation: 0.71224193945569]
	TIME [epoch: 2.59 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5215954143413498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5215954143413498 | validation: 0.5483213646520995]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_215.pth
	Model improved!!!
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5240122380513347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5240122380513347 | validation: 0.897033433204854]
	TIME [epoch: 2.61 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5634766278524491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5634766278524491 | validation: 0.5920846203830376]
	TIME [epoch: 2.59 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.523283815938274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.523283815938274 | validation: 0.7620162799977068]
	TIME [epoch: 2.61 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158424879634032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7158424879634032 | validation: 1.0015174120976842]
	TIME [epoch: 2.59 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7184939800667735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7184939800667735 | validation: 1.0084261874997622]
	TIME [epoch: 2.6 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6444034979263478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6444034979263478 | validation: 0.8052851714449638]
	TIME [epoch: 2.61 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6092777242223677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6092777242223677 | validation: 0.6637512745562519]
	TIME [epoch: 2.6 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336394894390564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5336394894390564 | validation: 0.6365216626340952]
	TIME [epoch: 2.6 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4889872860693663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4889872860693663 | validation: 0.6076872573322736]
	TIME [epoch: 2.62 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5388713226240149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5388713226240149 | validation: 0.6553745779454279]
	TIME [epoch: 2.6 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5675914450377425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5675914450377425 | validation: 0.7006292516980054]
	TIME [epoch: 2.61 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5712611013579182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5712611013579182 | validation: 0.6226154910174236]
	TIME [epoch: 2.59 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48923941401707516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48923941401707516 | validation: 0.5680409791500579]
	TIME [epoch: 2.6 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43730744729154497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43730744729154497 | validation: 0.5616459341313763]
	TIME [epoch: 2.59 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.453633036883734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.453633036883734 | validation: 0.5834279607952567]
	TIME [epoch: 2.6 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4310291576248851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4310291576248851 | validation: 0.595614871521158]
	TIME [epoch: 2.59 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45876016952502136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45876016952502136 | validation: 0.558469846961341]
	TIME [epoch: 2.59 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4048442297989851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4048442297989851 | validation: 0.5679465091377806]
	TIME [epoch: 2.59 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3874736475241251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3874736475241251 | validation: 0.5263606318966476]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4099092939753014		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4099092939753014 | validation: 0.872758463797066]
	TIME [epoch: 2.59 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6089606573461858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6089606573461858 | validation: 0.8713777826649935]
	TIME [epoch: 2.61 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4860608874003623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4860608874003623 | validation: 0.5400178858533616]
	TIME [epoch: 2.61 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38958821250069925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38958821250069925 | validation: 0.6061692024134575]
	TIME [epoch: 2.62 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4061669778132194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4061669778132194 | validation: 0.46175987230863225]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_239.pth
	Model improved!!!
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4954653953463178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4954653953463178 | validation: 0.6267063566295589]
	TIME [epoch: 2.59 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40523059068393996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40523059068393996 | validation: 0.5000204673224017]
	TIME [epoch: 2.57 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34100885629717154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34100885629717154 | validation: 0.5089701386785016]
	TIME [epoch: 2.59 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3667612123790563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3667612123790563 | validation: 0.8037393901115243]
	TIME [epoch: 2.57 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5240668428044909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5240668428044909 | validation: 0.7460542329835282]
	TIME [epoch: 2.6 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4089777711702024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4089777711702024 | validation: 0.45235789008150323]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3169514737028017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3169514737028017 | validation: 0.4734086357163538]
	TIME [epoch: 2.6 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33758500985675116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33758500985675116 | validation: 0.7962654784923746]
	TIME [epoch: 2.59 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7207378421722214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7207378421722214 | validation: 0.5753119741427314]
	TIME [epoch: 2.59 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5397487572138528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5397487572138528 | validation: 0.5515800778870926]
	TIME [epoch: 2.57 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4577497134248784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4577497134248784 | validation: 0.5910764623101908]
	TIME [epoch: 2.59 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3668413826591158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3668413826591158 | validation: 0.46855263179244633]
	TIME [epoch: 2.59 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30240062617232555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30240062617232555 | validation: 0.4106139423525306]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28168073373480906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28168073373480906 | validation: 0.57425348415372]
	TIME [epoch: 2.6 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29696136882146257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29696136882146257 | validation: 0.39247355874416884]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30798844496084227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30798844496084227 | validation: 0.6499622829758739]
	TIME [epoch: 2.61 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3163899665673301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3163899665673301 | validation: 0.4090505160916152]
	TIME [epoch: 2.6 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2867221911458785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2867221911458785 | validation: 0.5425724618012968]
	TIME [epoch: 2.61 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32028767173742906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32028767173742906 | validation: 0.504701998648595]
	TIME [epoch: 2.6 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49234816404757775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49234816404757775 | validation: 0.6201419616866417]
	TIME [epoch: 2.61 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3194858499428358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3194858499428358 | validation: 0.388069363600964]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_260.pth
	Model improved!!!
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24486711925084884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24486711925084884 | validation: 0.45984876023064986]
	TIME [epoch: 2.6 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23837274390220883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23837274390220883 | validation: 0.40382257932714777]
	TIME [epoch: 2.59 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21818849315549385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21818849315549385 | validation: 0.4297052946214326]
	TIME [epoch: 2.6 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24880788012203117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24880788012203117 | validation: 0.6383481924058968]
	TIME [epoch: 2.59 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4650691428563922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4650691428563922 | validation: 0.41769465910090614]
	TIME [epoch: 2.59 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3969797560232799		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3969797560232799 | validation: 0.7269925020296752]
	TIME [epoch: 2.58 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34324852178473864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34324852178473864 | validation: 0.3817641359802832]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_267.pth
	Model improved!!!
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2673491684757512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2673491684757512 | validation: 0.4545012248586593]
	TIME [epoch: 2.57 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2096061103663373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2096061103663373 | validation: 0.46599148064192975]
	TIME [epoch: 2.58 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21556689940041907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21556689940041907 | validation: 0.35997235352398205]
	TIME [epoch: 2.58 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2238678925948706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2238678925948706 | validation: 0.44156299809333144]
	TIME [epoch: 2.61 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24742341290733677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24742341290733677 | validation: 0.40087448583874324]
	TIME [epoch: 2.63 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28058227726412727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28058227726412727 | validation: 0.5454425138654581]
	TIME [epoch: 2.61 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2783027388047857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2783027388047857 | validation: 0.35153704856685314]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_274.pth
	Model improved!!!
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23340779656875663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23340779656875663 | validation: 0.46505714694778294]
	TIME [epoch: 2.61 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20718711099460393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20718711099460393 | validation: 0.3556076370648107]
	TIME [epoch: 2.62 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18202214131060598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18202214131060598 | validation: 0.4096977958113328]
	TIME [epoch: 2.58 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1753451332744563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1753451332744563 | validation: 0.35472915854030684]
	TIME [epoch: 2.58 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1780996855523796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1780996855523796 | validation: 0.45546492557218105]
	TIME [epoch: 2.59 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25053311521373944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25053311521373944 | validation: 0.41377643064015174]
	TIME [epoch: 2.58 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.37486031078138604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.37486031078138604 | validation: 0.7124984733428414]
	TIME [epoch: 2.58 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34047789703562453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34047789703562453 | validation: 0.37635900800303324]
	TIME [epoch: 2.59 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551923695199738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551923695199738 | validation: 0.5526563398364586]
	TIME [epoch: 2.59 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2558524688728056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2558524688728056 | validation: 0.44505725357661663]
	TIME [epoch: 2.59 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1887830192090782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1887830192090782 | validation: 0.37011141047021456]
	TIME [epoch: 2.59 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1702844907099356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1702844907099356 | validation: 0.3672867069331154]
	TIME [epoch: 2.59 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15963422591499973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15963422591499973 | validation: 0.4441844024349688]
	TIME [epoch: 2.59 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1983454798602166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1983454798602166 | validation: 0.35201853160623]
	TIME [epoch: 2.6 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23958167938869518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23958167938869518 | validation: 0.5180570714967431]
	TIME [epoch: 2.58 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26825765618977254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26825765618977254 | validation: 0.40605640853904607]
	TIME [epoch: 2.6 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2115543787422063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2115543787422063 | validation: 0.40023259577017767]
	TIME [epoch: 2.59 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17093628270211395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17093628270211395 | validation: 0.35056711887235903]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_292.pth
	Model improved!!!
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670055114422477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1670055114422477 | validation: 0.4334880889827744]
	TIME [epoch: 2.63 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1558132461389612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1558132461389612 | validation: 0.3117655274800155]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_294.pth
	Model improved!!!
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1366955995694771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1366955995694771 | validation: 0.4233061489601889]
	TIME [epoch: 2.62 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466589699642037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1466589699642037 | validation: 0.33232942145605515]
	TIME [epoch: 2.6 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2089100054937018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2089100054937018 | validation: 0.5792187073341113]
	TIME [epoch: 2.61 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22478181979819603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22478181979819603 | validation: 0.33238234481760637]
	TIME [epoch: 2.6 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28169592042711844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28169592042711844 | validation: 0.523201379463089]
	TIME [epoch: 2.61 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36003708226051245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36003708226051245 | validation: 0.5299482490191161]
	TIME [epoch: 2.61 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23885866368728717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23885866368728717 | validation: 0.442166084291837]
	TIME [epoch: 2.61 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1702324749686579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1702324749686579 | validation: 0.34858433875275274]
	TIME [epoch: 2.61 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1649018903092484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1649018903092484 | validation: 0.3870633728545679]
	TIME [epoch: 2.62 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13340604870494308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13340604870494308 | validation: 0.3799308700691808]
	TIME [epoch: 2.61 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1329089423143627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1329089423143627 | validation: 0.3096974014709768]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_305.pth
	Model improved!!!
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16376517415085026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16376517415085026 | validation: 0.46712992577651424]
	TIME [epoch: 2.6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22164226834748355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22164226834748355 | validation: 0.37905976989602386]
	TIME [epoch: 2.61 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23316265215020568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23316265215020568 | validation: 0.44426037255143846]
	TIME [epoch: 2.6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19447498889810136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19447498889810136 | validation: 0.33215904938804974]
	TIME [epoch: 2.61 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12862504987287063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12862504987287063 | validation: 0.31140034867400634]
	TIME [epoch: 2.58 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1249022005282194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1249022005282194 | validation: 0.33465949861445454]
	TIME [epoch: 2.63 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11713793200667776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11713793200667776 | validation: 0.29863621353088654]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1097594649124587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1097594649124587 | validation: 0.32543017933441604]
	TIME [epoch: 2.61 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10714849813464868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10714849813464868 | validation: 0.29678624081207333]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_314.pth
	Model improved!!!
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1247263983646871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1247263983646871 | validation: 0.4011169560433663]
	TIME [epoch: 2.61 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18278643971373257		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18278643971373257 | validation: 0.3461720474596879]
	TIME [epoch: 2.61 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26984593331721457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26984593331721457 | validation: 0.5490324524343272]
	TIME [epoch: 2.6 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31169237614819956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31169237614819956 | validation: 0.31968419431386225]
	TIME [epoch: 2.6 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2450207956424149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2450207956424149 | validation: 0.36518097155443663]
	TIME [epoch: 2.6 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1728224830623546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1728224830623546 | validation: 0.3102006035625959]
	TIME [epoch: 2.6 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11501960249697137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11501960249697137 | validation: 0.3440007989088746]
	TIME [epoch: 2.6 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12900898831340032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12900898831340032 | validation: 0.30030358439376803]
	TIME [epoch: 2.6 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1175858534571861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1175858534571861 | validation: 0.34366924748924815]
	TIME [epoch: 2.6 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11114561105147429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11114561105147429 | validation: 0.2820446074688771]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11472600494298016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11472600494298016 | validation: 0.5426499894452069]
	TIME [epoch: 2.61 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16893654491616783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16893654491616783 | validation: 0.2997164484257058]
	TIME [epoch: 2.6 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1518457244204772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1518457244204772 | validation: 0.3343164687783642]
	TIME [epoch: 2.61 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12425927496399661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12425927496399661 | validation: 0.2972864730060722]
	TIME [epoch: 2.6 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11869109226250256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11869109226250256 | validation: 0.2765293582768923]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14697396192600581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14697396192600581 | validation: 0.49833985872607]
	TIME [epoch: 2.6 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28957784272624937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28957784272624937 | validation: 0.3475182910441068]
	TIME [epoch: 2.6 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2282554059627333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2282554059627333 | validation: 0.3107835712838997]
	TIME [epoch: 2.6 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13362689199608463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13362689199608463 | validation: 0.3292887902083598]
	TIME [epoch: 2.6 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12681925659975374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12681925659975374 | validation: 0.29848180411965936]
	TIME [epoch: 2.6 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10635576067002127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10635576067002127 | validation: 0.2781030478019756]
	TIME [epoch: 2.6 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10162446795685276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10162446795685276 | validation: 0.28970508183644755]
	TIME [epoch: 2.61 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09927443325520979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09927443325520979 | validation: 0.27200809717683827]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_337.pth
	Model improved!!!
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10005522812613951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10005522812613951 | validation: 0.3431592539097812]
	TIME [epoch: 2.6 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10800517861259301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10800517861259301 | validation: 0.27222239416551364]
	TIME [epoch: 2.6 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11203568939327484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11203568939327484 | validation: 0.3930335896192031]
	TIME [epoch: 2.6 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.186394416378357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.186394416378357 | validation: 0.38372768003834284]
	TIME [epoch: 2.6 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2551562402188677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2551562402188677 | validation: 0.25683800254448164]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_342.pth
	Model improved!!!
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1291520529679272		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1291520529679272 | validation: 0.4525396758887629]
	TIME [epoch: 2.61 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1758762190862171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1758762190862171 | validation: 0.2903077005358274]
	TIME [epoch: 2.6 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22380642334010517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22380642334010517 | validation: 0.43509804248786355]
	TIME [epoch: 2.6 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13359963766834645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13359963766834645 | validation: 0.2972663907564969]
	TIME [epoch: 2.6 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09433415442353088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09433415442353088 | validation: 0.25809228719436605]
	TIME [epoch: 2.6 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10916801800925612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10916801800925612 | validation: 0.2930471531913203]
	TIME [epoch: 2.6 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11101241516448518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11101241516448518 | validation: 0.3427768156693506]
	TIME [epoch: 2.6 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1224867103488921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1224867103488921 | validation: 0.2893468774858809]
	TIME [epoch: 2.6 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11647868049505698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11647868049505698 | validation: 0.2872852436603319]
	TIME [epoch: 2.6 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10626995103011362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10626995103011362 | validation: 0.29267663352823653]
	TIME [epoch: 2.6 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11235869773186087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11235869773186087 | validation: 0.2860843307752115]
	TIME [epoch: 2.6 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11707166701459859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11707166701459859 | validation: 0.29416584542281526]
	TIME [epoch: 2.61 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13770060152055671		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13770060152055671 | validation: 0.35649739041268413]
	TIME [epoch: 2.6 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17377282265670002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17377282265670002 | validation: 0.3309372296771271]
	TIME [epoch: 2.6 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16501236551855156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16501236551855156 | validation: 0.26586859122014955]
	TIME [epoch: 2.6 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12383478493915895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12383478493915895 | validation: 0.40550893974996305]
	TIME [epoch: 2.6 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12470777132195085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12470777132195085 | validation: 0.25065726380587416]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_359.pth
	Model improved!!!
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08454653720665213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08454653720665213 | validation: 0.23572448030031934]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_360.pth
	Model improved!!!
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08720399306778634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08720399306778634 | validation: 0.32997268200509994]
	TIME [epoch: 2.6 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09568223566539957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09568223566539957 | validation: 0.24596377178342566]
	TIME [epoch: 2.59 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1260252637940814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1260252637940814 | validation: 0.33774821217082485]
	TIME [epoch: 2.59 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1573069055166836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1573069055166836 | validation: 0.3363285700565535]
	TIME [epoch: 2.61 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18454384743835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18454384743835 | validation: 0.328397299491352]
	TIME [epoch: 2.6 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14078323118708486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14078323118708486 | validation: 0.2562278973429378]
	TIME [epoch: 2.6 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11489307061346837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11489307061346837 | validation: 0.28702204989692953]
	TIME [epoch: 2.6 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12085927518344436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12085927518344436 | validation: 0.27705458969547914]
	TIME [epoch: 2.61 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11851878309900159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11851878309900159 | validation: 0.2553689631985429]
	TIME [epoch: 2.59 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10067516931623188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10067516931623188 | validation: 0.28224679943517056]
	TIME [epoch: 2.6 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08932339638850516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08932339638850516 | validation: 0.22738266460004186]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10290926359954096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10290926359954096 | validation: 0.36733493715908266]
	TIME [epoch: 2.61 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10709192094807683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10709192094807683 | validation: 0.22819875140614287]
	TIME [epoch: 2.61 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12734993733938887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12734993733938887 | validation: 0.2773699077242773]
	TIME [epoch: 2.61 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12208970736883246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12208970736883246 | validation: 0.29365408253157776]
	TIME [epoch: 2.6 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12896835783448238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12896835783448238 | validation: 0.3226092272380968]
	TIME [epoch: 2.61 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16519609436077395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16519609436077395 | validation: 0.2502078633060835]
	TIME [epoch: 2.61 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11929076945701823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11929076945701823 | validation: 0.2586469355779683]
	TIME [epoch: 2.61 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10892113552344085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10892113552344085 | validation: 0.23091300636302595]
	TIME [epoch: 2.61 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11040264129466243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11040264129466243 | validation: 0.25669469356791663]
	TIME [epoch: 2.61 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1327004801291779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1327004801291779 | validation: 0.25233004132313647]
	TIME [epoch: 2.61 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08283900264522098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08283900264522098 | validation: 0.1992892121623525]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_382.pth
	Model improved!!!
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704505447547756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0704505447547756 | validation: 0.2217469799093129]
	TIME [epoch: 2.6 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754344984813346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06754344984813346 | validation: 0.21827723911459318]
	TIME [epoch: 2.61 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06739275732761049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06739275732761049 | validation: 0.24737219321178508]
	TIME [epoch: 2.61 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07774683273995688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07774683273995688 | validation: 0.23179339768061055]
	TIME [epoch: 2.61 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11241134114112555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11241134114112555 | validation: 0.40812998491286195]
	TIME [epoch: 2.6 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18592266565452734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18592266565452734 | validation: 0.2557150317627341]
	TIME [epoch: 2.61 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1669579587586545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1669579587586545 | validation: 0.2771073196300948]
	TIME [epoch: 2.6 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07951430334421529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07951430334421529 | validation: 0.20216150482171047]
	TIME [epoch: 2.61 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07129346106022398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07129346106022398 | validation: 0.1912974413453212]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002711671712609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1002711671712609 | validation: 0.36280127771728626]
	TIME [epoch: 2.61 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2261889186805038		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2261889186805038 | validation: 0.3008111651996121]
	TIME [epoch: 2.59 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20677381220849267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20677381220849267 | validation: 0.27929897413872173]
	TIME [epoch: 2.61 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13586464564018363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13586464564018363 | validation: 0.23051112632199866]
	TIME [epoch: 2.6 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08846561776603289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08846561776603289 | validation: 0.25273885619353537]
	TIME [epoch: 2.61 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13051248968237822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13051248968237822 | validation: 0.23322747408022995]
	TIME [epoch: 2.61 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08447148124088998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08447148124088998 | validation: 0.24499355163403913]
	TIME [epoch: 2.6 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07379598566914865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07379598566914865 | validation: 0.2029686694577765]
	TIME [epoch: 2.61 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06866832709525234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06866832709525234 | validation: 0.22174484888374565]
	TIME [epoch: 2.6 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06512085906860468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06512085906860468 | validation: 0.19144435196301002]
	TIME [epoch: 2.61 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06613267059797742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06613267059797742 | validation: 0.21401739812829224]
	TIME [epoch: 2.6 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06417304179128952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06417304179128952 | validation: 0.1933996806919842]
	TIME [epoch: 2.6 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07653852530148215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07653852530148215 | validation: 0.3207283856455709]
	TIME [epoch: 2.61 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11630320638444978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11630320638444978 | validation: 0.271451902140527]
	TIME [epoch: 2.6 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19752118469331437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19752118469331437 | validation: 0.24053103282660393]
	TIME [epoch: 2.61 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09358224411593925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09358224411593925 | validation: 0.20520808872731858]
	TIME [epoch: 2.61 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0741781184924655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0741781184924655 | validation: 0.20983749272093963]
	TIME [epoch: 2.6 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08198031369631885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08198031369631885 | validation: 0.3325356897446318]
	TIME [epoch: 2.61 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15382848754520076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15382848754520076 | validation: 0.3420831679518438]
	TIME [epoch: 2.61 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.237553921920541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.237553921920541 | validation: 0.19522683739228736]
	TIME [epoch: 2.6 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11621536773023657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11621536773023657 | validation: 0.25240911814864536]
	TIME [epoch: 2.61 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165359344313415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165359344313415 | validation: 0.19423207071872606]
	TIME [epoch: 2.6 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06915863090081403		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06915863090081403 | validation: 0.24485849727677644]
	TIME [epoch: 2.6 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07804214941356455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07804214941356455 | validation: 0.19457629360812914]
	TIME [epoch: 2.59 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05737931507032007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05737931507032007 | validation: 0.20618538535752942]
	TIME [epoch: 2.6 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05540778511592736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05540778511592736 | validation: 0.20176368680814025]
	TIME [epoch: 2.6 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06048903940300345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06048903940300345 | validation: 0.18356249092197766]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_418.pth
	Model improved!!!
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.075461016916734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.075461016916734 | validation: 0.4041930459965226]
	TIME [epoch: 2.62 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1599913460828843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1599913460828843 | validation: 0.21254297535048983]
	TIME [epoch: 2.62 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11831892867925126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11831892867925126 | validation: 0.21584169962468516]
	TIME [epoch: 2.62 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11758326228406621		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11758326228406621 | validation: 0.20334821528160393]
	TIME [epoch: 2.6 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1135555239047495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1135555239047495 | validation: 0.27839522348424006]
	TIME [epoch: 2.61 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18013880391799258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18013880391799258 | validation: 0.1840992532004975]
	TIME [epoch: 2.6 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461347608266261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1461347608266261 | validation: 0.20535740907187844]
	TIME [epoch: 2.61 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08090438897716673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08090438897716673 | validation: 0.23062391108985106]
	TIME [epoch: 2.6 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07227536630783823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07227536630783823 | validation: 0.17696514938464222]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_427.pth
	Model improved!!!
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06555691071141245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06555691071141245 | validation: 0.18419621868973707]
	TIME [epoch: 2.61 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825457229402554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06825457229402554 | validation: 0.20956420195304531]
	TIME [epoch: 2.6 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07445525511027432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07445525511027432 | validation: 0.20339327897466092]
	TIME [epoch: 2.6 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09655186443448271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09655186443448271 | validation: 0.5127899949139606]
	TIME [epoch: 2.59 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1466500372791755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1466500372791755 | validation: 0.17787954654770885]
	TIME [epoch: 2.6 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09644794521731281		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09644794521731281 | validation: 0.2016966444182554]
	TIME [epoch: 2.6 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636597981232937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08636597981232937 | validation: 0.1641844984799465]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06222298800765369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06222298800765369 | validation: 0.17919370376044622]
	TIME [epoch: 2.62 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06515628059685417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06515628059685417 | validation: 0.16845587039948928]
	TIME [epoch: 2.6 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0695367749219628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0695367749219628 | validation: 0.21364661886937011]
	TIME [epoch: 2.61 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10205817012514282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10205817012514282 | validation: 0.22462129589874685]
	TIME [epoch: 2.61 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19218183098097236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19218183098097236 | validation: 0.2947487638723671]
	TIME [epoch: 2.61 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18647916477521823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18647916477521823 | validation: 0.21124765238320237]
	TIME [epoch: 2.61 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08127559302579192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08127559302579192 | validation: 0.20945618154580684]
	TIME [epoch: 2.61 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06492440341125026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06492440341125026 | validation: 0.18432746303182168]
	TIME [epoch: 2.61 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06985489039116928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06985489039116928 | validation: 0.19282080420395176]
	TIME [epoch: 2.61 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06080869928950134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06080869928950134 | validation: 0.19044361496204157]
	TIME [epoch: 2.61 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06515295427228586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06515295427228586 | validation: 0.18648468017636627]
	TIME [epoch: 2.61 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08827334653360641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08827334653360641 | validation: 0.23297557145277278]
	TIME [epoch: 2.61 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10893028762483274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10893028762483274 | validation: 0.20756894894464548]
	TIME [epoch: 2.61 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11276259384523091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11276259384523091 | validation: 0.2866548218209756]
	TIME [epoch: 2.6 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10428390809322678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10428390809322678 | validation: 0.1795165465292058]
	TIME [epoch: 2.62 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08331289130711513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08331289130711513 | validation: 0.18523411027017067]
	TIME [epoch: 2.61 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06148931258931309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06148931258931309 | validation: 0.173706260876097]
	TIME [epoch: 2.61 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08225276871517082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08225276871517082 | validation: 0.177541854314934]
	TIME [epoch: 2.6 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08342564809242291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08342564809242291 | validation: 0.2636204748499731]
	TIME [epoch: 2.61 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08503881424582307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08503881424582307 | validation: 0.19374719846397934]
	TIME [epoch: 2.61 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11689414516574527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11689414516574527 | validation: 0.23459161769665454]
	TIME [epoch: 2.61 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13312968832164312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13312968832164312 | validation: 0.2348019532955699]
	TIME [epoch: 2.61 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09142726481421275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09142726481421275 | validation: 0.16607078821232277]
	TIME [epoch: 2.6 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05484982165955323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05484982165955323 | validation: 0.14520574394237112]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04945496981645373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04945496981645373 | validation: 0.1640565069290275]
	TIME [epoch: 2.61 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05231993350832293		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05231993350832293 | validation: 0.14869240467220704]
	TIME [epoch: 2.6 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053337259655564025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053337259655564025 | validation: 0.16079403133266315]
	TIME [epoch: 2.61 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06563603708775351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06563603708775351 | validation: 0.20264060281728097]
	TIME [epoch: 2.6 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12639772954227246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12639772954227246 | validation: 0.31348965160365705]
	TIME [epoch: 2.61 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15029775771794224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15029775771794224 | validation: 0.22799081489701287]
	TIME [epoch: 2.6 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538527526274867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1538527526274867 | validation: 0.1769575334053127]
	TIME [epoch: 2.61 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298483546182515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06298483546182515 | validation: 0.18421212358815842]
	TIME [epoch: 2.61 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058326510519031365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058326510519031365 | validation: 0.23587379872654043]
	TIME [epoch: 2.61 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06769012713254771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06769012713254771 | validation: 0.1632475463592106]
	TIME [epoch: 2.6 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06303246871656695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06303246871656695 | validation: 0.17179921524043246]
	TIME [epoch: 2.63 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07039785085247967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07039785085247967 | validation: 0.1826364148818625]
	TIME [epoch: 2.61 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08872033974033942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08872033974033942 | validation: 0.2963794776731575]
	TIME [epoch: 2.61 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09120185366001035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09120185366001035 | validation: 0.2025995402989839]
	TIME [epoch: 2.61 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13515610269076514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13515610269076514 | validation: 0.2882739379278772]
	TIME [epoch: 2.61 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1974269004914124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1974269004914124 | validation: 0.2721373333400902]
	TIME [epoch: 2.61 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11328073445260359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11328073445260359 | validation: 0.16635821568717857]
	TIME [epoch: 2.61 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056749125920387104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056749125920387104 | validation: 0.1573169555167069]
	TIME [epoch: 2.6 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052482403001679466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052482403001679466 | validation: 0.1784487573588432]
	TIME [epoch: 2.61 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05572472021595516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05572472021595516 | validation: 0.156628704504395]
	TIME [epoch: 2.61 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047826647652949625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047826647652949625 | validation: 0.15315996407376328]
	TIME [epoch: 2.61 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04780150350091794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04780150350091794 | validation: 0.14018116816545215]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_480.pth
	Model improved!!!
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04691328323554836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04691328323554836 | validation: 0.15496418554358785]
	TIME [epoch: 2.61 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0504407297254299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0504407297254299 | validation: 0.156172335247201]
	TIME [epoch: 2.61 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08195573083449538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08195573083449538 | validation: 0.2964658340551645]
	TIME [epoch: 2.61 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1881652199067486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1881652199067486 | validation: 0.28717671780084947]
	TIME [epoch: 2.61 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12729970989851153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12729970989851153 | validation: 0.22618947860814714]
	TIME [epoch: 2.62 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07074630983736334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07074630983736334 | validation: 0.1808695160651326]
	TIME [epoch: 2.61 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07674472797327313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07674472797327313 | validation: 0.20281864609544098]
	TIME [epoch: 2.61 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07047267488114858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07047267488114858 | validation: 0.20361287941537123]
	TIME [epoch: 2.62 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10308484744689551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10308484744689551 | validation: 0.23017463212183104]
	TIME [epoch: 2.62 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12754022273152107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12754022273152107 | validation: 0.18146766763160135]
	TIME [epoch: 2.6 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0887300808944934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0887300808944934 | validation: 0.17882060134279565]
	TIME [epoch: 2.63 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0537863229007744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0537863229007744 | validation: 0.16096145577152096]
	TIME [epoch: 2.6 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043276359205902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05043276359205902 | validation: 0.17275915971023226]
	TIME [epoch: 2.62 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05898869824977201		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05898869824977201 | validation: 0.17209540866362888]
	TIME [epoch: 2.61 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08489027907737162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08489027907737162 | validation: 0.22150667663060175]
	TIME [epoch: 2.62 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13170434968171413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13170434968171413 | validation: 0.19473013386662386]
	TIME [epoch: 2.61 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11307609851344856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11307609851344856 | validation: 0.1843531141547566]
	TIME [epoch: 2.62 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08469904587300722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08469904587300722 | validation: 0.19021645279861688]
	TIME [epoch: 2.6 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05113046620322619		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05113046620322619 | validation: 0.13406970153928474]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_499.pth
	Model improved!!!
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048223749543464256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048223749543464256 | validation: 0.16639292425712632]
	TIME [epoch: 2.59 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053748348549496776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053748348549496776 | validation: 0.1398513972377288]
	TIME [epoch: 169 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061875632728338806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061875632728338806 | validation: 0.2246891852494879]
	TIME [epoch: 5.66 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07467106376533987		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07467106376533987 | validation: 0.16660279675553233]
	TIME [epoch: 5.65 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08284679980587871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08284679980587871 | validation: 0.17860690856987332]
	TIME [epoch: 5.63 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06779095462196369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06779095462196369 | validation: 0.17782092648415013]
	TIME [epoch: 5.66 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336456182317247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06336456182317247 | validation: 0.1907041259509774]
	TIME [epoch: 5.64 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054728118724271406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054728118724271406 | validation: 0.15355401645752811]
	TIME [epoch: 5.67 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05621845084955261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05621845084955261 | validation: 0.16988032449668627]
	TIME [epoch: 5.63 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08819542644011953		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08819542644011953 | validation: 0.2461149549860815]
	TIME [epoch: 5.67 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13798136029176697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13798136029176697 | validation: 0.26320118554257127]
	TIME [epoch: 5.64 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21024114394402638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21024114394402638 | validation: 0.23799796100200019]
	TIME [epoch: 5.67 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12318996620993082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12318996620993082 | validation: 0.21512595951528143]
	TIME [epoch: 5.64 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1179179627652367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1179179627652367 | validation: 0.1607188136029234]
	TIME [epoch: 5.64 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08540693580065227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08540693580065227 | validation: 0.22428862206710934]
	TIME [epoch: 5.64 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06171829839184386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06171829839184386 | validation: 0.16308941345760625]
	TIME [epoch: 5.67 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04605352495417593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04605352495417593 | validation: 0.14601003811373642]
	TIME [epoch: 5.64 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0520572397550548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0520572397550548 | validation: 0.16921202457041462]
	TIME [epoch: 5.67 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050884219776849406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050884219776849406 | validation: 0.1240035300871222]
	TIME [epoch: 5.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_518.pth
	Model improved!!!
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04401016979429786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04401016979429786 | validation: 0.13541764363572656]
	TIME [epoch: 5.65 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04024947666347393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04024947666347393 | validation: 0.13128697609599024]
	TIME [epoch: 5.64 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04321182792115351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04321182792115351 | validation: 0.14394328967294526]
	TIME [epoch: 5.65 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048166691416664825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048166691416664825 | validation: 0.16689694317598847]
	TIME [epoch: 5.63 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08570975040498326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08570975040498326 | validation: 0.26015327224956175]
	TIME [epoch: 5.65 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18378929580854889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18378929580854889 | validation: 0.20188696032074077]
	TIME [epoch: 5.63 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1259789489875385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1259789489875385 | validation: 0.16609661369395956]
	TIME [epoch: 5.66 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06999835347328397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06999835347328397 | validation: 0.19942594028999155]
	TIME [epoch: 5.65 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058236509033570354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058236509033570354 | validation: 0.18923655932268016]
	TIME [epoch: 5.66 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08685447985485867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08685447985485867 | validation: 0.177401418864411]
	TIME [epoch: 5.67 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06480191499286127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06480191499286127 | validation: 0.1565070387819263]
	TIME [epoch: 5.67 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823760585141948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05823760585141948 | validation: 0.14045019852949944]
	TIME [epoch: 5.67 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06797263072384414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06797263072384414 | validation: 0.17671798837809932]
	TIME [epoch: 5.66 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965102084775383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06965102084775383 | validation: 0.1608702960617568]
	TIME [epoch: 5.63 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08461044558177509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08461044558177509 | validation: 0.2580799419126863]
	TIME [epoch: 5.66 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09865442227204163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09865442227204163 | validation: 0.18395635381426784]
	TIME [epoch: 5.64 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10303759151980531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10303759151980531 | validation: 0.19045536402495922]
	TIME [epoch: 5.67 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07250400611683043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07250400611683043 | validation: 0.17923326937380157]
	TIME [epoch: 5.67 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06780974352646206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06780974352646206 | validation: 0.14270340231878773]
	TIME [epoch: 5.66 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05941337812920633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05941337812920633 | validation: 0.16825136326108267]
	TIME [epoch: 5.66 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054247252149605955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054247252149605955 | validation: 0.15502927625067442]
	TIME [epoch: 5.65 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058921287434845454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058921287434845454 | validation: 0.17692750965855192]
	TIME [epoch: 5.66 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07488697833389235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07488697833389235 | validation: 0.1524899681572723]
	TIME [epoch: 5.64 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08269739915394611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08269739915394611 | validation: 0.15297249282127706]
	TIME [epoch: 5.61 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06510612062184506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06510612062184506 | validation: 0.14181868705221548]
	TIME [epoch: 5.66 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05811775593636119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05811775593636119 | validation: 0.16649855704231828]
	TIME [epoch: 5.6 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08192010276380506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08192010276380506 | validation: 0.19786834338643589]
	TIME [epoch: 5.64 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11487410151823627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11487410151823627 | validation: 0.17944767199631884]
	TIME [epoch: 5.63 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10498789223158386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10498789223158386 | validation: 0.17423845981353378]
	TIME [epoch: 5.63 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06166978444311968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06166978444311968 | validation: 0.12696561273408377]
	TIME [epoch: 5.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04862377987225012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04862377987225012 | validation: 0.1476166901867789]
	TIME [epoch: 5.64 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05278830497771841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05278830497771841 | validation: 0.1486745426905703]
	TIME [epoch: 5.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807825120449502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06807825120449502 | validation: 0.17122733765009923]
	TIME [epoch: 5.65 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07696201430048323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07696201430048323 | validation: 0.13810757728279377]
	TIME [epoch: 5.63 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215235959137525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07215235959137525 | validation: 0.13105212177095132]
	TIME [epoch: 5.67 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058529718363858695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058529718363858695 | validation: 0.16136092167129631]
	TIME [epoch: 5.62 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047542751739671534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047542751739671534 | validation: 0.1580524887895888]
	TIME [epoch: 5.65 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06283239586226175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06283239586226175 | validation: 0.23966360519441193]
	TIME [epoch: 5.63 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08561702343825077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08561702343825077 | validation: 0.20483538372274912]
	TIME [epoch: 5.65 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13524024823203337		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13524024823203337 | validation: 0.1976065226610907]
	TIME [epoch: 5.61 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09579587116918141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09579587116918141 | validation: 0.1633475355643937]
	TIME [epoch: 5.66 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06806922081416349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06806922081416349 | validation: 0.14135733805633605]
	TIME [epoch: 5.61 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053474812171463625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053474812171463625 | validation: 0.24865752403410157]
	TIME [epoch: 5.64 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0683590850604048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0683590850604048 | validation: 0.15353059883781495]
	TIME [epoch: 5.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06632539780278178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06632539780278178 | validation: 0.18371539666553144]
	TIME [epoch: 5.63 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07260755618705103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07260755618705103 | validation: 0.1453803454402316]
	TIME [epoch: 5.61 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949030594822329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05949030594822329 | validation: 0.1563147058065083]
	TIME [epoch: 5.64 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060973241070833205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060973241070833205 | validation: 0.15869494749306223]
	TIME [epoch: 5.62 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06258849974123527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06258849974123527 | validation: 0.16341593988742154]
	TIME [epoch: 5.65 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07267268291662149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07267268291662149 | validation: 0.15202046991056328]
	TIME [epoch: 5.65 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07056599706017438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07056599706017438 | validation: 0.1523368270840486]
	TIME [epoch: 5.68 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07972214610073262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07972214610073262 | validation: 0.16151831451666837]
	TIME [epoch: 5.64 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06407941465463904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06407941465463904 | validation: 0.12659692558965108]
	TIME [epoch: 5.66 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06003119494774442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06003119494774442 | validation: 0.1461025328675443]
	TIME [epoch: 5.64 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061331261832350444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061331261832350444 | validation: 0.1488813784620843]
	TIME [epoch: 5.64 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07638747475323843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07638747475323843 | validation: 0.2697448570475751]
	TIME [epoch: 5.64 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094371582310314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.094371582310314 | validation: 0.15309419137799407]
	TIME [epoch: 5.64 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0816005562308997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0816005562308997 | validation: 0.16257988954063193]
	TIME [epoch: 5.66 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0519140691795782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0519140691795782 | validation: 0.1516681165368431]
	TIME [epoch: 5.64 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04791290881076643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04791290881076643 | validation: 0.1400142431604836]
	TIME [epoch: 5.65 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04712645724492076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04712645724492076 | validation: 0.136269858133193]
	TIME [epoch: 5.64 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060219299595297676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060219299595297676 | validation: 0.21122543111567255]
	TIME [epoch: 5.66 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13854487204734683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13854487204734683 | validation: 0.2549119468498473]
	TIME [epoch: 5.64 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1730693895194143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1730693895194143 | validation: 0.1379561573469839]
	TIME [epoch: 5.64 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05150453243901667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05150453243901667 | validation: 0.16021321825834042]
	TIME [epoch: 5.64 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058046399600632126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058046399600632126 | validation: 0.14380416857207265]
	TIME [epoch: 5.63 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059488844410447425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.059488844410447425 | validation: 0.14906268569891845]
	TIME [epoch: 5.67 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04573789606012264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04573789606012264 | validation: 0.14743648425426406]
	TIME [epoch: 5.63 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05206766944790946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05206766944790946 | validation: 0.14474286464403519]
	TIME [epoch: 5.66 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06015179214776176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06015179214776176 | validation: 0.13310557593132688]
	TIME [epoch: 5.63 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507650111128975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07507650111128975 | validation: 0.16076006510848467]
	TIME [epoch: 5.65 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07786837196878142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07786837196878142 | validation: 0.14902756070064835]
	TIME [epoch: 5.64 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07029435270322214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07029435270322214 | validation: 0.19183959642123985]
	TIME [epoch: 5.67 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06521012701180412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06521012701180412 | validation: 0.14243708311386227]
	TIME [epoch: 5.65 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06785087715838461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06785087715838461 | validation: 0.19442683024713037]
	TIME [epoch: 5.65 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05204460840512375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05204460840512375 | validation: 0.1186213728519046]
	TIME [epoch: 5.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_594.pth
	Model improved!!!
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05129946390318711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05129946390318711 | validation: 0.1517423536170908]
	TIME [epoch: 5.64 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07478642637463541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07478642637463541 | validation: 0.21707861373224216]
	TIME [epoch: 5.67 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11852173524267798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11852173524267798 | validation: 0.1893131109114108]
	TIME [epoch: 5.64 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1191322945506711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1191322945506711 | validation: 0.1369208337053098]
	TIME [epoch: 6.67 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05162659419016185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05162659419016185 | validation: 0.1331003482107101]
	TIME [epoch: 5.66 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04957549131207754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04957549131207754 | validation: 0.1307777933984709]
	TIME [epoch: 5.67 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04689034793572321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04689034793572321 | validation: 0.12393283542287062]
	TIME [epoch: 5.66 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04117656997649769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04117656997649769 | validation: 0.12556261525424994]
	TIME [epoch: 5.64 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0470905441744251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0470905441744251 | validation: 0.12854325084585194]
	TIME [epoch: 5.66 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06428082066370956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06428082066370956 | validation: 0.1949990512021977]
	TIME [epoch: 5.64 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08801151721199597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08801151721199597 | validation: 0.15454367267798716]
	TIME [epoch: 5.67 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1006735816893074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1006735816893074 | validation: 0.23091875017827965]
	TIME [epoch: 5.63 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06911691276307481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06911691276307481 | validation: 0.13525087881519668]
	TIME [epoch: 5.65 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06508394501027753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06508394501027753 | validation: 0.1715129248460162]
	TIME [epoch: 5.65 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08613457397273372		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08613457397273372 | validation: 0.16719925616304054]
	TIME [epoch: 5.65 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09221533641829606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09221533641829606 | validation: 0.14702153611041188]
	TIME [epoch: 5.64 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06732144411732056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06732144411732056 | validation: 0.147834039979832]
	TIME [epoch: 5.66 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043756812790907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043756812790907 | validation: 0.12684307480677837]
	TIME [epoch: 5.66 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0332637938331718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0332637938331718 | validation: 0.1538202826207422]
	TIME [epoch: 5.66 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05789040927825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05789040927825 | validation: 0.17389333411398705]
	TIME [epoch: 5.64 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05304310697738972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05304310697738972 | validation: 0.147279373938677]
	TIME [epoch: 5.66 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05363483754291314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05363483754291314 | validation: 0.14250527506244395]
	TIME [epoch: 5.63 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06293412161435744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06293412161435744 | validation: 0.136239165403572]
	TIME [epoch: 5.63 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060331810404914234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060331810404914234 | validation: 0.140266234423378]
	TIME [epoch: 5.65 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060643009341277275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060643009341277275 | validation: 0.14997300427660892]
	TIME [epoch: 5.65 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09090589101817545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09090589101817545 | validation: 0.21383291522608272]
	TIME [epoch: 5.66 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13840702515832756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13840702515832756 | validation: 0.15967520453424014]
	TIME [epoch: 5.64 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728445274861901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06728445274861901 | validation: 0.1945051759957418]
	TIME [epoch: 5.64 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06584899381793333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06584899381793333 | validation: 0.1702165291804978]
	TIME [epoch: 5.63 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0762298450211736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0762298450211736 | validation: 0.19061617993798646]
	TIME [epoch: 5.63 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06741939931170617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06741939931170617 | validation: 0.1590360818180276]
	TIME [epoch: 5.63 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07225034581964034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07225034581964034 | validation: 0.15358095556320045]
	TIME [epoch: 5.62 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054242327742092165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054242327742092165 | validation: 0.12580264653126916]
	TIME [epoch: 5.64 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038468003927006125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038468003927006125 | validation: 0.12164656843739974]
	TIME [epoch: 5.62 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03506123145972353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03506123145972353 | validation: 0.11347947439074924]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_629.pth
	Model improved!!!
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041095742557963194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041095742557963194 | validation: 0.14624227334822743]
	TIME [epoch: 5.62 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05064255921162841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05064255921162841 | validation: 0.16249105029130834]
	TIME [epoch: 5.63 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0925780645653398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0925780645653398 | validation: 0.20135432407224008]
	TIME [epoch: 5.62 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13369572479095587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13369572479095587 | validation: 0.13358527074011803]
	TIME [epoch: 5.65 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06744408743365356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06744408743365356 | validation: 0.11165486586695653]
	TIME [epoch: 5.64 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_634.pth
	Model improved!!!
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04287180764688039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04287180764688039 | validation: 0.11580392627307529]
	TIME [epoch: 5.58 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03324662400788748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03324662400788748 | validation: 0.12661532976604736]
	TIME [epoch: 5.55 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04740197763951825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04740197763951825 | validation: 0.1431769283371789]
	TIME [epoch: 5.53 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06980488975632029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06980488975632029 | validation: 0.14044881587477817]
	TIME [epoch: 5.55 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0681788161174952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0681788161174952 | validation: 0.1437859021221411]
	TIME [epoch: 5.57 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07810614652085915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07810614652085915 | validation: 0.19570492903948764]
	TIME [epoch: 5.56 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07778445261557419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07778445261557419 | validation: 0.16111943830018183]
	TIME [epoch: 5.58 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09071918482270228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09071918482270228 | validation: 0.18190596820829433]
	TIME [epoch: 5.58 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051768927871943086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051768927871943086 | validation: 0.12629395815869823]
	TIME [epoch: 5.59 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03620860685101402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03620860685101402 | validation: 0.1495293342670975]
	TIME [epoch: 5.58 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04587972268178518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04587972268178518 | validation: 0.1317920986499171]
	TIME [epoch: 5.58 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053062968659004554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053062968659004554 | validation: 0.14744750802217027]
	TIME [epoch: 5.56 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06619699088773043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06619699088773043 | validation: 0.1952642156593061]
	TIME [epoch: 5.55 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10525263239090497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10525263239090497 | validation: 0.15767199218969155]
	TIME [epoch: 5.55 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09091830982279678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09091830982279678 | validation: 0.14393739294786428]
	TIME [epoch: 5.58 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057192045115861706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057192045115861706 | validation: 0.14066742544051297]
	TIME [epoch: 5.54 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03622778629281836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03622778629281836 | validation: 0.14807515228137821]
	TIME [epoch: 5.55 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04489423787046192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04489423787046192 | validation: 0.1324318059195053]
	TIME [epoch: 5.62 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05911332788243948		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05911332788243948 | validation: 0.14713974681879147]
	TIME [epoch: 5.54 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07199047023042222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07199047023042222 | validation: 0.1632020682155591]
	TIME [epoch: 5.59 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09422650563556698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09422650563556698 | validation: 0.17514881248863257]
	TIME [epoch: 5.51 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07266932779670988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07266932779670988 | validation: 0.11965672075420514]
	TIME [epoch: 5.58 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0526955118522693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0526955118522693 | validation: 0.14217737024171342]
	TIME [epoch: 5.56 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040962813686653256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040962813686653256 | validation: 0.1199182989775967]
	TIME [epoch: 5.58 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04287653868384414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04287653868384414 | validation: 0.12168049151677618]
	TIME [epoch: 5.63 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05058161728541604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05058161728541604 | validation: 0.13368570879998504]
	TIME [epoch: 5.57 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06639332680989532		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06639332680989532 | validation: 0.16925198166749622]
	TIME [epoch: 5.64 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10818567946570866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10818567946570866 | validation: 0.16531913026122044]
	TIME [epoch: 5.62 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08667891689284804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08667891689284804 | validation: 0.16073486770701637]
	TIME [epoch: 5.62 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05991317715717516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05991317715717516 | validation: 0.14280528756583008]
	TIME [epoch: 5.61 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039004401707165855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039004401707165855 | validation: 0.12095098372386288]
	TIME [epoch: 5.63 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033994420891011574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033994420891011574 | validation: 0.12257258717572146]
	TIME [epoch: 5.58 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05050277740982545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05050277740982545 | validation: 0.13652057936536482]
	TIME [epoch: 5.6 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076438886792511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06076438886792511 | validation: 0.1398559917831249]
	TIME [epoch: 5.58 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06999452654206424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06999452654206424 | validation: 0.13191244005927807]
	TIME [epoch: 5.61 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061115177430030816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061115177430030816 | validation: 0.13127943078780288]
	TIME [epoch: 5.64 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06070456381130788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06070456381130788 | validation: 0.1931063014200634]
	TIME [epoch: 5.57 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06807797498397991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06807797498397991 | validation: 0.16733887780030174]
	TIME [epoch: 5.64 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10669585705419131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10669585705419131 | validation: 0.1978070464680152]
	TIME [epoch: 5.55 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09229783802796764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09229783802796764 | validation: 0.12228414564884145]
	TIME [epoch: 5.6 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0454818656889883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0454818656889883 | validation: 0.09756720441376006]
	TIME [epoch: 5.56 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_675.pth
	Model improved!!!
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0400223313248874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0400223313248874 | validation: 0.13502716789663477]
	TIME [epoch: 5.64 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041261005811435096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041261005811435096 | validation: 0.12455556134079498]
	TIME [epoch: 5.65 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04382167656572007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04382167656572007 | validation: 0.12406984423072709]
	TIME [epoch: 5.63 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05023668746227471		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05023668746227471 | validation: 0.14226534471559732]
	TIME [epoch: 5.62 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083892132853553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07083892132853553 | validation: 0.17911266410272222]
	TIME [epoch: 5.63 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09744229054013388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09744229054013388 | validation: 0.16037492876767434]
	TIME [epoch: 5.66 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826591689614188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08826591689614188 | validation: 0.12573055370349626]
	TIME [epoch: 5.64 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06676032798314091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06676032798314091 | validation: 0.14036401369181828]
	TIME [epoch: 5.65 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04120366941418063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04120366941418063 | validation: 0.20782995868378032]
	TIME [epoch: 5.63 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10687901921889832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10687901921889832 | validation: 0.24297436872988973]
	TIME [epoch: 5.66 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11001130468091697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11001130468091697 | validation: 0.20859542861077027]
	TIME [epoch: 5.62 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11881402186574694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11881402186574694 | validation: 0.18833712865408733]
	TIME [epoch: 5.65 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13779555130296298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13779555130296298 | validation: 0.15031083672257797]
	TIME [epoch: 5.62 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057736373213403545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057736373213403545 | validation: 0.14597394877805067]
	TIME [epoch: 5.65 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045497169178816056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045497169178816056 | validation: 0.12090304154559764]
	TIME [epoch: 5.62 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044625432987542235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044625432987542235 | validation: 0.11972571513027525]
	TIME [epoch: 5.63 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03849129118883506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03849129118883506 | validation: 0.10339980704623825]
	TIME [epoch: 5.62 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03661829590433028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03661829590433028 | validation: 0.11668133205182762]
	TIME [epoch: 5.66 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039416679325108624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039416679325108624 | validation: 0.11118296502801624]
	TIME [epoch: 5.64 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06677340399295827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06677340399295827 | validation: 0.14181322412379604]
	TIME [epoch: 5.65 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06967557163993517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06967557163993517 | validation: 0.18359550928522528]
	TIME [epoch: 5.65 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12359566484656596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12359566484656596 | validation: 0.19426714193271952]
	TIME [epoch: 5.62 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12060795338988303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12060795338988303 | validation: 0.09445025906656634]
	TIME [epoch: 5.63 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03520628567981564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03520628567981564 | validation: 0.11343272323332979]
	TIME [epoch: 5.62 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04170301095195251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04170301095195251 | validation: 0.12295664567252912]
	TIME [epoch: 5.61 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04778829683940805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04778829683940805 | validation: 0.08914357109046175]
	TIME [epoch: 5.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_701.pth
	Model improved!!!
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031393894943117735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031393894943117735 | validation: 0.10714701765637871]
	TIME [epoch: 5.68 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03154178869666381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03154178869666381 | validation: 0.12208232747948498]
	TIME [epoch: 5.67 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054943636037242775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054943636037242775 | validation: 0.15841367458026756]
	TIME [epoch: 5.62 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08376092945463615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08376092945463615 | validation: 0.1447555570872283]
	TIME [epoch: 5.64 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155421031995167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10155421031995167 | validation: 0.14374698220767637]
	TIME [epoch: 5.65 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05363408015432944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05363408015432944 | validation: 0.09544606926842544]
	TIME [epoch: 5.62 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033866374272427835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033866374272427835 | validation: 0.09667529794293755]
	TIME [epoch: 5.66 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034605363714625706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034605363714625706 | validation: 0.10913231026371935]
	TIME [epoch: 5.61 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03389536802738483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03389536802738483 | validation: 0.10214114630355048]
	TIME [epoch: 5.66 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04545965676692026		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04545965676692026 | validation: 0.1347883554770223]
	TIME [epoch: 5.65 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07942288055300138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07942288055300138 | validation: 0.16957084486323604]
	TIME [epoch: 5.6 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1044674770717642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1044674770717642 | validation: 0.1624173764847737]
	TIME [epoch: 5.65 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08734364398054355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08734364398054355 | validation: 0.10009491409035416]
	TIME [epoch: 5.62 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04016843827632219		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04016843827632219 | validation: 0.09065217269887021]
	TIME [epoch: 5.67 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151069111502745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03151069111502745 | validation: 0.1302264178915433]
	TIME [epoch: 5.64 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03804022422423074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03804022422423074 | validation: 0.12123321046274832]
	TIME [epoch: 5.65 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04989881178588707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04989881178588707 | validation: 0.16754890378631326]
	TIME [epoch: 5.63 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07265962794658845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07265962794658845 | validation: 0.12109762435339229]
	TIME [epoch: 5.63 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058099000873824094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058099000873824094 | validation: 0.11982770825380862]
	TIME [epoch: 5.61 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044942707125321155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044942707125321155 | validation: 0.13392430098884592]
	TIME [epoch: 5.63 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05189533383644569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05189533383644569 | validation: 0.17271585208852241]
	TIME [epoch: 5.6 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04974140640458984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04974140640458984 | validation: 0.13076850501346213]
	TIME [epoch: 5.64 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05522228056802574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05522228056802574 | validation: 0.1481936373803719]
	TIME [epoch: 5.64 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06873223046329109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06873223046329109 | validation: 0.1580586668176965]
	TIME [epoch: 5.65 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08784144868729238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08784144868729238 | validation: 0.12004722782967697]
	TIME [epoch: 5.65 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0567512208091514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0567512208091514 | validation: 0.12677104125340274]
	TIME [epoch: 5.64 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04644310020767545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04644310020767545 | validation: 0.11287109127076907]
	TIME [epoch: 5.67 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04456495231818159		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04456495231818159 | validation: 0.1318414178744361]
	TIME [epoch: 5.63 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055246041805988536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055246041805988536 | validation: 0.1565286419106654]
	TIME [epoch: 5.66 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06754726798179424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06754726798179424 | validation: 0.1361759609440278]
	TIME [epoch: 5.64 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04282824199964394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04282824199964394 | validation: 0.1229864768368587]
	TIME [epoch: 5.65 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06076929557653596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06076929557653596 | validation: 0.14273900000825987]
	TIME [epoch: 5.64 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0656024862482269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0656024862482269 | validation: 0.11667741759898936]
	TIME [epoch: 5.64 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06252975094316485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06252975094316485 | validation: 0.13820582380974578]
	TIME [epoch: 5.63 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05055626805977517		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05055626805977517 | validation: 0.12348078554507125]
	TIME [epoch: 5.65 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060540265661251376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060540265661251376 | validation: 0.15770236854093264]
	TIME [epoch: 5.65 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09679629547355674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09679629547355674 | validation: 0.14444297705236483]
	TIME [epoch: 5.65 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07935742539998841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07935742539998841 | validation: 0.1198247148661123]
	TIME [epoch: 5.64 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0457655525645552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0457655525645552 | validation: 0.09777550442710366]
	TIME [epoch: 5.64 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03214009836968527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03214009836968527 | validation: 0.08680183155715859]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_741.pth
	Model improved!!!
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03201653748279155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03201653748279155 | validation: 0.12791297473434102]
	TIME [epoch: 5.64 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046728951095366185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046728951095366185 | validation: 0.10777893038727578]
	TIME [epoch: 5.66 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05999604232090886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05999604232090886 | validation: 0.13636920838681477]
	TIME [epoch: 5.64 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07354347584605361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07354347584605361 | validation: 0.11681934753215767]
	TIME [epoch: 5.64 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049708865247390675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049708865247390675 | validation: 0.13068105414478878]
	TIME [epoch: 5.62 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04182532346156478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04182532346156478 | validation: 0.10790235331934804]
	TIME [epoch: 5.65 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047499447176511896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047499447176511896 | validation: 0.1417279882294503]
	TIME [epoch: 5.62 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600602248630582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600602248630582 | validation: 0.16451631682842793]
	TIME [epoch: 5.65 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10961133200651901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10961133200651901 | validation: 0.15899308731120376]
	TIME [epoch: 5.62 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06259323966727061		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06259323966727061 | validation: 0.11505011875971323]
	TIME [epoch: 5.65 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0386283337985433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0386283337985433 | validation: 0.11076828954585663]
	TIME [epoch: 5.62 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036500877032257024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036500877032257024 | validation: 0.12175575908738297]
	TIME [epoch: 5.63 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03858287083083048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03858287083083048 | validation: 0.09372034461104785]
	TIME [epoch: 5.62 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036929689861933734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036929689861933734 | validation: 0.12249869597714856]
	TIME [epoch: 5.64 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04941435421580822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04941435421580822 | validation: 0.14949333892166705]
	TIME [epoch: 5.62 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08250964177439475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08250964177439475 | validation: 0.15920776713325493]
	TIME [epoch: 5.62 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07264386332231813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07264386332231813 | validation: 0.10172639448362293]
	TIME [epoch: 5.62 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039654810835431833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039654810835431833 | validation: 0.11064939441102464]
	TIME [epoch: 5.64 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030222794066841134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030222794066841134 | validation: 0.09844729207324612]
	TIME [epoch: 5.62 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027160314140942415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027160314140942415 | validation: 0.09580024180260056]
	TIME [epoch: 5.66 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02752846519659224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02752846519659224 | validation: 0.10202885507425499]
	TIME [epoch: 5.62 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04116301077908863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04116301077908863 | validation: 0.18586043553238604]
	TIME [epoch: 5.66 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1367186417855683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1367186417855683 | validation: 0.23652848388539277]
	TIME [epoch: 5.63 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16310924020004566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16310924020004566 | validation: 0.11622662106169823]
	TIME [epoch: 5.66 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03516630203001315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03516630203001315 | validation: 0.14370154205762817]
	TIME [epoch: 5.63 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0611169136457282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0611169136457282 | validation: 0.14682434744577555]
	TIME [epoch: 5.65 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0576744014816903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0576744014816903 | validation: 0.11471489635329189]
	TIME [epoch: 5.62 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04263186970270559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04263186970270559 | validation: 0.12286100121220103]
	TIME [epoch: 5.66 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058118949723131495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058118949723131495 | validation: 0.1545177892356911]
	TIME [epoch: 5.64 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08242925464561633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08242925464561633 | validation: 0.1245424192608144]
	TIME [epoch: 5.65 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053496027750627326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053496027750627326 | validation: 0.12246897928558168]
	TIME [epoch: 5.64 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050545327956615135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050545327956615135 | validation: 0.09376974164578986]
	TIME [epoch: 5.61 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0348673673622672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0348673673622672 | validation: 0.09157225271837183]
	TIME [epoch: 5.61 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027578503258251157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027578503258251157 | validation: 0.09100787024258783]
	TIME [epoch: 5.62 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02419976570434888		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02419976570434888 | validation: 0.09339915993761305]
	TIME [epoch: 5.59 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026985033075915963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026985033075915963 | validation: 0.09358331909044756]
	TIME [epoch: 5.64 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02420992984113545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02420992984113545 | validation: 0.07921230892442305]
	TIME [epoch: 5.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_778.pth
	Model improved!!!
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025076911807255586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025076911807255586 | validation: 0.13292742725221807]
	TIME [epoch: 5.63 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906113467727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03906113467727 | validation: 0.14248586430116802]
	TIME [epoch: 5.65 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054332351006362425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054332351006362425 | validation: 0.18125539972177285]
	TIME [epoch: 5.63 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13716168686106092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13716168686106092 | validation: 0.2074140019637635]
	TIME [epoch: 5.65 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14253639599646245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14253639599646245 | validation: 0.17184251739405995]
	TIME [epoch: 5.64 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880434950129349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0880434950129349 | validation: 0.15119931442621604]
	TIME [epoch: 5.64 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0647284960409238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0647284960409238 | validation: 0.13941883896055235]
	TIME [epoch: 5.64 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05180575808770876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05180575808770876 | validation: 0.1315787960261346]
	TIME [epoch: 5.67 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03948074806320189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03948074806320189 | validation: 0.11276510794178735]
	TIME [epoch: 5.63 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0370585977636485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0370585977636485 | validation: 0.11593111166293435]
	TIME [epoch: 5.63 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03561373986209834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03561373986209834 | validation: 0.10977534446193915]
	TIME [epoch: 5.68 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03776559946115546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03776559946115546 | validation: 0.10686025472352022]
	TIME [epoch: 5.63 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0355146571926624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0355146571926624 | validation: 0.10937780778077438]
	TIME [epoch: 5.64 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051439478229480036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051439478229480036 | validation: 0.111181413326577]
	TIME [epoch: 5.63 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054120763243760805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054120763243760805 | validation: 0.1535919404127419]
	TIME [epoch: 5.66 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08140679597975652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08140679597975652 | validation: 0.16986306038018928]
	TIME [epoch: 5.63 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10624524536809256		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10624524536809256 | validation: 0.12208534638446622]
	TIME [epoch: 5.66 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06386553039894104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06386553039894104 | validation: 0.09404700134279464]
	TIME [epoch: 5.67 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029868321406202582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029868321406202582 | validation: 0.07902864375374598]
	TIME [epoch: 5.66 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_797.pth
	Model improved!!!
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027694738441696964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027694738441696964 | validation: 0.10045289023810716]
	TIME [epoch: 5.63 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03488939549223114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03488939549223114 | validation: 0.10724005977438206]
	TIME [epoch: 5.64 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04888284203356029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04888284203356029 | validation: 0.134498556821929]
	TIME [epoch: 5.63 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06767731883914373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06767731883914373 | validation: 0.11427271148904468]
	TIME [epoch: 5.65 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05766986881823957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05766986881823957 | validation: 0.10782317939028406]
	TIME [epoch: 5.63 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04389016368918228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04389016368918228 | validation: 0.10097591218542551]
	TIME [epoch: 5.65 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04701156942941168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04701156942941168 | validation: 0.17764238609701866]
	TIME [epoch: 5.63 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07675260883068248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07675260883068248 | validation: 0.15347886989811776]
	TIME [epoch: 5.61 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08677294195255175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08677294195255175 | validation: 0.15118788991361223]
	TIME [epoch: 5.61 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04927909295164603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04927909295164603 | validation: 0.12218162093563581]
	TIME [epoch: 5.64 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03723995063678054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03723995063678054 | validation: 0.1108083859426281]
	TIME [epoch: 5.62 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04613287046768072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04613287046768072 | validation: 0.11837606468079331]
	TIME [epoch: 5.64 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05963600511008782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05963600511008782 | validation: 0.10895395823332038]
	TIME [epoch: 5.61 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05297742354375025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05297742354375025 | validation: 0.10465682801245704]
	TIME [epoch: 5.64 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049385620773770754		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049385620773770754 | validation: 0.11236634991244801]
	TIME [epoch: 5.62 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04288525482563426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04288525482563426 | validation: 0.12066436443212672]
	TIME [epoch: 5.65 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04353062988126839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04353062988126839 | validation: 0.1261831731077023]
	TIME [epoch: 5.62 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041075087597667745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041075087597667745 | validation: 0.11037430322256077]
	TIME [epoch: 5.64 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049928673150841985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049928673150841985 | validation: 0.12428632677435095]
	TIME [epoch: 5.63 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725752139669704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0725752139669704 | validation: 0.15584655570799577]
	TIME [epoch: 5.62 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08758530922643097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08758530922643097 | validation: 0.1138777950272079]
	TIME [epoch: 5.63 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901535054591428		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05901535054591428 | validation: 0.10601997174366454]
	TIME [epoch: 5.6 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03447311450016097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03447311450016097 | validation: 0.1032642373944218]
	TIME [epoch: 5.61 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623326347995787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03623326347995787 | validation: 0.12280616149451258]
	TIME [epoch: 5.61 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05525032809390584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05525032809390584 | validation: 0.14055227421326763]
	TIME [epoch: 5.58 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08995209036584186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08995209036584186 | validation: 0.10190353488135853]
	TIME [epoch: 5.61 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07940228014233602		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07940228014233602 | validation: 0.13013274794146043]
	TIME [epoch: 5.58 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04892635037581298		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04892635037581298 | validation: 0.08040319450628614]
	TIME [epoch: 5.62 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028547245561061984		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028547245561061984 | validation: 0.11447918401034524]
	TIME [epoch: 5.62 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029296820882765716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029296820882765716 | validation: 0.1090918169023446]
	TIME [epoch: 5.6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04328000082925144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04328000082925144 | validation: 0.1025615272343309]
	TIME [epoch: 5.61 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03321312030194761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03321312030194761 | validation: 0.12259045400648759]
	TIME [epoch: 5.64 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054403171346127016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054403171346127016 | validation: 0.17973140763480264]
	TIME [epoch: 5.62 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09234269500946578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09234269500946578 | validation: 0.14718933828363695]
	TIME [epoch: 5.64 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10229779399710057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10229779399710057 | validation: 0.13082573220131152]
	TIME [epoch: 5.62 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05263282657199422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05263282657199422 | validation: 0.10348286943386853]
	TIME [epoch: 5.63 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03250904582128251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03250904582128251 | validation: 0.10775299582501541]
	TIME [epoch: 5.59 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040900752207046656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040900752207046656 | validation: 0.11545602040272146]
	TIME [epoch: 5.63 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03687061769166908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03687061769166908 | validation: 0.09493769889920749]
	TIME [epoch: 5.6 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027517085606175003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027517085606175003 | validation: 0.0879065851124278]
	TIME [epoch: 5.63 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023676661779923194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023676661779923194 | validation: 0.08255318986878649]
	TIME [epoch: 5.6 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027367726838811057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027367726838811057 | validation: 0.0923513637594461]
	TIME [epoch: 5.61 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04610688455914649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04610688455914649 | validation: 0.1705463720139862]
	TIME [epoch: 5.61 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12998618250153832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12998618250153832 | validation: 0.19001885647050998]
	TIME [epoch: 5.61 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.132625869441587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.132625869441587 | validation: 0.08014213528983027]
	TIME [epoch: 5.61 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852261734448807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03852261734448807 | validation: 0.12535681511014338]
	TIME [epoch: 5.61 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05496382304939759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05496382304939759 | validation: 0.11645975559126515]
	TIME [epoch: 5.58 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06229083812419309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06229083812419309 | validation: 0.10446815682194972]
	TIME [epoch: 5.61 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045144181912529974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045144181912529974 | validation: 0.0884311712551667]
	TIME [epoch: 5.59 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04358277010513439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04358277010513439 | validation: 0.09775590013991822]
	TIME [epoch: 5.63 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0342962660502802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0342962660502802 | validation: 0.07766134257954324]
	TIME [epoch: 5.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_848.pth
	Model improved!!!
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03001039500871362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03001039500871362 | validation: 0.09234650743134276]
	TIME [epoch: 5.65 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028407081044557962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028407081044557962 | validation: 0.10511648279471682]
	TIME [epoch: 5.62 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04133927200505357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04133927200505357 | validation: 0.17643449779237935]
	TIME [epoch: 5.66 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055753497234989506		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055753497234989506 | validation: 0.10833562762266169]
	TIME [epoch: 5.61 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057232722052743414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057232722052743414 | validation: 0.11710633473668484]
	TIME [epoch: 5.69 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04719108002580976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04719108002580976 | validation: 0.08724478533109649]
	TIME [epoch: 5.6 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322428265392939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04322428265392939 | validation: 0.10026554227268335]
	TIME [epoch: 5.68 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045816367291030946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045816367291030946 | validation: 0.13050152828122316]
	TIME [epoch: 5.66 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705736894088478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0705736894088478 | validation: 0.13722974631942156]
	TIME [epoch: 5.65 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08934662156254648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08934662156254648 | validation: 0.11496729880676951]
	TIME [epoch: 5.67 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05949295955434688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05949295955434688 | validation: 0.0864640647983353]
	TIME [epoch: 5.65 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027100080592357263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027100080592357263 | validation: 0.10209750492933821]
	TIME [epoch: 5.66 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03416912130847047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03416912130847047 | validation: 0.11233561556057776]
	TIME [epoch: 5.62 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05565555343167579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05565555343167579 | validation: 0.14653259182115888]
	TIME [epoch: 5.63 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07344063917781692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07344063917781692 | validation: 0.10150272610449318]
	TIME [epoch: 5.61 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054875120286338265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054875120286338265 | validation: 0.1120810048450782]
	TIME [epoch: 5.64 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026870520410714704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026870520410714704 | validation: 0.07852913704832648]
	TIME [epoch: 5.6 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026027927997577496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026027927997577496 | validation: 0.09182400564919849]
	TIME [epoch: 5.64 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033132266559257204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.033132266559257204 | validation: 0.10732887238074099]
	TIME [epoch: 5.58 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04623865224233205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04623865224233205 | validation: 0.0872739754215704]
	TIME [epoch: 5.62 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843346381894223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03843346381894223 | validation: 0.10033080705295949]
	TIME [epoch: 5.58 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052381098455336124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052381098455336124 | validation: 0.1739170516233862]
	TIME [epoch: 5.63 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13543152132699326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13543152132699326 | validation: 0.15132131676601657]
	TIME [epoch: 5.61 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09244593561376323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09244593561376323 | validation: 0.085756826241334]
	TIME [epoch: 5.63 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03789367670602187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03789367670602187 | validation: 0.13927017730331234]
	TIME [epoch: 5.59 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06713187318697102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06713187318697102 | validation: 0.1071535815450033]
	TIME [epoch: 5.62 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336792723231223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06336792723231223 | validation: 0.10676166362800493]
	TIME [epoch: 5.61 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052406511938159865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052406511938159865 | validation: 0.08842252695267558]
	TIME [epoch: 5.63 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035038377181739626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035038377181739626 | validation: 0.09140837520008639]
	TIME [epoch: 5.6 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025924084082611133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025924084082611133 | validation: 0.07598297834893872]
	TIME [epoch: 5.62 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_878.pth
	Model improved!!!
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02432423486183528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02432423486183528 | validation: 0.07067213727490917]
	TIME [epoch: 5.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023029128759963834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023029128759963834 | validation: 0.06659049847444524]
	TIME [epoch: 5.55 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_880.pth
	Model improved!!!
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.023468681583326472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.023468681583326472 | validation: 0.08834649756613736]
	TIME [epoch: 5.6 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019262574203333778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019262574203333778 | validation: 0.0701731567964224]
	TIME [epoch: 5.6 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.01877927799037181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.01877927799037181 | validation: 0.07195959455830322]
	TIME [epoch: 5.58 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02444897112610537		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02444897112610537 | validation: 0.11028402600032515]
	TIME [epoch: 5.61 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04899202346020656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04899202346020656 | validation: 0.24281401924607138]
	TIME [epoch: 5.6 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06242810389544922		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06242810389544922 | validation: 0.17204284586077678]
	TIME [epoch: 5.59 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.161608292333704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.161608292333704 | validation: 0.18895188134959562]
	TIME [epoch: 5.58 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10360485062294529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10360485062294529 | validation: 0.08994078182581493]
	TIME [epoch: 5.58 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03352126657856909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03352126657856909 | validation: 0.09561701358675423]
	TIME [epoch: 5.59 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051630371820926405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051630371820926405 | validation: 0.11883021165734087]
	TIME [epoch: 5.57 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05424774819670221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05424774819670221 | validation: 0.1126658059633212]
	TIME [epoch: 5.58 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04446726702286485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04446726702286485 | validation: 0.10251183626975774]
	TIME [epoch: 5.58 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0537119462897304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0537119462897304 | validation: 0.12366205881998882]
	TIME [epoch: 5.58 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03883485866149247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03883485866149247 | validation: 0.08071620241175914]
	TIME [epoch: 5.58 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03005556962953559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03005556962953559 | validation: 0.09509071384998158]
	TIME [epoch: 5.59 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512357373327694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03512357373327694 | validation: 0.0958643447586149]
	TIME [epoch: 5.58 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03415468037211986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03415468037211986 | validation: 0.08448672912706573]
	TIME [epoch: 5.6 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03290220352509837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03290220352509837 | validation: 0.10197350718939147]
	TIME [epoch: 5.59 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037610700157521076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037610700157521076 | validation: 0.1211696888162479]
	TIME [epoch: 5.57 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06700636822728698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06700636822728698 | validation: 0.17248030130842143]
	TIME [epoch: 5.58 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07911319129745463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07911319129745463 | validation: 0.11826798349896511]
	TIME [epoch: 5.55 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07663428110097198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07663428110097198 | validation: 0.16059644177982257]
	TIME [epoch: 5.56 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07914662326471227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07914662326471227 | validation: 0.10315208291583727]
	TIME [epoch: 5.57 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03169423094425688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03169423094425688 | validation: 0.08959529120813012]
	TIME [epoch: 5.56 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02322292494863975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02322292494863975 | validation: 0.09879549779469561]
	TIME [epoch: 5.62 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03044073118507143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03044073118507143 | validation: 0.08881417329004938]
	TIME [epoch: 5.57 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030805546672156306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030805546672156306 | validation: 0.0822461430954638]
	TIME [epoch: 5.57 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04157824311275112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04157824311275112 | validation: 0.12209848148844693]
	TIME [epoch: 5.59 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0557023082514303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0557023082514303 | validation: 0.09658846147715607]
	TIME [epoch: 5.6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345770479276966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06345770479276966 | validation: 0.15692773615911096]
	TIME [epoch: 5.6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06087684227769254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06087684227769254 | validation: 0.10123199439760554]
	TIME [epoch: 5.57 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06148950789295775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06148950789295775 | validation: 0.10755834217162717]
	TIME [epoch: 5.61 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049575426493986705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049575426493986705 | validation: 0.08274996329372482]
	TIME [epoch: 5.6 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02502292075059355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02502292075059355 | validation: 0.06654496144832904]
	TIME [epoch: 5.61 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_914.pth
	Model improved!!!
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019281273481961764		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019281273481961764 | validation: 0.08183250156084]
	TIME [epoch: 5.6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022476151192847366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022476151192847366 | validation: 0.09304048565758222]
	TIME [epoch: 5.56 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03886125537157433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03886125537157433 | validation: 0.14651757187702913]
	TIME [epoch: 5.59 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07767860503146952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07767860503146952 | validation: 0.136758215689138]
	TIME [epoch: 5.6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09204150852033394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09204150852033394 | validation: 0.09452366892401363]
	TIME [epoch: 5.59 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031384517777706226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031384517777706226 | validation: 0.10331005141339476]
	TIME [epoch: 5.6 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04266706936812767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04266706936812767 | validation: 0.09753895994788947]
	TIME [epoch: 5.58 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0533819150584707		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0533819150584707 | validation: 0.10910021935562082]
	TIME [epoch: 5.61 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04427595103316239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04427595103316239 | validation: 0.08456244169341327]
	TIME [epoch: 5.59 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0395699318374746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0395699318374746 | validation: 0.11002241996445666]
	TIME [epoch: 5.61 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058936129777199024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058936129777199024 | validation: 0.11784660285684836]
	TIME [epoch: 5.59 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05970022136587565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05970022136587565 | validation: 0.10699142949006722]
	TIME [epoch: 5.6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05046284307213186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05046284307213186 | validation: 0.07676970159657713]
	TIME [epoch: 5.57 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040814481740524204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040814481740524204 | validation: 0.10227313948955988]
	TIME [epoch: 5.62 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04039287591421166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04039287591421166 | validation: 0.11632481085825769]
	TIME [epoch: 5.55 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04559337355386043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04559337355386043 | validation: 0.08135745840018742]
	TIME [epoch: 5.6 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031729072373973634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031729072373973634 | validation: 0.08469889371635661]
	TIME [epoch: 5.59 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0439833529615658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0439833529615658 | validation: 0.09496297650982541]
	TIME [epoch: 5.6 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042062570802379365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042062570802379365 | validation: 0.10349102392659787]
	TIME [epoch: 5.59 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04322859893126628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04322859893126628 | validation: 0.08356243419626519]
	TIME [epoch: 5.62 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045235843477633646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.045235843477633646 | validation: 0.11817787821943547]
	TIME [epoch: 5.59 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624608021058899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04624608021058899 | validation: 0.1232782180778181]
	TIME [epoch: 5.62 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05874487225059679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05874487225059679 | validation: 0.12582774323767582]
	TIME [epoch: 5.55 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05317997525712313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05317997525712313 | validation: 0.11897943791417893]
	TIME [epoch: 5.63 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0497997053589199		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0497997053589199 | validation: 0.10318455909809701]
	TIME [epoch: 5.56 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011247233425983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05011247233425983 | validation: 0.09135255975256511]
	TIME [epoch: 5.6 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0437099467280507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0437099467280507 | validation: 0.10215567297385411]
	TIME [epoch: 5.59 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043746744632462045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043746744632462045 | validation: 0.08748627710483316]
	TIME [epoch: 5.61 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03311105552609843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03311105552609843 | validation: 0.08255358266585971]
	TIME [epoch: 5.58 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03583597107504288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03583597107504288 | validation: 0.12075564678628128]
	TIME [epoch: 5.6 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05473348391470588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05473348391470588 | validation: 0.11558506475623918]
	TIME [epoch: 5.57 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05815494621655005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05815494621655005 | validation: 0.11399285787637217]
	TIME [epoch: 5.61 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04045870829968158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04045870829968158 | validation: 0.1260769763413512]
	TIME [epoch: 5.6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039554160681949835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039554160681949835 | validation: 0.08753575338419928]
	TIME [epoch: 5.61 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03373328871346605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03373328871346605 | validation: 0.10568470603810459]
	TIME [epoch: 5.59 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046206407355948186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046206407355948186 | validation: 0.125975348060036]
	TIME [epoch: 5.61 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07348942188654572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07348942188654572 | validation: 0.13876050907973678]
	TIME [epoch: 5.56 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07546832478984328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07546832478984328 | validation: 0.09925497117311893]
	TIME [epoch: 5.61 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040193719545035894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040193719545035894 | validation: 0.10021165791880526]
	TIME [epoch: 5.56 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03339702406063851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03339702406063851 | validation: 0.09236584271023046]
	TIME [epoch: 5.6 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030981084226068642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030981084226068642 | validation: 0.08902787910378893]
	TIME [epoch: 5.58 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024818167019286918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024818167019286918 | validation: 0.07670779437563216]
	TIME [epoch: 5.6 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0223067655380568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0223067655380568 | validation: 0.07093414552485568]
	TIME [epoch: 5.59 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025508220265504495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025508220265504495 | validation: 0.08641626931537982]
	TIME [epoch: 5.59 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03737246156189374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03737246156189374 | validation: 0.12856227348472335]
	TIME [epoch: 5.58 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07878335741926859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07878335741926859 | validation: 0.12471068416947569]
	TIME [epoch: 5.6 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10353638886613516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10353638886613516 | validation: 0.1420945579852345]
	TIME [epoch: 5.6 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052502831835551045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052502831835551045 | validation: 0.09182213732194944]
	TIME [epoch: 5.61 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0293041445045203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0293041445045203 | validation: 0.07886465082247143]
	TIME [epoch: 5.59 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028540577696146183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028540577696146183 | validation: 0.2141302270302633]
	TIME [epoch: 5.6 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042623024948871595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042623024948871595 | validation: 0.09007752392253653]
	TIME [epoch: 5.58 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04818185314462771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04818185314462771 | validation: 0.14113657824694534]
	TIME [epoch: 5.61 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10113636313962006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10113636313962006 | validation: 0.09619100405038619]
	TIME [epoch: 5.57 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0696030035915172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0696030035915172 | validation: 0.08052793008100459]
	TIME [epoch: 5.62 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0343741712325357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0343741712325357 | validation: 0.05912234318445084]
	TIME [epoch: 5.55 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_969.pth
	Model improved!!!
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021216128671231058		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021216128671231058 | validation: 0.09407409359572039]
	TIME [epoch: 5.61 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041108456757324795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041108456757324795 | validation: 0.09934374877515689]
	TIME [epoch: 5.54 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056950822781227366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.056950822781227366 | validation: 0.13949654952867996]
	TIME [epoch: 5.61 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11153302085019387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11153302085019387 | validation: 0.11931637599712473]
	TIME [epoch: 5.53 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10760453342214103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10760453342214103 | validation: 0.7750249824736797]
	TIME [epoch: 5.63 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36426696174751144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36426696174751144 | validation: 0.5456311959622487]
	TIME [epoch: 5.6 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3243094006787586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3243094006787586 | validation: 0.3224001597747805]
	TIME [epoch: 5.61 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16785413568802077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16785413568802077 | validation: 0.15579944497846465]
	TIME [epoch: 5.59 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06623990643924153		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06623990643924153 | validation: 0.0880440457530297]
	TIME [epoch: 5.57 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093194547451326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07093194547451326 | validation: 0.10977906093639112]
	TIME [epoch: 5.61 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058920084905028355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.058920084905028355 | validation: 0.10958387122895318]
	TIME [epoch: 5.58 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06198538359420032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06198538359420032 | validation: 0.09508356699333498]
	TIME [epoch: 5.6 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04843996297068193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04843996297068193 | validation: 0.09594075477203318]
	TIME [epoch: 5.6 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0426499674879231		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0426499674879231 | validation: 0.07391358634365568]
	TIME [epoch: 5.58 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031058570387811433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031058570387811433 | validation: 0.07117401678020577]
	TIME [epoch: 5.6 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02680176673710796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02680176673710796 | validation: 0.06380884816454678]
	TIME [epoch: 5.58 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0266167708674963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0266167708674963 | validation: 0.08102780987104308]
	TIME [epoch: 5.59 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02887501860779728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02887501860779728 | validation: 0.04868108004021404]
	TIME [epoch: 5.59 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_987.pth
	Model improved!!!
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030665999458296043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030665999458296043 | validation: 0.07102630177473866]
	TIME [epoch: 5.6 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032402215279674414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032402215279674414 | validation: 0.06717219263606918]
	TIME [epoch: 5.59 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04334583739720793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04334583739720793 | validation: 0.09051568602337831]
	TIME [epoch: 5.6 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055030264668578306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055030264668578306 | validation: 0.08981721881803613]
	TIME [epoch: 5.6 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05792613316942166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05792613316942166 | validation: 0.08404038655739855]
	TIME [epoch: 5.62 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04837593405046075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04837593405046075 | validation: 0.06447203412315576]
	TIME [epoch: 5.59 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03167984377015316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03167984377015316 | validation: 0.07471391788912207]
	TIME [epoch: 5.61 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024459620329630605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.024459620329630605 | validation: 0.06648543449854792]
	TIME [epoch: 5.58 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02236404209988794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02236404209988794 | validation: 0.07041943482505186]
	TIME [epoch: 5.58 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032543241199263745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032543241199263745 | validation: 0.0986323591730368]
	TIME [epoch: 5.54 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06426902323034012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06426902323034012 | validation: 0.11229892610831867]
	TIME [epoch: 5.58 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07912922825011581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07912922825011581 | validation: 0.09280281431309924]
	TIME [epoch: 5.59 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041096664688485135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041096664688485135 | validation: 0.0870397001769451]
	TIME [epoch: 5.62 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04756649889724246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04756649889724246 | validation: 0.10458708455540139]
	TIME [epoch: 176 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048561817997805415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.048561817997805415 | validation: 0.0941226198267171]
	TIME [epoch: 11.9 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04241085658392613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04241085658392613 | validation: 0.08938656912029956]
	TIME [epoch: 12 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0280409882745826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0280409882745826 | validation: 0.07927649797767611]
	TIME [epoch: 11.9 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0404875420126789		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0404875420126789 | validation: 0.13593490825133128]
	TIME [epoch: 12.1 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06395899737614548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06395899737614548 | validation: 0.10322366628150831]
	TIME [epoch: 11.8 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06361469073241646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06361469073241646 | validation: 0.10242738066224395]
	TIME [epoch: 12 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084968839514734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04084968839514734 | validation: 0.08596206290438414]
	TIME [epoch: 12 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022697823818729167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022697823818729167 | validation: 0.07358206214442804]
	TIME [epoch: 12 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.019133313126778694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.019133313126778694 | validation: 0.07471156019765478]
	TIME [epoch: 12 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020321314995928534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020321314995928534 | validation: 0.07667360159043161]
	TIME [epoch: 12 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027601103821989208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027601103821989208 | validation: 0.08419449634264975]
	TIME [epoch: 12 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030245495143166625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030245495143166625 | validation: 0.08328829208710674]
	TIME [epoch: 12 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04537275447935107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04537275447935107 | validation: 0.12178391149438882]
	TIME [epoch: 12 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07508082278878414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07508082278878414 | validation: 0.1523107713869902]
	TIME [epoch: 12 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10736218507954469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10736218507954469 | validation: 0.1410322105799201]
	TIME [epoch: 12 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052870492874000224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052870492874000224 | validation: 0.08084747739330755]
	TIME [epoch: 12 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018372720426676162		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018372720426676162 | validation: 0.08384452767895906]
	TIME [epoch: 12 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.021888034103639268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.021888034103639268 | validation: 0.08066801909601622]
	TIME [epoch: 12 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02439986177661294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02439986177661294 | validation: 0.07410236166293498]
	TIME [epoch: 12 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026434635517977787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026434635517977787 | validation: 0.09292479433951298]
	TIME [epoch: 12 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03348755843418111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03348755843418111 | validation: 0.07762947233938432]
	TIME [epoch: 11.9 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05252752394936815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05252752394936815 | validation: 0.14904539693925054]
	TIME [epoch: 12 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07888188477203396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07888188477203396 | validation: 0.13013129868159318]
	TIME [epoch: 12 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07309812794596925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07309812794596925 | validation: 0.11708757087410589]
	TIME [epoch: 11.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055027672620736406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055027672620736406 | validation: 0.07767098067103335]
	TIME [epoch: 11.9 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0303956738666841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0303956738666841 | validation: 0.08660642196731783]
	TIME [epoch: 11.9 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707310983616551		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03707310983616551 | validation: 0.07980270369147147]
	TIME [epoch: 12 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03472552686264698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03472552686264698 | validation: 0.07787486721430313]
	TIME [epoch: 12 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0362388279164726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0362388279164726 | validation: 0.07893389160342086]
	TIME [epoch: 12 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02484836736751136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02484836736751136 | validation: 0.0841542188354788]
	TIME [epoch: 12 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0236994435337785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0236994435337785 | validation: 0.08603050519620478]
	TIME [epoch: 12 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531335597197943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03531335597197943 | validation: 0.10988923429786315]
	TIME [epoch: 11.9 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055944524377468455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055944524377468455 | validation: 0.12764526112744418]
	TIME [epoch: 12 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060069662177022434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060069662177022434 | validation: 0.09579221699420158]
	TIME [epoch: 11.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03985423503394109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03985423503394109 | validation: 0.0836089058200284]
	TIME [epoch: 12 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027778059263358072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.027778059263358072 | validation: 0.08137029739780523]
	TIME [epoch: 12 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039330394863201945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039330394863201945 | validation: 0.14206826916658116]
	TIME [epoch: 11.9 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10311854948064283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10311854948064283 | validation: 0.13368888655263741]
	TIME [epoch: 12 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08641374394570861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08641374394570861 | validation: 0.06908965993721472]
	TIME [epoch: 11.9 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02936609313966756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02936609313966756 | validation: 0.10150908393516633]
	TIME [epoch: 12 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04290777551486292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04290777551486292 | validation: 0.10774746188528327]
	TIME [epoch: 11.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032972389342325686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032972389342325686 | validation: 0.08366410403182722]
	TIME [epoch: 12 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02374931514000088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02374931514000088 | validation: 0.08943852997733909]
	TIME [epoch: 11.9 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029273497411898726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029273497411898726 | validation: 0.10093113636931844]
	TIME [epoch: 12 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040567318833653644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.040567318833653644 | validation: 0.09687370780819893]
	TIME [epoch: 11.9 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05599354834970903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05599354834970903 | validation: 0.2415500477804014]
	TIME [epoch: 12 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09024741488374194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09024741488374194 | validation: 0.0733132436933109]
	TIME [epoch: 11.8 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04131586703327989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04131586703327989 | validation: 0.09372161353544044]
	TIME [epoch: 12 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025252406113776785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.025252406113776785 | validation: 0.07003054421857606]
	TIME [epoch: 11.9 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02331158202470483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02331158202470483 | validation: 0.08431746130966333]
	TIME [epoch: 12 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02542810616413202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02542810616413202 | validation: 0.07349416526346533]
	TIME [epoch: 11.9 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03157724264246306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03157724264246306 | validation: 0.09465457088535824]
	TIME [epoch: 12 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05534089871342376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05534089871342376 | validation: 0.12550276046652006]
	TIME [epoch: 11.9 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06917438581090687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06917438581090687 | validation: 0.1069727843380989]
	TIME [epoch: 12 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05971179816976964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05971179816976964 | validation: 0.07997963892769862]
	TIME [epoch: 11.9 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03923542866608389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03923542866608389 | validation: 0.10685268646629498]
	TIME [epoch: 12 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03498703706664269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03498703706664269 | validation: 0.0701811786612646]
	TIME [epoch: 11.9 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.018859546776362923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.018859546776362923 | validation: 0.07714831044529531]
	TIME [epoch: 12 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029500883380180264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.029500883380180264 | validation: 0.08906618416633956]
	TIME [epoch: 11.9 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03904745160582653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03904745160582653 | validation: 0.12840799030135314]
	TIME [epoch: 12 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06568789283014094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06568789283014094 | validation: 0.09879434818853872]
	TIME [epoch: 12 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705652439706068		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06705652439706068 | validation: 0.08313982772034703]
	TIME [epoch: 12 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03524039572850829		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03524039572850829 | validation: 0.06529872550426483]
	TIME [epoch: 12 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02206567767235163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02206567767235163 | validation: 0.060438046678565366]
	TIME [epoch: 12 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020467311802185292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020467311802185292 | validation: 0.05949710857834298]
	TIME [epoch: 12 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.022912933874313663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.022912933874313663 | validation: 0.09806475691991595]
	TIME [epoch: 11.9 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702501733727284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03702501733727284 | validation: 0.10571173182029568]
	TIME [epoch: 11.9 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07272355649873932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07272355649873932 | validation: 0.11417360546263498]
	TIME [epoch: 11.9 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352755056666825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07352755056666825 | validation: 0.10932247560197346]
	TIME [epoch: 11.9 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04607925302077686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04607925302077686 | validation: 0.06660335429353653]
	TIME [epoch: 12 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02476557380575495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02476557380575495 | validation: 0.11001332321098488]
	TIME [epoch: 11.8 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03841631034531157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03841631034531157 | validation: 0.12155032402561146]
	TIME [epoch: 12 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326172610428437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05326172610428437 | validation: 0.0792925925840032]
	TIME [epoch: 11.9 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026868275038218332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026868275038218332 | validation: 0.10282689017148111]
	TIME [epoch: 12 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035229020748902204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.035229020748902204 | validation: 0.08846401631513962]
	TIME [epoch: 11.9 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031152500669848215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031152500669848215 | validation: 0.11384281716927623]
	TIME [epoch: 12 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05019959299340667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05019959299340667 | validation: 0.09449601994096757]
	TIME [epoch: 11.9 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06848612563732655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06848612563732655 | validation: 0.12107847544419409]
	TIME [epoch: 12 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055108831315115535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055108831315115535 | validation: 0.08530738794547275]
	TIME [epoch: 11.9 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03862274885057845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03862274885057845 | validation: 0.08595476245893245]
	TIME [epoch: 12 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02473007781191443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02473007781191443 | validation: 0.06797352995631749]
	TIME [epoch: 11.9 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.020933777108682935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.020933777108682935 | validation: 0.07554545700322685]
	TIME [epoch: 11.9 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026522277694278368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026522277694278368 | validation: 0.0875153411054644]
	TIME [epoch: 11.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039594995331049654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.039594995331049654 | validation: 0.09848977826092864]
	TIME [epoch: 12 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055551674336418735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.055551674336418735 | validation: 0.08442732119428246]
	TIME [epoch: 11.9 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04146208000818533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04146208000818533 | validation: 0.060236518632802084]
	TIME [epoch: 12 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02808255002775588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.02808255002775588 | validation: 0.07308822991797898]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd2_20241125_154137/states/model_phi1_4b_v_mmd2_1088.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5671.546 seconds.
