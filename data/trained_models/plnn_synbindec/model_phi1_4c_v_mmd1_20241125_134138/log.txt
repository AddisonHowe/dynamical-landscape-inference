Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1442498013

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.370953279596661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.370953279596661 | validation: 5.274961833915926]
	TIME [epoch: 171 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.2242525148889625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.2242525148889625 | validation: 4.03499316351094]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9423681369062407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9423681369062407 | validation: 4.530584508080991]
	TIME [epoch: 2.84 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.386540965053753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.386540965053753 | validation: 4.17091455544682]
	TIME [epoch: 2.85 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.137892696941371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.137892696941371 | validation: 4.052475005652614]
	TIME [epoch: 2.84 sec]
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.01857592442456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.01857592442456 | validation: 3.697096552815177]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6262029935823894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6262029935823894 | validation: 3.885787799759095]
	TIME [epoch: 2.85 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7708747550861252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7708747550861252 | validation: 3.6816805675965316]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6067795744978812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6067795744978812 | validation: 3.6948242037632952]
	TIME [epoch: 2.83 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6232319943841684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6232319943841684 | validation: 3.683305599993604]
	TIME [epoch: 2.83 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.598401469594328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.598401469594328 | validation: 3.716381708562129]
	TIME [epoch: 2.83 sec]
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.613424178449544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.613424178449544 | validation: 3.694072199388852]
	TIME [epoch: 2.83 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5992913107915783		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5992913107915783 | validation: 3.6538677454212394]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5643286854235097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5643286854235097 | validation: 3.638261839055855]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.541948588532041		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.541948588532041 | validation: 3.628255733294038]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5288808326377024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5288808326377024 | validation: 3.614326561427624]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5183317214780345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5183317214780345 | validation: 3.6031569021745735]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_17.pth
	Model improved!!!
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.512519703846986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.512519703846986 | validation: 3.6150311648885096]
	TIME [epoch: 2.85 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5139406050168027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5139406050168027 | validation: 3.6135150376441545]
	TIME [epoch: 2.85 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.519952835432157		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.519952835432157 | validation: 3.6375220458972417]
	TIME [epoch: 2.84 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5386828813031195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5386828813031195 | validation: 3.602575204771734]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.514287003592154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.514287003592154 | validation: 3.595538186917962]
	TIME [epoch: 2.82 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4988412628645507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4988412628645507 | validation: 3.5595218262829103]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4713699168569305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4713699168569305 | validation: 3.557100974343999]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4594813675444933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4594813675444933 | validation: 3.5342531226423177]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.449546129903015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.449546129903015 | validation: 3.5338569221035643]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4415478153591224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4415478153591224 | validation: 3.52104694902567]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.432744669048936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.432744669048936 | validation: 3.514425455024131]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4257144515564124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4257144515564124 | validation: 3.5017840088805245]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4229670130716934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4229670130716934 | validation: 3.5474217938534376]
	TIME [epoch: 2.84 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.436601536466622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.436601536466622 | validation: 3.4916056410574456]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.422307806340243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.422307806340243 | validation: 3.5398569943093463]
	TIME [epoch: 2.84 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4142787186180166		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4142787186180166 | validation: 3.4320334596514033]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3705394052046804		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3705394052046804 | validation: 3.4647201495225417]
	TIME [epoch: 2.83 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.347907665692342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.347907665692342 | validation: 3.4002371285736785]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3318848147542957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3318848147542957 | validation: 3.425913380181703]
	TIME [epoch: 2.83 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.31206719535131		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.31206719535131 | validation: 3.382873987294]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_37.pth
	Model improved!!!
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2995710842741106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2995710842741106 | validation: 3.3893406296345243]
	TIME [epoch: 2.83 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2859880413630913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2859880413630913 | validation: 3.361423389695073]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2728756286717315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2728756286717315 | validation: 3.4387348249340977]
	TIME [epoch: 2.83 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.295318033279879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.295318033279879 | validation: 3.4776594082630314]
	TIME [epoch: 2.83 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.455139631549839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.455139631549839 | validation: 3.5043697446193933]
	TIME [epoch: 2.83 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3523679495662466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3523679495662466 | validation: 3.3428328878147155]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.227165203103135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.227165203103135 | validation: 3.3162179524175515]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2615510074562155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2615510074562155 | validation: 3.3511255819081684]
	TIME [epoch: 2.84 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2218453349542995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2218453349542995 | validation: 3.272930025291642]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1795065178990582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1795065178990582 | validation: 3.2630640866881957]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1713889128834825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1713889128834825 | validation: 3.268936291723783]
	TIME [epoch: 2.84 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1561793297212173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.1561793297212173 | validation: 3.2178586401610634]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.140777856772327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.140777856772327 | validation: 3.2733823301064184]
	TIME [epoch: 2.83 sec]
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.135585176539702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.135585176539702 | validation: 3.2504058146437673]
	TIME [epoch: 2.82 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2294665745267492		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.2294665745267492 | validation: 3.3486043478687515]
	TIME [epoch: 2.82 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1866636237815302		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.1866636237815302 | validation: 3.169472220975333]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1121926252644845		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.1121926252644845 | validation: 3.1887959132609716]
	TIME [epoch: 2.83 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0640278556710427		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.0640278556710427 | validation: 3.1592812835084185]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.051675066505422		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.051675066505422 | validation: 3.1164177425776196]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0415774066002723		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.0415774066002723 | validation: 3.279456775295597]
	TIME [epoch: 2.85 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1005454279243456		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.1005454279243456 | validation: 3.2671732655385757]
	TIME [epoch: 2.84 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.254913248231337		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.254913248231337 | validation: 3.1416890805905258]
	TIME [epoch: 2.85 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0560049980340773		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.0560049980340773 | validation: 3.235163870036595]
	TIME [epoch: 2.82 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0703974041642694		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.0703974041642694 | validation: 3.060369359381136]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0308761308619148		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.0308761308619148 | validation: 3.09132516981702]
	TIME [epoch: 2.83 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9679177459454342		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.9679177459454342 | validation: 3.0574452596610673]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9442522429624893		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.9442522429624893 | validation: 3.020269498795296]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.927010798082887		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.927010798082887 | validation: 3.0396713428500366]
	TIME [epoch: 2.83 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.912449175586195		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 2.912449175586195 | validation: 2.984091730879914]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9492029499722783		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.9492029499722783 | validation: 3.3759401423468614]
	TIME [epoch: 2.83 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1524707181074314		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.1524707181074314 | validation: 2.892146536002668]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8757108590617952		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.8757108590617952 | validation: 2.8198963706932023]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.723661893720299		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 2.723661893720299 | validation: 2.7666694050942318]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.595428056851475		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 2.595428056851475 | validation: 2.364635634279155]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_71.pth
	Model improved!!!
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2654301223477886		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 2.2654301223477886 | validation: 1.7856393236156252]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8532590011375965		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.8532590011375965 | validation: 2.0596306803817135]
	TIME [epoch: 2.84 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9938248097667393		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.9938248097667393 | validation: 1.573692390059299]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7721502776333227		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.7721502776333227 | validation: 1.8241431208418923]
	TIME [epoch: 2.83 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7183732045810276		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.7183732045810276 | validation: 1.284847500637868]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2926705797619786		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.2926705797619786 | validation: 1.0508411339946913]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.175164460267702		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.175164460267702 | validation: 1.0559913011154058]
	TIME [epoch: 2.83 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0747329900798643		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.0747329900798643 | validation: 0.9761890176176543]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0982290892655728		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 1.0982290892655728 | validation: 1.0032991231006707]
	TIME [epoch: 2.83 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0356593587806584		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.0356593587806584 | validation: 0.8771092849519868]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9668870384661438		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 0.9668870384661438 | validation: 0.9939848064811838]
	TIME [epoch: 2.83 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1884302248384235		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.1884302248384235 | validation: 1.1275056977863447]
	TIME [epoch: 2.83 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1171527423659189		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 1.1171527423659189 | validation: 0.8876235084082369]
	TIME [epoch: 2.83 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9771476713473944		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.9771476713473944 | validation: 0.8591293759207279]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_85.pth
	Model improved!!!
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040564559260737		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.040564559260737 | validation: 1.0664866557273887]
	TIME [epoch: 2.83 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0968679992361405		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 1.0968679992361405 | validation: 0.8141583997070145]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9542212326722935		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.9542212326722935 | validation: 0.800946339794222]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9436516181362872		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.9436516181362872 | validation: 0.8630935268520838]
	TIME [epoch: 2.83 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9642874335657592		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9642874335657592 | validation: 0.8046901328429903]
	TIME [epoch: 2.83 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9604202387891198		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9604202387891198 | validation: 0.8182262675068929]
	TIME [epoch: 2.82 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.932854587082061		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.932854587082061 | validation: 0.7994444686525823]
	TIME [epoch: 2.83 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9330270973101561		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.9330270973101561 | validation: 0.795779132521403]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9225292676741578		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.9225292676741578 | validation: 0.9203687689910167]
	TIME [epoch: 2.85 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9996908845859895		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.9996908845859895 | validation: 0.8126279196641288]
	TIME [epoch: 2.85 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9725721286947848		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9725721286947848 | validation: 0.7875647298187145]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_96.pth
	Model improved!!!
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9164621616235092		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.9164621616235092 | validation: 0.7590796322070585]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9131292341827169		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.9131292341827169 | validation: 0.7818564902488991]
	TIME [epoch: 2.84 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8987375538017137		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8987375538017137 | validation: 0.7767793876997878]
	TIME [epoch: 2.84 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8985187815737703		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.8985187815737703 | validation: 0.7817635742855565]
	TIME [epoch: 2.84 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.903726414652248		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 0.903726414652248 | validation: 0.7675687977186255]
	TIME [epoch: 2.84 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.894104033831523		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 0.894104033831523 | validation: 0.7838206745120028]
	TIME [epoch: 2.84 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9116898721780435		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.9116898721780435 | validation: 0.8549383442414776]
	TIME [epoch: 2.84 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9845386591880927		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.9845386591880927 | validation: 0.8085640119854797]
	TIME [epoch: 2.85 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9761042547444752		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.9761042547444752 | validation: 0.8602970373559901]
	TIME [epoch: 2.84 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.998440207041408		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.998440207041408 | validation: 0.7867851721594651]
	TIME [epoch: 2.84 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9148539109387462		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.9148539109387462 | validation: 0.7708745799784618]
	TIME [epoch: 2.84 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9114207314729953		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.9114207314729953 | validation: 0.9558097900916146]
	TIME [epoch: 2.84 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0302191948982364		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 1.0302191948982364 | validation: 0.8855798229434758]
	TIME [epoch: 2.84 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0550969433852777		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 1.0550969433852777 | validation: 0.8146612427742517]
	TIME [epoch: 2.84 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9293239245966005		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.9293239245966005 | validation: 0.7953912963864108]
	TIME [epoch: 2.84 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9100670079886493		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9100670079886493 | validation: 0.7710291688750353]
	TIME [epoch: 2.84 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027060539271312		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.9027060539271312 | validation: 0.7626235933616553]
	TIME [epoch: 2.84 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9041606590941504		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9041606590941504 | validation: 0.7982406953179231]
	TIME [epoch: 2.84 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9155710632017645		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.9155710632017645 | validation: 0.8224972740564719]
	TIME [epoch: 2.84 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9662549114852644		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.9662549114852644 | validation: 0.8282524741804371]
	TIME [epoch: 2.84 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9450634148786015		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.9450634148786015 | validation: 0.7774726860199611]
	TIME [epoch: 2.84 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9120678832593501		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.9120678832593501 | validation: 0.7734949755379703]
	TIME [epoch: 2.84 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9011175396809417		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 0.9011175396809417 | validation: 0.7545661829994783]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966300803753776		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.8966300803753776 | validation: 0.7625023052442608]
	TIME [epoch: 2.84 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8943407165314428		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8943407165314428 | validation: 0.7705110820346934]
	TIME [epoch: 2.84 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8885168027371886		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8885168027371886 | validation: 0.7870206208590577]
	TIME [epoch: 2.84 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.906738359878651		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.906738359878651 | validation: 0.8953257505453713]
	TIME [epoch: 2.84 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.033046112255545		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.033046112255545 | validation: 0.807031495613705]
	TIME [epoch: 2.84 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.953586185268524		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.953586185268524 | validation: 0.7762548763401202]
	TIME [epoch: 2.85 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9253452564051423		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.9253452564051423 | validation: 0.766368503195219]
	TIME [epoch: 2.84 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9055554101462511		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.9055554101462511 | validation: 0.7993116400486087]
	TIME [epoch: 2.84 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9285421204704462		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.9285421204704462 | validation: 0.9141257520373209]
	TIME [epoch: 2.84 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0232317649102765		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 1.0232317649102765 | validation: 0.7774164433474371]
	TIME [epoch: 2.84 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9315998509336154		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.9315998509336154 | validation: 0.7682296260838435]
	TIME [epoch: 2.84 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8991136257332576		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.8991136257332576 | validation: 0.7658438209109513]
	TIME [epoch: 2.84 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992725424228775		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.8992725424228775 | validation: 0.7425110851863389]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8871974219057478		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.8871974219057478 | validation: 0.781014190520303]
	TIME [epoch: 2.84 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916457311790386		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.8916457311790386 | validation: 0.8064680968021467]
	TIME [epoch: 2.84 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9446126556867139		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9446126556867139 | validation: 0.865861842772762]
	TIME [epoch: 2.84 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9914108759767589		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9914108759767589 | validation: 0.7387233462083066]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8755128474477212		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8755128474477212 | validation: 0.7846750316045836]
	TIME [epoch: 2.84 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9344653398890619		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.9344653398890619 | validation: 0.8033198847885731]
	TIME [epoch: 2.84 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9292695713821948		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.9292695713821948 | validation: 0.7756421086887997]
	TIME [epoch: 2.85 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8971769968124028		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8971769968124028 | validation: 0.7637907884392375]
	TIME [epoch: 2.84 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8901035102740059		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.8901035102740059 | validation: 0.8291291517502011]
	TIME [epoch: 2.84 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9992758972705926		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.9992758972705926 | validation: 0.7793868662529262]
	TIME [epoch: 2.84 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9048408891131962		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.9048408891131962 | validation: 0.7509372993062894]
	TIME [epoch: 2.84 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8801976011957194		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8801976011957194 | validation: 0.7530060667684835]
	TIME [epoch: 2.84 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8703073678647453		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8703073678647453 | validation: 0.759974866815425]
	TIME [epoch: 2.84 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8788765688497843		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8788765688497843 | validation: 0.7966325774060725]
	TIME [epoch: 2.84 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9081513285546412		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.9081513285546412 | validation: 0.8234098676115537]
	TIME [epoch: 2.84 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9525183551503826		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.9525183551503826 | validation: 0.792586421689342]
	TIME [epoch: 2.84 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9222289565696916		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.9222289565696916 | validation: 0.7941535477136239]
	TIME [epoch: 2.84 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8898611247641475		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.8898611247641475 | validation: 0.7462618822419244]
	TIME [epoch: 2.84 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847349505150208		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8847349505150208 | validation: 0.7475827267630214]
	TIME [epoch: 2.85 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8626966311883684		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8626966311883684 | validation: 0.7498595447520455]
	TIME [epoch: 2.84 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.875762691699086		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.875762691699086 | validation: 0.7634393790230747]
	TIME [epoch: 2.84 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9271957662943369		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.9271957662943369 | validation: 0.7688268263837928]
	TIME [epoch: 2.84 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880219467066921		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8880219467066921 | validation: 0.7557235723363103]
	TIME [epoch: 2.84 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9088955937261135		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.9088955937261135 | validation: 0.7362832904680364]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8526611585017386		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8526611585017386 | validation: 0.8158347003451238]
	TIME [epoch: 2.85 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9262219057657243		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.9262219057657243 | validation: 0.7459334302721321]
	TIME [epoch: 2.84 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828659217261237		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.8828659217261237 | validation: 0.7573075604328494]
	TIME [epoch: 2.84 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533314669190167		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8533314669190167 | validation: 0.7439774063813038]
	TIME [epoch: 2.84 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8421946527209642		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8421946527209642 | validation: 0.7273708111634424]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_161.pth
	Model improved!!!
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8112873123593162		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.8112873123593162 | validation: 0.7430739582735619]
	TIME [epoch: 2.84 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840628646436096		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7840628646436096 | validation: 0.7605413555021111]
	TIME [epoch: 2.84 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8821934006159241		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.8821934006159241 | validation: 1.113495585153992]
	TIME [epoch: 2.84 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2455710125798287		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 1.2455710125798287 | validation: 0.7944427157479379]
	TIME [epoch: 2.84 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161907377667744		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.8161907377667744 | validation: 0.847735729942643]
	TIME [epoch: 2.84 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0361583382949935		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 1.0361583382949935 | validation: 0.6949283596961822]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609366550303479		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.7609366550303479 | validation: 0.7808953429439405]
	TIME [epoch: 2.84 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084380215435912		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8084380215435912 | validation: 0.6934930579130252]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8078964648221572		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.8078964648221572 | validation: 0.7379057691379276]
	TIME [epoch: 2.84 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7469711781258138		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7469711781258138 | validation: 0.670977816811992]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7232082193567491		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.7232082193567491 | validation: 0.7510345078568254]
	TIME [epoch: 2.84 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7483225017227083		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7483225017227083 | validation: 0.7743181707510391]
	TIME [epoch: 2.84 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8350209838274469		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.8350209838274469 | validation: 0.8597529682901043]
	TIME [epoch: 2.84 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.853116308288492		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.853116308288492 | validation: 0.668191673782585]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_175.pth
	Model improved!!!
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7168183830083673		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7168183830083673 | validation: 0.6624093289022679]
	TIME [epoch: 2.84 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_176.pth
	Model improved!!!
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.715254062415834		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.715254062415834 | validation: 0.6786282974898303]
	TIME [epoch: 2.86 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7035117054767608		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7035117054767608 | validation: 0.6319231593776827]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6763589058140012		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.6763589058140012 | validation: 0.671436762230705]
	TIME [epoch: 2.86 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6671295177295846		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.6671295177295846 | validation: 0.6157829159137072]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6839044100367228		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.6839044100367228 | validation: 0.7394289982100098]
	TIME [epoch: 2.86 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7211713581227052		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7211713581227052 | validation: 0.6547180885760167]
	TIME [epoch: 2.86 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7867001672793458		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.7867001672793458 | validation: 0.7072910985689428]
	TIME [epoch: 2.85 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6966595632253193		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.6966595632253193 | validation: 0.5914493874558437]
	TIME [epoch: 2.85 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_184.pth
	Model improved!!!
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.635628717832206		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.635628717832206 | validation: 0.5868168203323195]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_185.pth
	Model improved!!!
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6177882120082334		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.6177882120082334 | validation: 0.6279474617660773]
	TIME [epoch: 2.86 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6205225221672843		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6205225221672843 | validation: 0.5919306459472141]
	TIME [epoch: 2.85 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6517816296728034		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.6517816296728034 | validation: 0.7682660061831877]
	TIME [epoch: 2.85 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7604992394963219		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7604992394963219 | validation: 0.5985433007348642]
	TIME [epoch: 2.85 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365647331513127		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.6365647331513127 | validation: 0.6032441192709908]
	TIME [epoch: 2.85 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5950512637060045		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5950512637060045 | validation: 0.5639352376622562]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_191.pth
	Model improved!!!
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6021721212501022		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.6021721212501022 | validation: 0.6090492190674563]
	TIME [epoch: 2.86 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5883040990520775		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.5883040990520775 | validation: 0.5763715204921515]
	TIME [epoch: 2.85 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018599504439314		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.6018599504439314 | validation: 0.7462817680484791]
	TIME [epoch: 2.85 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6761085401978901		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.6761085401978901 | validation: 0.5836133977632855]
	TIME [epoch: 2.85 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6502014875633901		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.6502014875633901 | validation: 0.6003679299897043]
	TIME [epoch: 2.85 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6151070938233028		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.6151070938233028 | validation: 0.6964028640933055]
	TIME [epoch: 2.85 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7501329180414112		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7501329180414112 | validation: 0.7400681085851262]
	TIME [epoch: 2.85 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7869271308877377		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7869271308877377 | validation: 0.5842362025027157]
	TIME [epoch: 2.85 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6240001810315744		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.6240001810315744 | validation: 0.5484299211280222]
	TIME [epoch: 2.86 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_200.pth
	Model improved!!!
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6186967655926939		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.6186967655926939 | validation: 0.5872393556516635]
	TIME [epoch: 181 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590697639615702		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.590697639615702 | validation: 0.5183368193013393]
	TIME [epoch: 6.14 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5316424462366193		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.5316424462366193 | validation: 0.49674419186526225]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_203.pth
	Model improved!!!
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5280818567581064		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.5280818567581064 | validation: 0.5258504853872211]
	TIME [epoch: 6.13 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5195519788613722		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5195519788613722 | validation: 0.5063049028129115]
	TIME [epoch: 6.13 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5414114496030432		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.5414114496030432 | validation: 0.5573361310957585]
	TIME [epoch: 6.13 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5664795518137696		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.5664795518137696 | validation: 0.5280819315786577]
	TIME [epoch: 6.13 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5832723051315652		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5832723051315652 | validation: 0.5755428699226401]
	TIME [epoch: 6.13 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6129855906012819		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.6129855906012819 | validation: 0.5133727882036503]
	TIME [epoch: 6.13 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5152320062362489		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.5152320062362489 | validation: 0.4592065691055788]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_210.pth
	Model improved!!!
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5141136824052336		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5141136824052336 | validation: 0.48485887006849837]
	TIME [epoch: 6.12 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4888118799420052		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.4888118799420052 | validation: 0.4908781215790525]
	TIME [epoch: 6.12 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4907108292140784		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.4907108292140784 | validation: 0.48555455734658637]
	TIME [epoch: 6.13 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5127927202123219		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.5127927202123219 | validation: 0.42963401094025794]
	TIME [epoch: 6.13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4760720116572762		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.4760720116572762 | validation: 0.536139751996799]
	TIME [epoch: 6.13 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5222874681132194		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5222874681132194 | validation: 0.4825158461651949]
	TIME [epoch: 6.12 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6022816265865197		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.6022816265865197 | validation: 0.5619886981303502]
	TIME [epoch: 6.13 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970745060002228		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.5970745060002228 | validation: 0.43280954922065096]
	TIME [epoch: 6.12 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4584343506959647		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.4584343506959647 | validation: 0.4030934427217968]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46996098633136657		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.46996098633136657 | validation: 0.43089253904628233]
	TIME [epoch: 6.1 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.459597372898743		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.459597372898743 | validation: 0.4152314732174796]
	TIME [epoch: 6.1 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4400897176318739		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.4400897176318739 | validation: 0.4112587783000474]
	TIME [epoch: 6.1 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4483460946828632		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.4483460946828632 | validation: 0.3960480268780199]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41861637969012394		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.41861637969012394 | validation: 0.3758037987221666]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4101362748078449		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.4101362748078449 | validation: 0.42441000487541136]
	TIME [epoch: 6.08 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41487444949163127		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.41487444949163127 | validation: 0.38442805048195194]
	TIME [epoch: 6.06 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4428672824049567		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.4428672824049567 | validation: 0.48109571650944283]
	TIME [epoch: 6.06 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4374511490237687		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.4374511490237687 | validation: 0.37382021216461]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_228.pth
	Model improved!!!
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.415977595733382		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.415977595733382 | validation: 0.42229942895936023]
	TIME [epoch: 6.1 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43251115750013164		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.43251115750013164 | validation: 0.5536280572795754]
	TIME [epoch: 6.11 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5460612915406343		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.5460612915406343 | validation: 0.5587759131283684]
	TIME [epoch: 6.11 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6954296056050041		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.6954296056050041 | validation: 0.4792450605412135]
	TIME [epoch: 6.11 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.567464478474058		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.567464478474058 | validation: 0.4791914983081125]
	TIME [epoch: 6.1 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6271886490494835		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.6271886490494835 | validation: 0.34113709478056636]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_234.pth
	Model improved!!!
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.384421580491057		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.384421580491057 | validation: 0.5184614616718739]
	TIME [epoch: 6.07 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4825193613970628		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.4825193613970628 | validation: 0.33721591102075577]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38766155523334034		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.38766155523334034 | validation: 0.34853416957997174]
	TIME [epoch: 6.1 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4031083040605976		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.4031083040605976 | validation: 0.356378578322353]
	TIME [epoch: 6.1 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36591527554267816		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.36591527554267816 | validation: 0.4253225634110752]
	TIME [epoch: 6.1 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39238694427184484		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.39238694427184484 | validation: 0.32022364548423066]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36871807997492034		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.36871807997492034 | validation: 0.3227702237364383]
	TIME [epoch: 6.12 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3558045562520167		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.3558045562520167 | validation: 0.3290915271904621]
	TIME [epoch: 6.13 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34408460260183243		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.34408460260183243 | validation: 0.33656571077295017]
	TIME [epoch: 6.13 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3373123924414551		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.3373123924414551 | validation: 0.31502520347536495]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_244.pth
	Model improved!!!
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3348747202592587		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.3348747202592587 | validation: 0.3336981239904664]
	TIME [epoch: 6.11 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3384731356443453		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.3384731356443453 | validation: 0.3400467524778402]
	TIME [epoch: 6.1 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35199825303509763		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.35199825303509763 | validation: 0.4273468672947203]
	TIME [epoch: 6.11 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4310229550098459		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.4310229550098459 | validation: 0.4254558171151134]
	TIME [epoch: 6.11 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5288024711761404		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.5288024711761404 | validation: 0.31479880408068284]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_249.pth
	Model improved!!!
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36618226720844294		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.36618226720844294 | validation: 0.42097674855009726]
	TIME [epoch: 6.1 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4182470037821173		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.4182470037821173 | validation: 0.3119406740330436]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3654931915075859		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.3654931915075859 | validation: 0.2980282448679159]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_252.pth
	Model improved!!!
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3303483866740467		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.3303483866740467 | validation: 0.31673041628720155]
	TIME [epoch: 6.09 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142958801272455		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.3142958801272455 | validation: 0.2569542813149383]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_254.pth
	Model improved!!!
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3106941790413426		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.3106941790413426 | validation: 0.2942301154747057]
	TIME [epoch: 6.09 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3066773274275753		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.3066773274275753 | validation: 0.2694263197720915]
	TIME [epoch: 6.1 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2926680997509971		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.2926680997509971 | validation: 0.265411634479121]
	TIME [epoch: 6.1 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28335947136677636		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.28335947136677636 | validation: 0.2667237809443498]
	TIME [epoch: 6.09 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2849095167738726		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.2849095167738726 | validation: 0.2585256918925051]
	TIME [epoch: 6.09 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27718797865110956		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.27718797865110956 | validation: 0.26723609604433324]
	TIME [epoch: 6.09 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2727842980465435		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.2727842980465435 | validation: 0.32788986223122446]
	TIME [epoch: 6.1 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.304629023405253		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.304629023405253 | validation: 0.38570944350115816]
	TIME [epoch: 6.09 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.436682024317262		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.436682024317262 | validation: 0.3918610613895357]
	TIME [epoch: 6.09 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47166976007740086		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.47166976007740086 | validation: 0.3380561495554102]
	TIME [epoch: 6.09 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35795738575084896		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.35795738575084896 | validation: 0.27814085523219084]
	TIME [epoch: 6.09 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30668338276404805		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.30668338276404805 | validation: 0.23713974477014582]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_266.pth
	Model improved!!!
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2759640489552256		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.2759640489552256 | validation: 0.24080130399537608]
	TIME [epoch: 6.09 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27217096992695955		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.27217096992695955 | validation: 0.2616550498073056]
	TIME [epoch: 6.09 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25941355513480974		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.25941355513480974 | validation: 0.20990951631996266]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_269.pth
	Model improved!!!
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27225711747250486		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.27225711747250486 | validation: 0.4915454782030912]
	TIME [epoch: 6.11 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3739786662816044		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.3739786662816044 | validation: 0.24601204456472914]
	TIME [epoch: 6.1 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25344755416416376		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.25344755416416376 | validation: 0.22894167085329067]
	TIME [epoch: 6.1 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28338426013152657		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.28338426013152657 | validation: 0.23898046819253246]
	TIME [epoch: 6.1 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25638370300888313		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.25638370300888313 | validation: 0.25397049396780524]
	TIME [epoch: 6.09 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24585010904043952		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.24585010904043952 | validation: 0.2657411682915817]
	TIME [epoch: 6.1 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2651955899548177		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.2651955899548177 | validation: 0.22859521366043245]
	TIME [epoch: 6.1 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23518062667870077		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.23518062667870077 | validation: 0.32885518890950394]
	TIME [epoch: 6.1 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2685691810742331		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.2685691810742331 | validation: 0.22874874530691314]
	TIME [epoch: 6.1 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25829184898420027		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.25829184898420027 | validation: 0.21662500548323776]
	TIME [epoch: 6.09 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2371490905672485		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.2371490905672485 | validation: 0.23422092644915216]
	TIME [epoch: 6.11 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22414821945555438		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.22414821945555438 | validation: 0.22278471049503584]
	TIME [epoch: 6.11 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20765305405409443		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.20765305405409443 | validation: 0.19275827209042096]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_282.pth
	Model improved!!!
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20499915416656384		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.20499915416656384 | validation: 0.2565768411091098]
	TIME [epoch: 6.08 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25053249393931964		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.25053249393931964 | validation: 0.34435147360817364]
	TIME [epoch: 6.07 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35619155521588475		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.35619155521588475 | validation: 0.1956471616769512]
	TIME [epoch: 6.07 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22104928924765463		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.22104928924765463 | validation: 0.3767640449369785]
	TIME [epoch: 6.06 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29851997493748544		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.29851997493748544 | validation: 0.2712790271343472]
	TIME [epoch: 6.09 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31203119927926537		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.31203119927926537 | validation: 0.2147111502446742]
	TIME [epoch: 6.1 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26129993347913644		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.26129993347913644 | validation: 0.3241290926963558]
	TIME [epoch: 6.1 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25984546654579005		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.25984546654579005 | validation: 0.17646614139352837]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_290.pth
	Model improved!!!
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2114696587439314		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.2114696587439314 | validation: 0.1631717284422424]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20371989843284768		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.20371989843284768 | validation: 0.1974734785519018]
	TIME [epoch: 6.07 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18416528437533689		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.18416528437533689 | validation: 0.17292002766500636]
	TIME [epoch: 6.07 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19188828124205903		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.19188828124205903 | validation: 0.22798034732399583]
	TIME [epoch: 6.06 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19462016482162037		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.19462016482162037 | validation: 0.18490875863334202]
	TIME [epoch: 6.07 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21747172500196377		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.21747172500196377 | validation: 0.24942796418468538]
	TIME [epoch: 6.07 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22105778386668867		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.22105778386668867 | validation: 0.18653788754471404]
	TIME [epoch: 6.1 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18955637252966212		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.18955637252966212 | validation: 0.19177559025990357]
	TIME [epoch: 6.1 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20549503696106441		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.20549503696106441 | validation: 0.19918592443148786]
	TIME [epoch: 6.1 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.185153592456817		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.185153592456817 | validation: 0.20448590061292224]
	TIME [epoch: 6.1 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20182344586121886		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.20182344586121886 | validation: 0.1713891987514502]
	TIME [epoch: 6.11 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17813332075556887		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.17813332075556887 | validation: 0.22641650978525707]
	TIME [epoch: 6.11 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1905301293252116		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.1905301293252116 | validation: 0.1578243117915488]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_303.pth
	Model improved!!!
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21001869118463856		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.21001869118463856 | validation: 0.29651070160378795]
	TIME [epoch: 6.07 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20256740624781885		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.20256740624781885 | validation: 0.16187753731222856]
	TIME [epoch: 6.07 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20436964710591995		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.20436964710591995 | validation: 0.2327790798678772]
	TIME [epoch: 6.08 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18476255486606644		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.18476255486606644 | validation: 0.2319246751369963]
	TIME [epoch: 6.1 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2232805041028757		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2232805041028757 | validation: 0.13372373764414558]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_308.pth
	Model improved!!!
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.163287642006247		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.163287642006247 | validation: 0.18636612257462506]
	TIME [epoch: 6.11 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15854272444040338		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.15854272444040338 | validation: 0.22089686546169154]
	TIME [epoch: 6.11 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19256541900582153		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.19256541900582153 | validation: 0.17327219981236264]
	TIME [epoch: 6.1 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2228777552691501		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.2228777552691501 | validation: 0.24628884106336138]
	TIME [epoch: 6.1 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18873475733492492		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.18873475733492492 | validation: 0.14056073571888458]
	TIME [epoch: 6.12 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1529107034229503		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.1529107034229503 | validation: 0.1393881575880677]
	TIME [epoch: 6.11 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15016621918073506		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.15016621918073506 | validation: 0.16509855006556426]
	TIME [epoch: 6.11 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15580524226526435		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.15580524226526435 | validation: 0.18781506657956162]
	TIME [epoch: 6.11 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22538100940116435		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.22538100940116435 | validation: 0.22530588887676575]
	TIME [epoch: 6.11 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.161456350166467		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.161456350166467 | validation: 0.12849036534263675]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1582070512640885		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.1582070512640885 | validation: 0.17982231143313332]
	TIME [epoch: 6.1 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.152842787422493		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.152842787422493 | validation: 0.17409690587160506]
	TIME [epoch: 6.1 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16726017136343416		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.16726017136343416 | validation: 0.24396454100084553]
	TIME [epoch: 6.09 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22993042360776333		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.22993042360776333 | validation: 0.15390391044929008]
	TIME [epoch: 6.1 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15341822544778036		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.15341822544778036 | validation: 0.15962469599522688]
	TIME [epoch: 6.09 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1400723321898518		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.1400723321898518 | validation: 0.12601882848453191]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_324.pth
	Model improved!!!
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13356489676064373		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.13356489676064373 | validation: 0.11529692893037538]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_325.pth
	Model improved!!!
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12815807390177533		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.12815807390177533 | validation: 0.18061988203097973]
	TIME [epoch: 6.11 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13118098994773983		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.13118098994773983 | validation: 0.16332241185587174]
	TIME [epoch: 6.11 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21010519825635599		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.21010519825635599 | validation: 0.5001392967155175]
	TIME [epoch: 6.1 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3142964978205507		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.3142964978205507 | validation: 0.2451174271099136]
	TIME [epoch: 6.12 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2784705650071807		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.2784705650071807 | validation: 0.20977256275256623]
	TIME [epoch: 6.12 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24712766482406281		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.24712766482406281 | validation: 0.16539481686825014]
	TIME [epoch: 6.11 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14138856568522745		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.14138856568522745 | validation: 0.1942443454984506]
	TIME [epoch: 6.11 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14444248166311688		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.14444248166311688 | validation: 0.1444516450144028]
	TIME [epoch: 6.11 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14721122239697093		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.14721122239697093 | validation: 0.1199798937896683]
	TIME [epoch: 6.11 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13114055954174889		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.13114055954174889 | validation: 0.15341418964393527]
	TIME [epoch: 6.11 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13329325881857707		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.13329325881857707 | validation: 0.18215740049827858]
	TIME [epoch: 6.1 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16232679269329675		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.16232679269329675 | validation: 0.20180137716990076]
	TIME [epoch: 6.11 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18173651187570797		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.18173651187570797 | validation: 0.11587574606185605]
	TIME [epoch: 6.11 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1377002746174622		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.1377002746174622 | validation: 0.2423683567262348]
	TIME [epoch: 6.11 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14920092133074767		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.14920092133074767 | validation: 0.23156703255480038]
	TIME [epoch: 6.11 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2578621400620839		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.2578621400620839 | validation: 0.1644612066946215]
	TIME [epoch: 6.11 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18347551513444174		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.18347551513444174 | validation: 0.18737804433905192]
	TIME [epoch: 6.11 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13447810376029787		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.13447810376029787 | validation: 0.15611335089582987]
	TIME [epoch: 6.11 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16684179648341807		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.16684179648341807 | validation: 0.13712372384740207]
	TIME [epoch: 6.1 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14480101240902357		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.14480101240902357 | validation: 0.137011398754426]
	TIME [epoch: 6.11 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12062631141684768		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.12062631141684768 | validation: 0.15057924548469767]
	TIME [epoch: 6.11 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12474798996607439		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.12474798996607439 | validation: 0.11356528847177434]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312716758263144		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.1312716758263144 | validation: 0.2503545074410956]
	TIME [epoch: 6.11 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479147299291139		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.1479147299291139 | validation: 0.09752165111420621]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_349.pth
	Model improved!!!
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13147389925226297		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.13147389925226297 | validation: 0.18316719933911108]
	TIME [epoch: 6.11 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12761794424142978		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.12761794424142978 | validation: 0.11701320756158738]
	TIME [epoch: 6.11 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13708664276098745		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.13708664276098745 | validation: 0.22571152222315202]
	TIME [epoch: 6.11 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16362808422909603		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.16362808422909603 | validation: 0.17222551939889552]
	TIME [epoch: 6.11 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15811598291456164		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.15811598291456164 | validation: 0.12837839009762209]
	TIME [epoch: 6.11 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17369819125020178		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.17369819125020178 | validation: 0.19930774743569424]
	TIME [epoch: 6.11 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1216576506307533		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.1216576506307533 | validation: 0.09444580524986027]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_356.pth
	Model improved!!!
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10657538372023019		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.10657538372023019 | validation: 0.10435175882278064]
	TIME [epoch: 6.11 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10232707356166401		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.10232707356166401 | validation: 0.16262609916896772]
	TIME [epoch: 6.11 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10409986720429668		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.10409986720429668 | validation: 0.1363426895741921]
	TIME [epoch: 6.11 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17722125687780257		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.17722125687780257 | validation: 0.8079674936337189]
	TIME [epoch: 6.1 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6191017921879016		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.6191017921879016 | validation: 0.9007683875264876]
	TIME [epoch: 6.11 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341265076325312		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.7341265076325312 | validation: 0.5845581235882714]
	TIME [epoch: 6.1 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4843411458202281		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.4843411458202281 | validation: 0.32664970160336887]
	TIME [epoch: 6.11 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2598003476819011		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.2598003476819011 | validation: 0.15976091154305047]
	TIME [epoch: 6.12 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1987553745138594		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.1987553745138594 | validation: 0.13380404612935268]
	TIME [epoch: 6.11 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2026429677695178		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.2026429677695178 | validation: 0.10761020404306638]
	TIME [epoch: 6.12 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15538140487936425		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.15538140487936425 | validation: 0.13368779395206984]
	TIME [epoch: 6.12 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1426710893630683		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.1426710893630683 | validation: 0.16677712498911657]
	TIME [epoch: 6.12 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1407794341126227		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.1407794341126227 | validation: 0.1368934626097168]
	TIME [epoch: 6.11 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1342632608378549		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.1342632608378549 | validation: 0.11792507479992805]
	TIME [epoch: 6.11 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12540952881426226		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.12540952881426226 | validation: 0.13971044404958818]
	TIME [epoch: 6.12 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12428966650582489		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.12428966650582489 | validation: 0.12090619198845753]
	TIME [epoch: 6.12 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11597953580707242		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.11597953580707242 | validation: 0.11353173987904763]
	TIME [epoch: 6.12 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11533037452535762		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.11533037452535762 | validation: 0.12996873397611378]
	TIME [epoch: 6.11 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11128294077744492		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.11128294077744492 | validation: 0.11655373961906412]
	TIME [epoch: 6.11 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13385257669216696		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.13385257669216696 | validation: 0.1460687953822932]
	TIME [epoch: 6.11 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12524665837955204		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.12524665837955204 | validation: 0.10368282367148705]
	TIME [epoch: 6.11 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11783426790948648		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.11783426790948648 | validation: 0.15578625555932782]
	TIME [epoch: 6.11 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10746513777424138		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.10746513777424138 | validation: 0.09407528633230773]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_379.pth
	Model improved!!!
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10483678120364384		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.10483678120364384 | validation: 0.13141881032230024]
	TIME [epoch: 6.11 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13456147399599874		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.13456147399599874 | validation: 0.1768025194412044]
	TIME [epoch: 6.11 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14982954720503422		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.14982954720503422 | validation: 0.26221906675462436]
	TIME [epoch: 6.11 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17569436758386853		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.17569436758386853 | validation: 0.10834884543336738]
	TIME [epoch: 6.11 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1459108548559002		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.1459108548559002 | validation: 0.09977883303709656]
	TIME [epoch: 6.1 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.106704123914094		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.106704123914094 | validation: 0.21954404664635724]
	TIME [epoch: 6.11 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13295694950049225		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.13295694950049225 | validation: 0.1081964848443683]
	TIME [epoch: 6.1 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12840596230983461		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.12840596230983461 | validation: 0.1037898404681902]
	TIME [epoch: 6.11 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09796867639844198		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.09796867639844198 | validation: 0.6274636044829781]
	TIME [epoch: 6.11 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4659423923828036		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.4659423923828036 | validation: 0.4470606593978813]
	TIME [epoch: 6.11 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28631053845667137		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.28631053845667137 | validation: 0.12424624390330806]
	TIME [epoch: 6.12 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12254485217824802		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.12254485217824802 | validation: 0.09929536875130394]
	TIME [epoch: 6.11 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16525633992343927		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.16525633992343927 | validation: 0.23648344504258084]
	TIME [epoch: 6.11 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19725360901220435		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.19725360901220435 | validation: 0.24442671795952095]
	TIME [epoch: 6.1 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15520985057936823		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.15520985057936823 | validation: 0.20820016496259142]
	TIME [epoch: 6.11 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15113817680199973		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.15113817680199973 | validation: 0.09353240344489645]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_395.pth
	Model improved!!!
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11993302948854105		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.11993302948854105 | validation: 0.10212301774890367]
	TIME [epoch: 6.11 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12272982395933621		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.12272982395933621 | validation: 0.1291266583527164]
	TIME [epoch: 6.11 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1067978287600063		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.1067978287600063 | validation: 0.17361632463461707]
	TIME [epoch: 6.11 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11309982309410266		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.11309982309410266 | validation: 0.08429110551117337]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10792799504918169		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.10792799504918169 | validation: 0.1259375968014935]
	TIME [epoch: 6.11 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10460372386756905		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.10460372386756905 | validation: 0.19904589563915034]
	TIME [epoch: 6.1 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21590923371353904		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.21590923371353904 | validation: 0.10687523410347698]
	TIME [epoch: 6.1 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11056414847234602		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.11056414847234602 | validation: 0.1917173407692414]
	TIME [epoch: 6.1 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1311429476749031		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.1311429476749031 | validation: 0.16854228657820836]
	TIME [epoch: 6.1 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11365459653459516		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.11365459653459516 | validation: 0.10402844966335595]
	TIME [epoch: 6.1 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09797993301244472		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.09797993301244472 | validation: 0.13449920618056752]
	TIME [epoch: 6.11 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09538313190202477		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.09538313190202477 | validation: 0.133057471704197]
	TIME [epoch: 6.11 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09902540717062946		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.09902540717062946 | validation: 0.08917324313673088]
	TIME [epoch: 6.1 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09372039255212795		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.09372039255212795 | validation: 0.11758349625552329]
	TIME [epoch: 6.1 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09215527324773584		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.09215527324773584 | validation: 0.10417466647022017]
	TIME [epoch: 6.1 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09000979142641867		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.09000979142641867 | validation: 0.09843100990931591]
	TIME [epoch: 6.1 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878809726185849		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.0878809726185849 | validation: 0.11130389857207997]
	TIME [epoch: 6.1 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09001460795608424		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.09001460795608424 | validation: 0.09719530136883593]
	TIME [epoch: 6.11 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0960996088675222		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.0960996088675222 | validation: 0.12229167603330848]
	TIME [epoch: 6.11 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10507987369927334		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.10507987369927334 | validation: 0.2923709778452765]
	TIME [epoch: 6.11 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2333467802717622		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.2333467802717622 | validation: 0.16778228207984647]
	TIME [epoch: 6.11 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2020518982405659		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.2020518982405659 | validation: 0.10474581740417209]
	TIME [epoch: 6.1 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10564588877109758		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.10564588877109758 | validation: 0.19665716300755035]
	TIME [epoch: 6.1 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14869954646100186		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.14869954646100186 | validation: 0.09164336044678684]
	TIME [epoch: 6.1 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08801694680190206		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.08801694680190206 | validation: 0.09712772549723353]
	TIME [epoch: 6.1 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09491029773508526		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.09491029773508526 | validation: 0.11100051324789542]
	TIME [epoch: 6.1 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09573958838737394		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.09573958838737394 | validation: 0.07946986882100208]
	TIME [epoch: 6.11 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_422.pth
	Model improved!!!
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08895919256575872		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.08895919256575872 | validation: 0.08247816679719418]
	TIME [epoch: 6.1 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08321140973787877		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.08321140973787877 | validation: 0.10948398317577507]
	TIME [epoch: 6.1 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08444422881607729		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.08444422881607729 | validation: 0.07559294915170893]
	TIME [epoch: 6.09 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_425.pth
	Model improved!!!
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08006814756877771		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.08006814756877771 | validation: 0.12624819965307799]
	TIME [epoch: 6.06 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0889585757530512		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.0889585757530512 | validation: 0.08317675340983267]
	TIME [epoch: 6.07 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12090609949569152		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.12090609949569152 | validation: 0.19094610830267356]
	TIME [epoch: 6.07 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10288800888308344		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.10288800888308344 | validation: 0.09756222206656498]
	TIME [epoch: 6.07 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13875149741587284		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.13875149741587284 | validation: 0.21962908229900735]
	TIME [epoch: 6.06 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12273818953987656		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.12273818953987656 | validation: 0.11625205483079576]
	TIME [epoch: 6.1 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0827368340480352		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.0827368340480352 | validation: 0.07050429891641188]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0951442720904605		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.0951442720904605 | validation: 0.1658392007049462]
	TIME [epoch: 6.1 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10115593281149797		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.10115593281149797 | validation: 0.10506989867308364]
	TIME [epoch: 6.09 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104906890291314		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.09104906890291314 | validation: 0.5200178475432874]
	TIME [epoch: 6.1 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6009325374822296		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.6009325374822296 | validation: 0.4520559533694766]
	TIME [epoch: 6.09 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5045906560404159		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.5045906560404159 | validation: 0.1382576623476682]
	TIME [epoch: 6.1 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17345839859341225		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.17345839859341225 | validation: 0.1601714492334926]
	TIME [epoch: 6.09 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12737095156939973		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.12737095156939973 | validation: 0.20538289508488586]
	TIME [epoch: 6.1 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12873050120342666		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.12873050120342666 | validation: 0.15921600005681136]
	TIME [epoch: 6.11 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09949453129096593		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.09949453129096593 | validation: 0.10893460994484139]
	TIME [epoch: 6.1 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09761290038849453		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.09761290038849453 | validation: 0.08335112880510699]
	TIME [epoch: 6.1 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09575960643999683		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.09575960643999683 | validation: 0.09330851640991011]
	TIME [epoch: 6.09 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08864383501098377		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.08864383501098377 | validation: 0.10395625091017946]
	TIME [epoch: 6.09 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0882016921166317		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.0882016921166317 | validation: 0.11118729583971276]
	TIME [epoch: 6.1 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880552811631954		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.0880552811631954 | validation: 0.09148988850354045]
	TIME [epoch: 6.1 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08827153528882555		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.08827153528882555 | validation: 0.1126151370176892]
	TIME [epoch: 6.1 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0876773001728704		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.0876773001728704 | validation: 0.08819714334870565]
	TIME [epoch: 6.1 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08710396485061779		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.08710396485061779 | validation: 0.10669466888902922]
	TIME [epoch: 6.1 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08800388370437572		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.08800388370437572 | validation: 0.1299866588950429]
	TIME [epoch: 6.09 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0986012687811213		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.0986012687811213 | validation: 0.14148350392775563]
	TIME [epoch: 6.08 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08749862458539953		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.08749862458539953 | validation: 0.06577955171483503]
	TIME [epoch: 6.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0862745124842746		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.0862745124842746 | validation: 0.12323243805246449]
	TIME [epoch: 6.12 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08739531496005337		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.08739531496005337 | validation: 0.07981928113903389]
	TIME [epoch: 6.12 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11037475965229467		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.11037475965229467 | validation: 0.11207643029318988]
	TIME [epoch: 6.12 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08386762271959014		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.08386762271959014 | validation: 0.0860002709849024]
	TIME [epoch: 6.12 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07971691207885348		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.07971691207885348 | validation: 0.09175745547546688]
	TIME [epoch: 6.12 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.082380933235005		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.082380933235005 | validation: 0.07669915962062113]
	TIME [epoch: 6.11 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083475743735385		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.07083475743735385 | validation: 0.08316295664120041]
	TIME [epoch: 6.12 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07812311463179863		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.07812311463179863 | validation: 0.10500251263015158]
	TIME [epoch: 6.12 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09482885850099963		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.09482885850099963 | validation: 0.12066984016036407]
	TIME [epoch: 6.11 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11715397562981174		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.11715397562981174 | validation: 0.1212113988173992]
	TIME [epoch: 6.11 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1176941953116129		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.1176941953116129 | validation: 0.14304830045267633]
	TIME [epoch: 6.12 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09837688197217613		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.09837688197217613 | validation: 0.09001061002579563]
	TIME [epoch: 6.13 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1177251685796621		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.1177251685796621 | validation: 0.0778424052930008]
	TIME [epoch: 6.12 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07218897779709177		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.07218897779709177 | validation: 0.12532149733438086]
	TIME [epoch: 6.13 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08222088085431636		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.08222088085431636 | validation: 0.057444007527712816]
	TIME [epoch: 6.12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_467.pth
	Model improved!!!
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07367342817014783		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.07367342817014783 | validation: 0.14326213386173456]
	TIME [epoch: 6.08 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09204808031070226		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.09204808031070226 | validation: 0.08683116049069642]
	TIME [epoch: 6.07 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09475467150281111		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.09475467150281111 | validation: 0.16757588440721624]
	TIME [epoch: 6.09 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14403089834016944		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.14403089834016944 | validation: 0.22202912149190196]
	TIME [epoch: 6.11 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11064946408498089		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.11064946408498089 | validation: 0.08896352464437564]
	TIME [epoch: 6.13 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12332196978743816		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.12332196978743816 | validation: 0.10866530803743873]
	TIME [epoch: 6.12 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09046484681844781		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.09046484681844781 | validation: 0.07427141969136229]
	TIME [epoch: 6.13 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08740480022663945		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.08740480022663945 | validation: 0.08213319432909226]
	TIME [epoch: 6.13 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08153754417202029		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.08153754417202029 | validation: 0.09867649632228635]
	TIME [epoch: 6.13 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0735622756642123		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.0735622756642123 | validation: 0.07514085749408828]
	TIME [epoch: 6.12 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08138435227819256		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.08138435227819256 | validation: 0.0649120730463924]
	TIME [epoch: 6.12 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08371707616186828		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.08371707616186828 | validation: 0.13670234457707428]
	TIME [epoch: 6.07 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09272787300743478		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.09272787300743478 | validation: 0.09579369768404945]
	TIME [epoch: 6.12 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08680309486009154		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.08680309486009154 | validation: 0.07138525844439322]
	TIME [epoch: 6.08 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06909409301044182		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.06909409301044182 | validation: 0.07001194991478009]
	TIME [epoch: 6.13 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06992951562366088		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.06992951562366088 | validation: 0.07491777717581775]
	TIME [epoch: 6.07 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06770747605716956		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.06770747605716956 | validation: 0.07959523587008437]
	TIME [epoch: 6.12 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08341089744697307		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.08341089744697307 | validation: 0.07765168257336375]
	TIME [epoch: 6.07 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08988709390974652		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.08988709390974652 | validation: 0.12224872465781021]
	TIME [epoch: 6.07 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0728407811199065		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.0728407811199065 | validation: 0.05374648535384561]
	TIME [epoch: 6.07 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07471489129513004		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.07471489129513004 | validation: 0.10835521103675548]
	TIME [epoch: 6.13 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07364182860905519		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.07364182860905519 | validation: 0.0648444072103738]
	TIME [epoch: 6.12 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07052875597437966		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.07052875597437966 | validation: 0.09090441573642388]
	TIME [epoch: 6.12 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07662771480549246		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.07662771480549246 | validation: 0.06957835943154667]
	TIME [epoch: 6.12 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829965250173928		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.08829965250173928 | validation: 0.14581480338376893]
	TIME [epoch: 6.11 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1785490998422042		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.1785490998422042 | validation: 0.15153268003415554]
	TIME [epoch: 6.12 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0850350204161962		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.0850350204161962 | validation: 0.09508749441960272]
	TIME [epoch: 6.12 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07732715321563954		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.07732715321563954 | validation: 0.2582858919813335]
	TIME [epoch: 6.13 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1274701577422759		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.1274701577422759 | validation: 0.07431939186313899]
	TIME [epoch: 6.12 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09844589639734683		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.09844589639734683 | validation: 0.05785558192469523]
	TIME [epoch: 6.12 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07017737162638883		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.07017737162638883 | validation: 0.5302963885680875]
	TIME [epoch: 6.12 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4264273579662619		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.4264273579662619 | validation: 0.5825086041538385]
	TIME [epoch: 6.12 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4505659669586163		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.4505659669586163 | validation: 0.26809820816727864]
	TIME [epoch: 6.12 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1907709344289814		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.1907709344289814 | validation: 0.12856464556064484]
	TIME [epoch: 190 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13672481047711696		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.13672481047711696 | validation: 0.08260348994641369]
	TIME [epoch: 13 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09384110726339503		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.09384110726339503 | validation: 0.09920693599879589]
	TIME [epoch: 13 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1007521702262843		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.1007521702262843 | validation: 0.1493778991397697]
	TIME [epoch: 13 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11871926414825441		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.11871926414825441 | validation: 0.13605717248505242]
	TIME [epoch: 13 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09994498676955271		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.09994498676955271 | validation: 0.06092904460706429]
	TIME [epoch: 13 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07705569269395163		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.07705569269395163 | validation: 0.05414901350862649]
	TIME [epoch: 13 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0765677822760106		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.0765677822760106 | validation: 0.08660694244450445]
	TIME [epoch: 13 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07543590764255248		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.07543590764255248 | validation: 0.07905905304029617]
	TIME [epoch: 13 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07030482708789731		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.07030482708789731 | validation: 0.05586127140665971]
	TIME [epoch: 13 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07855673447555905		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.07855673447555905 | validation: 0.07875077264883852]
	TIME [epoch: 13 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07062799435769058		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.07062799435769058 | validation: 0.09229173162437157]
	TIME [epoch: 13 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07575569831001878		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.07575569831001878 | validation: 0.06288308393627559]
	TIME [epoch: 13 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08150525627749296		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.08150525627749296 | validation: 0.08724116460471812]
	TIME [epoch: 13 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06968795819632037		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.06968795819632037 | validation: 0.05235836643127663]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_515.pth
	Model improved!!!
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525974684220028		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.06525974684220028 | validation: 0.06020593294251362]
	TIME [epoch: 13 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06067477920985727		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.06067477920985727 | validation: 0.18689668680451868]
	TIME [epoch: 13 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10922635589985781		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.10922635589985781 | validation: 0.05940533852414331]
	TIME [epoch: 13 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06735824297915295		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.06735824297915295 | validation: 0.0560453945652063]
	TIME [epoch: 13 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07515167195857018		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.07515167195857018 | validation: 0.060337186657715064]
	TIME [epoch: 13 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06424539105673403		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.06424539105673403 | validation: 0.1019880473079588]
	TIME [epoch: 13 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0770855846768184		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.0770855846768184 | validation: 0.10536528958677696]
	TIME [epoch: 13 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11132683218247387		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.11132683218247387 | validation: 0.066129182196458]
	TIME [epoch: 13 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08002459571964972		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.08002459571964972 | validation: 0.08596024290606495]
	TIME [epoch: 13 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06728617894188087		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.06728617894188087 | validation: 0.0617288567631955]
	TIME [epoch: 13 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475459729623352		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.06475459729623352 | validation: 0.0760984786718326]
	TIME [epoch: 13 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06624250763817979		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.06624250763817979 | validation: 0.07470056374523751]
	TIME [epoch: 13 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06227267320205293		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.06227267320205293 | validation: 0.05205513205617063]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_528.pth
	Model improved!!!
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06048664841681415		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.06048664841681415 | validation: 0.086803861516411]
	TIME [epoch: 13 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644425666554723		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.0644425666554723 | validation: 0.06514463591848617]
	TIME [epoch: 13 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08755832440634813		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.08755832440634813 | validation: 0.11680628136209491]
	TIME [epoch: 13 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0765736147733634		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.0765736147733634 | validation: 0.0663996784783628]
	TIME [epoch: 13 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06789103735773525		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.06789103735773525 | validation: 0.085391276924619]
	TIME [epoch: 13 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06591537588549688		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.06591537588549688 | validation: 0.06441508002935419]
	TIME [epoch: 13 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0646980904517597		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.0646980904517597 | validation: 0.13028201002823958]
	TIME [epoch: 13 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15637651779526995		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.15637651779526995 | validation: 0.1381635114141686]
	TIME [epoch: 13 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08138374069433507		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.08138374069433507 | validation: 0.12985848010111423]
	TIME [epoch: 13 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0926588707047322		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.0926588707047322 | validation: 0.08478914545314731]
	TIME [epoch: 13 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0840644147688349		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.0840644147688349 | validation: 0.07619196124302201]
	TIME [epoch: 13 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08253956676211967		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.08253956676211967 | validation: 0.06639629710253907]
	TIME [epoch: 13 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06385921941723818		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.06385921941723818 | validation: 0.054313035734895536]
	TIME [epoch: 13 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061607477108410524		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.061607477108410524 | validation: 0.0838933790430832]
	TIME [epoch: 13 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06818755653718443		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.06818755653718443 | validation: 0.05038588456500113]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_543.pth
	Model improved!!!
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06458663647013346		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.06458663647013346 | validation: 0.09419935937445162]
	TIME [epoch: 13 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06505300136049327		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.06505300136049327 | validation: 0.05085800517071462]
	TIME [epoch: 13 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05382549247282352		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.05382549247282352 | validation: 0.04655612119076833]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_546.pth
	Model improved!!!
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05579376076684353		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.05579376076684353 | validation: 0.13195110260912166]
	TIME [epoch: 13 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06938478690410561		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.06938478690410561 | validation: 0.048318783566569636]
	TIME [epoch: 13 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06862146912127468		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.06862146912127468 | validation: 0.1009206324983678]
	TIME [epoch: 13 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.069906077297931		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.069906077297931 | validation: 0.07068533496200528]
	TIME [epoch: 13 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06740832755064814		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.06740832755064814 | validation: 0.07311359702400157]
	TIME [epoch: 13 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06074065298576369		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.06074065298576369 | validation: 0.04717372326781508]
	TIME [epoch: 13 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06103680429504577		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.06103680429504577 | validation: 0.08984269319850413]
	TIME [epoch: 13 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06743895130260903		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.06743895130260903 | validation: 0.07159630990821579]
	TIME [epoch: 13 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08509618617206577		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.08509618617206577 | validation: 0.09881813229016218]
	TIME [epoch: 13 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06458625342524414		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.06458625342524414 | validation: 0.051521180062072874]
	TIME [epoch: 13 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05878492961427794		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.05878492961427794 | validation: 0.059060258021241]
	TIME [epoch: 13 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06420495893876428		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.06420495893876428 | validation: 0.10633960784561779]
	TIME [epoch: 13 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07306095028548153		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.07306095028548153 | validation: 0.16310471791260836]
	TIME [epoch: 13 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19082732660932963		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.19082732660932963 | validation: 0.06046879938947776]
	TIME [epoch: 13 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435658656543921		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.07435658656543921 | validation: 0.11083221206088718]
	TIME [epoch: 13 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07301426364817579		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.07301426364817579 | validation: 0.05740240201082656]
	TIME [epoch: 13 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056258553308943395		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.056258553308943395 | validation: 0.04847972606122716]
	TIME [epoch: 13 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446722681985289		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.05446722681985289 | validation: 0.050610151992221845]
	TIME [epoch: 13 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05153877236875173		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.05153877236875173 | validation: 0.0627826166166485]
	TIME [epoch: 13 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563469454124264		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.0563469454124264 | validation: 0.050023445720108285]
	TIME [epoch: 13 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05981244914504799		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.05981244914504799 | validation: 0.06544336264454767]
	TIME [epoch: 13 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05761940450048263		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.05761940450048263 | validation: 0.057610204985815774]
	TIME [epoch: 13 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06355386209776612		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.06355386209776612 | validation: 0.06848254778643315]
	TIME [epoch: 13 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07004055491517294		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.07004055491517294 | validation: 0.060762140199547246]
	TIME [epoch: 13 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060472233736278865		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.060472233736278865 | validation: 0.04379555117967468]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_571.pth
	Model improved!!!
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05585115018173869		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.05585115018173869 | validation: 0.06635943629802196]
	TIME [epoch: 13 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05324477627510528		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.05324477627510528 | validation: 0.052696378132731495]
	TIME [epoch: 13 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05310073761064409		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.05310073761064409 | validation: 0.0799035084650076]
	TIME [epoch: 13 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0644912691054196		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.0644912691054196 | validation: 0.06759657892650593]
	TIME [epoch: 13 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07171387883760633		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.07171387883760633 | validation: 0.060868997917542814]
	TIME [epoch: 13 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07060613633856601		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.07060613633856601 | validation: 0.23159929300539375]
	TIME [epoch: 13 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10998627751854738		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.10998627751854738 | validation: 0.051874612049863704]
	TIME [epoch: 13 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05704560722193325		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.05704560722193325 | validation: 0.4839274117640576]
	TIME [epoch: 13 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2875626245992131		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.2875626245992131 | validation: 0.4207913836481574]
	TIME [epoch: 13 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1968641366303239		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.1968641366303239 | validation: 0.07858104912727955]
	TIME [epoch: 13 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0610976865631341		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.0610976865631341 | validation: 0.04702931642067997]
	TIME [epoch: 13 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08346654445287936		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.08346654445287936 | validation: 0.0439390146015973]
	TIME [epoch: 13 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06727582182458469		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.06727582182458469 | validation: 0.06721565186099086]
	TIME [epoch: 12.9 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05731884559364334		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.05731884559364334 | validation: 0.06145877789661811]
	TIME [epoch: 13 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06057272060835127		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.06057272060835127 | validation: 0.04690031820070268]
	TIME [epoch: 13 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05106094251411624		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.05106094251411624 | validation: 0.05763224503218714]
	TIME [epoch: 13 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056910629820176944		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.056910629820176944 | validation: 0.07329093740153254]
	TIME [epoch: 13 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05354540574154072		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.05354540574154072 | validation: 0.24585299260319632]
	TIME [epoch: 13 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17372345683941945		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.17372345683941945 | validation: 0.18956158520511718]
	TIME [epoch: 13 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10451181310034471		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.10451181310034471 | validation: 0.080256230633618]
	TIME [epoch: 13 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06799647720674423		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.06799647720674423 | validation: 0.05438519276503189]
	TIME [epoch: 12.9 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0818331909652951		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.0818331909652951 | validation: 0.039627956951706594]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_593.pth
	Model improved!!!
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06327027242348915		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.06327027242348915 | validation: 0.05218055200476382]
	TIME [epoch: 12.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054903403573463044		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.054903403573463044 | validation: 0.07368621138881795]
	TIME [epoch: 12.9 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05369150364561165		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.05369150364561165 | validation: 0.05264666665830536]
	TIME [epoch: 12.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05073623479774559		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.05073623479774559 | validation: 0.04370415442083628]
	TIME [epoch: 12.9 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04975855692450839		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.04975855692450839 | validation: 0.058047533092246445]
	TIME [epoch: 12.9 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05480765102044348		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.05480765102044348 | validation: 0.054842100560992306]
	TIME [epoch: 12.9 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0522778610465158		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.0522778610465158 | validation: 0.0465874795359718]
	TIME [epoch: 12.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05212989413863058		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.05212989413863058 | validation: 0.08056456920163063]
	TIME [epoch: 13 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06768968096310374		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.06768968096310374 | validation: 0.07675929144937162]
	TIME [epoch: 13 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05437614059134044		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.05437614059134044 | validation: 0.046937591955405526]
	TIME [epoch: 13 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052296865798033276		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.052296865798033276 | validation: 0.28940522396152185]
	TIME [epoch: 13 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15341161497321945		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.15341161497321945 | validation: 0.11591614777009261]
	TIME [epoch: 13 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569790457999293		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.07569790457999293 | validation: 0.047057420932929576]
	TIME [epoch: 13 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07841826985389454		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.07841826985389454 | validation: 0.06697692706296816]
	TIME [epoch: 13 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05914201998714708		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.05914201998714708 | validation: 0.08383330389666745]
	TIME [epoch: 13 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05851721529169014		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.05851721529169014 | validation: 0.06967463329807452]
	TIME [epoch: 13 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05635524462908599		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.05635524462908599 | validation: 0.04826262575864302]
	TIME [epoch: 13 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05211598317515577		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.05211598317515577 | validation: 0.05525809083825367]
	TIME [epoch: 13 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05374876587718694		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.05374876587718694 | validation: 0.09920825112158506]
	TIME [epoch: 13 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08318463235658385		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.08318463235658385 | validation: 0.0819506004397232]
	TIME [epoch: 13 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06034164567190476		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.06034164567190476 | validation: 0.045026379893267944]
	TIME [epoch: 13 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06346657083807145		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.06346657083807145 | validation: 0.06320005025158469]
	TIME [epoch: 13 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054229854244274234		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.054229854244274234 | validation: 0.10187815979363757]
	TIME [epoch: 13 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057346168783289304		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.057346168783289304 | validation: 0.037664190819125344]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_617.pth
	Model improved!!!
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05307807828285571		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.05307807828285571 | validation: 0.04173947872565367]
	TIME [epoch: 12.9 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049244759301200834		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.049244759301200834 | validation: 0.08994517236781069]
	TIME [epoch: 12.9 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05561592360379133		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.05561592360379133 | validation: 0.06691894272787599]
	TIME [epoch: 12.9 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055300072336439236		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.055300072336439236 | validation: 0.060251975817609243]
	TIME [epoch: 12.9 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757586371921809		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.04757586371921809 | validation: 0.05535351981552472]
	TIME [epoch: 12.9 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04825994677213331		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.04825994677213331 | validation: 0.09253819834483698]
	TIME [epoch: 12.9 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12293750380598362		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.12293750380598362 | validation: 0.06226287874913122]
	TIME [epoch: 12.9 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06244027470098051		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.06244027470098051 | validation: 0.2535453856149503]
	TIME [epoch: 12.9 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11787792623286794		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.11787792623286794 | validation: 0.05694791267894886]
	TIME [epoch: 12.9 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0473499154758257		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.0473499154758257 | validation: 0.056217180324183436]
	TIME [epoch: 12.9 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06662922174834755		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.06662922174834755 | validation: 0.061773730206071624]
	TIME [epoch: 12.9 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058725990293205986		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.058725990293205986 | validation: 0.04868450372975091]
	TIME [epoch: 12.9 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788913564985377		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.04788913564985377 | validation: 0.05102145077538194]
	TIME [epoch: 12.9 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046634293248957945		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.046634293248957945 | validation: 0.04616483425148326]
	TIME [epoch: 12.9 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04558351383541282		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.04558351383541282 | validation: 0.041893510018758344]
	TIME [epoch: 12.9 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04911617922743061		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.04911617922743061 | validation: 0.04761187987348497]
	TIME [epoch: 12.9 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042965525022223304		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.042965525022223304 | validation: 0.04690366866058814]
	TIME [epoch: 12.9 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432274899502132		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.04432274899502132 | validation: 0.4982103301559375]
	TIME [epoch: 12.9 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3219528706846448		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.3219528706846448 | validation: 0.40279420349158745]
	TIME [epoch: 12.9 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23937675590482307		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.23937675590482307 | validation: 0.0642203297104335]
	TIME [epoch: 12.9 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0616196038685659		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.0616196038685659 | validation: 0.10788758275732664]
	TIME [epoch: 12.9 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.091825126935648		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.091825126935648 | validation: 0.059878894818418786]
	TIME [epoch: 12.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08020999356058649		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.08020999356058649 | validation: 0.056217158331842966]
	TIME [epoch: 12.9 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07288107406221082		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.07288107406221082 | validation: 0.09943038863520896]
	TIME [epoch: 12.9 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06935813716036071		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.06935813716036071 | validation: 0.09374386880775026]
	TIME [epoch: 12.9 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06997823221154247		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.06997823221154247 | validation: 0.06963653490492079]
	TIME [epoch: 12.9 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06373120278091961		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.06373120278091961 | validation: 0.06064684823169051]
	TIME [epoch: 12.9 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05219315383182324		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.05219315383182324 | validation: 0.04582056853352763]
	TIME [epoch: 12.9 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047284830282397675		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.047284830282397675 | validation: 0.04210740556824732]
	TIME [epoch: 12.9 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04875414868038624		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.04875414868038624 | validation: 0.04543412221085054]
	TIME [epoch: 12.9 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04785484166211239		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.04785484166211239 | validation: 0.04229689015400595]
	TIME [epoch: 12.9 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046622590788570326		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.046622590788570326 | validation: 0.04639445211649669]
	TIME [epoch: 12.9 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951698764920508		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.04951698764920508 | validation: 0.06109903199741954]
	TIME [epoch: 12.9 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04653585475288617		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.04653585475288617 | validation: 0.048779515836182924]
	TIME [epoch: 12.9 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04452318794594335		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.04452318794594335 | validation: 0.12841737059923358]
	TIME [epoch: 12.9 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06946295267616201		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.06946295267616201 | validation: 0.04871960321080936]
	TIME [epoch: 12.9 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05283465948672769		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.05283465948672769 | validation: 0.05095139997703344]
	TIME [epoch: 12.9 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05314777379438979		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.05314777379438979 | validation: 0.038590320245118714]
	TIME [epoch: 12.9 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0516224108854935		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.0516224108854935 | validation: 0.06340266726444328]
	TIME [epoch: 12.9 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04836483583056368		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.04836483583056368 | validation: 0.060834175327455976]
	TIME [epoch: 12.9 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0484667522189954		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.0484667522189954 | validation: 0.040146880813367786]
	TIME [epoch: 12.9 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509036988485549		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.04509036988485549 | validation: 0.03366846235621046]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_659.pth
	Model improved!!!
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710702399792501		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.04710702399792501 | validation: 0.05839395691551774]
	TIME [epoch: 13 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04373330911893692		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.04373330911893692 | validation: 0.08151770287520854]
	TIME [epoch: 13 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06161584570054416		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.06161584570054416 | validation: 0.09903506235044654]
	TIME [epoch: 13 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08891429790798977		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.08891429790798977 | validation: 0.05251264686941405]
	TIME [epoch: 13 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06705096813303475		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.06705096813303475 | validation: 0.1670308824893059]
	TIME [epoch: 13 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12354147048698577		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.12354147048698577 | validation: 0.06631225076428623]
	TIME [epoch: 12.9 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05336897868303442		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.05336897868303442 | validation: 0.048477317404847065]
	TIME [epoch: 12.9 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0591160963761077		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.0591160963761077 | validation: 0.04293555378001375]
	TIME [epoch: 12.9 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052921565959140014		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.052921565959140014 | validation: 0.05188547653541986]
	TIME [epoch: 13 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04742941568152704		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.04742941568152704 | validation: 0.04891169346551719]
	TIME [epoch: 13 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046028759683647745		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.046028759683647745 | validation: 0.0454983002032823]
	TIME [epoch: 13 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286962944014693		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.04286962944014693 | validation: 0.03995839142792049]
	TIME [epoch: 13 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04266864789533179		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.04266864789533179 | validation: 0.05395545082227996]
	TIME [epoch: 13 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04681985884533986		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.04681985884533986 | validation: 0.04227021522738168]
	TIME [epoch: 13 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04233671398297755		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.04233671398297755 | validation: 0.03863039606853283]
	TIME [epoch: 13 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0442023198141617		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.0442023198141617 | validation: 0.0425450149889137]
	TIME [epoch: 13 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04428487546907199		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.04428487546907199 | validation: 0.050235787118278356]
	TIME [epoch: 13 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04676635033086646		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.04676635033086646 | validation: 0.038650225238710416]
	TIME [epoch: 13 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04706682933838193		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.04706682933838193 | validation: 0.07068189038078868]
	TIME [epoch: 13 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05413885742473406		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.05413885742473406 | validation: 0.06735996911172136]
	TIME [epoch: 13 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08013795786129758		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.08013795786129758 | validation: 0.07302662754987396]
	TIME [epoch: 13 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059241468766459915		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.059241468766459915 | validation: 0.038012697491367424]
	TIME [epoch: 13 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04765925838861314		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.04765925838861314 | validation: 0.04862899422121456]
	TIME [epoch: 13 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047508633622676884		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.047508633622676884 | validation: 0.04780226337166213]
	TIME [epoch: 13 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04307479236185376		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.04307479236185376 | validation: 0.0446681647258541]
	TIME [epoch: 13 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04179007019735315		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.04179007019735315 | validation: 0.03639754213409838]
	TIME [epoch: 13 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04337896249200746		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.04337896249200746 | validation: 0.04196263423518252]
	TIME [epoch: 13 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04251924236031767		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.04251924236031767 | validation: 0.03916606897299044]
	TIME [epoch: 13 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042314154847324266		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.042314154847324266 | validation: 0.054704985124110594]
	TIME [epoch: 13 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044303483398959516		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.044303483398959516 | validation: 0.041375435387578]
	TIME [epoch: 13 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05052185396995165		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.05052185396995165 | validation: 0.0436356282473281]
	TIME [epoch: 13 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042252093018826806		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.042252093018826806 | validation: 0.05541757051231962]
	TIME [epoch: 13 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0446078438991932		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.0446078438991932 | validation: 0.04748065301023713]
	TIME [epoch: 13 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052792732709212656		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.052792732709212656 | validation: 0.0747462679824943]
	TIME [epoch: 13 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06138346962122618		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.06138346962122618 | validation: 0.04535425424594695]
	TIME [epoch: 13 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05104346814655178		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.05104346814655178 | validation: 0.06833735646295283]
	TIME [epoch: 12.9 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04734771604492565		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.04734771604492565 | validation: 0.06709850438349427]
	TIME [epoch: 12.9 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05196394889429795		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.05196394889429795 | validation: 0.05345575486690024]
	TIME [epoch: 12.9 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06257156565609404		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.06257156565609404 | validation: 0.04605998979859297]
	TIME [epoch: 12.9 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04059263159062278		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.04059263159062278 | validation: 0.042707949932623814]
	TIME [epoch: 12.9 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04504664560850415		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.04504664560850415 | validation: 0.035530756762637]
	TIME [epoch: 12.9 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04700368638766264		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.04700368638766264 | validation: 0.080028365992512]
	TIME [epoch: 13 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0500250515824736		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.0500250515824736 | validation: 0.04727042934194986]
	TIME [epoch: 13 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04207690025222112		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.04207690025222112 | validation: 0.04143286223967944]
	TIME [epoch: 13 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05102363180044854		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.05102363180044854 | validation: 0.06245543135269702]
	TIME [epoch: 13 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04248295966420711		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.04248295966420711 | validation: 0.04535672703982205]
	TIME [epoch: 13 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04572140364430619		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.04572140364430619 | validation: 0.0482157285264134]
	TIME [epoch: 12.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04827212664685038		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.04827212664685038 | validation: 0.04169137746315021]
	TIME [epoch: 12.9 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04284713402779761		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.04284713402779761 | validation: 0.04361458978979437]
	TIME [epoch: 12.9 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04358679380583018		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.04358679380583018 | validation: 0.04237408167677764]
	TIME [epoch: 12.9 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04301321607163349		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.04301321607163349 | validation: 0.0564942024745439]
	TIME [epoch: 13 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04449360303723884		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.04449360303723884 | validation: 0.034773649171276046]
	TIME [epoch: 13 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04316818296701975		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.04316818296701975 | validation: 0.08999901358267015]
	TIME [epoch: 13 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757791347900051		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.04757791347900051 | validation: 0.03706207749860779]
	TIME [epoch: 13 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04822947185902624		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.04822947185902624 | validation: 0.03872604423546675]
	TIME [epoch: 13 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04082941836466853		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.04082941836466853 | validation: 0.0634891638951513]
	TIME [epoch: 13 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05376616506496934		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.05376616506496934 | validation: 0.05855276031404712]
	TIME [epoch: 13 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07154320833104956		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.07154320833104956 | validation: 0.061190969479541515]
	TIME [epoch: 13 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050171094681893326		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.050171094681893326 | validation: 0.03578858684466052]
	TIME [epoch: 13 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04624352306081621		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.04624352306081621 | validation: 0.06965838220744502]
	TIME [epoch: 13 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05825382060992253		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.05825382060992253 | validation: 0.053294652110326336]
	TIME [epoch: 13 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06622236722232573		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.06622236722232573 | validation: 0.03490393370289938]
	TIME [epoch: 13 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043764494675211464		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.043764494675211464 | validation: 0.05121450960291645]
	TIME [epoch: 12.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04402529320308838		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.04402529320308838 | validation: 0.04025029484592471]
	TIME [epoch: 12.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03729484263569941		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.03729484263569941 | validation: 0.059400383012192394]
	TIME [epoch: 12.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041072755795607103		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.041072755795607103 | validation: 0.03831286576898523]
	TIME [epoch: 12.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0380564892246223		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.0380564892246223 | validation: 0.04141815492820832]
	TIME [epoch: 13 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038190518154242306		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.038190518154242306 | validation: 0.040164094997296396]
	TIME [epoch: 12.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04462646509999626		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.04462646509999626 | validation: 0.18912274487682068]
	TIME [epoch: 13 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10749417103430556		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.10749417103430556 | validation: 0.08238625352031717]
	TIME [epoch: 13 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0469009280982734		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.0469009280982734 | validation: 0.044488100882611785]
	TIME [epoch: 13 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0540895058947491		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.0540895058947491 | validation: 0.05027595040726658]
	TIME [epoch: 12.9 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04377182679530938		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.04377182679530938 | validation: 0.05450981067923018]
	TIME [epoch: 13 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04245116175489479		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.04245116175489479 | validation: 0.04506067096678055]
	TIME [epoch: 13 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03843638858164718		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.03843638858164718 | validation: 0.034457223379205704]
	TIME [epoch: 12.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03880106508481715		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.03880106508481715 | validation: 0.04464911984857348]
	TIME [epoch: 13 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03775956295943073		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.03775956295943073 | validation: 0.0366484749005325]
	TIME [epoch: 13 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03994542486925376		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.03994542486925376 | validation: 0.0455240849595474]
	TIME [epoch: 13 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04003901125907894		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.04003901125907894 | validation: 0.032626516653794534]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_738.pth
	Model improved!!!
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04345203562151479		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.04345203562151479 | validation: 0.07709752316773695]
	TIME [epoch: 12.9 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04601549139188145		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.04601549139188145 | validation: 0.03745201853692597]
	TIME [epoch: 13 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04721566159494303		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.04721566159494303 | validation: 0.038528041846505705]
	TIME [epoch: 13 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047166465212777044		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.047166465212777044 | validation: 0.049319415839293335]
	TIME [epoch: 13 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03732234790822147		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.03732234790822147 | validation: 0.1446387309292967]
	TIME [epoch: 13 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09308764665240805		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.09308764665240805 | validation: 0.032191968585296264]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_744.pth
	Model improved!!!
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662444167854567		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.03662444167854567 | validation: 0.03450061929127385]
	TIME [epoch: 13 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041590376289706986		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.041590376289706986 | validation: 0.10059946014044635]
	TIME [epoch: 13 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06131655999783445		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.06131655999783445 | validation: 0.038484576883242576]
	TIME [epoch: 12.9 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041588071232976066		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.041588071232976066 | validation: 0.031956050491014264]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_748.pth
	Model improved!!!
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717993562920059		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.03717993562920059 | validation: 0.04142500979868074]
	TIME [epoch: 12.9 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038221461689137225		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.038221461689137225 | validation: 0.09332060733501191]
	TIME [epoch: 13 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631160977836892		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.0631160977836892 | validation: 0.035767174020905304]
	TIME [epoch: 12.9 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038049646856525725		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.038049646856525725 | validation: 0.03151189563365192]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_752.pth
	Model improved!!!
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037554348434751826		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.037554348434751826 | validation: 0.04087127867264637]
	TIME [epoch: 12.9 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0369229588289802		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.0369229588289802 | validation: 0.04037986638450283]
	TIME [epoch: 13 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03780031937408044		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.03780031937408044 | validation: 0.03904272495599627]
	TIME [epoch: 12.9 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0371964197700795		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.0371964197700795 | validation: 0.05139885412948234]
	TIME [epoch: 13 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03848770363699969		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.03848770363699969 | validation: 0.03587420413068998]
	TIME [epoch: 12.9 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04148107137124184		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.04148107137124184 | validation: 0.07618240692894322]
	TIME [epoch: 13 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05460830915328554		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.05460830915328554 | validation: 0.04198145247795912]
	TIME [epoch: 12.9 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05662489666114467		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.05662489666114467 | validation: 0.04349471893119558]
	TIME [epoch: 12.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042331211056393876		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.042331211056393876 | validation: 0.040537582542860634]
	TIME [epoch: 12.9 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0357624383013296		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.0357624383013296 | validation: 0.03620851711948323]
	TIME [epoch: 13 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036474256884334595		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.036474256884334595 | validation: 0.04242855881278171]
	TIME [epoch: 12.9 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0414040981922565		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.0414040981922565 | validation: 0.0582506198521793]
	TIME [epoch: 12.9 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307311855111511		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.06307311855111511 | validation: 0.041892790541728724]
	TIME [epoch: 13 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03609409752381914		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.03609409752381914 | validation: 0.057638565351236626]
	TIME [epoch: 13 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0430858345438711		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.0430858345438711 | validation: 0.036784860149482894]
	TIME [epoch: 12.9 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693429856935159		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.03693429856935159 | validation: 0.031413579938434036]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_768.pth
	Model improved!!!
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03666221876511051		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.03666221876511051 | validation: 0.04654793953312743]
	TIME [epoch: 13 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036725308996411736		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.036725308996411736 | validation: 0.03319244028899412]
	TIME [epoch: 13 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038666330121727696		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.038666330121727696 | validation: 0.04596247615912627]
	TIME [epoch: 13 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03626241094427012		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.03626241094427012 | validation: 0.03573038692197702]
	TIME [epoch: 13 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036791089360709635		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.036791089360709635 | validation: 0.04253201029537891]
	TIME [epoch: 13 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038981306361237165		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.038981306361237165 | validation: 0.03258486336732558]
	TIME [epoch: 13 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03738681245072452		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.03738681245072452 | validation: 0.04636582488526364]
	TIME [epoch: 13 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037742965845752485		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.037742965845752485 | validation: 0.03240567594978979]
	TIME [epoch: 13 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040170162544960863		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.040170162544960863 | validation: 0.04007877122751762]
	TIME [epoch: 13 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040072136909333686		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.040072136909333686 | validation: 0.04058738432273176]
	TIME [epoch: 13 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05052936855715711		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.05052936855715711 | validation: 0.054401904789981484]
	TIME [epoch: 13 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04192658410775508		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.04192658410775508 | validation: 0.03488596836971874]
	TIME [epoch: 13 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614963127871494		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.03614963127871494 | validation: 0.04720006525236943]
	TIME [epoch: 13 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700645796378325		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.03700645796378325 | validation: 0.035129629065987725]
	TIME [epoch: 13 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03595530761681822		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.03595530761681822 | validation: 0.02854717130872595]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_783.pth
	Model improved!!!
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039248626941255624		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.039248626941255624 | validation: 0.08649984500777569]
	TIME [epoch: 13 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05065290439316447		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.05065290439316447 | validation: 0.03634985456789515]
	TIME [epoch: 13 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037021011606046365		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.037021011606046365 | validation: 0.035975009860125716]
	TIME [epoch: 12.9 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0443872143688663		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.0443872143688663 | validation: 0.07497294248383628]
	TIME [epoch: 12.9 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0462283104672858		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.0462283104672858 | validation: 0.03400335246879371]
	TIME [epoch: 12.9 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03570470809401775		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.03570470809401775 | validation: 0.033492704539472284]
	TIME [epoch: 12.9 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03532144346998101		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.03532144346998101 | validation: 0.0388729403821529]
	TIME [epoch: 12.9 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03468357642957897		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.03468357642957897 | validation: 0.03150734259077133]
	TIME [epoch: 12.9 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034064856090553965		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.034064856090553965 | validation: 0.03304255207626492]
	TIME [epoch: 12.9 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03464692462698278		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.03464692462698278 | validation: 0.07812723783542382]
	TIME [epoch: 13 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0439228520286644		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.0439228520286644 | validation: 0.038051402465375916]
	TIME [epoch: 13 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04437891788782996		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.04437891788782996 | validation: 0.21757202615294952]
	TIME [epoch: 13 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15267277880277122		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.15267277880277122 | validation: 0.1424521266556248]
	TIME [epoch: 13 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08583411137860943		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.08583411137860943 | validation: 0.05339151656249296]
	TIME [epoch: 13 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084722093745596		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.04084722093745596 | validation: 0.03455626634013244]
	TIME [epoch: 13 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04995236883597107		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.04995236883597107 | validation: 0.03789639019936628]
	TIME [epoch: 13 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04739992287403287		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.04739992287403287 | validation: 0.04990811426329274]
	TIME [epoch: 13 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038727948619678536		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.038727948619678536 | validation: 0.04436287601820993]
	TIME [epoch: 13 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03790907348577654		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.03790907348577654 | validation: 0.10721748128881048]
	TIME [epoch: 13 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06630419546231156		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.06630419546231156 | validation: 0.07873798064690514]
	TIME [epoch: 12.9 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05701542438747075		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.05701542438747075 | validation: 0.11088194945032961]
	TIME [epoch: 13 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12488855409628855		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.12488855409628855 | validation: 0.08415452845752279]
	TIME [epoch: 13 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10476514814989368		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.10476514814989368 | validation: 0.028272893057671514]
	TIME [epoch: 13 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_806.pth
	Model improved!!!
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03684329558474798		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.03684329558474798 | validation: 0.08220092016313513]
	TIME [epoch: 12.9 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051974323238063		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.051974323238063 | validation: 0.08238661237042416]
	TIME [epoch: 12.9 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057971464456532174		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.057971464456532174 | validation: 0.043673490946993625]
	TIME [epoch: 12.9 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04774506679883917		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.04774506679883917 | validation: 0.07989869341870055]
	TIME [epoch: 12.9 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07392503556100419		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.07392503556100419 | validation: 0.058517547137326036]
	TIME [epoch: 12.9 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06408897897940503		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.06408897897940503 | validation: 0.04262693329621447]
	TIME [epoch: 12.9 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05301828712083696		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.05301828712083696 | validation: 0.029901072754355697]
	TIME [epoch: 12.9 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03680004697396272		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.03680004697396272 | validation: 0.03750461320085933]
	TIME [epoch: 13.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037250712649833415		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.037250712649833415 | validation: 0.08924551446270099]
	TIME [epoch: 13 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048734572242454394		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.048734572242454394 | validation: 0.07539911982711761]
	TIME [epoch: 12.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04441429055940029		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.04441429055940029 | validation: 0.03161791282656872]
	TIME [epoch: 12.9 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0375027223696525		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.0375027223696525 | validation: 0.0504508863583985]
	TIME [epoch: 12.9 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047636410347043884		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.047636410347043884 | validation: 0.03218122139049997]
	TIME [epoch: 12.9 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0380155941962174		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.0380155941962174 | validation: 0.035118646914907625]
	TIME [epoch: 12.9 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03490624651587463		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.03490624651587463 | validation: 0.036324768908709726]
	TIME [epoch: 12.9 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03927481083607587		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.03927481083607587 | validation: 0.043918864030673146]
	TIME [epoch: 12.9 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03928924706222934		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.03928924706222934 | validation: 0.035387335421645474]
	TIME [epoch: 12.9 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0361238414487348		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.0361238414487348 | validation: 0.034083805461283034]
	TIME [epoch: 12.9 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036702249944265714		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.036702249944265714 | validation: 0.034050539072341804]
	TIME [epoch: 12.9 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036207758742502774		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.036207758742502774 | validation: 0.035498283656059226]
	TIME [epoch: 12.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327602460437111		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.0327602460437111 | validation: 0.035313226363451626]
	TIME [epoch: 12.9 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230954331064043		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.03230954331064043 | validation: 0.03530642028473448]
	TIME [epoch: 12.9 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034599948911262474		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.034599948911262474 | validation: 0.036281912870822455]
	TIME [epoch: 12.9 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03491152627265281		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.03491152627265281 | validation: 0.03328979401335789]
	TIME [epoch: 12.9 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034925027108913105		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.034925027108913105 | validation: 0.029670394071909037]
	TIME [epoch: 12.9 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03237450991894056		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.03237450991894056 | validation: 0.034962198618836905]
	TIME [epoch: 12.9 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03367734056722403		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.03367734056722403 | validation: 0.03174865976834963]
	TIME [epoch: 12.9 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033904380366030586		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.033904380366030586 | validation: 0.0406913943657652]
	TIME [epoch: 12.9 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04556981099064229		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.04556981099064229 | validation: 0.03218736127943239]
	TIME [epoch: 12.9 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03581493836897164		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.03581493836897164 | validation: 0.05164699321048366]
	TIME [epoch: 12.9 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036945357464862626		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.036945357464862626 | validation: 0.059074523954177574]
	TIME [epoch: 12.9 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038172035348171054		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.038172035348171054 | validation: 0.030076515730175626]
	TIME [epoch: 12.9 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035136007260833886		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.035136007260833886 | validation: 0.036113748480825346]
	TIME [epoch: 12.9 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04249433896105707		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.04249433896105707 | validation: 0.030218054317276734]
	TIME [epoch: 12.9 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033471979681957434		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.033471979681957434 | validation: 0.041675010923239365]
	TIME [epoch: 12.9 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037854811168799574		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.037854811168799574 | validation: 0.05191033392371082]
	TIME [epoch: 12.9 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059279458449137436		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.059279458449137436 | validation: 0.02764444631606422]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_843.pth
	Model improved!!!
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04591755039309555		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.04591755039309555 | validation: 0.04090470414173886]
	TIME [epoch: 12.9 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040577689593347815		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.040577689593347815 | validation: 0.05578045808260489]
	TIME [epoch: 12.9 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04278729342685014		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.04278729342685014 | validation: 0.03228292079511171]
	TIME [epoch: 12.9 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03433038864527339		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.03433038864527339 | validation: 0.028409998167967354]
	TIME [epoch: 12.9 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03285451553032417		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.03285451553032417 | validation: 0.03106584675271882]
	TIME [epoch: 12.9 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03256157489096939		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.03256157489096939 | validation: 0.046138506472653044]
	TIME [epoch: 12.9 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03600998129204825		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.03600998129204825 | validation: 0.03655633425243722]
	TIME [epoch: 12.9 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03484080450411585		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.03484080450411585 | validation: 0.033688543704056995]
	TIME [epoch: 12.9 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035061020265258556		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.035061020265258556 | validation: 0.03295256863571313]
	TIME [epoch: 12.9 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03289446777317693		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.03289446777317693 | validation: 0.046522242621495014]
	TIME [epoch: 12.9 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0350001258665906		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.0350001258665906 | validation: 0.028508109051837628]
	TIME [epoch: 12.9 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03540677781133434		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.03540677781133434 | validation: 0.03414269342142854]
	TIME [epoch: 12.9 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035115848316613095		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.035115848316613095 | validation: 0.06244592483761288]
	TIME [epoch: 12.9 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046121935856303205		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.046121935856303205 | validation: 0.03518557264538163]
	TIME [epoch: 12.9 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03396265568145475		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.03396265568145475 | validation: 0.028258062271947784]
	TIME [epoch: 12.9 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034209517552285684		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.034209517552285684 | validation: 0.029307501702402396]
	TIME [epoch: 12.9 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03186880095746912		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.03186880095746912 | validation: 0.032213744717081626]
	TIME [epoch: 12.9 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033936417966527264		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.033936417966527264 | validation: 0.032951962396226976]
	TIME [epoch: 12.9 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032070831332012474		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.032070831332012474 | validation: 0.034837428471256926]
	TIME [epoch: 12.9 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028543977697044483		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.028543977697044483 | validation: 0.04036856917688049]
	TIME [epoch: 12.9 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033800321170846484		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.033800321170846484 | validation: 0.03762107489172757]
	TIME [epoch: 12.9 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03694666871445422		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.03694666871445422 | validation: 0.031242686619996053]
	TIME [epoch: 12.9 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036476370447101454		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.036476370447101454 | validation: 0.03589907118255481]
	TIME [epoch: 12.9 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03503947639428587		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.03503947639428587 | validation: 0.04944461137989814]
	TIME [epoch: 12.9 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04293810572642226		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.04293810572642226 | validation: 0.03885638577560661]
	TIME [epoch: 12.9 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03176495656537538		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.03176495656537538 | validation: 0.034466274101954165]
	TIME [epoch: 12.9 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0340503674832257		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.0340503674832257 | validation: 0.03852027215762856]
	TIME [epoch: 12.9 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03335442345344699		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.03335442345344699 | validation: 0.03233606909417763]
	TIME [epoch: 12.9 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03364024418725407		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.03364024418725407 | validation: 0.030328717951196984]
	TIME [epoch: 12.9 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03703542048330022		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.03703542048330022 | validation: 0.03383319402603322]
	TIME [epoch: 12.9 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033421256389710406		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.033421256389710406 | validation: 0.028749997755033153]
	TIME [epoch: 12.9 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03400355558023319		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.03400355558023319 | validation: 0.047317316467567566]
	TIME [epoch: 12.9 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03450968499266921		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.03450968499266921 | validation: 0.029737619763028113]
	TIME [epoch: 12.9 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039213512313830044		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.039213512313830044 | validation: 0.04425424675332865]
	TIME [epoch: 12.9 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032570238839608204		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.032570238839608204 | validation: 0.03594506955271002]
	TIME [epoch: 12.9 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03203367805047942		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.03203367805047942 | validation: 0.029594885164434283]
	TIME [epoch: 12.9 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03242388782581144		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.03242388782581144 | validation: 0.11586372543099362]
	TIME [epoch: 12.9 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.080349852079105		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.080349852079105 | validation: 0.03457688321660666]
	TIME [epoch: 12.9 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03669316171482643		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.03669316171482643 | validation: 0.04879745287133539]
	TIME [epoch: 12.9 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05470593534487223		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.05470593534487223 | validation: 0.04167655844698161]
	TIME [epoch: 12.9 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05333430542325737		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.05333430542325737 | validation: 0.029690909519138733]
	TIME [epoch: 12.9 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0333048539849228		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.0333048539849228 | validation: 0.04135120423960862]
	TIME [epoch: 12.9 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034523844142439176		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.034523844142439176 | validation: 0.04040698176076091]
	TIME [epoch: 12.9 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033937669172117255		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.033937669172117255 | validation: 0.03884364995340263]
	TIME [epoch: 12.9 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03206538006563064		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.03206538006563064 | validation: 0.19765609939378603]
	TIME [epoch: 12.9 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11704631390181248		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.11704631390181248 | validation: 0.2200417600369871]
	TIME [epoch: 12.9 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375380698957482		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.1375380698957482 | validation: 0.16237765792308123]
	TIME [epoch: 12.9 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784258568777032		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.0784258568777032 | validation: 0.06410100241728205]
	TIME [epoch: 12.9 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0430352513966906		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.0430352513966906 | validation: 0.03152975776086649]
	TIME [epoch: 12.9 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035923497332866886		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.035923497332866886 | validation: 0.028806921638778116]
	TIME [epoch: 12.9 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03589035995309978		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.03589035995309978 | validation: 0.03233532802067558]
	TIME [epoch: 12.9 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03586901060208654		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.03586901060208654 | validation: 0.03339832273392986]
	TIME [epoch: 12.9 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353686384442768		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.03353686384442768 | validation: 0.04092287424753336]
	TIME [epoch: 12.9 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05118208000256543		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.05118208000256543 | validation: 0.04787804314419595]
	TIME [epoch: 12.9 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051109746082080604		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.051109746082080604 | validation: 0.02878744797075743]
	TIME [epoch: 12.9 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03535324785422025		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.03535324785422025 | validation: 0.037561339928309005]
	TIME [epoch: 12.9 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03193128410573382		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.03193128410573382 | validation: 0.051555567981480394]
	TIME [epoch: 12.9 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034315201031061324		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.034315201031061324 | validation: 0.02602231689627621]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_901.pth
	Model improved!!!
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03147722864866453		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.03147722864866453 | validation: 0.025116034354138894]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_902.pth
	Model improved!!!
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031359476112017365		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.031359476112017365 | validation: 0.02737946302917469]
	TIME [epoch: 12.9 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027903320419783148		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.027903320419783148 | validation: 0.03043498343607979]
	TIME [epoch: 12.9 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0308555755353367		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.0308555755353367 | validation: 0.02852948439354406]
	TIME [epoch: 12.9 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032003906690396954		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.032003906690396954 | validation: 0.030575505496330535]
	TIME [epoch: 12.9 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03150851741920255		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.03150851741920255 | validation: 0.03762567694345884]
	TIME [epoch: 12.9 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03291031627746245		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.03291031627746245 | validation: 0.029347584030549302]
	TIME [epoch: 12.9 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03120894100735888		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.03120894100735888 | validation: 0.03469623242289548]
	TIME [epoch: 12.9 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030668281371406512		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.030668281371406512 | validation: 0.03327593727952517]
	TIME [epoch: 12.9 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02986684994932439		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.02986684994932439 | validation: 0.030216487284566707]
	TIME [epoch: 12.9 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03066211573242053		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.03066211573242053 | validation: 0.03251512882409602]
	TIME [epoch: 12.9 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03084222226474588		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.03084222226474588 | validation: 0.06324448400988235]
	TIME [epoch: 12.9 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0659671162019716		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.0659671162019716 | validation: 0.03316198913881483]
	TIME [epoch: 12.9 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820024869854579		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.03820024869854579 | validation: 0.04149000736062578]
	TIME [epoch: 12.9 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031835291252799276		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.031835291252799276 | validation: 0.04484419344227164]
	TIME [epoch: 12.9 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03397625228713357		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.03397625228713357 | validation: 0.030305172650486125]
	TIME [epoch: 12.9 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03030747038522999		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.03030747038522999 | validation: 0.026384394292436976]
	TIME [epoch: 12.9 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029052975958922034		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.029052975958922034 | validation: 0.030618005647748782]
	TIME [epoch: 12.9 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03268305456158621		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.03268305456158621 | validation: 0.04871620746884843]
	TIME [epoch: 12.9 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037843295119222846		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.037843295119222846 | validation: 0.043105911978976966]
	TIME [epoch: 12.9 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036110399862717635		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.036110399862717635 | validation: 0.028588375477033325]
	TIME [epoch: 12.9 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03441823521161918		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.03441823521161918 | validation: 0.029368158201819497]
	TIME [epoch: 12.9 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03307758555775778		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.03307758555775778 | validation: 0.028174846051773485]
	TIME [epoch: 12.9 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03127069688457865		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.03127069688457865 | validation: 0.035046264281158]
	TIME [epoch: 12.9 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030886382414461473		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.030886382414461473 | validation: 0.03353742550926543]
	TIME [epoch: 12.9 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030224516007919835		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.030224516007919835 | validation: 0.02949896885923549]
	TIME [epoch: 12.9 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031213748761686384		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.031213748761686384 | validation: 0.03017504035865126]
	TIME [epoch: 12.9 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028413126801250282		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.028413126801250282 | validation: 0.031195699500024654]
	TIME [epoch: 12.9 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028269099144942134		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.028269099144942134 | validation: 0.03392710043072892]
	TIME [epoch: 12.9 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031703669633887585		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.031703669633887585 | validation: 0.03181970188223491]
	TIME [epoch: 12.9 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031193255862969737		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.031193255862969737 | validation: 0.035957925591789355]
	TIME [epoch: 12.9 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030161167429010406		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.030161167429010406 | validation: 0.039434975217726875]
	TIME [epoch: 12.9 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03349407263733367		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.03349407263733367 | validation: 0.028263627105586477]
	TIME [epoch: 12.9 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03022544130112457		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.03022544130112457 | validation: 0.026850787634145903]
	TIME [epoch: 12.9 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030401435293038032		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.030401435293038032 | validation: 0.033276081624072154]
	TIME [epoch: 12.9 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03094577542980842		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.03094577542980842 | validation: 0.028882877115477934]
	TIME [epoch: 12.9 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028528987235759723		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.028528987235759723 | validation: 0.026598366834112853]
	TIME [epoch: 12.9 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0294197949938623		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.0294197949938623 | validation: 0.043286472744336835]
	TIME [epoch: 12.9 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032539605991171616		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.032539605991171616 | validation: 0.026430289307180523]
	TIME [epoch: 12.9 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031691239297530385		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.031691239297530385 | validation: 0.03716639490254964]
	TIME [epoch: 12.9 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03732936042228587		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.03732936042228587 | validation: 0.04028460470317466]
	TIME [epoch: 12.9 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03386544961048672		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.03386544961048672 | validation: 0.03478119041852281]
	TIME [epoch: 12.9 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03137978981513124		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.03137978981513124 | validation: 0.027795790182098146]
	TIME [epoch: 12.9 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03119920700699801		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.03119920700699801 | validation: 0.05440678401405145]
	TIME [epoch: 12.9 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03749956749907949		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.03749956749907949 | validation: 0.033408414258682]
	TIME [epoch: 12.9 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029372152973433766		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.029372152973433766 | validation: 0.026349123990673896]
	TIME [epoch: 12.9 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03131583395923356		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.03131583395923356 | validation: 0.029570065730257247]
	TIME [epoch: 12.9 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151787626068463		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.03151787626068463 | validation: 0.041894020772788504]
	TIME [epoch: 12.9 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032990737561113655		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.032990737561113655 | validation: 0.029498979342865373]
	TIME [epoch: 12.9 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03159122030817333		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.03159122030817333 | validation: 0.18974714189475905]
	TIME [epoch: 12.9 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15620386447543388		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.15620386447543388 | validation: 0.19254149479557825]
	TIME [epoch: 12.9 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13187139522403485		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.13187139522403485 | validation: 0.145580270225205]
	TIME [epoch: 12.9 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07301553423215204		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.07301553423215204 | validation: 0.04421180091305826]
	TIME [epoch: 12.9 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857841190451301		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.03857841190451301 | validation: 0.038051509668137355]
	TIME [epoch: 12.9 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033473659392667256		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.033473659392667256 | validation: 0.029864053184984654]
	TIME [epoch: 12.9 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03703785104992147		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.03703785104992147 | validation: 0.0352002438863462]
	TIME [epoch: 12.9 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03538984714050442		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.03538984714050442 | validation: 0.036754730760815105]
	TIME [epoch: 12.9 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03230238129514269		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.03230238129514269 | validation: 0.03940981660817597]
	TIME [epoch: 12.9 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463598920271273		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.03463598920271273 | validation: 0.041879206052121844]
	TIME [epoch: 12.9 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034111725546380534		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.034111725546380534 | validation: 0.041203281066504095]
	TIME [epoch: 12.9 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034786399133053696		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.034786399133053696 | validation: 0.04046652382823354]
	TIME [epoch: 12.9 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03440493061930622		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.03440493061930622 | validation: 0.04101731460088547]
	TIME [epoch: 12.9 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034468677248157165		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.034468677248157165 | validation: 0.029393628282027753]
	TIME [epoch: 12.9 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034432996312735276		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.034432996312735276 | validation: 0.02746454396665692]
	TIME [epoch: 12.9 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030341062676894247		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.030341062676894247 | validation: 0.032156094386408395]
	TIME [epoch: 12.9 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03253616351039156		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.03253616351039156 | validation: 0.03337106082822379]
	TIME [epoch: 12.9 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030541917108801463		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.030541917108801463 | validation: 0.02906725602848298]
	TIME [epoch: 12.9 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03180580111673915		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.03180580111673915 | validation: 0.03433025757743504]
	TIME [epoch: 12.9 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030920070146293003		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.030920070146293003 | validation: 0.0320626530566829]
	TIME [epoch: 12.9 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03123780095312971		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.03123780095312971 | validation: 0.030810073488880242]
	TIME [epoch: 12.9 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03076884950948534		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.03076884950948534 | validation: 0.03110185174947784]
	TIME [epoch: 12.9 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03142781961032742		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.03142781961032742 | validation: 0.02847711721830709]
	TIME [epoch: 12.9 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032687934844599496		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.032687934844599496 | validation: 0.025070788033668658]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_974.pth
	Model improved!!!
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0281627121762491		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.0281627121762491 | validation: 0.028511772201911336]
	TIME [epoch: 12.9 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03257801173728916		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.03257801173728916 | validation: 0.04165979733844867]
	TIME [epoch: 12.9 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03377907271845294		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.03377907271845294 | validation: 0.040349009522532925]
	TIME [epoch: 12.9 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03426539634686034		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.03426539634686034 | validation: 0.027090598449012585]
	TIME [epoch: 12.9 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030430464926182454		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.030430464926182454 | validation: 0.038163742823896334]
	TIME [epoch: 12.9 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0315787902852027		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.0315787902852027 | validation: 0.03724401232274029]
	TIME [epoch: 12.9 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0352130051151138		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.0352130051151138 | validation: 0.028602349716495702]
	TIME [epoch: 12.9 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028634472010321536		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.028634472010321536 | validation: 0.024870128190258092]
	TIME [epoch: 12.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_982.pth
	Model improved!!!
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0310207600796059		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.0310207600796059 | validation: 0.02749732842953605]
	TIME [epoch: 12.9 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03055221558844723		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.03055221558844723 | validation: 0.04424778798349639]
	TIME [epoch: 12.9 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03340329458219338		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.03340329458219338 | validation: 0.03983077238160152]
	TIME [epoch: 12.9 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03087172863574298		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.03087172863574298 | validation: 0.026374497772118023]
	TIME [epoch: 12.9 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03359931087425349		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.03359931087425349 | validation: 0.07455573293605283]
	TIME [epoch: 12.9 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053776544130267924		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.053776544130267924 | validation: 0.06565738277099334]
	TIME [epoch: 12.9 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03721921986930234		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.03721921986930234 | validation: 0.03287139739329559]
	TIME [epoch: 12.9 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842573401337085		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.03842573401337085 | validation: 0.032952529610023366]
	TIME [epoch: 12.9 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902919364354138		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.03902919364354138 | validation: 0.030477955888682653]
	TIME [epoch: 12.9 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03002763476755037		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.03002763476755037 | validation: 0.040798102231445844]
	TIME [epoch: 12.9 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03409996307842377		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.03409996307842377 | validation: 0.030868257767631458]
	TIME [epoch: 12.9 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02926271040331401		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.02926271040331401 | validation: 0.029916016108835167]
	TIME [epoch: 12.9 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030015995870971545		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.030015995870971545 | validation: 0.03968114564684127]
	TIME [epoch: 12.9 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031845283757978474		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.031845283757978474 | validation: 0.028940515684635126]
	TIME [epoch: 12.9 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030819044512303488		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.030819044512303488 | validation: 0.027090319562211487]
	TIME [epoch: 12.9 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029672997277321422		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.029672997277321422 | validation: 0.026276398328609285]
	TIME [epoch: 12.9 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028792234380900332		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.028792234380900332 | validation: 0.03272481229322564]
	TIME [epoch: 12.9 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030574934796222867		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.030574934796222867 | validation: 0.028172164852675066]
	TIME [epoch: 12.9 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030173556666419615		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.030173556666419615 | validation: 0.02979031281119986]
	TIME [epoch: 205 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028439559476153432		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.028439559476153432 | validation: 0.030327670823814656]
	TIME [epoch: 26.5 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031229694755500862		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.031229694755500862 | validation: 0.02911343926990533]
	TIME [epoch: 26.5 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030485335569552935		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.030485335569552935 | validation: 0.03837854288272612]
	TIME [epoch: 26.5 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031327821815148815		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.031327821815148815 | validation: 0.03396103964634463]
	TIME [epoch: 26.5 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028854126909527794		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.028854126909527794 | validation: 0.032351063006181534]
	TIME [epoch: 26.5 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028008902499436036		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.028008902499436036 | validation: 0.029557869932203686]
	TIME [epoch: 26.5 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02723631476874331		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.02723631476874331 | validation: 0.03248815709569099]
	TIME [epoch: 26.5 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034453658582427805		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.034453658582427805 | validation: 0.04608418789246748]
	TIME [epoch: 26.5 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03267315763969846		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.03267315763969846 | validation: 0.026179734415221978]
	TIME [epoch: 26.5 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02950184793928196		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.02950184793928196 | validation: 0.13260654399843724]
	TIME [epoch: 26.5 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07640696858522762		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.07640696858522762 | validation: 0.1780444531136683]
	TIME [epoch: 26.6 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08785445596191313		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.08785445596191313 | validation: 0.0860545687887909]
	TIME [epoch: 26.5 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050422007467690676		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.050422007467690676 | validation: 0.04191420897524474]
	TIME [epoch: 26.5 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03523733491232606		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.03523733491232606 | validation: 0.035257396027505376]
	TIME [epoch: 26.5 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217210451087052		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.03217210451087052 | validation: 0.029164754693998842]
	TIME [epoch: 26.6 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032974935830299805		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.032974935830299805 | validation: 0.029466974737606556]
	TIME [epoch: 26.5 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03005515440730293		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.03005515440730293 | validation: 0.03537728742667136]
	TIME [epoch: 26.5 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03200550794518927		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.03200550794518927 | validation: 0.0422027818707596]
	TIME [epoch: 26.5 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03178748623161753		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.03178748623161753 | validation: 0.0342686977204315]
	TIME [epoch: 26.6 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032096112879396996		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.032096112879396996 | validation: 0.029786771690359884]
	TIME [epoch: 26.5 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029722169537026284		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.029722169537026284 | validation: 0.03463724309304105]
	TIME [epoch: 26.5 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031002477932086655		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.031002477932086655 | validation: 0.033198209196691766]
	TIME [epoch: 26.5 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03210970417539731		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.03210970417539731 | validation: 0.02861835673107248]
	TIME [epoch: 26.5 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03056754441726586		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.03056754441726586 | validation: 0.038408434585805375]
	TIME [epoch: 26.5 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03080822545153427		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.03080822545153427 | validation: 0.03605650133043816]
	TIME [epoch: 26.5 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03151119473233211		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.03151119473233211 | validation: 0.03272567617425586]
	TIME [epoch: 26.5 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02964261528732302		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.02964261528732302 | validation: 0.03462278041046908]
	TIME [epoch: 26.5 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030045255409684595		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.030045255409684595 | validation: 0.027000977329235665]
	TIME [epoch: 26.5 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029313162192671403		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.029313162192671403 | validation: 0.03538353613362152]
	TIME [epoch: 26.5 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04401524640200224		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.04401524640200224 | validation: 0.04272983092523613]
	TIME [epoch: 26.5 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042485308930849106		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.042485308930849106 | validation: 0.027922163191785577]
	TIME [epoch: 26.5 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0276329300612914		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.0276329300612914 | validation: 0.03464739215732995]
	TIME [epoch: 26.6 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03117014042862226		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.03117014042862226 | validation: 0.03150508370108392]
	TIME [epoch: 26.5 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030044126860894763		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.030044126860894763 | validation: 0.029847368313544842]
	TIME [epoch: 26.6 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028181267904018672		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.028181267904018672 | validation: 0.0253444669596553]
	TIME [epoch: 26.5 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02857932628418939		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.02857932628418939 | validation: 0.02826832924527466]
	TIME [epoch: 26.6 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028223823049220328		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.028223823049220328 | validation: 0.041871386367880084]
	TIME [epoch: 26.6 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034021586994419845		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.034021586994419845 | validation: 0.034515716499917685]
	TIME [epoch: 26.6 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03021160509439447		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.03021160509439447 | validation: 0.025710967269178677]
	TIME [epoch: 26.6 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028854968818431692		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.028854968818431692 | validation: 0.02853767046323279]
	TIME [epoch: 26.6 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027885841420264637		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.027885841420264637 | validation: 0.03358831937824158]
	TIME [epoch: 26.5 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029347309058625343		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.029347309058625343 | validation: 0.028807561946550653]
	TIME [epoch: 26.6 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02922165197423487		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.02922165197423487 | validation: 0.02739282768509186]
	TIME [epoch: 26.5 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032074580974126445		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.032074580974126445 | validation: 0.02740279803947261]
	TIME [epoch: 26.6 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028538862395820104		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.028538862395820104 | validation: 0.02685222274122222]
	TIME [epoch: 26.6 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027954115761705953		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.027954115761705953 | validation: 0.033124104422817935]
	TIME [epoch: 26.5 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029034369125365098		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.029034369125365098 | validation: 0.029203325745839816]
	TIME [epoch: 26.6 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02919889514918615		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.02919889514918615 | validation: 0.027070602823877212]
	TIME [epoch: 26.5 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02862879431692631		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.02862879431692631 | validation: 0.028439193067872806]
	TIME [epoch: 26.5 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031580481020519996		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.031580481020519996 | validation: 0.02890638505483766]
	TIME [epoch: 26.5 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029270663391183928		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.029270663391183928 | validation: 0.03168627989438453]
	TIME [epoch: 26.6 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028926794906921342		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.028926794906921342 | validation: 0.0222829937947836]
	TIME [epoch: 26.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1053.pth
	Model improved!!!
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02741688099268319		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.02741688099268319 | validation: 0.028397510848425192]
	TIME [epoch: 26.6 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027926678433640034		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.027926678433640034 | validation: 0.03284646508879339]
	TIME [epoch: 26.5 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030402456660754435		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.030402456660754435 | validation: 0.03364433061640133]
	TIME [epoch: 26.6 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029646919073975316		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.029646919073975316 | validation: 0.043583906551966214]
	TIME [epoch: 26.5 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03132794411448574		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.03132794411448574 | validation: 0.03656103077811934]
	TIME [epoch: 26.5 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03274904401796367		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.03274904401796367 | validation: 0.035675329028589]
	TIME [epoch: 26.5 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028867120768274836		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.028867120768274836 | validation: 0.022803729176991618]
	TIME [epoch: 26.5 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02837384552034649		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.02837384552034649 | validation: 0.03790776450338144]
	TIME [epoch: 26.5 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028871101971678446		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.028871101971678446 | validation: 0.024806450801624982]
	TIME [epoch: 26.6 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027955901793796794		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.027955901793796794 | validation: 0.02834111174369205]
	TIME [epoch: 26.5 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029543349383957206		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.029543349383957206 | validation: 0.030980079239144677]
	TIME [epoch: 26.5 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027987871116166085		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.027987871116166085 | validation: 0.028908388720714273]
	TIME [epoch: 26.5 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02777729315790249		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.02777729315790249 | validation: 0.02624820564346795]
	TIME [epoch: 26.6 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029544547993697334		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.029544547993697334 | validation: 0.030832347379555383]
	TIME [epoch: 26.5 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02822223194125871		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.02822223194125871 | validation: 0.031434230448005175]
	TIME [epoch: 26.5 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028786426876209036		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.028786426876209036 | validation: 0.028071106852494723]
	TIME [epoch: 26.5 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02669695614034965		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.02669695614034965 | validation: 0.034549900777330105]
	TIME [epoch: 26.6 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0327137836567291		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.0327137836567291 | validation: 0.026057820309722136]
	TIME [epoch: 26.5 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02695331465039158		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.02695331465039158 | validation: 0.03650591891486665]
	TIME [epoch: 26.6 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028763582882037866		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.028763582882037866 | validation: 0.024119951439597432]
	TIME [epoch: 26.5 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027115444123175313		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.027115444123175313 | validation: 0.03020333786781726]
	TIME [epoch: 26.6 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02830009356875451		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.02830009356875451 | validation: 0.030742042368234735]
	TIME [epoch: 26.5 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02607640339400408		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.02607640339400408 | validation: 0.027365507425064186]
	TIME [epoch: 26.6 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027325300357008978		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.027325300357008978 | validation: 0.028281532065418814]
	TIME [epoch: 26.5 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026975505262897603		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.026975505262897603 | validation: 0.02726339425639698]
	TIME [epoch: 26.6 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02727303540017773		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.02727303540017773 | validation: 0.033321582658757044]
	TIME [epoch: 26.5 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02868349507297147		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.02868349507297147 | validation: 0.029311423067024313]
	TIME [epoch: 26.6 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02927243233358394		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.02927243233358394 | validation: 0.03299143651109567]
	TIME [epoch: 26.5 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028601775064494666		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.028601775064494666 | validation: 0.042386327921990374]
	TIME [epoch: 26.6 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03126079883099885		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.03126079883099885 | validation: 0.03338335295308398]
	TIME [epoch: 26.5 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028797850538186964		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.028797850538186964 | validation: 0.025065979473441316]
	TIME [epoch: 26.6 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026345363423765915		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.026345363423765915 | validation: 0.048415007963976055]
	TIME [epoch: 26.5 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030476658964536878		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.030476658964536878 | validation: 0.029418435316813263]
	TIME [epoch: 26.6 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.033187870523752806		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.033187870523752806 | validation: 0.02904352240302979]
	TIME [epoch: 26.6 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027996953821261005		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.027996953821261005 | validation: 0.05444585697234096]
	TIME [epoch: 26.6 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032367908842448		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.032367908842448 | validation: 0.030575905916136594]
	TIME [epoch: 26.5 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02998810170010189		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.02998810170010189 | validation: 0.02906971552935014]
	TIME [epoch: 26.5 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029207974189919682		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.029207974189919682 | validation: 0.023682274261471892]
	TIME [epoch: 26.5 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027319848904949363		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.027319848904949363 | validation: 0.03071098145956478]
	TIME [epoch: 26.5 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02861749374807527		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.02861749374807527 | validation: 0.023948222688309564]
	TIME [epoch: 26.5 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027561423281637857		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.027561423281637857 | validation: 0.023812239481092803]
	TIME [epoch: 26.5 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811955009519302		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.02811955009519302 | validation: 0.025989336834580448]
	TIME [epoch: 26.5 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028538594290770398		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.028538594290770398 | validation: 0.025651900121616878]
	TIME [epoch: 26.5 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02534492680984527		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.02534492680984527 | validation: 0.02372710937256961]
	TIME [epoch: 26.5 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027719983993448185		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.027719983993448185 | validation: 0.02549634860546981]
	TIME [epoch: 26.5 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027516946630488138		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.027516946630488138 | validation: 0.026451073666718807]
	TIME [epoch: 26.5 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028680249937292995		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.028680249937292995 | validation: 0.02808183322016107]
	TIME [epoch: 26.5 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02811738233367137		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.02811738233367137 | validation: 0.028795402980891685]
	TIME [epoch: 26.5 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02618846384392305		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.02618846384392305 | validation: 0.034965675283414786]
	TIME [epoch: 26.6 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478877622442012		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.03478877622442012 | validation: 0.028626461358953484]
	TIME [epoch: 26.5 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027753216606823124		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.027753216606823124 | validation: 0.02444797373864648]
	TIME [epoch: 26.5 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027763922610272128		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.027763922610272128 | validation: 0.024991884037231807]
	TIME [epoch: 26.5 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026541187907854785		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.026541187907854785 | validation: 0.03636126407249204]
	TIME [epoch: 26.5 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02880436564746775		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.02880436564746775 | validation: 0.031926003190061183]
	TIME [epoch: 26.5 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02906689209020114		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.02906689209020114 | validation: 0.026179445691917682]
	TIME [epoch: 26.5 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02680634685367136		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.02680634685367136 | validation: 0.027523453601869165]
	TIME [epoch: 26.5 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02729905312581594		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.02729905312581594 | validation: 0.029200448961263382]
	TIME [epoch: 26.5 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02858145192850256		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.02858145192850256 | validation: 0.02444939637632401]
	TIME [epoch: 26.5 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02793744079023469		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.02793744079023469 | validation: 0.025249753152890065]
	TIME [epoch: 26.5 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025643392502284465		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.025643392502284465 | validation: 0.029593147319595028]
	TIME [epoch: 26.5 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027987237454691824		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.027987237454691824 | validation: 0.026360059738018526]
	TIME [epoch: 26.5 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028176457028405846		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.028176457028405846 | validation: 0.029008988252173285]
	TIME [epoch: 26.5 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028075031740606465		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.028075031740606465 | validation: 0.03737080135650146]
	TIME [epoch: 26.5 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02755568005060117		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.02755568005060117 | validation: 0.023357515857204327]
	TIME [epoch: 26.5 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030118194283294084		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.030118194283294084 | validation: 0.023882770515445587]
	TIME [epoch: 26.6 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02772877727537519		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.02772877727537519 | validation: 0.02909305295639052]
	TIME [epoch: 26.5 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03017377240002962		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.03017377240002962 | validation: 0.030623586551471182]
	TIME [epoch: 26.5 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025984878451622243		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.025984878451622243 | validation: 0.023513825127822997]
	TIME [epoch: 26.5 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027488400528829164		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.027488400528829164 | validation: 0.028069132277726283]
	TIME [epoch: 26.5 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0264325887582375		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.0264325887582375 | validation: 0.06531455593233888]
	TIME [epoch: 26.5 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046535549433404315		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.046535549433404315 | validation: 0.04119410880270921]
	TIME [epoch: 26.5 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031874646656706064		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.031874646656706064 | validation: 0.025737737087537384]
	TIME [epoch: 26.5 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026957542040134518		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.026957542040134518 | validation: 0.02354150061061814]
	TIME [epoch: 26.5 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027819858845882714		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.027819858845882714 | validation: 0.021940531502103258]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1127.pth
	Model improved!!!
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027260070326257883		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.027260070326257883 | validation: 0.027448927192767005]
	TIME [epoch: 26.5 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02822550933677389		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.02822550933677389 | validation: 0.02714374205956115]
	TIME [epoch: 26.5 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030240298648166252		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.030240298648166252 | validation: 0.030926448598436032]
	TIME [epoch: 26.5 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02787508507449565		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.02787508507449565 | validation: 0.03047444638499257]
	TIME [epoch: 26.5 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026633984809019226		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.026633984809019226 | validation: 0.02547894918707455]
	TIME [epoch: 26.5 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027680682964245116		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.027680682964245116 | validation: 0.03130293397100453]
	TIME [epoch: 26.5 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03013254600320707		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.03013254600320707 | validation: 0.030316567148438323]
	TIME [epoch: 26.5 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03218362138717538		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.03218362138717538 | validation: 0.027443187933579274]
	TIME [epoch: 26.5 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02741677648273724		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.02741677648273724 | validation: 0.02638141305646098]
	TIME [epoch: 26.5 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028833479435931383		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.028833479435931383 | validation: 0.030770661719605574]
	TIME [epoch: 26.5 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03077199565911772		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.03077199565911772 | validation: 0.029058030753753884]
	TIME [epoch: 26.5 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029788292130766725		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.029788292130766725 | validation: 0.03212643000351252]
	TIME [epoch: 26.5 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028617126866963796		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.028617126866963796 | validation: 0.025200306132688168]
	TIME [epoch: 26.5 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02685368534379237		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.02685368534379237 | validation: 0.02700715946382165]
	TIME [epoch: 26.5 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025527980341975943		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.025527980341975943 | validation: 0.027245767457017803]
	TIME [epoch: 26.5 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027946978279984692		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.027946978279984692 | validation: 0.027405340804742065]
	TIME [epoch: 26.5 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02533169807317261		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.02533169807317261 | validation: 0.030746093179221903]
	TIME [epoch: 26.5 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027219567878018817		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.027219567878018817 | validation: 0.03686754037567835]
	TIME [epoch: 26.5 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028134136246180486		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.028134136246180486 | validation: 0.02706898016864935]
	TIME [epoch: 26.5 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029054978253826427		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.029054978253826427 | validation: 0.026600448934420564]
	TIME [epoch: 26.5 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028164951701568786		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.028164951701568786 | validation: 0.025720471602734475]
	TIME [epoch: 26.5 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02608281303885697		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.02608281303885697 | validation: 0.03174566038028218]
	TIME [epoch: 26.5 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025963464652008095		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.025963464652008095 | validation: 0.03267200549295698]
	TIME [epoch: 26.6 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025102341712582285		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.025102341712582285 | validation: 0.047017638020118914]
	TIME [epoch: 26.5 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03083356114506051		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.03083356114506051 | validation: 0.025877387557629628]
	TIME [epoch: 26.6 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025157229588372614		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.025157229588372614 | validation: 0.025603290860749573]
	TIME [epoch: 26.5 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029201202185865943		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.029201202185865943 | validation: 0.023236185513654785]
	TIME [epoch: 26.5 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0266422213162107		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.0266422213162107 | validation: 0.032985565419869765]
	TIME [epoch: 26.5 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026323671433643947		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.026323671433643947 | validation: 0.02841212104351264]
	TIME [epoch: 26.5 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027159118760513908		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.027159118760513908 | validation: 0.026566667201707774]
	TIME [epoch: 26.6 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02520139175610557		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.02520139175610557 | validation: 0.02405731648245273]
	TIME [epoch: 26.5 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02526370186652387		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.02526370186652387 | validation: 0.027744178545347088]
	TIME [epoch: 26.5 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02499436559705598		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.02499436559705598 | validation: 0.028721944438563785]
	TIME [epoch: 26.5 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026149082935069626		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.026149082935069626 | validation: 0.025407796282646944]
	TIME [epoch: 26.5 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0279074572595791		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.0279074572595791 | validation: 0.026928401913346445]
	TIME [epoch: 26.5 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02617175366280966		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.02617175366280966 | validation: 0.02379178057560474]
	TIME [epoch: 26.6 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02684779064870496		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.02684779064870496 | validation: 0.02528249857304502]
	TIME [epoch: 26.5 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028953560048352634		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.028953560048352634 | validation: 0.02455584624615528]
	TIME [epoch: 26.6 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026595884953027366		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.026595884953027366 | validation: 0.022405807051487203]
	TIME [epoch: 26.5 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025473086647196698		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.025473086647196698 | validation: 0.021559331111589575]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1167.pth
	Model improved!!!
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027154795905290127		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.027154795905290127 | validation: 0.024536648514634565]
	TIME [epoch: 26.5 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026178563535943482		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.026178563535943482 | validation: 0.026641773577303008]
	TIME [epoch: 26.5 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02849281127612069		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.02849281127612069 | validation: 0.029048492544811344]
	TIME [epoch: 26.5 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028783953294474453		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.028783953294474453 | validation: 0.02687072270893942]
	TIME [epoch: 26.5 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02565538229619388		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.02565538229619388 | validation: 0.02756235304154352]
	TIME [epoch: 26.5 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028501481479587545		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.028501481479587545 | validation: 0.027332859706425253]
	TIME [epoch: 26.5 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02570071288109066		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.02570071288109066 | validation: 0.02576192565046318]
	TIME [epoch: 26.5 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025364549546926955		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.025364549546926955 | validation: 0.029319197395476906]
	TIME [epoch: 26.6 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024675067856406767		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.024675067856406767 | validation: 0.02824614668095864]
	TIME [epoch: 26.5 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0261487556011039		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.0261487556011039 | validation: 0.029868323601428038]
	TIME [epoch: 26.5 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027879952168785315		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.027879952168785315 | validation: 0.025697567145402823]
	TIME [epoch: 26.5 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025202169590371897		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.025202169590371897 | validation: 0.02382090440923721]
	TIME [epoch: 26.5 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0248813275084963		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.0248813275084963 | validation: 0.02153936331181048]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1180.pth
	Model improved!!!
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02607236901274365		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.02607236901274365 | validation: 0.0230331421427996]
	TIME [epoch: 26.5 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026288651688495594		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.026288651688495594 | validation: 0.04129998757279925]
	TIME [epoch: 26.5 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030951210851287617		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.030951210851287617 | validation: 0.02772800706116535]
	TIME [epoch: 26.5 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02610298225216477		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.02610298225216477 | validation: 0.02277800659800331]
	TIME [epoch: 26.5 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026270642103984426		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.026270642103984426 | validation: 0.02717486968861045]
	TIME [epoch: 26.5 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026773338542139903		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.026773338542139903 | validation: 0.03194994246506804]
	TIME [epoch: 26.5 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02623453630584222		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.02623453630584222 | validation: 0.02484739085791552]
	TIME [epoch: 26.5 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02503088483425427		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.02503088483425427 | validation: 0.023870872216539676]
	TIME [epoch: 26.5 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025872232573694857		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.025872232573694857 | validation: 0.025615843123550953]
	TIME [epoch: 26.5 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027367498083650445		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.027367498083650445 | validation: 0.023306490476140584]
	TIME [epoch: 26.6 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02678116765649512		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.02678116765649512 | validation: 0.022813737432818473]
	TIME [epoch: 26.5 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026644158448749934		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.026644158448749934 | validation: 0.07645825803003647]
	TIME [epoch: 26.6 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547159779419161		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.04547159779419161 | validation: 0.09792718590185501]
	TIME [epoch: 26.5 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06345858564983202		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.06345858564983202 | validation: 0.07835566945457945]
	TIME [epoch: 26.5 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788268578475744		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.04788268578475744 | validation: 0.0653049020260829]
	TIME [epoch: 26.5 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03669111949064791		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.03669111949064791 | validation: 0.04522932477963265]
	TIME [epoch: 26.5 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0304956672889339		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.0304956672889339 | validation: 0.03380118682695595]
	TIME [epoch: 26.5 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028038939112508113		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.028038939112508113 | validation: 0.029565351477409543]
	TIME [epoch: 26.5 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028203199796177376		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.028203199796177376 | validation: 0.02994199629080581]
	TIME [epoch: 26.5 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025130438730255222		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.025130438730255222 | validation: 0.029930605020866932]
	TIME [epoch: 26.5 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027672333147139284		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.027672333147139284 | validation: 0.030578691568264038]
	TIME [epoch: 26.5 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027138342158962717		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.027138342158962717 | validation: 0.029610365037292054]
	TIME [epoch: 26.5 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025974293185485612		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.025974293185485612 | validation: 0.03527483227982965]
	TIME [epoch: 26.5 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026580885633792635		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.026580885633792635 | validation: 0.02336361836218233]
	TIME [epoch: 26.5 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02651136977121636		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.02651136977121636 | validation: 0.0252663840032493]
	TIME [epoch: 26.5 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026424270858969808		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.026424270858969808 | validation: 0.024299112118264844]
	TIME [epoch: 26.5 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02695379140105261		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.02695379140105261 | validation: 0.02175021639067102]
	TIME [epoch: 26.5 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026367971194245388		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.026367971194245388 | validation: 0.026057740355851802]
	TIME [epoch: 26.5 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02695101067042964		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.02695101067042964 | validation: 0.029711593926178406]
	TIME [epoch: 26.5 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024979759339106015		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.024979759339106015 | validation: 0.02543250323539932]
	TIME [epoch: 26.5 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02692308750882253		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.02692308750882253 | validation: 0.025739518776625704]
	TIME [epoch: 26.5 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026038999171448404		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.026038999171448404 | validation: 0.029168822762911707]
	TIME [epoch: 26.5 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026409747282934087		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.026409747282934087 | validation: 0.030692532292617116]
	TIME [epoch: 26.5 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025066019286324872		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.025066019286324872 | validation: 0.029183460583351264]
	TIME [epoch: 26.5 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027029613471496496		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.027029613471496496 | validation: 0.023075273387162056]
	TIME [epoch: 26.5 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02643983989109806		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.02643983989109806 | validation: 0.027485156472537242]
	TIME [epoch: 26.5 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026305393971279827		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.026305393971279827 | validation: 0.030844772439416357]
	TIME [epoch: 26.5 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02658603425993668		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.02658603425993668 | validation: 0.028425367409599847]
	TIME [epoch: 26.5 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025811692697762942		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.025811692697762942 | validation: 0.026988957029148122]
	TIME [epoch: 26.5 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026868041319861678		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.026868041319861678 | validation: 0.025784825882295505]
	TIME [epoch: 26.5 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028140503416679136		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.028140503416679136 | validation: 0.021725421314821327]
	TIME [epoch: 26.5 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025082604909544476		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.025082604909544476 | validation: 0.02643081911097337]
	TIME [epoch: 26.5 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0262389577165214		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.0262389577165214 | validation: 0.03071668915438921]
	TIME [epoch: 26.5 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025263885059431078		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.025263885059431078 | validation: 0.03238169915510638]
	TIME [epoch: 26.5 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02458802156627848		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.02458802156627848 | validation: 0.02529815820665701]
	TIME [epoch: 26.5 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02696075559872095		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.02696075559872095 | validation: 0.03216601365139606]
	TIME [epoch: 26.5 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025918948941197673		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.025918948941197673 | validation: 0.028702359026881744]
	TIME [epoch: 26.5 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027365680077940092		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.027365680077940092 | validation: 0.02676484397920741]
	TIME [epoch: 26.5 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028202838061226913		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.028202838061226913 | validation: 0.031007844203734116]
	TIME [epoch: 26.5 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02625596648844081		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.02625596648844081 | validation: 0.0332455414618541]
	TIME [epoch: 26.5 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027778691530358907		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.027778691530358907 | validation: 0.022457765976590452]
	TIME [epoch: 26.5 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.029977237847665882		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.029977237847665882 | validation: 0.03277107893206251]
	TIME [epoch: 26.5 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030064365333787632		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.030064365333787632 | validation: 0.02499377072921155]
	TIME [epoch: 26.5 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028788930979632945		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.028788930979632945 | validation: 0.035735464848121824]
	TIME [epoch: 26.5 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026022566288377184		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.026022566288377184 | validation: 0.03044817904385303]
	TIME [epoch: 26.5 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02626752064907189		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.02626752064907189 | validation: 0.022833744244817156]
	TIME [epoch: 26.5 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02708715473074264		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.02708715473074264 | validation: 0.024152437615423773]
	TIME [epoch: 26.5 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027429466517109348		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.027429466517109348 | validation: 0.018729725495833682]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1238.pth
	Model improved!!!
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024129659559250515		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.024129659559250515 | validation: 0.02014799933720718]
	TIME [epoch: 26.5 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025823700611803647		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.025823700611803647 | validation: 0.02064560185971631]
	TIME [epoch: 26.5 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024191017112622577		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.024191017112622577 | validation: 0.020531287745190308]
	TIME [epoch: 26.5 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02330622306704829		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.02330622306704829 | validation: 0.023176723433379565]
	TIME [epoch: 26.5 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02454139759519815		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.02454139759519815 | validation: 0.025434478977203023]
	TIME [epoch: 26.5 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026154432335599997		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.026154432335599997 | validation: 0.028828378884394004]
	TIME [epoch: 26.5 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027194124437720575		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.027194124437720575 | validation: 0.03324477988330748]
	TIME [epoch: 26.5 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0265802012193603		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.0265802012193603 | validation: 0.02628831918630068]
	TIME [epoch: 26.5 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02408431713562145		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.02408431713562145 | validation: 0.02372174494525178]
	TIME [epoch: 26.5 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025097750888029978		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.025097750888029978 | validation: 0.02699411744141572]
	TIME [epoch: 26.5 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02509036516471243		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.02509036516471243 | validation: 0.03063347878858599]
	TIME [epoch: 26.5 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02638241961802236		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.02638241961802236 | validation: 0.028086280767747985]
	TIME [epoch: 26.5 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025621534827917465		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.025621534827917465 | validation: 0.024825340459435977]
	TIME [epoch: 26.5 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02504229108199008		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.02504229108199008 | validation: 0.029281815988358374]
	TIME [epoch: 26.5 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0258450751408342		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.0258450751408342 | validation: 0.02692238841485525]
	TIME [epoch: 26.5 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02540351332772919		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.02540351332772919 | validation: 0.02127363171642357]
	TIME [epoch: 26.5 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025044523277501025		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.025044523277501025 | validation: 0.060268424675968914]
	TIME [epoch: 26.5 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03968890044946163		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.03968890044946163 | validation: 0.06853416747026851]
	TIME [epoch: 26.5 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046156205236045286		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.046156205236045286 | validation: 0.06007076144835723]
	TIME [epoch: 26.5 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04253946010521183		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.04253946010521183 | validation: 0.03922933435409858]
	TIME [epoch: 26.5 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03109104556467851		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.03109104556467851 | validation: 0.03309248404718051]
	TIME [epoch: 26.5 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02638874760708877		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.02638874760708877 | validation: 0.025276011911751085]
	TIME [epoch: 26.5 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026918165487189824		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.026918165487189824 | validation: 0.028117613285846655]
	TIME [epoch: 26.5 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027499586516019657		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.027499586516019657 | validation: 0.025989966628216045]
	TIME [epoch: 26.5 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026210244461797854		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.026210244461797854 | validation: 0.027827131036253772]
	TIME [epoch: 26.6 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026141498986333972		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.026141498986333972 | validation: 0.03367331730134683]
	TIME [epoch: 26.5 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0256324420417923		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.0256324420417923 | validation: 0.026593355847100743]
	TIME [epoch: 26.6 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02550635412543992		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.02550635412543992 | validation: 0.026835594875604486]
	TIME [epoch: 26.5 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026826820244896094		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.026826820244896094 | validation: 0.028103338859257587]
	TIME [epoch: 26.5 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026836891015280806		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.026836891015280806 | validation: 0.028412279188749768]
	TIME [epoch: 26.5 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02714747978636956		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.02714747978636956 | validation: 0.028054082568284136]
	TIME [epoch: 26.5 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02712943705513674		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.02712943705513674 | validation: 0.023481222279507642]
	TIME [epoch: 26.5 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02538597831273392		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.02538597831273392 | validation: 0.025899614930819704]
	TIME [epoch: 26.5 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0267649397886619		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.0267649397886619 | validation: 0.025130607998280777]
	TIME [epoch: 26.5 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02535567845135747		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.02535567845135747 | validation: 0.02322977560883819]
	TIME [epoch: 26.5 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025922423916297187		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.025922423916297187 | validation: 0.02890732693021496]
	TIME [epoch: 26.5 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027790888110183327		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.027790888110183327 | validation: 0.028412400898439705]
	TIME [epoch: 26.5 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028093933180715097		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.028093933180715097 | validation: 0.023767049060763892]
	TIME [epoch: 26.5 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025433928372466188		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.025433928372466188 | validation: 0.024399507306639978]
	TIME [epoch: 26.5 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025549248947501903		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.025549248947501903 | validation: 0.021001519844926667]
	TIME [epoch: 26.5 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024783821528524968		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.024783821528524968 | validation: 0.025906859374873217]
	TIME [epoch: 26.5 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025367315949705475		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.025367315949705475 | validation: 0.02658816887569552]
	TIME [epoch: 26.5 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026136965733124268		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.026136965733124268 | validation: 0.02839118532046038]
	TIME [epoch: 26.5 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025832369975324447		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.025832369975324447 | validation: 0.021136938099432248]
	TIME [epoch: 26.5 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024189889958100678		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.024189889958100678 | validation: 0.024132557800716492]
	TIME [epoch: 26.6 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02558697152232944		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.02558697152232944 | validation: 0.06725312391459719]
	TIME [epoch: 26.5 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03823026657387941		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.03823026657387941 | validation: 0.07348729443940512]
	TIME [epoch: 26.5 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04206182028281057		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.04206182028281057 | validation: 0.0609196758287564]
	TIME [epoch: 26.5 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038958562398519705		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.038958562398519705 | validation: 0.03637072134680256]
	TIME [epoch: 26.5 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02696136212539539		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.02696136212539539 | validation: 0.026539795192027905]
	TIME [epoch: 26.5 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02899644954473504		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.02899644954473504 | validation: 0.025237131725342633]
	TIME [epoch: 26.5 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025954429049248048		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.025954429049248048 | validation: 0.025378913696651507]
	TIME [epoch: 26.5 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02530367931806678		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.02530367931806678 | validation: 0.021183579699533852]
	TIME [epoch: 26.5 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025894143984555404		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.025894143984555404 | validation: 0.02533537667641005]
	TIME [epoch: 26.5 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02728310183634232		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.02728310183634232 | validation: 0.02957351457313341]
	TIME [epoch: 26.5 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028515942572725016		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.028515942572725016 | validation: 0.026308240788151905]
	TIME [epoch: 26.5 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027147031184776065		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.027147031184776065 | validation: 0.028404912454721612]
	TIME [epoch: 26.5 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027224225637627497		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.027224225637627497 | validation: 0.026904968451393133]
	TIME [epoch: 26.5 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026136582878970548		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.026136582878970548 | validation: 0.025197107867093095]
	TIME [epoch: 26.5 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024280651393221636		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.024280651393221636 | validation: 0.02888470651262345]
	TIME [epoch: 26.5 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026380108858940023		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.026380108858940023 | validation: 0.029920781884005678]
	TIME [epoch: 26.5 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02595097128200211		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.02595097128200211 | validation: 0.023461901599294333]
	TIME [epoch: 26.5 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0243680105940476		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.0243680105940476 | validation: 0.023339950041613367]
	TIME [epoch: 26.6 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02778627350246314		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.02778627350246314 | validation: 0.025958812429094136]
	TIME [epoch: 26.6 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026622331732878354		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.026622331732878354 | validation: 0.029225708448808774]
	TIME [epoch: 26.6 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025836494031664083		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.025836494031664083 | validation: 0.029422372730404735]
	TIME [epoch: 26.6 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02816586328067658		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.02816586328067658 | validation: 0.02940654218801593]
	TIME [epoch: 26.6 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026436570519201674		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.026436570519201674 | validation: 0.02732629034210875]
	TIME [epoch: 26.6 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02776880927892438		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.02776880927892438 | validation: 0.02473226986593843]
	TIME [epoch: 26.6 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02612854951239591		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.02612854951239591 | validation: 0.023765948064771215]
	TIME [epoch: 26.7 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025396235431873054		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.025396235431873054 | validation: 0.027125135301248583]
	TIME [epoch: 26.6 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02450940520681469		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.02450940520681469 | validation: 0.028898755820233005]
	TIME [epoch: 26.5 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025653334733976924		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.025653334733976924 | validation: 0.0294317162227555]
	TIME [epoch: 26.5 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02415170171513509		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.02415170171513509 | validation: 0.0272997142453548]
	TIME [epoch: 26.5 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02517493749924097		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.02517493749924097 | validation: 0.026740281720366854]
	TIME [epoch: 26.5 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026950294135928848		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.026950294135928848 | validation: 0.026244566575068764]
	TIME [epoch: 26.5 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026479549916124556		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.026479549916124556 | validation: 0.020905886846337166]
	TIME [epoch: 26.5 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02679200194895354		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.02679200194895354 | validation: 0.025078253442384582]
	TIME [epoch: 26.5 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026075445025606755		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.026075445025606755 | validation: 0.0224799441148427]
	TIME [epoch: 26.5 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025146879484910362		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.025146879484910362 | validation: 0.021130745901311112]
	TIME [epoch: 26.5 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026277314176528902		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.026277314176528902 | validation: 0.02802903626288963]
	TIME [epoch: 26.5 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026039610435223556		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.026039610435223556 | validation: 0.02474925760667144]
	TIME [epoch: 26.5 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02437206213935063		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.02437206213935063 | validation: 0.02663027247339539]
	TIME [epoch: 26.5 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025053792900912458		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.025053792900912458 | validation: 0.02466175534596651]
	TIME [epoch: 26.5 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024549577866614637		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.024549577866614637 | validation: 0.0259138612124174]
	TIME [epoch: 26.5 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025323392370471		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.025323392370471 | validation: 0.02422979503088343]
	TIME [epoch: 26.5 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025134857672628828		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.025134857672628828 | validation: 0.024292611259462173]
	TIME [epoch: 26.5 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02602579019584843		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.02602579019584843 | validation: 0.027694952461870495]
	TIME [epoch: 26.5 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025146103598859125		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.025146103598859125 | validation: 0.028224834670551247]
	TIME [epoch: 26.5 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.027459804707583847		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.027459804707583847 | validation: 0.02891458479368625]
	TIME [epoch: 26.5 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02855979919440794		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.02855979919440794 | validation: 0.02440270298041294]
	TIME [epoch: 26.5 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02534965040657839		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.02534965040657839 | validation: 0.02680040788036128]
	TIME [epoch: 26.5 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02443468835803694		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.02443468835803694 | validation: 0.025076803580975682]
	TIME [epoch: 26.5 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024529479510532807		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.024529479510532807 | validation: 0.029992296491625537]
	TIME [epoch: 26.5 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025836202584751354		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.025836202584751354 | validation: 0.0319175611346469]
	TIME [epoch: 26.5 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026191188889977263		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.026191188889977263 | validation: 0.02653688450686652]
	TIME [epoch: 26.5 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02585552008283879		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.02585552008283879 | validation: 0.02412457208750454]
	TIME [epoch: 26.5 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.02584129646627534		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.02584129646627534 | validation: 0.024011398114650397]
	TIME [epoch: 26.5 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026021119968885075		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.026021119968885075 | validation: 0.021609678605724714]
	TIME [epoch: 26.5 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.024608994133170744		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.024608994133170744 | validation: 0.028661227855025173]
	TIME [epoch: 26.5 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.025974995351705915		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.025974995351705915 | validation: 0.024992490596009977]
	TIME [epoch: 26.5 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241125_134138/states/model_phi1_4c_v_mmd1_1339.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 18698.324 seconds.
