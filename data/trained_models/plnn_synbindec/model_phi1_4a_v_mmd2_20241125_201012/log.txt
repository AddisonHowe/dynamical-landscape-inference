Args:
Namespace(name='model_phi1_4a_v_mmd2', outdir='out/model_training/model_phi1_4a_v_mmd2', training_data='data/training_data/basic/data_phi1_4a/training', validation_data='data/training_data/basic/data_phi1_4a/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1991461642

Training model...

Saving initial model state to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.426699966153889		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.426699966153889 | validation: 3.017588355816829]
	TIME [epoch: 158 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.117959904753444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.117959904753444 | validation: 2.8554047648237297]
	TIME [epoch: 0.68 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.767316048934709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.767316048934709 | validation: 2.9871809890189733]
	TIME [epoch: 0.673 sec]
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.375669377252479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.375669377252479 | validation: 4.254477656525991]
	TIME [epoch: 0.671 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6976617911103826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6976617911103826 | validation: 2.6564474986489515]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2677554895796885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2677554895796885 | validation: 2.3245457516858554]
	TIME [epoch: 0.677 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.057986554969598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.057986554969598 | validation: 2.3274556957832133]
	TIME [epoch: 0.674 sec]
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8381842326224076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.8381842326224076 | validation: 2.1397986302608123]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.6518994717372117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.6518994717372117 | validation: 1.9220080815077567]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.4937677814879424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.4937677814879424 | validation: 1.8342676580283523]
	TIME [epoch: 0.675 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.253123603765503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.253123603765503 | validation: 1.6457559979602494]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9909140021866685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9909140021866685 | validation: 1.832148060283151]
	TIME [epoch: 0.671 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8397038223419748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8397038223419748 | validation: 2.6991655568490076]
	TIME [epoch: 0.673 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.368359392985846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.368359392985846 | validation: 2.074195939706131]
	TIME [epoch: 0.671 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7131384386390884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.7131384386390884 | validation: 1.6262974611422494]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9660512112804798		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9660512112804798 | validation: 1.8622999704073218]
	TIME [epoch: 0.672 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0370034099673884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0370034099673884 | validation: 1.6323178826150326]
	TIME [epoch: 0.672 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7277514005072747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7277514005072747 | validation: 1.5663554137332423]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7236359206930503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7236359206930503 | validation: 1.494343544646437]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_19.pth
	Model improved!!!
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5638653334167196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5638653334167196 | validation: 1.539045388799893]
	TIME [epoch: 0.68 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5642949331549045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5642949331549045 | validation: 1.4588056371645863]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4880641120840787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4880641120840787 | validation: 1.3610967236633342]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.456624357946323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.456624357946323 | validation: 1.355326098814385]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.430379243542596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.430379243542596 | validation: 1.318500598721181]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3900637951554686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3900637951554686 | validation: 1.232859881759528]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_25.pth
	Model improved!!!
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3676492937173856		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3676492937173856 | validation: 1.3287423914041618]
	TIME [epoch: 0.67 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.364206027929003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.364206027929003 | validation: 1.0860935486252605]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3599565597114371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3599565597114371 | validation: 1.414761939680006]
	TIME [epoch: 0.674 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3782672948232735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3782672948232735 | validation: 1.1042591512260576]
	TIME [epoch: 0.668 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.282735037390172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.282735037390172 | validation: 1.0774696030976776]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2762807170572419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2762807170572419 | validation: 1.219038267600183]
	TIME [epoch: 0.67 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2882120652050895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2882120652050895 | validation: 1.0396619830117029]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2881538231487542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2881538231487542 | validation: 1.0363092393414173]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3404825860912917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3404825860912917 | validation: 1.3598979283747912]
	TIME [epoch: 0.67 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.471330439284097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.471330439284097 | validation: 0.920283623822105]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3168659700299015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3168659700299015 | validation: 1.1592624170589745]
	TIME [epoch: 0.674 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2059517786971714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2059517786971714 | validation: 1.0159859416572894]
	TIME [epoch: 0.669 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.17421433885396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.17421433885396 | validation: 0.8609425063321468]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1837530408772354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1837530408772354 | validation: 1.0508037533126204]
	TIME [epoch: 0.664 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1617109188633843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1617109188633843 | validation: 0.8729667357563516]
	TIME [epoch: 0.669 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1277657395255904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1277657395255904 | validation: 0.8486607850278549]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_41.pth
	Model improved!!!
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1161573648135397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1161573648135397 | validation: 1.0070278199129892]
	TIME [epoch: 0.672 sec]
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.124502945226629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.124502945226629 | validation: 0.8370235449648649]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_43.pth
	Model improved!!!
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1071862280857039		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1071862280857039 | validation: 0.8575795378563345]
	TIME [epoch: 0.674 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1024247658201694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1024247658201694 | validation: 0.922554406766039]
	TIME [epoch: 0.669 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1417456924008502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1417456924008502 | validation: 0.7954433052620127]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1441958904719838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1441958904719838 | validation: 1.0405416179520885]
	TIME [epoch: 0.675 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.133822184506771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.133822184506771 | validation: 0.7452573612928801]
	TIME [epoch: 0.675 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0496590897430522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0496590897430522 | validation: 0.8090689323379039]
	TIME [epoch: 0.675 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0264265034667768		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0264265034667768 | validation: 0.7194955662996986]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0093847182504583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0093847182504583 | validation: 0.7997103401569845]
	TIME [epoch: 0.674 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0066635975173441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0066635975173441 | validation: 0.718530745293625]
	TIME [epoch: 0.675 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0022397627425155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0022397627425155 | validation: 0.8347400866945045]
	TIME [epoch: 0.671 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0113619270595686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0113619270595686 | validation: 0.662168214951177]
	TIME [epoch: 0.674 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0080675932826757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0080675932826757 | validation: 0.8216364010434986]
	TIME [epoch: 0.674 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9936122386940326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9936122386940326 | validation: 0.705960470366909]
	TIME [epoch: 0.679 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9855134456634013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9855134456634013 | validation: 0.7308785955562351]
	TIME [epoch: 0.671 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.983885822872018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.983885822872018 | validation: 0.8725336112488927]
	TIME [epoch: 0.671 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0531057910748378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0531057910748378 | validation: 0.7627725069063724]
	TIME [epoch: 0.67 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1327534841084113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1327534841084113 | validation: 1.136914699837091]
	TIME [epoch: 0.67 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1007508193430677		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1007508193430677 | validation: 0.7055212152257573]
	TIME [epoch: 0.67 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9433147152610308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9433147152610308 | validation: 0.648776046274492]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9751263676218241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9751263676218241 | validation: 0.8235179950100259]
	TIME [epoch: 0.672 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9656115510761292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9656115510761292 | validation: 0.6839537040938073]
	TIME [epoch: 0.672 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9263446022526876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9263446022526876 | validation: 0.6834552560337069]
	TIME [epoch: 0.673 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.925358352259158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.925358352259158 | validation: 0.6798435466937349]
	TIME [epoch: 0.677 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9313770324712634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9313770324712634 | validation: 0.7482135022619212]
	TIME [epoch: 0.672 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.928525094871801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.928525094871801 | validation: 0.6253409901151943]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9334455599302949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9334455599302949 | validation: 0.8930335754313007]
	TIME [epoch: 0.673 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9510116630249661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9510116630249661 | validation: 0.5981461482832174]
	TIME [epoch: 0.671 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9414960075280303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9414960075280303 | validation: 0.7316878789516402]
	TIME [epoch: 0.677 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9021967104435402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9021967104435402 | validation: 0.7040652357815218]
	TIME [epoch: 0.675 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896104449711103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.896104449711103 | validation: 0.7355519684832768]
	TIME [epoch: 0.676 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9204439350440989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9204439350440989 | validation: 0.7318688239703143]
	TIME [epoch: 0.671 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9463649455151193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9463649455151193 | validation: 1.0759533120502085]
	TIME [epoch: 0.677 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0525160536237226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0525160536237226 | validation: 0.5980266076362941]
	TIME [epoch: 0.674 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9190891924987321		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9190891924987321 | validation: 0.7683754900629726]
	TIME [epoch: 0.671 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8917422184707442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8917422184707442 | validation: 0.7541544500867738]
	TIME [epoch: 0.67 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9004565980848644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9004565980848644 | validation: 0.6671767424120367]
	TIME [epoch: 0.668 sec]
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769767031713755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8769767031713755 | validation: 0.6842063277110109]
	TIME [epoch: 0.668 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8592545119833389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8592545119833389 | validation: 0.7254805809962135]
	TIME [epoch: 0.667 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8530196902373036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8530196902373036 | validation: 0.6371886208491819]
	TIME [epoch: 0.668 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8561463350015542		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8561463350015542 | validation: 0.872445563475227]
	TIME [epoch: 0.664 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9013133523536945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9013133523536945 | validation: 0.6036224321824989]
	TIME [epoch: 0.668 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9229017579524594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9229017579524594 | validation: 0.7539177068404073]
	TIME [epoch: 0.664 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8100897914357752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8100897914357752 | validation: 0.7056201508247506]
	TIME [epoch: 0.667 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8026454446441834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8026454446441834 | validation: 0.7500137464962346]
	TIME [epoch: 0.665 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8576409779308535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8576409779308535 | validation: 0.7765377013735437]
	TIME [epoch: 0.662 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074603190952141		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074603190952141 | validation: 0.8539431620691582]
	TIME [epoch: 0.665 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8754911578346415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8754911578346415 | validation: 0.631325916443937]
	TIME [epoch: 0.664 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7760105145586504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7760105145586504 | validation: 0.6759348538527145]
	TIME [epoch: 0.664 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7771566915635378		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7771566915635378 | validation: 0.7149760413642219]
	TIME [epoch: 0.666 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7875066794749611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7875066794749611 | validation: 0.7800461152444023]
	TIME [epoch: 0.664 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8240839575937361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8240839575937361 | validation: 0.5762399481112391]
	TIME [epoch: 0.664 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_94.pth
	Model improved!!!
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934301382584713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934301382584713 | validation: 0.8105033149426147]
	TIME [epoch: 0.669 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7709408999937358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7709408999937358 | validation: 0.6474326087848401]
	TIME [epoch: 0.666 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7371501968597539		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7371501968597539 | validation: 0.620653914509459]
	TIME [epoch: 0.667 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.742826146462702		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.742826146462702 | validation: 0.7476582366131027]
	TIME [epoch: 0.665 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7388169558471879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7388169558471879 | validation: 0.6303619173581635]
	TIME [epoch: 0.663 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401136013879431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7401136013879431 | validation: 0.6848109954070265]
	TIME [epoch: 0.663 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7345704574807064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7345704574807064 | validation: 0.6914998571544035]
	TIME [epoch: 0.667 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7445422442415394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7445422442415394 | validation: 0.6280506841678074]
	TIME [epoch: 0.669 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7605707314839808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7605707314839808 | validation: 0.7411687089497976]
	TIME [epoch: 0.665 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7220145778577493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7220145778577493 | validation: 0.568605679868324]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_104.pth
	Model improved!!!
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6913605793145488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6913605793145488 | validation: 0.6775033492498861]
	TIME [epoch: 0.674 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6796039567486912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6796039567486912 | validation: 0.5563980917255329]
	TIME [epoch: 0.673 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7080624099837239		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7080624099837239 | validation: 0.7824601311769634]
	TIME [epoch: 0.677 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.730998920576611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.730998920576611 | validation: 0.5379677973577833]
	TIME [epoch: 0.677 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7228795553081941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7228795553081941 | validation: 0.637170506867391]
	TIME [epoch: 0.673 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6678738500582173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6678738500582173 | validation: 0.6566678603461613]
	TIME [epoch: 0.67 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581896037037397		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6581896037037397 | validation: 0.52064195240108]
	TIME [epoch: 0.668 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7567863087097476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7567863087097476 | validation: 0.9200612352756625]
	TIME [epoch: 0.672 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.833287617759277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.833287617759277 | validation: 0.6102281472827076]
	TIME [epoch: 0.671 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.65266550584568		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.65266550584568 | validation: 0.5172181946509284]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6467201738737091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6467201738737091 | validation: 0.6269309660882009]
	TIME [epoch: 0.67 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335600053632641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6335600053632641 | validation: 0.5629723180313299]
	TIME [epoch: 0.677 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6052353054395221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6052353054395221 | validation: 0.5401818262078879]
	TIME [epoch: 0.667 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5891256086506172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5891256086506172 | validation: 0.52012180176964]
	TIME [epoch: 0.668 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5721781435627429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5721781435627429 | validation: 0.5083537322806907]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5623360093759306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5623360093759306 | validation: 0.5241287003936748]
	TIME [epoch: 0.676 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5557716908575389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5557716908575389 | validation: 0.4881419577578574]
	TIME [epoch: 0.676 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5748953658806251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5748953658806251 | validation: 0.7883413021953267]
	TIME [epoch: 0.668 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.776147879765782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.776147879765782 | validation: 0.670060891377082]
	TIME [epoch: 0.671 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6592701040570669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6592701040570669 | validation: 0.5866513727955119]
	TIME [epoch: 0.673 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7127273469671362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7127273469671362 | validation: 0.4638408994067816]
	TIME [epoch: 0.664 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_125.pth
	Model improved!!!
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7826545809740344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7826545809740344 | validation: 0.8143800928059994]
	TIME [epoch: 0.668 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684238202840811		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7684238202840811 | validation: 0.501401084591604]
	TIME [epoch: 0.669 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5736707811027081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5736707811027081 | validation: 0.4206960091749715]
	TIME [epoch: 0.666 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_128.pth
	Model improved!!!
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.641923587727009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.641923587727009 | validation: 0.5537672812350136]
	TIME [epoch: 0.671 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5632072307151187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5632072307151187 | validation: 0.5155666586046678]
	TIME [epoch: 0.672 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5311600493988884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5311600493988884 | validation: 0.43951792910438026]
	TIME [epoch: 0.67 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5119237170318678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5119237170318678 | validation: 0.46684974435750565]
	TIME [epoch: 0.671 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5042140738376814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5042140738376814 | validation: 0.4667905141187954]
	TIME [epoch: 0.669 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48782807952612595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48782807952612595 | validation: 0.4690687014125093]
	TIME [epoch: 0.666 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4854114368673176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4854114368673176 | validation: 0.40443931712211517]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_135.pth
	Model improved!!!
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4860294178832581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4860294178832581 | validation: 0.5145834894526712]
	TIME [epoch: 0.669 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.503252144320252		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.503252144320252 | validation: 0.40909980498317683]
	TIME [epoch: 0.668 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5923038884156275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5923038884156275 | validation: 0.6275644654781711]
	TIME [epoch: 0.673 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5782850412788709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5782850412788709 | validation: 0.4005890243186233]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47168637910143363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47168637910143363 | validation: 0.40889244617868653]
	TIME [epoch: 0.672 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4650767219755017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4650767219755017 | validation: 0.5618569084267845]
	TIME [epoch: 0.671 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516953707918301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.516953707918301 | validation: 0.43686084157629124]
	TIME [epoch: 0.673 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.573056687311939		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.573056687311939 | validation: 0.38930720055659396]
	TIME [epoch: 0.665 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4389963256199944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4389963256199944 | validation: 0.45467263051097223]
	TIME [epoch: 0.671 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4407775654126965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4407775654126965 | validation: 0.3439037594398875]
	TIME [epoch: 0.669 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4734674092422121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4734674092422121 | validation: 0.4921411055841084]
	TIME [epoch: 0.669 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4789584811403565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4789584811403565 | validation: 0.3863332853840807]
	TIME [epoch: 0.664 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4517221647104725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4517221647104725 | validation: 0.42292336130167396]
	TIME [epoch: 0.673 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4421171260613056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4421171260613056 | validation: 0.48883628446116867]
	TIME [epoch: 0.668 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47048482076154247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47048482076154247 | validation: 0.45714361106166623]
	TIME [epoch: 0.669 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.605036570441174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.605036570441174 | validation: 0.42289021510924424]
	TIME [epoch: 0.671 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4301838925761935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4301838925761935 | validation: 0.40981279662426395]
	TIME [epoch: 0.665 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4254913525397527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4254913525397527 | validation: 0.506828864525824]
	TIME [epoch: 0.667 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43992168103736873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43992168103736873 | validation: 0.3818446805331864]
	TIME [epoch: 0.664 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3950278932123921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3950278932123921 | validation: 0.33508384318538514]
	TIME [epoch: 0.665 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_155.pth
	Model improved!!!
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38498239861007194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38498239861007194 | validation: 0.4749347347185638]
	TIME [epoch: 0.662 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46299578572703126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46299578572703126 | validation: 0.4158659287362138]
	TIME [epoch: 0.675 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6446897039266376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6446897039266376 | validation: 0.4403622452577898]
	TIME [epoch: 0.672 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4663366532437489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4663366532437489 | validation: 0.6171965616471873]
	TIME [epoch: 0.668 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6357194987861526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6357194987861526 | validation: 0.36635165751718257]
	TIME [epoch: 0.671 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39498429233627547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39498429233627547 | validation: 0.5935192583058797]
	TIME [epoch: 0.667 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5953093270640468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5953093270640468 | validation: 0.3580492755148872]
	TIME [epoch: 0.669 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47254454333344587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.47254454333344587 | validation: 0.4505304878765743]
	TIME [epoch: 0.668 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41562488481632925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41562488481632925 | validation: 0.4546512234536255]
	TIME [epoch: 0.663 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4009328155732108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4009328155732108 | validation: 0.33309628506161965]
	TIME [epoch: 0.667 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_165.pth
	Model improved!!!
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35085318952132355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35085318952132355 | validation: 0.3764979648915764]
	TIME [epoch: 0.672 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3317491864172591		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3317491864172591 | validation: 0.3545015940801055]
	TIME [epoch: 0.673 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34631457300195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34631457300195 | validation: 0.36026761773196736]
	TIME [epoch: 0.671 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33775981176245806		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33775981176245806 | validation: 0.3157915132950481]
	TIME [epoch: 0.67 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_169.pth
	Model improved!!!
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36777309073899916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36777309073899916 | validation: 0.46232550454309956]
	TIME [epoch: 0.675 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4224630993179095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4224630993179095 | validation: 0.32181520950268666]
	TIME [epoch: 0.677 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42680605263000765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42680605263000765 | validation: 0.43452749515547223]
	TIME [epoch: 0.673 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34059499125294473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34059499125294473 | validation: 0.4585958211603555]
	TIME [epoch: 0.673 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39822732968578306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39822732968578306 | validation: 0.5518709385550535]
	TIME [epoch: 0.67 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4508991202685725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4508991202685725 | validation: 0.37025517514400874]
	TIME [epoch: 0.672 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4530288125753792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4530288125753792 | validation: 0.40615386998591385]
	TIME [epoch: 0.67 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3525417009248603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3525417009248603 | validation: 0.4627825827394455]
	TIME [epoch: 0.674 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3482074491137081		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3482074491137081 | validation: 0.3391898650844214]
	TIME [epoch: 0.673 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2856599740197644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2856599740197644 | validation: 0.285781094409251]
	TIME [epoch: 0.672 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_179.pth
	Model improved!!!
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2686515381084049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2686515381084049 | validation: 0.401045779981664]
	TIME [epoch: 0.673 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29189713616491825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29189713616491825 | validation: 0.25360421462279814]
	TIME [epoch: 0.663 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28682527314986644		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28682527314986644 | validation: 0.5043410659519666]
	TIME [epoch: 0.659 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4480386446792652		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4480386446792652 | validation: 0.49072233021354067]
	TIME [epoch: 0.662 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.517224116954917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.517224116954917 | validation: 0.44633220510048477]
	TIME [epoch: 0.659 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5628549820290654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5628549820290654 | validation: 0.5265355169299184]
	TIME [epoch: 0.662 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3991211174129133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3991211174129133 | validation: 0.5168422071602429]
	TIME [epoch: 0.658 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3572665277186215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3572665277186215 | validation: 0.35490892332452767]
	TIME [epoch: 0.658 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3290046294994636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3290046294994636 | validation: 0.3417325805667324]
	TIME [epoch: 0.653 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27041893860126287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27041893860126287 | validation: 0.334659656633183]
	TIME [epoch: 0.663 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2684964476215722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2684964476215722 | validation: 0.29952555087308097]
	TIME [epoch: 0.655 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.254119597286882		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.254119597286882 | validation: 0.34818553995322665]
	TIME [epoch: 0.661 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2636558906494626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2636558906494626 | validation: 0.34422037019530255]
	TIME [epoch: 0.655 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31264815208372404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31264815208372404 | validation: 0.39373818327080756]
	TIME [epoch: 0.661 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33029305697265515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33029305697265515 | validation: 0.3259242892542471]
	TIME [epoch: 0.659 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3091991652851396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3091991652851396 | validation: 0.38052967305475477]
	TIME [epoch: 0.658 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2707810649974382		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2707810649974382 | validation: 0.40662940247454343]
	TIME [epoch: 0.658 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36798596989863336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36798596989863336 | validation: 0.3075080165641979]
	TIME [epoch: 0.663 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22588868992102704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22588868992102704 | validation: 0.3094852210848002]
	TIME [epoch: 0.657 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20678776046385458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20678776046385458 | validation: 0.2914481545793654]
	TIME [epoch: 0.66 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2072663784816681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2072663784816681 | validation: 0.321498184373269]
	TIME [epoch: 0.657 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2493059587039326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2493059587039326 | validation: 0.45942809266225454]
	TIME [epoch: 165 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31673751961544977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31673751961544977 | validation: 0.5850360459756668]
	TIME [epoch: 1.33 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5689660572440155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5689660572440155 | validation: 0.5256129018763956]
	TIME [epoch: 1.32 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31480362026144093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31480362026144093 | validation: 0.3871238054185988]
	TIME [epoch: 1.32 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28281035275245286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28281035275245286 | validation: 0.31112810340267905]
	TIME [epoch: 1.32 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30173936267918755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30173936267918755 | validation: 0.2961690910823269]
	TIME [epoch: 1.32 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19068943752248835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19068943752248835 | validation: 0.27951167318048037]
	TIME [epoch: 1.36 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17820295875189815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17820295875189815 | validation: 0.27809231288804076]
	TIME [epoch: 1.31 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18978950953251275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18978950953251275 | validation: 0.2934606246198119]
	TIME [epoch: 1.32 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23587037323937415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23587037323937415 | validation: 0.4485049379266264]
	TIME [epoch: 1.31 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40066988232568834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40066988232568834 | validation: 0.23808023891086388]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18416682139373172		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18416682139373172 | validation: 0.34637679055309567]
	TIME [epoch: 1.31 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2031872524744794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2031872524744794 | validation: 0.285644305499583]
	TIME [epoch: 1.32 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22580860190842386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22580860190842386 | validation: 0.37389756997097157]
	TIME [epoch: 1.32 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3213835180780593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3213835180780593 | validation: 0.36671310290340653]
	TIME [epoch: 1.31 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3854014970604142		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3854014970604142 | validation: 0.4161732094856399]
	TIME [epoch: 1.31 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2754627219613523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2754627219613523 | validation: 0.37909518473293435]
	TIME [epoch: 1.31 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2088142363370744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2088142363370744 | validation: 0.24092385378735826]
	TIME [epoch: 1.31 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17732530687659265		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17732530687659265 | validation: 0.32325366297082225]
	TIME [epoch: 1.31 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18023176976363167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18023176976363167 | validation: 0.28612748838843255]
	TIME [epoch: 1.31 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1952126958851558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1952126958851558 | validation: 0.2577879579640347]
	TIME [epoch: 1.32 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2948943262831545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2948943262831545 | validation: 0.37787661640367776]
	TIME [epoch: 1.31 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2650762341236089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2650762341236089 | validation: 0.2911616575911661]
	TIME [epoch: 1.32 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19532131813466846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19532131813466846 | validation: 0.2548814419793766]
	TIME [epoch: 1.3 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21036437026698523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21036437026698523 | validation: 0.44150032121292954]
	TIME [epoch: 1.32 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2601988451271868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2601988451271868 | validation: 0.2982918517362427]
	TIME [epoch: 1.31 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1830852493793753		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1830852493793753 | validation: 0.2807495682765548]
	TIME [epoch: 1.32 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29676636932847905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29676636932847905 | validation: 0.4308455896115862]
	TIME [epoch: 1.31 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2187442442372821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2187442442372821 | validation: 0.2573643001911735]
	TIME [epoch: 1.31 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18653116513086773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18653116513086773 | validation: 0.2760254628305267]
	TIME [epoch: 1.3 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19290038627336223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19290038627336223 | validation: 0.23738398687878717]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_231.pth
	Model improved!!!
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17308985099960736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17308985099960736 | validation: 0.3618861737949978]
	TIME [epoch: 1.31 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21442834510546932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21442834510546932 | validation: 0.3553096109195548]
	TIME [epoch: 1.32 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27975537347195906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27975537347195906 | validation: 0.3304520011132317]
	TIME [epoch: 1.31 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2761835365083748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2761835365083748 | validation: 0.31257685756973297]
	TIME [epoch: 1.32 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1662433403056879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1662433403056879 | validation: 0.2071903476890583]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1495338086542699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1495338086542699 | validation: 0.31930396043534226]
	TIME [epoch: 1.32 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1572388755208047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1572388755208047 | validation: 0.27353916018179425]
	TIME [epoch: 1.32 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31492922338082635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.31492922338082635 | validation: 0.34945649513816157]
	TIME [epoch: 1.32 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18872919869953714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18872919869953714 | validation: 0.22606865086698888]
	TIME [epoch: 1.32 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13928610608426448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13928610608426448 | validation: 0.2161042317483017]
	TIME [epoch: 1.32 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1629640404071008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1629640404071008 | validation: 0.33506218866565995]
	TIME [epoch: 1.32 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2027253834155097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2027253834155097 | validation: 0.28221437026677726]
	TIME [epoch: 1.32 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21181126756637084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21181126756637084 | validation: 0.2391786803617174]
	TIME [epoch: 1.32 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17997228636600873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17997228636600873 | validation: 0.20279777908871927]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_245.pth
	Model improved!!!
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1222553727486593		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1222553727486593 | validation: 0.179179929333643]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_246.pth
	Model improved!!!
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11644249010759009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11644249010759009 | validation: 0.2530778593662746]
	TIME [epoch: 1.31 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.170022740999124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.170022740999124 | validation: 0.24111753066492547]
	TIME [epoch: 1.32 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23290009919307292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23290009919307292 | validation: 0.28666365134106647]
	TIME [epoch: 1.31 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20196454275944387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20196454275944387 | validation: 0.2501609375314377]
	TIME [epoch: 1.32 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12987741007680093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12987741007680093 | validation: 0.17695889566024747]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_251.pth
	Model improved!!!
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11932435134854079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11932435134854079 | validation: 0.21742427292955774]
	TIME [epoch: 1.32 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12902852200388126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12902852200388126 | validation: 0.2816875184633217]
	TIME [epoch: 1.32 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26225257971855453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26225257971855453 | validation: 0.42454556628588086]
	TIME [epoch: 1.32 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33790588386845477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33790588386845477 | validation: 0.35125067993366116]
	TIME [epoch: 1.31 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19639967685126852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19639967685126852 | validation: 0.16169932334759987]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_256.pth
	Model improved!!!
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10658359563137022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10658359563137022 | validation: 0.1891670461141136]
	TIME [epoch: 1.32 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09277746787363843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09277746787363843 | validation: 0.17948406886900556]
	TIME [epoch: 1.31 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11191467700130649		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11191467700130649 | validation: 0.34780542811078863]
	TIME [epoch: 1.32 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16830970655104857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16830970655104857 | validation: 0.24274000398420714]
	TIME [epoch: 1.31 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1933928556925772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1933928556925772 | validation: 0.350127684543315]
	TIME [epoch: 1.32 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1641562540276871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1641562540276871 | validation: 0.16349545138810356]
	TIME [epoch: 1.31 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11938796991426678		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11938796991426678 | validation: 0.2524989248503484]
	TIME [epoch: 1.32 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563050206936292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1563050206936292 | validation: 0.34136008087978453]
	TIME [epoch: 1.32 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22510909903550833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22510909903550833 | validation: 0.4297874375742736]
	TIME [epoch: 1.32 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33303572955541105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33303572955541105 | validation: 0.2806154826269391]
	TIME [epoch: 1.31 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25994478229688356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25994478229688356 | validation: 0.2549260915064284]
	TIME [epoch: 1.32 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21669694615627016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21669694615627016 | validation: 0.1870093683702604]
	TIME [epoch: 1.31 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13419788763482315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13419788763482315 | validation: 0.23938738581621335]
	TIME [epoch: 1.32 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16384842817621242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16384842817621242 | validation: 0.19563944418591636]
	TIME [epoch: 1.32 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973848071222692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0973848071222692 | validation: 0.1682971680336078]
	TIME [epoch: 1.31 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10205366218645406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10205366218645406 | validation: 0.17398642063049674]
	TIME [epoch: 1.31 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10906360564441886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10906360564441886 | validation: 0.2316649532747492]
	TIME [epoch: 1.31 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12414511055159279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12414511055159279 | validation: 0.2245039197022198]
	TIME [epoch: 1.31 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14451433102491937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14451433102491937 | validation: 0.39525346397536565]
	TIME [epoch: 1.32 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18076542205353355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18076542205353355 | validation: 0.14713143244338958]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_276.pth
	Model improved!!!
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10054596051028021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10054596051028021 | validation: 0.1540988533698501]
	TIME [epoch: 1.31 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0743075310151772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0743075310151772 | validation: 0.13662130015850651]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_278.pth
	Model improved!!!
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07853248081844877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07853248081844877 | validation: 0.17074474038267265]
	TIME [epoch: 1.33 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14075680637875582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14075680637875582 | validation: 0.3306435172777876]
	TIME [epoch: 1.33 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2872173145372555		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2872173145372555 | validation: 0.5372005270576627]
	TIME [epoch: 1.33 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4078615569579347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4078615569579347 | validation: 0.21471890322272047]
	TIME [epoch: 1.33 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1643106791040963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1643106791040963 | validation: 0.15139509812831586]
	TIME [epoch: 1.32 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09411976498970664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09411976498970664 | validation: 0.20468603834348317]
	TIME [epoch: 1.32 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11979074096221655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11979074096221655 | validation: 0.19113545826152978]
	TIME [epoch: 1.32 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10695552965807716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10695552965807716 | validation: 0.14776661024345816]
	TIME [epoch: 1.33 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0957875765734986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0957875765734986 | validation: 0.15410306710890984]
	TIME [epoch: 1.31 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07854924500016174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07854924500016174 | validation: 0.13173074751092687]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07582382092321086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07582382092321086 | validation: 0.18067070763516457]
	TIME [epoch: 1.33 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0867977379155189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0867977379155189 | validation: 0.15074349436620127]
	TIME [epoch: 1.33 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12864184914744312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12864184914744312 | validation: 0.39131330607745785]
	TIME [epoch: 1.32 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22858245271296646		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22858245271296646 | validation: 0.1327132380463788]
	TIME [epoch: 1.33 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11351552155448667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11351552155448667 | validation: 0.12972208971805518]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_293.pth
	Model improved!!!
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0909715356814574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0909715356814574 | validation: 0.24674223014711194]
	TIME [epoch: 1.32 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16050541619113687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16050541619113687 | validation: 0.3157565246594372]
	TIME [epoch: 1.32 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24683706424485816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24683706424485816 | validation: 0.33520640316012323]
	TIME [epoch: 1.32 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2481918143133227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2481918143133227 | validation: 0.24907636997284854]
	TIME [epoch: 1.32 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16632969879606202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16632969879606202 | validation: 0.13488710377958468]
	TIME [epoch: 1.32 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10452507411920635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10452507411920635 | validation: 0.14530285630989415]
	TIME [epoch: 1.32 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11360649189178043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11360649189178043 | validation: 0.12446534513380708]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0901821106953943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0901821106953943 | validation: 0.15734089393941897]
	TIME [epoch: 1.31 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08880701211371246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08880701211371246 | validation: 0.1696713653663986]
	TIME [epoch: 1.31 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10660318660020743		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10660318660020743 | validation: 0.1340877762951063]
	TIME [epoch: 1.31 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09220982711352672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09220982711352672 | validation: 0.1719403417867183]
	TIME [epoch: 1.31 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09993072561724989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09993072561724989 | validation: 0.15519002168563767]
	TIME [epoch: 1.31 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11683511865591363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11683511865591363 | validation: 0.20131866138199192]
	TIME [epoch: 1.3 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13572189202965443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13572189202965443 | validation: 0.16879889712582966]
	TIME [epoch: 1.31 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11130633154245959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11130633154245959 | validation: 0.1973506591359102]
	TIME [epoch: 1.31 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.128052673729165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.128052673729165 | validation: 0.38312963474836187]
	TIME [epoch: 1.31 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21024686076608823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21024686076608823 | validation: 0.16782503020821685]
	TIME [epoch: 1.31 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13178112867425962		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13178112867425962 | validation: 0.17061699779197181]
	TIME [epoch: 1.31 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08815076894528867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08815076894528867 | validation: 0.10037888415550843]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08151118953875024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08151118953875024 | validation: 0.14133737583345524]
	TIME [epoch: 1.32 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10043056882536945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10043056882536945 | validation: 0.13630114058698056]
	TIME [epoch: 1.32 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10967994663826047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10967994663826047 | validation: 0.1679467493248216]
	TIME [epoch: 1.32 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.113133228337125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.113133228337125 | validation: 0.17881850246890452]
	TIME [epoch: 1.32 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10238522519251682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10238522519251682 | validation: 0.20670889020493252]
	TIME [epoch: 1.31 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12661445522481724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12661445522481724 | validation: 0.22558489471289295]
	TIME [epoch: 1.32 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.209764949943048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.209764949943048 | validation: 0.32521576154894016]
	TIME [epoch: 1.32 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24171087936791572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24171087936791572 | validation: 0.19482774033837214]
	TIME [epoch: 1.32 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12032342747765082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12032342747765082 | validation: 0.20338574675649745]
	TIME [epoch: 1.31 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11757040854235137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11757040854235137 | validation: 0.16182491754125053]
	TIME [epoch: 1.31 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10739459237246735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10739459237246735 | validation: 0.24867306008830822]
	TIME [epoch: 1.32 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11267036429710892		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11267036429710892 | validation: 0.12920196027009653]
	TIME [epoch: 1.31 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.077088450380208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.077088450380208 | validation: 0.11573234965128121]
	TIME [epoch: 1.32 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08928605153307086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08928605153307086 | validation: 0.17402143452920035]
	TIME [epoch: 1.32 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08893960866095825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08893960866095825 | validation: 0.19201093363345512]
	TIME [epoch: 1.32 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10089914009121742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10089914009121742 | validation: 0.12869314453885713]
	TIME [epoch: 1.32 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07033104095682688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07033104095682688 | validation: 0.09241117905941128]
	TIME [epoch: 1.32 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_329.pth
	Model improved!!!
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07013951900260479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07013951900260479 | validation: 0.1495639399189589]
	TIME [epoch: 1.33 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11337549789733886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11337549789733886 | validation: 0.1360025891648105]
	TIME [epoch: 1.33 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284889681910761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1284889681910761 | validation: 0.26719777145438917]
	TIME [epoch: 1.33 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1375959052342569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1375959052342569 | validation: 0.25598679921337436]
	TIME [epoch: 1.33 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18446564332808543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18446564332808543 | validation: 0.2746472757979024]
	TIME [epoch: 1.33 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23952397675280931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23952397675280931 | validation: 0.26482919939507504]
	TIME [epoch: 1.33 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16296212606554406		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16296212606554406 | validation: 0.10138714939385436]
	TIME [epoch: 1.32 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06391387919089946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06391387919089946 | validation: 0.1228202928018603]
	TIME [epoch: 1.33 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057468642846563604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057468642846563604 | validation: 0.1348431615268382]
	TIME [epoch: 1.33 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06604275615164198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06604275615164198 | validation: 0.14205017810103113]
	TIME [epoch: 1.33 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07167941664909748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07167941664909748 | validation: 0.15973191134019507]
	TIME [epoch: 1.32 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09565967989056094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09565967989056094 | validation: 0.24562252440386823]
	TIME [epoch: 1.32 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.129089427796434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.129089427796434 | validation: 0.151157127467516]
	TIME [epoch: 1.32 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10723943358837111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10723943358837111 | validation: 0.1686803522032756]
	TIME [epoch: 1.32 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13124184053043653		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13124184053043653 | validation: 0.1830284255649019]
	TIME [epoch: 1.32 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13508041287419226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13508041287419226 | validation: 0.14978677192589177]
	TIME [epoch: 1.32 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12673156915854622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12673156915854622 | validation: 0.21316488432895075]
	TIME [epoch: 1.33 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13428717845550486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13428717845550486 | validation: 0.1357378101904942]
	TIME [epoch: 1.32 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11742599290877387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11742599290877387 | validation: 0.16052974416594828]
	TIME [epoch: 1.32 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11949584774696477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11949584774696477 | validation: 0.19798516244210873]
	TIME [epoch: 1.33 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1041960588743106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1041960588743106 | validation: 0.12774519334148415]
	TIME [epoch: 1.32 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714422160819663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0714422160819663 | validation: 0.09545992853462047]
	TIME [epoch: 1.32 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0550599404050079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0550599404050079 | validation: 0.08397501319614414]
	TIME [epoch: 1.33 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_352.pth
	Model improved!!!
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06940979826066249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06940979826066249 | validation: 0.12585293571566783]
	TIME [epoch: 1.31 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10397736159947893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10397736159947893 | validation: 0.11357830068211357]
	TIME [epoch: 1.31 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12014726056984001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12014726056984001 | validation: 0.13666610537488455]
	TIME [epoch: 1.31 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0784825170496302		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0784825170496302 | validation: 0.12668922413512596]
	TIME [epoch: 1.31 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08023270072757932		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08023270072757932 | validation: 0.19039630848083222]
	TIME [epoch: 1.31 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1420145229912559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1420145229912559 | validation: 0.2338320084780067]
	TIME [epoch: 1.31 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2113054228507445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2113054228507445 | validation: 0.18609707344846285]
	TIME [epoch: 1.31 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1293577381672794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1293577381672794 | validation: 0.1442720738138104]
	TIME [epoch: 1.31 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08234263517463478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08234263517463478 | validation: 0.20457671273714287]
	TIME [epoch: 1.31 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10215398472138222		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10215398472138222 | validation: 0.16094781053842527]
	TIME [epoch: 1.32 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1286634110600931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1286634110600931 | validation: 0.22457027702306617]
	TIME [epoch: 1.3 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17530054165991488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17530054165991488 | validation: 0.09379593130000603]
	TIME [epoch: 1.3 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06775910500849412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06775910500849412 | validation: 0.10708432908151232]
	TIME [epoch: 1.31 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057226982072842124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057226982072842124 | validation: 0.1504995501204028]
	TIME [epoch: 1.31 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815755418230597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0815755418230597 | validation: 0.17207644463996474]
	TIME [epoch: 1.31 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10339196202880885		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10339196202880885 | validation: 0.1579143297490755]
	TIME [epoch: 1.31 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10394045027280559		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10394045027280559 | validation: 0.13183770269609482]
	TIME [epoch: 1.31 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08505902402516093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08505902402516093 | validation: 0.157976339993803]
	TIME [epoch: 1.31 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08888998511155752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08888998511155752 | validation: 0.1799327459357436]
	TIME [epoch: 1.31 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12647790599785583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12647790599785583 | validation: 0.2791770395702719]
	TIME [epoch: 1.31 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14281535704856843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14281535704856843 | validation: 0.13680615834315743]
	TIME [epoch: 1.3 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07559807813463955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07559807813463955 | validation: 0.09198376082061255]
	TIME [epoch: 1.31 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07994221796606897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07994221796606897 | validation: 0.12526344184709012]
	TIME [epoch: 1.32 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07106716823468952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07106716823468952 | validation: 0.08559603105931968]
	TIME [epoch: 1.32 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06607153098630617		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06607153098630617 | validation: 0.08360065324912018]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06515357405919672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06515357405919672 | validation: 0.11330414082606328]
	TIME [epoch: 1.33 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07207208180508407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07207208180508407 | validation: 0.1117991232034818]
	TIME [epoch: 1.33 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06730137849668558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06730137849668558 | validation: 0.10912035238100104]
	TIME [epoch: 1.33 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062275844385803476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.062275844385803476 | validation: 0.11048106276032735]
	TIME [epoch: 1.33 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06020596354674611		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06020596354674611 | validation: 0.15252582523166414]
	TIME [epoch: 1.33 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10356941496877055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10356941496877055 | validation: 0.33844183207951883]
	TIME [epoch: 1.33 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30471971613469123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30471971613469123 | validation: 0.26221602652684944]
	TIME [epoch: 1.33 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18238703337633552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18238703337633552 | validation: 0.13383826432893176]
	TIME [epoch: 1.33 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12132268777801816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12132268777801816 | validation: 0.16953187716417412]
	TIME [epoch: 1.32 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09756398797704333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09756398797704333 | validation: 0.10306060222615798]
	TIME [epoch: 1.32 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060424450094199104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060424450094199104 | validation: 0.11753229783137487]
	TIME [epoch: 1.33 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05446550070526387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05446550070526387 | validation: 0.08954924431201222]
	TIME [epoch: 1.32 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05215942927525842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05215942927525842 | validation: 0.10339884559737139]
	TIME [epoch: 1.32 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05312411975946624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05312411975946624 | validation: 0.0899072825083735]
	TIME [epoch: 1.33 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05812126218682028		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05812126218682028 | validation: 0.14366198636705182]
	TIME [epoch: 1.32 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867456252942849		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08867456252942849 | validation: 0.1590894837286031]
	TIME [epoch: 1.33 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14792314359488046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14792314359488046 | validation: 0.1820560882325742]
	TIME [epoch: 1.33 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13680374980732407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13680374980732407 | validation: 0.28722959544218213]
	TIME [epoch: 1.33 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145294092802976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.145294092802976 | validation: 0.10665462872532701]
	TIME [epoch: 1.32 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06392880185784963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06392880185784963 | validation: 0.10533446456657158]
	TIME [epoch: 1.33 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1018938088180844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1018938088180844 | validation: 0.19955263923635674]
	TIME [epoch: 1.32 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15109455123287632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15109455123287632 | validation: 0.16552506164255298]
	TIME [epoch: 1.32 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12338570533946432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12338570533946432 | validation: 0.15967641211821396]
	TIME [epoch: 1.33 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10266542484383852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10266542484383852 | validation: 0.20897924088669942]
	TIME [epoch: 1.31 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12426639031463299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12426639031463299 | validation: 0.129115296282332]
	TIME [epoch: 1.31 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08216612321698545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08216612321698545 | validation: 0.08089753146640805]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_403.pth
	Model improved!!!
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637171251122461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08637171251122461 | validation: 0.1270723826976997]
	TIME [epoch: 1.3 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752236335950137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0752236335950137 | validation: 0.12130625263936344]
	TIME [epoch: 1.31 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06886609541848536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06886609541848536 | validation: 0.12254328913887456]
	TIME [epoch: 1.31 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0693483638162291		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0693483638162291 | validation: 0.12588927148093934]
	TIME [epoch: 1.31 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06937503146494145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06937503146494145 | validation: 0.1113018188035065]
	TIME [epoch: 1.31 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06994033489877323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06994033489877323 | validation: 0.23547392069747775]
	TIME [epoch: 1.31 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19621217915670322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19621217915670322 | validation: 0.36129126611638246]
	TIME [epoch: 1.31 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19950173124242465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19950173124242465 | validation: 0.43058129470236894]
	TIME [epoch: 1.31 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23654762493521658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23654762493521658 | validation: 0.32973091914682384]
	TIME [epoch: 1.3 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22349035252642557		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22349035252642557 | validation: 0.24459349662499968]
	TIME [epoch: 1.31 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23558219256535126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23558219256535126 | validation: 0.2284289319239648]
	TIME [epoch: 1.31 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15967842611730562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15967842611730562 | validation: 0.1845145037027074]
	TIME [epoch: 1.31 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13747411043129723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13747411043129723 | validation: 0.12592755303748912]
	TIME [epoch: 1.31 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103542801117785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07103542801117785 | validation: 0.10377607808953093]
	TIME [epoch: 1.31 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06197618225399576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06197618225399576 | validation: 0.08963867069889371]
	TIME [epoch: 1.31 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05511221023004598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05511221023004598 | validation: 0.08786262892320834]
	TIME [epoch: 1.32 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05375646746139369		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05375646746139369 | validation: 0.10196097144003531]
	TIME [epoch: 1.31 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0577812986023023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0577812986023023 | validation: 0.11642591962059577]
	TIME [epoch: 1.31 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631838556187864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0631838556187864 | validation: 0.10376590431392974]
	TIME [epoch: 1.31 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06656267945361813		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06656267945361813 | validation: 0.1028761808143627]
	TIME [epoch: 1.31 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07045141920210737		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07045141920210737 | validation: 0.10511258642762059]
	TIME [epoch: 1.31 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06673276247350034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06673276247350034 | validation: 0.11174703263240714]
	TIME [epoch: 1.31 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07030582201460982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07030582201460982 | validation: 0.10843562589605349]
	TIME [epoch: 1.31 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09157809482534816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09157809482534816 | validation: 0.1771167381391152]
	TIME [epoch: 1.31 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12177528609601519		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12177528609601519 | validation: 0.13106789829150364]
	TIME [epoch: 1.31 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09165024157163117		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09165024157163117 | validation: 0.10086012114514249]
	TIME [epoch: 1.31 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057560873634359785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057560873634359785 | validation: 0.08734647442147175]
	TIME [epoch: 1.31 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057446109683989006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057446109683989006 | validation: 0.11648663827980946]
	TIME [epoch: 1.31 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707980635154733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707980635154733 | validation: 0.13556007028192268]
	TIME [epoch: 1.31 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13212798946256105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13212798946256105 | validation: 0.12495842155608211]
	TIME [epoch: 1.31 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09155054377552622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09155054377552622 | validation: 0.08044079209977771]
	TIME [epoch: 1.3 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_434.pth
	Model improved!!!
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04889416483204095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04889416483204095 | validation: 0.07364788805301693]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_435.pth
	Model improved!!!
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04444969615492105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04444969615492105 | validation: 0.10162341528408106]
	TIME [epoch: 1.31 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05342931344715037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05342931344715037 | validation: 0.09950714924294636]
	TIME [epoch: 1.31 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06639326758519833		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06639326758519833 | validation: 0.15310199409640096]
	TIME [epoch: 1.31 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14349985093380793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14349985093380793 | validation: 0.275759815641991]
	TIME [epoch: 1.31 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21874086729754968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21874086729754968 | validation: 0.16984639239561572]
	TIME [epoch: 1.31 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11559643995216029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11559643995216029 | validation: 0.10237454725678188]
	TIME [epoch: 1.31 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051885006452021896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051885006452021896 | validation: 0.07659219055818452]
	TIME [epoch: 1.31 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05711712458517055		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05711712458517055 | validation: 0.08892456565524079]
	TIME [epoch: 1.31 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06947406092506608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06947406092506608 | validation: 0.09425817697830312]
	TIME [epoch: 1.31 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07415965878481073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07415965878481073 | validation: 0.09206516517796592]
	TIME [epoch: 1.31 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05687294296107722		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05687294296107722 | validation: 0.08413517989835102]
	TIME [epoch: 1.31 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052214205025820144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052214205025820144 | validation: 0.1042352917480808]
	TIME [epoch: 1.31 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050521763686036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050521763686036 | validation: 0.08390379280027016]
	TIME [epoch: 1.31 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04390713426128489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04390713426128489 | validation: 0.10638365781021726]
	TIME [epoch: 1.31 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06452915418340645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06452915418340645 | validation: 0.13334215694988866]
	TIME [epoch: 1.31 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08080637695020389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08080637695020389 | validation: 0.1557710968463776]
	TIME [epoch: 1.31 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10358827072634377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10358827072634377 | validation: 0.15064655763522072]
	TIME [epoch: 1.31 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12477244886894454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12477244886894454 | validation: 0.2057486977256943]
	TIME [epoch: 1.31 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1520883113601787		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1520883113601787 | validation: 0.20707573534955262]
	TIME [epoch: 1.31 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1312956756905586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1312956756905586 | validation: 0.10353127005339902]
	TIME [epoch: 1.31 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05502668630964484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05502668630964484 | validation: 0.08720632997898725]
	TIME [epoch: 1.31 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04479963135491511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04479963135491511 | validation: 0.06850215546986019]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603788169241805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03603788169241805 | validation: 0.07097769417844856]
	TIME [epoch: 1.31 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.031177052940736496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.031177052940736496 | validation: 0.060183465793602835]
	TIME [epoch: 1.31 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_459.pth
	Model improved!!!
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032241213221377685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.032241213221377685 | validation: 0.07335443017194936]
	TIME [epoch: 1.32 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03755543764939193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03755543764939193 | validation: 0.09502370937580318]
	TIME [epoch: 1.31 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0584767420872979		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0584767420872979 | validation: 0.15820150217593845]
	TIME [epoch: 1.31 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1149332320514318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1149332320514318 | validation: 0.16843995247835786]
	TIME [epoch: 1.31 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13564444181579785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13564444181579785 | validation: 0.09688104861373993]
	TIME [epoch: 1.31 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1230648461295944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1230648461295944 | validation: 0.12768869927691337]
	TIME [epoch: 1.31 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11761226275424282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11761226275424282 | validation: 0.22572913721675728]
	TIME [epoch: 1.31 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10928442844019175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10928442844019175 | validation: 0.11892463144433921]
	TIME [epoch: 1.31 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0902084807238135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0902084807238135 | validation: 0.15854744995339448]
	TIME [epoch: 1.31 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2091335370855336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2091335370855336 | validation: 0.18705963659489183]
	TIME [epoch: 1.31 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09942319531029824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09942319531029824 | validation: 0.13030614211487365]
	TIME [epoch: 1.31 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09837635279597576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09837635279597576 | validation: 0.17098314488875035]
	TIME [epoch: 1.31 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11564575971404287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11564575971404287 | validation: 0.24442469652467214]
	TIME [epoch: 1.31 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20250426193956259		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20250426193956259 | validation: 0.1526915383759947]
	TIME [epoch: 1.31 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792732602100469		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07792732602100469 | validation: 0.09641990781634285]
	TIME [epoch: 1.31 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05886867689694981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05886867689694981 | validation: 0.08079272084201006]
	TIME [epoch: 1.31 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04793745753262034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04793745753262034 | validation: 0.08247730505348239]
	TIME [epoch: 1.31 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04507378509018774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04507378509018774 | validation: 0.076276668212324]
	TIME [epoch: 1.31 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06336991765503198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06336991765503198 | validation: 0.10391111630088437]
	TIME [epoch: 1.31 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06462370979706844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06462370979706844 | validation: 0.06346538555882585]
	TIME [epoch: 1.31 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05351339270557991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05351339270557991 | validation: 0.0843991632732084]
	TIME [epoch: 1.31 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04549429350392676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04549429350392676 | validation: 0.07957863536071685]
	TIME [epoch: 1.31 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04760910233094634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04760910233094634 | validation: 0.09216838414571485]
	TIME [epoch: 1.31 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04860856577952782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04860856577952782 | validation: 0.10581969740057683]
	TIME [epoch: 1.32 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06920399865038562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06920399865038562 | validation: 0.13521523069767158]
	TIME [epoch: 1.32 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09257636148452263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09257636148452263 | validation: 0.11295510105656226]
	TIME [epoch: 1.32 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08735073374024221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08735073374024221 | validation: 0.1110029207633334]
	TIME [epoch: 1.32 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09400942658640467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09400942658640467 | validation: 0.16466207864116097]
	TIME [epoch: 1.32 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12061434607805478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12061434607805478 | validation: 0.21222354803133828]
	TIME [epoch: 1.32 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13077276423633605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13077276423633605 | validation: 0.19034878818112652]
	TIME [epoch: 1.32 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11460069820279857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11460069820279857 | validation: 0.13081510836110594]
	TIME [epoch: 1.32 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08697593139910835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08697593139910835 | validation: 0.12002321159936512]
	TIME [epoch: 1.31 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08057707420230958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08057707420230958 | validation: 0.16496258156833285]
	TIME [epoch: 1.32 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10056727292612241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10056727292612241 | validation: 0.14537252328462094]
	TIME [epoch: 1.32 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1110518583752624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1110518583752624 | validation: 0.15238341725965163]
	TIME [epoch: 1.31 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0826615483033929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0826615483033929 | validation: 0.11038927341956029]
	TIME [epoch: 1.31 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0590385222861792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0590385222861792 | validation: 0.0816435743658887]
	TIME [epoch: 1.32 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05461975842044067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05461975842044067 | validation: 0.10257633220222968]
	TIME [epoch: 1.32 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05455518987675383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05455518987675383 | validation: 0.07071989106628052]
	TIME [epoch: 1.32 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05317434706430488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05317434706430488 | validation: 0.08963730829848679]
	TIME [epoch: 1.32 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06119701699144654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06119701699144654 | validation: 0.06775496453299766]
	TIME [epoch: 1.31 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06828629923620108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06828629923620108 | validation: 0.07643396283814578]
	TIME [epoch: 169 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04906890768366875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04906890768366875 | validation: 0.05292978221935544]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_502.pth
	Model improved!!!
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03623560460379796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03623560460379796 | validation: 0.07899855083908469]
	TIME [epoch: 2.6 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036292820851768146		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036292820851768146 | validation: 0.07756813704505824]
	TIME [epoch: 2.61 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04433273232755583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04433273232755583 | validation: 0.12592846229728807]
	TIME [epoch: 2.61 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07592480491081333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07592480491081333 | validation: 0.19805142241447096]
	TIME [epoch: 2.61 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17368515323661635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17368515323661635 | validation: 0.20636018779940418]
	TIME [epoch: 2.59 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1969785892027636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1969785892027636 | validation: 0.17014857315024573]
	TIME [epoch: 2.6 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1426623160747271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1426623160747271 | validation: 0.16950381835197398]
	TIME [epoch: 2.61 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08369828032676158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08369828032676158 | validation: 0.07898998715478568]
	TIME [epoch: 2.61 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05013828132528188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05013828132528188 | validation: 0.05944574716074707]
	TIME [epoch: 2.59 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04592568348641638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04592568348641638 | validation: 0.07222887483573735]
	TIME [epoch: 2.59 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04869967155711507		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04869967155711507 | validation: 0.0670516684974214]
	TIME [epoch: 2.59 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04019360949749721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04019360949749721 | validation: 0.0568133948034233]
	TIME [epoch: 2.59 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038282757377660855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.038282757377660855 | validation: 0.0686495010053984]
	TIME [epoch: 2.59 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03866666561764657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03866666561764657 | validation: 0.06637273229589565]
	TIME [epoch: 2.59 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042086666014386584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042086666014386584 | validation: 0.08756698202615809]
	TIME [epoch: 2.58 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053936438685381345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053936438685381345 | validation: 0.11236996281436339]
	TIME [epoch: 2.59 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06867723016624827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06867723016624827 | validation: 0.13094084123436095]
	TIME [epoch: 2.6 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09479598985971016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09479598985971016 | validation: 0.14488534138393694]
	TIME [epoch: 2.61 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10783584186630356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10783584186630356 | validation: 0.12460687188880558]
	TIME [epoch: 2.59 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12329539670052463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12329539670052463 | validation: 0.10131462366417923]
	TIME [epoch: 2.6 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06276921223551601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06276921223551601 | validation: 0.1412190939712558]
	TIME [epoch: 2.6 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09313552348704972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09313552348704972 | validation: 0.187611131213397]
	TIME [epoch: 2.6 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11235608487090952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11235608487090952 | validation: 0.18557329679276915]
	TIME [epoch: 2.6 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13570099171877353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13570099171877353 | validation: 0.12378525335476494]
	TIME [epoch: 2.61 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11254828650244342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11254828650244342 | validation: 0.10881593816365576]
	TIME [epoch: 2.6 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07451802428880547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07451802428880547 | validation: 0.09749370041092276]
	TIME [epoch: 2.61 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03916988912667573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03916988912667573 | validation: 0.06732541667415413]
	TIME [epoch: 2.6 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04542933141577359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04542933141577359 | validation: 0.11572557015777188]
	TIME [epoch: 2.59 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06252700836358434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06252700836358434 | validation: 0.1244203482968449]
	TIME [epoch: 2.59 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08829994494375505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08829994494375505 | validation: 0.1561070855150864]
	TIME [epoch: 2.6 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09355955340887845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09355955340887845 | validation: 0.07516028602162173]
	TIME [epoch: 2.6 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141877722955724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05141877722955724 | validation: 0.07297017372934128]
	TIME [epoch: 2.61 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037380475709480776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037380475709480776 | validation: 0.07772756426881551]
	TIME [epoch: 2.6 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476145823206158		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476145823206158 | validation: 0.09755817995746636]
	TIME [epoch: 2.6 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0584734831735381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0584734831735381 | validation: 0.10257908234626414]
	TIME [epoch: 2.6 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08346468977963609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08346468977963609 | validation: 0.14089862591945862]
	TIME [epoch: 2.6 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10337806379407725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10337806379407725 | validation: 0.1000340528348208]
	TIME [epoch: 2.59 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0725352469019963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0725352469019963 | validation: 0.08339996417387563]
	TIME [epoch: 2.6 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054766998180209935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054766998180209935 | validation: 0.0921112172540795]
	TIME [epoch: 2.59 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486471123068438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0486471123068438 | validation: 0.06885495901886853]
	TIME [epoch: 2.59 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03855950872656083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03855950872656083 | validation: 0.07473766011045574]
	TIME [epoch: 2.59 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04079029315855685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04079029315855685 | validation: 0.09800622515450777]
	TIME [epoch: 2.61 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052999926961528374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052999926961528374 | validation: 0.09781457284596601]
	TIME [epoch: 2.6 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07298479061615261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07298479061615261 | validation: 0.11293269843722374]
	TIME [epoch: 2.6 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09151519284545007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09151519284545007 | validation: 0.1172904941080982]
	TIME [epoch: 2.61 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11762477993429152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11762477993429152 | validation: 0.1168912390661427]
	TIME [epoch: 2.58 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09957868577479854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09957868577479854 | validation: 0.16255390555276727]
	TIME [epoch: 2.59 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0792808814262982		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0792808814262982 | validation: 0.1221873645246931]
	TIME [epoch: 2.6 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07874075720812478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07874075720812478 | validation: 0.14266941731959634]
	TIME [epoch: 2.6 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1225648537258841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1225648537258841 | validation: 0.09795565423518297]
	TIME [epoch: 2.6 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09276794683496441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09276794683496441 | validation: 0.1316603532389491]
	TIME [epoch: 2.61 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07750280794410078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07750280794410078 | validation: 0.10803072579893609]
	TIME [epoch: 2.59 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707910413288578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707910413288578 | validation: 0.12138425654014413]
	TIME [epoch: 2.6 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061545793223380586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061545793223380586 | validation: 0.054794984413926585]
	TIME [epoch: 2.59 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0383895597354823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0383895597354823 | validation: 0.06284517507465791]
	TIME [epoch: 2.61 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03308393880780476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03308393880780476 | validation: 0.05934368163592573]
	TIME [epoch: 2.61 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030930368205421714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030930368205421714 | validation: 0.05479750411840804]
	TIME [epoch: 2.59 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03217783201611196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03217783201611196 | validation: 0.06000362017011396]
	TIME [epoch: 2.6 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03572110658038088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03572110658038088 | validation: 0.0745277181503272]
	TIME [epoch: 2.59 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05181302212656867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05181302212656867 | validation: 0.09389762833691152]
	TIME [epoch: 2.6 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08296920680813522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08296920680813522 | validation: 0.12289455648033415]
	TIME [epoch: 2.6 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09460237057300189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09460237057300189 | validation: 0.10756240659749972]
	TIME [epoch: 2.61 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06956133731446323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06956133731446323 | validation: 0.08221033052418825]
	TIME [epoch: 2.59 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05763428031907113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05763428031907113 | validation: 0.12981631843729577]
	TIME [epoch: 2.61 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09304692576579182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09304692576579182 | validation: 0.12771027935054952]
	TIME [epoch: 2.58 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11046528844686793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11046528844686793 | validation: 0.09151759797613618]
	TIME [epoch: 2.6 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05913988610247442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05913988610247442 | validation: 0.04877509938692015]
	TIME [epoch: 2.6 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_569.pth
	Model improved!!!
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05507491341126563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05507491341126563 | validation: 0.1172422505537895]
	TIME [epoch: 2.6 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08430076448681989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08430076448681989 | validation: 0.17624555159785904]
	TIME [epoch: 2.6 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15638126008107067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15638126008107067 | validation: 0.23454073743230014]
	TIME [epoch: 2.61 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17667067209766604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17667067209766604 | validation: 0.057153719688065555]
	TIME [epoch: 2.61 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03406727401078933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03406727401078933 | validation: 0.06364736249858291]
	TIME [epoch: 2.59 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047024846695557974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047024846695557974 | validation: 0.09407162758113023]
	TIME [epoch: 2.61 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04004471031348295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04004471031348295 | validation: 0.05837921258572748]
	TIME [epoch: 2.6 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03600928100536842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03600928100536842 | validation: 0.08128574294691872]
	TIME [epoch: 2.61 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03954726962937509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03954726962937509 | validation: 0.1047187306971221]
	TIME [epoch: 2.61 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0649966863748854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0649966863748854 | validation: 0.19764216949632193]
	TIME [epoch: 2.61 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1022130781169161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1022130781169161 | validation: 0.1453776744384061]
	TIME [epoch: 2.6 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09286881839407603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09286881839407603 | validation: 0.13335606184304827]
	TIME [epoch: 2.6 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10057254167263208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10057254167263208 | validation: 0.14316471108798695]
	TIME [epoch: 2.59 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13499194657647048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13499194657647048 | validation: 0.17359996653083157]
	TIME [epoch: 2.6 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12004451239643993		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12004451239643993 | validation: 0.13378915544790262]
	TIME [epoch: 2.61 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09676678525111548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09676678525111548 | validation: 0.10124500260433894]
	TIME [epoch: 2.61 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06558241714072616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06558241714072616 | validation: 0.07081712907057049]
	TIME [epoch: 2.59 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04537539283016605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04537539283016605 | validation: 0.06375580248099944]
	TIME [epoch: 2.61 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04692629889270996		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04692629889270996 | validation: 0.08262974607719137]
	TIME [epoch: 2.59 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03525845706923323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03525845706923323 | validation: 0.05178970370486577]
	TIME [epoch: 2.6 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030904416188329505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030904416188329505 | validation: 0.06555604857344295]
	TIME [epoch: 2.6 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028560345114598037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028560345114598037 | validation: 0.05449967256234215]
	TIME [epoch: 2.61 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03111799911338582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03111799911338582 | validation: 0.07660581659722952]
	TIME [epoch: 2.59 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037098142374982254		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037098142374982254 | validation: 0.0793859428419997]
	TIME [epoch: 2.61 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05901677275896249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05901677275896249 | validation: 0.1412361117411336]
	TIME [epoch: 2.61 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09375393562036097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09375393562036097 | validation: 0.10447524853261606]
	TIME [epoch: 2.61 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07059896385669853		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07059896385669853 | validation: 0.09633151397056611]
	TIME [epoch: 2.62 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04817420078388336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04817420078388336 | validation: 0.05566929975786381]
	TIME [epoch: 2.62 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05201855884670309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05201855884670309 | validation: 0.1138618711009321]
	TIME [epoch: 2.61 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0973855023378074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0973855023378074 | validation: 0.17737351131743653]
	TIME [epoch: 2.61 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1638681517082989		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1638681517082989 | validation: 0.08308172203344288]
	TIME [epoch: 2.61 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06348239359344875		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06348239359344875 | validation: 0.0744003446726515]
	TIME [epoch: 2.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07293186991634729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07293186991634729 | validation: 0.11374579579717313]
	TIME [epoch: 2.61 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08317716788780123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08317716788780123 | validation: 0.12720725802678243]
	TIME [epoch: 2.61 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07790632683291769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07790632683291769 | validation: 0.08264287775571785]
	TIME [epoch: 2.6 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04484176846973305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04484176846973305 | validation: 0.04929971061219199]
	TIME [epoch: 2.6 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03616463932573895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03616463932573895 | validation: 0.06934347217815183]
	TIME [epoch: 2.61 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03481944304050897		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03481944304050897 | validation: 0.06567764942968773]
	TIME [epoch: 2.61 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043250226504043185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.043250226504043185 | validation: 0.09101354164593835]
	TIME [epoch: 2.6 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06143633623940223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06143633623940223 | validation: 0.0839726911609252]
	TIME [epoch: 2.61 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07275616355266187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07275616355266187 | validation: 0.08332822836114845]
	TIME [epoch: 2.6 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07855541828062762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07855541828062762 | validation: 0.07304707854088187]
	TIME [epoch: 2.61 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06653400739503958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06653400739503958 | validation: 0.12027899446228739]
	TIME [epoch: 2.6 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08859688794156918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08859688794156918 | validation: 0.12338835563106731]
	TIME [epoch: 2.6 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07873823163108093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07873823163108093 | validation: 0.08509635536270026]
	TIME [epoch: 2.59 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04770882071425236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04770882071425236 | validation: 0.13685360378526165]
	TIME [epoch: 2.6 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08532695717058843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08532695717058843 | validation: 0.18601361290418064]
	TIME [epoch: 2.59 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0942698548378531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0942698548378531 | validation: 0.15262039656608317]
	TIME [epoch: 2.6 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08842113938486174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08842113938486174 | validation: 0.1337088616479406]
	TIME [epoch: 2.59 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1141496504462409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1141496504462409 | validation: 0.10947648207024888]
	TIME [epoch: 2.61 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08344101533667654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08344101533667654 | validation: 0.06751257377784134]
	TIME [epoch: 2.6 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036025207040750046		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.036025207040750046 | validation: 0.06220390986123303]
	TIME [epoch: 2.6 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03102505978614946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03102505978614946 | validation: 0.05733397929256401]
	TIME [epoch: 2.61 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03429199179507741		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03429199179507741 | validation: 0.07427261839821836]
	TIME [epoch: 2.61 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04159855588334577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04159855588334577 | validation: 0.0806144419251941]
	TIME [epoch: 2.6 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06036296424877624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06036296424877624 | validation: 0.16445235265283778]
	TIME [epoch: 2.6 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10633316606714263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10633316606714263 | validation: 0.12233381841787427]
	TIME [epoch: 2.61 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09737482066317679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09737482066317679 | validation: 0.08254815321210345]
	TIME [epoch: 2.61 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06572844573855716		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06572844573855716 | validation: 0.0732947466482556]
	TIME [epoch: 2.6 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05356671352242343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05356671352242343 | validation: 0.07769862799308447]
	TIME [epoch: 2.6 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05410493323757264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05410493323757264 | validation: 0.05660063152260685]
	TIME [epoch: 2.6 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05128541265449792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05128541265449792 | validation: 0.04951666974935039]
	TIME [epoch: 2.6 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04825356136785464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04825356136785464 | validation: 0.07063652626699304]
	TIME [epoch: 2.6 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04562963687610051		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04562963687610051 | validation: 0.05385790115240077]
	TIME [epoch: 2.6 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041270827009547977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.041270827009547977 | validation: 0.07717549728747627]
	TIME [epoch: 2.6 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05159657850661126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05159657850661126 | validation: 0.061661722265896925]
	TIME [epoch: 2.61 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05803326463933832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05803326463933832 | validation: 0.1156934123177555]
	TIME [epoch: 2.61 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06666523762331346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06666523762331346 | validation: 0.09476054157249689]
	TIME [epoch: 2.6 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07167558844104295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07167558844104295 | validation: 0.12424834536330234]
	TIME [epoch: 2.6 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0843532607348358		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0843532607348358 | validation: 0.09668136330068904]
	TIME [epoch: 2.6 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09178438841587067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09178438841587067 | validation: 0.11494489227407279]
	TIME [epoch: 2.6 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07211723392442373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07211723392442373 | validation: 0.09767678071112199]
	TIME [epoch: 2.6 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05686586055393263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05686586055393263 | validation: 0.07786185621165218]
	TIME [epoch: 2.6 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04661198843790198		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04661198843790198 | validation: 0.07206371615159939]
	TIME [epoch: 2.61 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044069422119124894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.044069422119124894 | validation: 0.07135741148426382]
	TIME [epoch: 2.61 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0458995367048303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0458995367048303 | validation: 0.06593100555327448]
	TIME [epoch: 2.6 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04118385099946695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04118385099946695 | validation: 0.06407067269915574]
	TIME [epoch: 2.59 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03978876417910349		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03978876417910349 | validation: 0.06477371592983822]
	TIME [epoch: 2.59 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0760344716936895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0760344716936895 | validation: 0.1357204114853474]
	TIME [epoch: 2.59 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11601955468132841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11601955468132841 | validation: 0.06820025810420399]
	TIME [epoch: 2.6 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060452778178438214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.060452778178438214 | validation: 0.060498078228785085]
	TIME [epoch: 2.6 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.026940587871218648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.026940587871218648 | validation: 0.05284946218745282]
	TIME [epoch: 2.61 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03513713138224169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03513713138224169 | validation: 0.07301397077813797]
	TIME [epoch: 2.6 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04735488356773353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04735488356773353 | validation: 0.09718347348360323]
	TIME [epoch: 2.61 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10381291305472325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10381291305472325 | validation: 0.16238050275425248]
	TIME [epoch: 2.61 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13155023805697524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13155023805697524 | validation: 0.15759282372198025]
	TIME [epoch: 2.61 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11674327878133073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11674327878133073 | validation: 0.13550775989625263]
	TIME [epoch: 2.59 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08081807998815121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08081807998815121 | validation: 0.08123895062104786]
	TIME [epoch: 2.61 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498831586883155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06498831586883155 | validation: 0.08219658421085801]
	TIME [epoch: 2.6 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04927686652124448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04927686652124448 | validation: 0.08217839521088283]
	TIME [epoch: 2.61 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04574788403148402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04574788403148402 | validation: 0.058170423410082044]
	TIME [epoch: 2.6 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.028325217589966478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.028325217589966478 | validation: 0.04933427778231282]
	TIME [epoch: 2.6 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03439056749468305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03439056749468305 | validation: 0.1052032573775688]
	TIME [epoch: 2.59 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05604018996193516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05604018996193516 | validation: 0.07899389982828892]
	TIME [epoch: 2.6 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05230651609804359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05230651609804359 | validation: 0.07543503699320599]
	TIME [epoch: 2.58 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03975038305548021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03975038305548021 | validation: 0.05192545204346306]
	TIME [epoch: 2.61 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.030736912006095383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.030736912006095383 | validation: 0.07655891044228698]
	TIME [epoch: 2.6 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03929093444752651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03929093444752651 | validation: 0.07597659216987403]
	TIME [epoch: 2.61 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871313898444019		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06871313898444019 | validation: 0.12028240900278764]
	TIME [epoch: 2.59 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10530601118813306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10530601118813306 | validation: 0.0983692532970678]
	TIME [epoch: 2.6 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09186395617121935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09186395617121935 | validation: 0.07710214626856815]
	TIME [epoch: 2.59 sec]
	Saving model to: out/model_training/model_phi1_4a_v_mmd2_20241125_201012/states/model_phi1_4a_v_mmd2_670.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 1525.617 seconds.
