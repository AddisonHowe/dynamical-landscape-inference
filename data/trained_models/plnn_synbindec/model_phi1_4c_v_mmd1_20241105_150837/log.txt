Args:
Namespace(name='model_phi1_4c_v_mmd1', outdir='out/model_training/model_phi1_4c_v_mmd1', training_data='data/training_data/basic/data_phi1_4c/training', validation_data='data/training_data/basic/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2927107827

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.3319866298952245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.3319866298952245 | validation: 6.202295851982332]
	TIME [epoch: 182 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.06634073377176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.06634073377176 | validation: 5.239333258909414]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.108014555339986		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.108014555339986 | validation: 5.149484569503669]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.127364996507415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.127364996507415 | validation: 4.837643114931708]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.786880271377676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.786880271377676 | validation: 4.288913157715469]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.16005162102244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.16005162102244 | validation: 4.210986083101681]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.066667470293585		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.066667470293585 | validation: 4.098864524475717]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.025244060966442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.025244060966442 | validation: 4.100729114303576]
	TIME [epoch: 2.8 sec]
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9404565087898527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9404565087898527 | validation: 4.399060819150788]
	TIME [epoch: 2.79 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.377738153216881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.377738153216881 | validation: 3.969890231758413]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.8226375737944727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.8226375737944727 | validation: 3.9166410473600877]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.805991206823053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.805991206823053 | validation: 3.985160211326747]
	TIME [epoch: 2.8 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.95578505144642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.95578505144642 | validation: 4.087749030004151]
	TIME [epoch: 2.77 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.918323594744819		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.918323594744819 | validation: 3.7840625210599352]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6917142656748756		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6917142656748756 | validation: 3.758644029396092]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.69220665843333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.69220665843333 | validation: 3.833072204701663]
	TIME [epoch: 2.8 sec]
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.694466822305887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.694466822305887 | validation: 3.7663797066574176]
	TIME [epoch: 2.79 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7115682547455204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7115682547455204 | validation: 3.9025167540605823]
	TIME [epoch: 2.79 sec]
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7531369596871027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7531369596871027 | validation: 3.7866571428308706]
	TIME [epoch: 2.79 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7416426981407676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7416426981407676 | validation: 3.7609964405136687]
	TIME [epoch: 2.8 sec]
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.627137674263391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.627137674263391 | validation: 3.6517938523140887]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_21.pth
	Model improved!!!
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5815501387757345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5815501387757345 | validation: 3.6563230047742907]
	TIME [epoch: 2.8 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.558420434313407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.558420434313407 | validation: 3.6342769963808133]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_23.pth
	Model improved!!!
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.539124219325096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.539124219325096 | validation: 3.6093827014447903]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.525556397963683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.525556397963683 | validation: 3.6413300345334947]
	TIME [epoch: 2.8 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.522080144330904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.522080144330904 | validation: 3.6177722846300924]
	TIME [epoch: 2.8 sec]
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5640565903094807		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5640565903094807 | validation: 3.9067054269714365]
	TIME [epoch: 2.8 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.729052987085723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.729052987085723 | validation: 3.6692161048573264]
	TIME [epoch: 2.8 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6317914029829126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6317914029829126 | validation: 3.600236717183247]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_29.pth
	Model improved!!!
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4815434873131212		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4815434873131212 | validation: 3.548889138426884]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4497589536853344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4497589536853344 | validation: 3.5226163700601547]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_31.pth
	Model improved!!!
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4387308419951013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4387308419951013 | validation: 3.5608531125838]
	TIME [epoch: 2.8 sec]
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4313490575050767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4313490575050767 | validation: 3.4900384396526007]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4248176218109094		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4248176218109094 | validation: 3.53710105704204]
	TIME [epoch: 2.79 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4080034274347284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4080034274347284 | validation: 3.465953290412889]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_35.pth
	Model improved!!!
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3957664389171227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3957664389171227 | validation: 3.5499441704452668]
	TIME [epoch: 2.78 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3968494004582475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3968494004582475 | validation: 3.46990983519908]
	TIME [epoch: 2.78 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.445179532011238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.445179532011238 | validation: 3.757733492713239]
	TIME [epoch: 2.78 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.548785013224135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.548785013224135 | validation: 3.411105600641016]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_39.pth
	Model improved!!!
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3313375414781934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3313375414781934 | validation: 3.4145171734625075]
	TIME [epoch: 2.78 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.318825911925314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.318825911925314 | validation: 3.51525022181917]
	TIME [epoch: 2.8 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3443644627535742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3443644627535742 | validation: 3.3806240781426284]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3156809971956434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3156809971956434 | validation: 3.4758816973812467]
	TIME [epoch: 2.78 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.303723903998563		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.303723903998563 | validation: 3.34421397996959]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2865426054992835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2865426054992835 | validation: 3.436490740738661]
	TIME [epoch: 2.81 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2702317985409866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2702317985409866 | validation: 3.322322824361882]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2724948534369744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2724948534369744 | validation: 3.4000950053908223]
	TIME [epoch: 2.78 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.237098418654594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.237098418654594 | validation: 3.2905694770492424]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.214537062790977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.214537062790977 | validation: 3.3659467796096196]
	TIME [epoch: 2.78 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.215600046522183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.215600046522183 | validation: 3.2652009939997018]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.203870510611399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.203870510611399 | validation: 3.3672611063329274]
	TIME [epoch: 2.78 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2080915696394663		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.2080915696394663 | validation: 3.216049473194086]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1454682180809925		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.1454682180809925 | validation: 3.265318401861878]
	TIME [epoch: 2.78 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.117588170312987		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.117588170312987 | validation: 3.2003807854464075]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1074811322561375		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.1074811322561375 | validation: 3.23531562200848]
	TIME [epoch: 2.8 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1004101303229348		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.1004101303229348 | validation: 3.149765440119792]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1003778441182273		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.1003778441182273 | validation: 3.3116260045460004]
	TIME [epoch: 2.79 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1348100669614434		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.1348100669614434 | validation: 3.1339786315833287]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_58.pth
	Model improved!!!
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0702331966234757		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.0702331966234757 | validation: 3.179595046455292]
	TIME [epoch: 2.8 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.033579662394659		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.033579662394659 | validation: 3.0916758881666695]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_60.pth
	Model improved!!!
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9989926010570223		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 2.9989926010570223 | validation: 3.108641761482245]
	TIME [epoch: 2.8 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9808687391177338		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 2.9808687391177338 | validation: 3.0469183784922103]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_62.pth
	Model improved!!!
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9490903392100307		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 2.9490903392100307 | validation: 3.068058183710609]
	TIME [epoch: 2.78 sec]
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.933064772168541		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 2.933064772168541 | validation: 2.9945292809504247]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9578409168906528		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 2.9578409168906528 | validation: 3.3377453389318017]
	TIME [epoch: 2.79 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0722470395391936		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.0722470395391936 | validation: 2.958067080865522]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.813798292509673		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 2.813798292509673 | validation: 2.8811031606101496]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_67.pth
	Model improved!!!
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8295431402014013		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 2.8295431402014013 | validation: 2.9277910246946]
	TIME [epoch: 2.8 sec]
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.678212026805342		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 2.678212026805342 | validation: 2.068390621632754]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_69.pth
	Model improved!!!
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9655433663503352		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 1.9655433663503352 | validation: 1.333034804987505]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.362807274964608		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 1.362807274964608 | validation: 3.399922356600176]
	TIME [epoch: 2.8 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.681719565353641		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.681719565353641 | validation: 1.290116589746086]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_72.pth
	Model improved!!!
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2942510595168355		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 1.2942510595168355 | validation: 1.6380533737809735]
	TIME [epoch: 2.8 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6977859217909625		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 1.6977859217909625 | validation: 1.1290774495962546]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2108169209478805		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 1.2108169209478805 | validation: 1.1806619892907728]
	TIME [epoch: 2.8 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2213785362214338		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 1.2213785362214338 | validation: 0.9674361727533839]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0620156929521218		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 1.0620156929521218 | validation: 1.0011067529872826]
	TIME [epoch: 2.79 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0923185583174695		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 1.0923185583174695 | validation: 0.9724253527468076]
	TIME [epoch: 2.8 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0130353757372759		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 1.0130353757372759 | validation: 0.9571057747774674]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9926389858899267		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 0.9926389858899267 | validation: 0.941672685445637]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0346592196848963		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 1.0346592196848963 | validation: 0.9755943787595028]
	TIME [epoch: 2.81 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.082851911782155		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 1.082851911782155 | validation: 0.9377630095731831]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_82.pth
	Model improved!!!
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0250032122963042		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 1.0250032122963042 | validation: 0.8549816991667862]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_83.pth
	Model improved!!!
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9508834253307975		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 0.9508834253307975 | validation: 0.8590721399895321]
	TIME [epoch: 2.8 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9465455932534158		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 0.9465455932534158 | validation: 1.0382630657227196]
	TIME [epoch: 2.81 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1093019079761337		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 1.1093019079761337 | validation: 0.8321588747072891]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9418629194588306		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 0.9418629194588306 | validation: 0.8184666289157569]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_87.pth
	Model improved!!!
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9156061561189666		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 0.9156061561189666 | validation: 0.842674825567163]
	TIME [epoch: 2.8 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915225651052571		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 0.915225651052571 | validation: 0.8318213329265212]
	TIME [epoch: 2.8 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9228122066122915		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 0.9228122066122915 | validation: 0.7996659615081425]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9160563334749802		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 0.9160563334749802 | validation: 0.8557665375845114]
	TIME [epoch: 2.8 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.936834323193911		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 0.936834323193911 | validation: 0.7891900685268451]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_92.pth
	Model improved!!!
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8951451958944077		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 0.8951451958944077 | validation: 0.7857903971080362]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_93.pth
	Model improved!!!
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8883946849999771		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 0.8883946849999771 | validation: 0.8486268823768506]
	TIME [epoch: 2.8 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9671416591855301		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 0.9671416591855301 | validation: 0.815029739890689]
	TIME [epoch: 2.8 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9205736034276545		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 0.9205736034276545 | validation: 0.7955879107704542]
	TIME [epoch: 2.8 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.902007384991297		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 0.902007384991297 | validation: 0.7654261354217415]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_97.pth
	Model improved!!!
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8656785997578124		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 0.8656785997578124 | validation: 0.7644820116579848]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_98.pth
	Model improved!!!
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8735429310517238		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 0.8735429310517238 | validation: 0.8079754824415449]
	TIME [epoch: 2.8 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.912811144413046		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 0.912811144413046 | validation: 0.8998541782250848]
	TIME [epoch: 2.79 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0015213371511176		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 1.0015213371511176 | validation: 0.9349767062513827]
	TIME [epoch: 2.78 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1224125636997462		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 1.1224125636997462 | validation: 0.7890868309157915]
	TIME [epoch: 2.78 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8796930155465387		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 0.8796930155465387 | validation: 0.8714011279659716]
	TIME [epoch: 2.79 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9661198122922496		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 0.9661198122922496 | validation: 0.7922743545280092]
	TIME [epoch: 2.78 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9140592239084517		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 0.9140592239084517 | validation: 0.7829537208767448]
	TIME [epoch: 2.81 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8769296535291806		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 0.8769296535291806 | validation: 0.7477867126125493]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_106.pth
	Model improved!!!
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8529832671692432		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 0.8529832671692432 | validation: 0.7568914915741004]
	TIME [epoch: 2.78 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8596775844062462		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 0.8596775844062462 | validation: 0.7375047375775269]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_108.pth
	Model improved!!!
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8459869596721962		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 0.8459869596721962 | validation: 0.7343593146759652]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_109.pth
	Model improved!!!
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.862768721020952		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 0.862768721020952 | validation: 0.7651416278195077]
	TIME [epoch: 2.81 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8616793746242687		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 0.8616793746242687 | validation: 0.8047511201030837]
	TIME [epoch: 2.81 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9132045482234316		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 0.9132045482234316 | validation: 0.7879240252262086]
	TIME [epoch: 2.81 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8727758414333482		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 0.8727758414333482 | validation: 0.8069814445388849]
	TIME [epoch: 2.81 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9291023232705274		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 0.9291023232705274 | validation: 0.7992988093814483]
	TIME [epoch: 2.81 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8779562417933551		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 0.8779562417933551 | validation: 0.7687150674887625]
	TIME [epoch: 2.81 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8517513194333267		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 0.8517513194333267 | validation: 0.7627179324577302]
	TIME [epoch: 2.81 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465021012803601		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 0.8465021012803601 | validation: 0.7444579796068246]
	TIME [epoch: 2.81 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499253958975661		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 0.8499253958975661 | validation: 0.9606243304972972]
	TIME [epoch: 2.81 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0432408696871225		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.0432408696871225 | validation: 0.8136618073780346]
	TIME [epoch: 2.81 sec]
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9603412699141984		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 0.9603412699141984 | validation: 0.7434702890141933]
	TIME [epoch: 2.81 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8107982350578655		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 0.8107982350578655 | validation: 0.7369197314173949]
	TIME [epoch: 2.81 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133543986384979		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 0.8133543986384979 | validation: 0.732853066181303]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8248851669433392		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 0.8248851669433392 | validation: 0.7069629099625394]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7747354003883684		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 0.7747354003883684 | validation: 0.7031407750008493]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7665052091997645		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 0.7665052091997645 | validation: 0.7369767889663437]
	TIME [epoch: 2.78 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767963482604561		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 0.767963482604561 | validation: 0.8277369843736718]
	TIME [epoch: 2.78 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9420530368406919		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 0.9420530368406919 | validation: 1.2233360126918473]
	TIME [epoch: 2.79 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.236707088572663		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 1.236707088572663 | validation: 0.7205329984944843]
	TIME [epoch: 2.78 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8482445068938569		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.8482445068938569 | validation: 0.7036293487147267]
	TIME [epoch: 2.78 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7906437479588141		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.7906437479588141 | validation: 0.7538399334913468]
	TIME [epoch: 2.78 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7988896444530451		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.7988896444530451 | validation: 0.6909388000481989]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_131.pth
	Model improved!!!
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7965819677298567		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.7965819677298567 | validation: 0.6663809158108966]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_132.pth
	Model improved!!!
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7373287470212466		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.7373287470212466 | validation: 0.6794053789588763]
	TIME [epoch: 2.8 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7205229542857796		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.7205229542857796 | validation: 0.6531631033457432]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7067511466578474		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.7067511466578474 | validation: 0.7116301707562158]
	TIME [epoch: 2.8 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7244315044221307		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.7244315044221307 | validation: 0.7569632706385168]
	TIME [epoch: 2.8 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8777883137822114		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.8777883137822114 | validation: 1.117671508790578]
	TIME [epoch: 2.8 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0577530808152837		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 1.0577530808152837 | validation: 0.7210695233576396]
	TIME [epoch: 2.81 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.841231019399503		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.841231019399503 | validation: 0.6246497381391816]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_139.pth
	Model improved!!!
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.701838375663647		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.701838375663647 | validation: 0.6692995115110969]
	TIME [epoch: 2.79 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7098885006601009		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.7098885006601009 | validation: 0.6424733326799862]
	TIME [epoch: 2.78 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7136182991937389		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.7136182991937389 | validation: 0.6981630373855308]
	TIME [epoch: 2.79 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7091119421145378		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.7091119421145378 | validation: 0.7172503847181971]
	TIME [epoch: 2.78 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8437338242382445		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8437338242382445 | validation: 0.7297935256693635]
	TIME [epoch: 2.79 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7703387221909228		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.7703387221909228 | validation: 0.6754746905484711]
	TIME [epoch: 2.78 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6838037582485493		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.6838037582485493 | validation: 0.7376271771927556]
	TIME [epoch: 2.78 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.825270709820903		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.825270709820903 | validation: 0.8701847575784175]
	TIME [epoch: 2.78 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8340128699276143		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8340128699276143 | validation: 0.6453090614664702]
	TIME [epoch: 2.79 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7245388907020217		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.7245388907020217 | validation: 0.6162980475289311]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_149.pth
	Model improved!!!
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6465026789745472		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.6465026789745472 | validation: 0.6884554631324176]
	TIME [epoch: 2.8 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6973285794831858		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.6973285794831858 | validation: 0.6598781873347015]
	TIME [epoch: 2.79 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7575915043867032		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.7575915043867032 | validation: 0.7668121521779931]
	TIME [epoch: 2.79 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7548051772504625		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.7548051772504625 | validation: 0.6126891326072242]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6776954045757987		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.6776954045757987 | validation: 0.6180131968737217]
	TIME [epoch: 2.8 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6509456534087346		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.6509456534087346 | validation: 0.6552094862908915]
	TIME [epoch: 2.91 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6674128860660145		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.6674128860660145 | validation: 0.7015176354872277]
	TIME [epoch: 2.8 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7056995807779359		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.7056995807779359 | validation: 0.5996451221122057]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_157.pth
	Model improved!!!
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6378586880254539		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.6378586880254539 | validation: 0.6234150170796515]
	TIME [epoch: 2.8 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6104800759571859		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.6104800759571859 | validation: 0.5825304815135351]
	TIME [epoch: 2.81 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_159.pth
	Model improved!!!
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6122373152753146		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.6122373152753146 | validation: 0.6145771968018248]
	TIME [epoch: 2.8 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6116469239461115		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.6116469239461115 | validation: 0.6233858139179319]
	TIME [epoch: 2.8 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6335091766297647		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.6335091766297647 | validation: 0.7569246102522831]
	TIME [epoch: 2.8 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7684504287880722		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.7684504287880722 | validation: 0.7172350626013254]
	TIME [epoch: 2.8 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018005219802109		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7018005219802109 | validation: 0.6398545225974606]
	TIME [epoch: 2.8 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7175895277652313		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7175895277652313 | validation: 0.6457924676995291]
	TIME [epoch: 2.8 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6694072552428856		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.6694072552428856 | validation: 0.6679269863158039]
	TIME [epoch: 2.8 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.77965087462263		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.77965087462263 | validation: 0.6405155429003228]
	TIME [epoch: 2.8 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6843901131164836		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.6843901131164836 | validation: 0.5843859503829102]
	TIME [epoch: 2.8 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6618119818123975		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.6618119818123975 | validation: 0.5929089384964457]
	TIME [epoch: 2.8 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.609551677264382		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.609551677264382 | validation: 0.580595619905847]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_170.pth
	Model improved!!!
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5936261935042719		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.5936261935042719 | validation: 0.5553908825445576]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_171.pth
	Model improved!!!
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6018512521535542		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.6018512521535542 | validation: 0.6418137122665126]
	TIME [epoch: 2.8 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6365064598875975		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.6365064598875975 | validation: 0.5945456082274596]
	TIME [epoch: 2.8 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6821243423573178		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.6821243423573178 | validation: 0.6856200800927661]
	TIME [epoch: 2.8 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.660542238162464		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.660542238162464 | validation: 0.6118484447680271]
	TIME [epoch: 2.8 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6768178776263823		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.6768178776263823 | validation: 0.6139380676853938]
	TIME [epoch: 2.8 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6201516196733623		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.6201516196733623 | validation: 0.5445313550657576]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_177.pth
	Model improved!!!
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.584410116114804		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.584410116114804 | validation: 0.5504880005730224]
	TIME [epoch: 2.79 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5672496806113508		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.5672496806113508 | validation: 0.5641385186539533]
	TIME [epoch: 2.79 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5735642642265215		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.5735642642265215 | validation: 0.5422026016763678]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_180.pth
	Model improved!!!
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5740076792986797		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.5740076792986797 | validation: 0.5639464666570898]
	TIME [epoch: 2.79 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5661930102261665		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.5661930102261665 | validation: 0.5818807836079805]
	TIME [epoch: 2.79 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6004418195535141		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.6004418195535141 | validation: 0.8185530214283555]
	TIME [epoch: 2.79 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7609391788451391		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.7609391788451391 | validation: 0.7876062320211241]
	TIME [epoch: 2.8 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578866192251369		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.8578866192251369 | validation: 0.5538288268196055]
	TIME [epoch: 2.8 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5893340159707728		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.5893340159707728 | validation: 0.6287289998866756]
	TIME [epoch: 2.8 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6581310868571602		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.6581310868571602 | validation: 0.6148547322374148]
	TIME [epoch: 2.8 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6643777804791339		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.6643777804791339 | validation: 0.5555205672384494]
	TIME [epoch: 2.81 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6496721151291287		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.6496721151291287 | validation: 0.6156361328217693]
	TIME [epoch: 2.81 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6113267086150549		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.6113267086150549 | validation: 0.526946770770146]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_190.pth
	Model improved!!!
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5834926012608728		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.5834926012608728 | validation: 0.562260501499522]
	TIME [epoch: 2.78 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5674992387675144		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.5674992387675144 | validation: 0.5133590193680939]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_192.pth
	Model improved!!!
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5738441765618585		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.5738441765618585 | validation: 0.5738327571381766]
	TIME [epoch: 2.78 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5631518904105134		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.5631518904105134 | validation: 0.5155313933450342]
	TIME [epoch: 2.8 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5619631325006006		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.5619631325006006 | validation: 0.5549519999186122]
	TIME [epoch: 2.8 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491014724810396		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.5491014724810396 | validation: 0.5247819105589552]
	TIME [epoch: 2.8 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.574846405790477		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.574846405790477 | validation: 0.7859753201690113]
	TIME [epoch: 2.8 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7263703687879687		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7263703687879687 | validation: 0.5892462980976391]
	TIME [epoch: 2.8 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6880580123021943		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.6880580123021943 | validation: 0.5414225367651858]
	TIME [epoch: 2.8 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5741828768007906		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.5741828768007906 | validation: 0.5678473488324859]
	TIME [epoch: 2.8 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.59164334849644		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.59164334849644 | validation: 0.5347458130637278]
	TIME [epoch: 188 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5491979278740702		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.5491979278740702 | validation: 0.5077170277573841]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_202.pth
	Model improved!!!
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5351812272616369		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.5351812272616369 | validation: 0.5438728152940383]
	TIME [epoch: 5.99 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5308420011646776		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.5308420011646776 | validation: 0.5121180884933844]
	TIME [epoch: 5.98 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5889670294987593		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.5889670294987593 | validation: 0.6089547260367166]
	TIME [epoch: 5.99 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.604166551259577		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.604166551259577 | validation: 0.5183463623139979]
	TIME [epoch: 5.98 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6031869414447952		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.6031869414447952 | validation: 0.5246964228011223]
	TIME [epoch: 5.99 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5291465921043396		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.5291465921043396 | validation: 0.5531178124380747]
	TIME [epoch: 5.98 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5489328387336357		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.5489328387336357 | validation: 0.5474895448154177]
	TIME [epoch: 5.99 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6118666023044864		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.6118666023044864 | validation: 0.6208705411151689]
	TIME [epoch: 5.99 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5817044148109153		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.5817044148109153 | validation: 0.4861648375087141]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_211.pth
	Model improved!!!
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5266505940500439		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.5266505940500439 | validation: 0.4952333810059469]
	TIME [epoch: 5.99 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060528506805605		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.5060528506805605 | validation: 0.5027032815505283]
	TIME [epoch: 5.99 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516646618917374		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.516646618917374 | validation: 0.5261198060076173]
	TIME [epoch: 5.98 sec]
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5242320026413103		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.5242320026413103 | validation: 0.5221761311738723]
	TIME [epoch: 5.99 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5294016583741342		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.5294016583741342 | validation: 0.5234285014083183]
	TIME [epoch: 5.99 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5461216019254621		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.5461216019254621 | validation: 0.4925849485738331]
	TIME [epoch: 5.99 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5424449289185492		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.5424449289185492 | validation: 0.5487949905277827]
	TIME [epoch: 5.99 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5261402220943463		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.5261402220943463 | validation: 0.4856228029781704]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_219.pth
	Model improved!!!
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5400008134240699		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.5400008134240699 | validation: 0.5482194194595288]
	TIME [epoch: 5.98 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5149693067148199		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.5149693067148199 | validation: 0.4935003993513677]
	TIME [epoch: 5.99 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5591925632202329		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.5591925632202329 | validation: 0.5388799548389721]
	TIME [epoch: 5.99 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5245218451610276		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.5245218451610276 | validation: 0.45728702037211577]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_223.pth
	Model improved!!!
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5143176102335536		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.5143176102335536 | validation: 0.45141003050051565]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_224.pth
	Model improved!!!
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4799030582814728		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.4799030582814728 | validation: 0.5273225429590991]
	TIME [epoch: 5.98 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5038272730486321		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.5038272730486321 | validation: 0.46433213877372037]
	TIME [epoch: 5.98 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5473820980658392		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.5473820980658392 | validation: 0.5100343204038646]
	TIME [epoch: 5.99 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5314945789523715		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.5314945789523715 | validation: 0.4803676831217555]
	TIME [epoch: 5.99 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5310462446936449		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.5310462446936449 | validation: 0.5100402371521969]
	TIME [epoch: 5.99 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4984163547393336		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.4984163547393336 | validation: 0.452090597379679]
	TIME [epoch: 5.99 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5119134473067978		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.5119134473067978 | validation: 0.5133072567076334]
	TIME [epoch: 5.99 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49233551694145106		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.49233551694145106 | validation: 0.4537860920250898]
	TIME [epoch: 5.99 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5088443268494715		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.5088443268494715 | validation: 0.41650781782333157]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_233.pth
	Model improved!!!
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45494779415971737		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.45494779415971737 | validation: 0.46935502518491545]
	TIME [epoch: 6.01 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47675993578414644		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.47675993578414644 | validation: 0.4449691409009324]
	TIME [epoch: 6.01 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.516490060617468		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.516490060617468 | validation: 0.5651268465602265]
	TIME [epoch: 6.02 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5336010456038031		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.5336010456038031 | validation: 0.4533049578501609]
	TIME [epoch: 6 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5170118069623411		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.5170118069623411 | validation: 0.4579085154933017]
	TIME [epoch: 6.01 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.472301364870835		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.472301364870835 | validation: 0.41738517711502426]
	TIME [epoch: 6.01 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4450951576981889		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.4450951576981889 | validation: 0.4036843103678811]
	TIME [epoch: 6.02 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_240.pth
	Model improved!!!
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4367300378647341		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.4367300378647341 | validation: 0.4487383603515038]
	TIME [epoch: 5.98 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393177513339955		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.4393177513339955 | validation: 0.4098720913180488]
	TIME [epoch: 5.98 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43959292026231467		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.43959292026231467 | validation: 0.44038177866810335]
	TIME [epoch: 5.99 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4315444753532086		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.4315444753532086 | validation: 0.41785731669294557]
	TIME [epoch: 5.99 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4656678053068367		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.4656678053068367 | validation: 0.5564353370952995]
	TIME [epoch: 5.99 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5146692383997287		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.5146692383997287 | validation: 0.4891150074666941]
	TIME [epoch: 5.99 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5855568044578423		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.5855568044578423 | validation: 0.533771943705554]
	TIME [epoch: 5.99 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5373465399028384		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.5373465399028384 | validation: 0.39137885576243175]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_248.pth
	Model improved!!!
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4225800887179392		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.4225800887179392 | validation: 0.4031334716177072]
	TIME [epoch: 5.98 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41795723077900515		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.41795723077900515 | validation: 0.4122956673171997]
	TIME [epoch: 5.99 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4299838525172544		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.4299838525172544 | validation: 0.3997024785716234]
	TIME [epoch: 5.99 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4106291850852905		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.4106291850852905 | validation: 0.4421153804682737]
	TIME [epoch: 5.99 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4252615642336087		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.4252615642336087 | validation: 0.4257649978768303]
	TIME [epoch: 5.99 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48608727018399167		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.48608727018399167 | validation: 0.45778172786958377]
	TIME [epoch: 5.99 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41875349228470937		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.41875349228470937 | validation: 0.3972951570191884]
	TIME [epoch: 5.99 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4306634643295172		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.4306634643295172 | validation: 0.4507167969073166]
	TIME [epoch: 5.98 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43337748756888145		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.43337748756888145 | validation: 0.3780073810761863]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_257.pth
	Model improved!!!
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4254963444599564		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.4254963444599564 | validation: 0.46193782255231686]
	TIME [epoch: 5.98 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4221480834478146		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.4221480834478146 | validation: 0.3822670589457879]
	TIME [epoch: 5.98 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42194485169115425		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.42194485169115425 | validation: 0.448094274940374]
	TIME [epoch: 5.98 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41706619224646285		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.41706619224646285 | validation: 0.38040745474793497]
	TIME [epoch: 5.98 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.414188538953103		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.414188538953103 | validation: 0.3881532275624807]
	TIME [epoch: 5.98 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3768876874881204		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.3768876874881204 | validation: 0.34524682700053866]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_263.pth
	Model improved!!!
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35616897567102707		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.35616897567102707 | validation: 0.35223960919179276]
	TIME [epoch: 5.98 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3544665563627569		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.3544665563627569 | validation: 0.3427650402361937]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_265.pth
	Model improved!!!
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34514580274294515		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.34514580274294515 | validation: 0.38048118947868464]
	TIME [epoch: 6.01 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.355010153205207		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.355010153205207 | validation: 0.4463465685937731]
	TIME [epoch: 6 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5582838784796365		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.5582838784796365 | validation: 0.7135162573972056]
	TIME [epoch: 6 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.677082283341395		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.677082283341395 | validation: 0.34871219692330596]
	TIME [epoch: 6.01 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3402131191406813		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.3402131191406813 | validation: 0.3425385671809309]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_270.pth
	Model improved!!!
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3819289136037773		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.3819289136037773 | validation: 0.46237558715237487]
	TIME [epoch: 6.01 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39944045692579755		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.39944045692579755 | validation: 0.3188748359853157]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_272.pth
	Model improved!!!
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3556006786482493		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.3556006786482493 | validation: 0.3714396582747889]
	TIME [epoch: 6.01 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3336643734670326		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.3336643734670326 | validation: 0.31890358512440065]
	TIME [epoch: 6.01 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3233839487692079		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.3233839487692079 | validation: 0.3635708295583171]
	TIME [epoch: 6.01 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32267707963442394		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.32267707963442394 | validation: 0.3343719263582593]
	TIME [epoch: 6.01 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38603318068744635		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.38603318068744635 | validation: 0.571352547303164]
	TIME [epoch: 6.01 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4619777918317074		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.4619777918317074 | validation: 0.3824012517315902]
	TIME [epoch: 6.01 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.405622538529598		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.405622538529598 | validation: 0.3549607934850805]
	TIME [epoch: 6.02 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33629014314201194		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.33629014314201194 | validation: 0.29809890219995955]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_280.pth
	Model improved!!!
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077327949368003		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.3077327949368003 | validation: 0.3969691147769673]
	TIME [epoch: 5.95 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3284508981915129		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.3284508981915129 | validation: 0.31067428294492183]
	TIME [epoch: 6.01 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3299713410414768		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.3299713410414768 | validation: 0.37694325506986426]
	TIME [epoch: 6 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31254065173555634		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.31254065173555634 | validation: 0.32953500132085667]
	TIME [epoch: 6.01 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36505736790243754		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.36505736790243754 | validation: 0.5807652571721355]
	TIME [epoch: 6.01 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4945687494530469		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.4945687494530469 | validation: 0.36593436554265296]
	TIME [epoch: 6.01 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4140478147989967		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.4140478147989967 | validation: 0.40878111687026764]
	TIME [epoch: 6.01 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3239601453657301		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.3239601453657301 | validation: 0.27602370434572515]
	TIME [epoch: 6.01 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_288.pth
	Model improved!!!
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2956196496903468		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.2956196496903468 | validation: 0.2986042769458067]
	TIME [epoch: 5.96 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2827183101824053		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.2827183101824053 | validation: 0.330853377435055]
	TIME [epoch: 5.96 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2716500127159322		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.2716500127159322 | validation: 0.2679836288635952]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_291.pth
	Model improved!!!
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29506168304206754		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.29506168304206754 | validation: 0.45228237756339623]
	TIME [epoch: 5.96 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34776916506237515		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.34776916506237515 | validation: 0.3376555868007181]
	TIME [epoch: 5.95 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4025784385580994		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.4025784385580994 | validation: 0.5868673189915316]
	TIME [epoch: 5.96 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46471533424928124		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.46471533424928124 | validation: 0.32116471804765245]
	TIME [epoch: 5.95 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29496793000107124		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.29496793000107124 | validation: 0.2675080628020588]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_296.pth
	Model improved!!!
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2792112360773517		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.2792112360773517 | validation: 0.36436098320305876]
	TIME [epoch: 6 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962080357143924		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.2962080357143924 | validation: 0.2791613916084274]
	TIME [epoch: 6.01 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3075833081797656		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.3075833081797656 | validation: 0.38416016070406456]
	TIME [epoch: 6 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30593692878308526		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.30593692878308526 | validation: 0.2703057746956817]
	TIME [epoch: 6 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29583335057654453		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.29583335057654453 | validation: 0.4315724494582366]
	TIME [epoch: 6 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3293181408579132		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.3293181408579132 | validation: 0.2554743178276537]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29176780925266727		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.29176780925266727 | validation: 0.38238544543257336]
	TIME [epoch: 6 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28900034371364824		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.28900034371364824 | validation: 0.25951921475330497]
	TIME [epoch: 6.01 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27773463219155353		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.27773463219155353 | validation: 0.36063302921911367]
	TIME [epoch: 6 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2888750481249607		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.2888750481249607 | validation: 0.26140898393782414]
	TIME [epoch: 6 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2666187224012132		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.2666187224012132 | validation: 0.3606933266583462]
	TIME [epoch: 6 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2938423393899324		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.2938423393899324 | validation: 0.3086906297289956]
	TIME [epoch: 6 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3436878344873114		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.3436878344873114 | validation: 0.46479260248288057]
	TIME [epoch: 5.99 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.339202909565758		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.339202909565758 | validation: 0.24183627502379676]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_310.pth
	Model improved!!!
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2610790905975092		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.2610790905975092 | validation: 0.30660891402427093]
	TIME [epoch: 5.96 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23960490528518372		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.23960490528518372 | validation: 0.22503241408858055]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_312.pth
	Model improved!!!
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23419471300503028		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.23419471300503028 | validation: 0.4293024608252828]
	TIME [epoch: 5.96 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962560010403904		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.2962560010403904 | validation: 0.2582122807252551]
	TIME [epoch: 5.97 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3157594295238432		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.3157594295238432 | validation: 0.44541719280040964]
	TIME [epoch: 5.97 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3324818058666408		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.3324818058666408 | validation: 0.42472056020798993]
	TIME [epoch: 5.96 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4228774281775437		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.4228774281775437 | validation: 0.31183595804035336]
	TIME [epoch: 5.97 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2764652372152204		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.2764652372152204 | validation: 0.2188089366382795]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_318.pth
	Model improved!!!
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2424458485107805		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.2424458485107805 | validation: 0.33591173128012336]
	TIME [epoch: 5.95 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24311854945645878		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.24311854945645878 | validation: 0.23355858337395438]
	TIME [epoch: 5.94 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23400419366011518		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.23400419366011518 | validation: 0.31265374691225767]
	TIME [epoch: 5.95 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2553560815632309		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.2553560815632309 | validation: 0.26613099457112793]
	TIME [epoch: 5.94 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.318123775705803		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.318123775705803 | validation: 0.44933841101965055]
	TIME [epoch: 5.95 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34136684155100716		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.34136684155100716 | validation: 0.2441160011652469]
	TIME [epoch: 5.94 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24756075724784637		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.24756075724784637 | validation: 0.2843014812297409]
	TIME [epoch: 5.95 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22875029532801439		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.22875029532801439 | validation: 0.2273875000841531]
	TIME [epoch: 5.94 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21205520226828412		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.21205520226828412 | validation: 0.21431141020991995]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_327.pth
	Model improved!!!
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20659017465119797		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.20659017465119797 | validation: 0.2159399713416732]
	TIME [epoch: 5.96 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20319671550458077		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.20319671550458077 | validation: 0.3342448185294946]
	TIME [epoch: 5.96 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24957199282129716		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.24957199282129716 | validation: 0.36641163657762976]
	TIME [epoch: 5.96 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43902167641304474		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.43902167641304474 | validation: 0.5926000714249321]
	TIME [epoch: 5.95 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4521652217360537		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.4521652217360537 | validation: 0.23905112637059728]
	TIME [epoch: 5.94 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20256998338395962		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.20256998338395962 | validation: 0.22177302322885062]
	TIME [epoch: 5.94 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27630222651712527		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.27630222651712527 | validation: 0.4151804729800177]
	TIME [epoch: 5.94 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3007576418373818		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.3007576418373818 | validation: 0.24946180712292165]
	TIME [epoch: 5.94 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21859485345450125		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.21859485345450125 | validation: 0.20141053356476865]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_336.pth
	Model improved!!!
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21106781166757876		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.21106781166757876 | validation: 0.24961099451910346]
	TIME [epoch: 5.94 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20454005563032598		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.20454005563032598 | validation: 0.23464053300104318]
	TIME [epoch: 5.95 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20194433612500007		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.20194433612500007 | validation: 0.22207051233783937]
	TIME [epoch: 5.95 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2087502660672513		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.2087502660672513 | validation: 0.2226634253791296]
	TIME [epoch: 5.97 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20976191984341902		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.20976191984341902 | validation: 0.3117203540139423]
	TIME [epoch: 5.96 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22850057513608638		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.22850057513608638 | validation: 0.2723483037624111]
	TIME [epoch: 5.96 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3016952902501938		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.3016952902501938 | validation: 0.5638964526762642]
	TIME [epoch: 5.95 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4239936792261325		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.4239936792261325 | validation: 0.26339591386415784]
	TIME [epoch: 5.95 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2685744647039651		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.2685744647039651 | validation: 0.23726282410000066]
	TIME [epoch: 5.94 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1950420444121368		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.1950420444121368 | validation: 0.210958376540216]
	TIME [epoch: 5.95 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19121273017253224		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.19121273017253224 | validation: 0.20770620765780842]
	TIME [epoch: 5.95 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18371047131716783		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.18371047131716783 | validation: 0.1885351627346652]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_348.pth
	Model improved!!!
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1867372394708262		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.1867372394708262 | validation: 0.24355940483857205]
	TIME [epoch: 5.94 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20442587020307076		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.20442587020307076 | validation: 0.2646947902691128]
	TIME [epoch: 5.94 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23471392536075947		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.23471392536075947 | validation: 0.2746557907067071]
	TIME [epoch: 5.94 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2874971382331675		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.2874971382331675 | validation: 0.524549655921563]
	TIME [epoch: 5.93 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36736314119753405		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.36736314119753405 | validation: 0.2363949602955927]
	TIME [epoch: 5.94 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2875646310290788		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.2875646310290788 | validation: 0.31288101677480257]
	TIME [epoch: 5.94 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21742832034891466		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.21742832034891466 | validation: 0.18074529090976973]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_355.pth
	Model improved!!!
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18213549221636022		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.18213549221636022 | validation: 0.21815884046990291]
	TIME [epoch: 5.95 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18664264222982735		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.18664264222982735 | validation: 0.18713836001870732]
	TIME [epoch: 5.95 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18635960479187552		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.18635960479187552 | validation: 0.253178389449643]
	TIME [epoch: 5.95 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19994409353433434		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.19994409353433434 | validation: 0.27058901865463575]
	TIME [epoch: 5.94 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2937252989739489		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.2937252989739489 | validation: 0.4798995350836269]
	TIME [epoch: 5.94 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3184540947425464		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.3184540947425464 | validation: 0.20292062038007766]
	TIME [epoch: 5.94 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23256869925457305		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.23256869925457305 | validation: 0.25137593786889484]
	TIME [epoch: 5.95 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18914219907941296		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.18914219907941296 | validation: 0.1967449771601904]
	TIME [epoch: 5.94 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19859484008580616		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.19859484008580616 | validation: 0.25058004890739033]
	TIME [epoch: 5.95 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20311143410836033		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.20311143410836033 | validation: 0.21749681180497069]
	TIME [epoch: 5.95 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21079808321017318		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.21079808321017318 | validation: 0.25105949353291196]
	TIME [epoch: 5.95 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21943190416707306		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.21943190416707306 | validation: 0.17191689456830447]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1810343150542758		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.1810343150542758 | validation: 0.2734482031130617]
	TIME [epoch: 5.95 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18989262779813168		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.18989262779813168 | validation: 0.20516243527478517]
	TIME [epoch: 5.94 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2539122212367896		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.2539122212367896 | validation: 0.4438280949543934]
	TIME [epoch: 5.94 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30945592463835225		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.30945592463835225 | validation: 0.17069172230213311]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_371.pth
	Model improved!!!
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20010699676354912		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.20010699676354912 | validation: 0.19905438412367646]
	TIME [epoch: 5.94 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1846415816343955		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.1846415816343955 | validation: 0.27701153426734154]
	TIME [epoch: 5.95 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1926331555299277		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.1926331555299277 | validation: 0.1609875484441501]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18826255274278636		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.18826255274278636 | validation: 0.21683689009914017]
	TIME [epoch: 5.94 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16458284471310763		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.16458284471310763 | validation: 0.1949147869595869]
	TIME [epoch: 5.95 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1507634647370701		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.1507634647370701 | validation: 0.1528396809224596]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17992558980787496		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.17992558980787496 | validation: 0.46600706556745664]
	TIME [epoch: 5.98 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32263812792201124		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.32263812792201124 | validation: 0.4101100745315878]
	TIME [epoch: 5.98 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47182904559816335		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.47182904559816335 | validation: 0.19468995455542748]
	TIME [epoch: 5.98 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1687402239848184		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.1687402239848184 | validation: 0.23309045715303278]
	TIME [epoch: 5.97 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17561956647008484		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.17561956647008484 | validation: 0.20614803558963654]
	TIME [epoch: 5.98 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21222331083454996		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.21222331083454996 | validation: 0.28027795391935356]
	TIME [epoch: 5.94 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21330305021572343		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.21330305021572343 | validation: 0.15099762750952966]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_384.pth
	Model improved!!!
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17264399368756167		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.17264399368756167 | validation: 0.23189979781802725]
	TIME [epoch: 5.98 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16503029484022946		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.16503029484022946 | validation: 0.15158531719540652]
	TIME [epoch: 5.98 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20106321295040389		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.20106321295040389 | validation: 0.4089058694398331]
	TIME [epoch: 5.97 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25440581117121536		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.25440581117121536 | validation: 0.15566934798912546]
	TIME [epoch: 5.98 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18981499736157303		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.18981499736157303 | validation: 0.19015834103168971]
	TIME [epoch: 5.98 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15892835826134655		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.15892835826134655 | validation: 0.1735604583704638]
	TIME [epoch: 5.98 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16002746440146262		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.16002746440146262 | validation: 0.20125090951973656]
	TIME [epoch: 5.98 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16609190907564092		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.16609190907564092 | validation: 0.20828405891530283]
	TIME [epoch: 5.98 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21887364462350625		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.21887364462350625 | validation: 0.4385674326956733]
	TIME [epoch: 5.97 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32877107416229934		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.32877107416229934 | validation: 0.20083213787042126]
	TIME [epoch: 5.97 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22013507968493543		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.22013507968493543 | validation: 0.30926576582642096]
	TIME [epoch: 5.97 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20241803302547304		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.20241803302547304 | validation: 0.16550293500518443]
	TIME [epoch: 5.98 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20215257370900383		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.20215257370900383 | validation: 0.19301215620022097]
	TIME [epoch: 5.97 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1538400292766821		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.1538400292766821 | validation: 0.18340208201960634]
	TIME [epoch: 5.98 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14631307479456382		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.14631307479456382 | validation: 0.14862989096398907]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_399.pth
	Model improved!!!
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1515632351337899		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.1515632351337899 | validation: 0.20044669880308083]
	TIME [epoch: 5.99 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14849628592801614		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.14849628592801614 | validation: 0.15120023158439636]
	TIME [epoch: 5.99 sec]
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15384648056516173		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.15384648056516173 | validation: 0.16460298291735095]
	TIME [epoch: 5.99 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15852509137090515		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.15852509137090515 | validation: 0.28495767964897606]
	TIME [epoch: 5.97 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2151798365314334		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.2151798365314334 | validation: 0.3519587357378673]
	TIME [epoch: 5.97 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4005494907503787		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.4005494907503787 | validation: 0.29594865762589734]
	TIME [epoch: 5.97 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19596111774068306		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.19596111774068306 | validation: 0.1475197286768414]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_406.pth
	Model improved!!!
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18536556885047203		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.18536556885047203 | validation: 0.25448057900837284]
	TIME [epoch: 5.98 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1753312382922173		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.1753312382922173 | validation: 0.14301709312450422]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1532638080358024		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.1532638080358024 | validation: 0.199667474380312]
	TIME [epoch: 5.98 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1627832218910135		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.1627832218910135 | validation: 0.1888828077666919]
	TIME [epoch: 5.97 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19990101767089194		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.19990101767089194 | validation: 0.310258332443458]
	TIME [epoch: 5.98 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2338233712528149		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.2338233712528149 | validation: 0.19837557730583463]
	TIME [epoch: 5.97 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21925976651119083		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.21925976651119083 | validation: 0.23894061965740754]
	TIME [epoch: 5.97 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1688004983407294		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.1688004983407294 | validation: 0.12351541295672863]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_414.pth
	Model improved!!!
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16482160451359829		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.16482160451359829 | validation: 0.2281586917889015]
	TIME [epoch: 5.98 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15894791672428255		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.15894791672428255 | validation: 0.12248027517879856]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_416.pth
	Model improved!!!
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1388889236991693		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.1388889236991693 | validation: 0.14244322123492054]
	TIME [epoch: 5.99 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13596318180680417		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.13596318180680417 | validation: 0.22114947887596942]
	TIME [epoch: 5.98 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1510567224505606		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.1510567224505606 | validation: 0.19062468129254828]
	TIME [epoch: 5.98 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20966672219091806		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.20966672219091806 | validation: 0.41078518565870514]
	TIME [epoch: 5.98 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2896085637633038		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.2896085637633038 | validation: 0.23910575705593318]
	TIME [epoch: 5.97 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23997465502133472		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.23997465502133472 | validation: 0.15866171686892697]
	TIME [epoch: 5.97 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1368620078067022		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.1368620078067022 | validation: 0.15709795051982078]
	TIME [epoch: 5.98 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13149512520890702		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.13149512520890702 | validation: 0.14221965583063742]
	TIME [epoch: 5.98 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15589967086088277		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.15589967086088277 | validation: 0.2487402753191618]
	TIME [epoch: 5.99 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1741981874929776		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.1741981874929776 | validation: 0.1686807065525194]
	TIME [epoch: 5.99 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17705097327297104		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.17705097327297104 | validation: 0.20406033305382476]
	TIME [epoch: 5.99 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1681651616147126		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.1681651616147126 | validation: 0.17323872455530942]
	TIME [epoch: 5.99 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1608201074934693		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.1608201074934693 | validation: 0.21431678360498263]
	TIME [epoch: 5.98 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16087297115419139		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.16087297115419139 | validation: 0.16145863038558497]
	TIME [epoch: 5.98 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17151283571673148		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.17151283571673148 | validation: 0.200136827176742]
	TIME [epoch: 5.99 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16700562917491674		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.16700562917491674 | validation: 0.15596840754016555]
	TIME [epoch: 5.98 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17594935911295614		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.17594935911295614 | validation: 0.3859427700964859]
	TIME [epoch: 5.99 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23195154624545883		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.23195154624545883 | validation: 0.15582233135290632]
	TIME [epoch: 5.98 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.211558471913632		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.211558471913632 | validation: 0.2609420933501749]
	TIME [epoch: 5.99 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16743718871889715		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.16743718871889715 | validation: 0.11264260631697871]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1381919503707445		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.1381919503707445 | validation: 0.1428046400055146]
	TIME [epoch: 5.99 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12712667716859433		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.12712667716859433 | validation: 0.13383255376349293]
	TIME [epoch: 5.99 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12369632864330962		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.12369632864330962 | validation: 0.15648555138469136]
	TIME [epoch: 5.98 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13334540630421426		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.13334540630421426 | validation: 0.15580705261618477]
	TIME [epoch: 5.98 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16685716174052936		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.16685716174052936 | validation: 0.27273807406147654]
	TIME [epoch: 5.99 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19424440920833191		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.19424440920833191 | validation: 0.1892569716258983]
	TIME [epoch: 5.98 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21206543977973383		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.21206543977973383 | validation: 0.24999705545929132]
	TIME [epoch: 5.98 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1652453918417698		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.1652453918417698 | validation: 0.12462529090273669]
	TIME [epoch: 5.99 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16225616252905503		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.16225616252905503 | validation: 0.23861096158577402]
	TIME [epoch: 5.98 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15249925009907814		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.15249925009907814 | validation: 0.12697755660634966]
	TIME [epoch: 5.99 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17391338300733225		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.17391338300733225 | validation: 0.25123229414077247]
	TIME [epoch: 5.98 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15572904619669845		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.15572904619669845 | validation: 0.1211453013292707]
	TIME [epoch: 5.99 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13906295896254775		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.13906295896254775 | validation: 0.16493170607297816]
	TIME [epoch: 5.99 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12812779850909323		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.12812779850909323 | validation: 0.1485589417534001]
	TIME [epoch: 6 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305351442055971		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.1305351442055971 | validation: 0.12625503355343218]
	TIME [epoch: 5.99 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13217506803886306		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.13217506803886306 | validation: 0.21242469957704185]
	TIME [epoch: 6 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17202972272143732		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.17202972272143732 | validation: 0.26647684349689654]
	TIME [epoch: 5.99 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28101827149161546		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.28101827149161546 | validation: 0.35413412567613795]
	TIME [epoch: 5.99 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22794663599260226		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.22794663599260226 | validation: 0.1499668511513872]
	TIME [epoch: 5.99 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18934227054548694		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.18934227054548694 | validation: 0.1744506791895597]
	TIME [epoch: 5.99 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12688855708233937		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.12688855708233937 | validation: 0.1206615239161073]
	TIME [epoch: 5.99 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11673483648234557		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.11673483648234557 | validation: 0.12738319783004215]
	TIME [epoch: 5.99 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11537486437426968		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.11537486437426968 | validation: 0.1334866751199218]
	TIME [epoch: 5.99 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11712349383127166		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.11712349383127166 | validation: 0.11585022634412817]
	TIME [epoch: 5.99 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13197526265899925		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.13197526265899925 | validation: 0.18885745335886256]
	TIME [epoch: 5.99 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.131980494490681		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.131980494490681 | validation: 0.13274881913820874]
	TIME [epoch: 5.99 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1579383033572736		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.1579383033572736 | validation: 0.4240184629437167]
	TIME [epoch: 5.98 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2755332466442767		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.2755332466442767 | validation: 0.15306489255188194]
	TIME [epoch: 5.98 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17017283161821467		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.17017283161821467 | validation: 0.11492689959120723]
	TIME [epoch: 5.98 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1229395948751622		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.1229395948751622 | validation: 0.24952631124474142]
	TIME [epoch: 5.99 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15588359900364146		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.15588359900364146 | validation: 0.12263452321138363]
	TIME [epoch: 5.98 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1670820534586735		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.1670820534586735 | validation: 0.25899856255192527]
	TIME [epoch: 5.99 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1574091368121625		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.1574091368121625 | validation: 0.09916977525879737]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_469.pth
	Model improved!!!
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1175193522458455		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.1175193522458455 | validation: 0.128770840514398]
	TIME [epoch: 5.99 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11231266907088294		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.11231266907088294 | validation: 0.16607224448102578]
	TIME [epoch: 5.97 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12292823601107312		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.12292823601107312 | validation: 0.13137773384308615]
	TIME [epoch: 5.97 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13591461447690353		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.13591461447690353 | validation: 0.17273564238494876]
	TIME [epoch: 5.98 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1660174989571365		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.1660174989571365 | validation: 0.20821119481635042]
	TIME [epoch: 5.99 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2009988654127672		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.2009988654127672 | validation: 0.24119023144365964]
	TIME [epoch: 5.98 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17463795106205396		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.17463795106205396 | validation: 0.11492240625400876]
	TIME [epoch: 5.99 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15389694404834017		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.15389694404834017 | validation: 0.22720221580077934]
	TIME [epoch: 5.98 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15109995982991883		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.15109995982991883 | validation: 0.1180473764960363]
	TIME [epoch: 5.98 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16165262516681064		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.16165262516681064 | validation: 0.14872497263255768]
	TIME [epoch: 5.99 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11299731214672434		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.11299731214672434 | validation: 0.11942055184061742]
	TIME [epoch: 5.98 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11082657698005723		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.11082657698005723 | validation: 0.13901820209979954]
	TIME [epoch: 5.98 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12361902220685558		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.12361902220685558 | validation: 0.12106294847548336]
	TIME [epoch: 5.97 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12610515764463048		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.12610515764463048 | validation: 0.24120595369290393]
	TIME [epoch: 5.98 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16337630643414527		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.16337630643414527 | validation: 0.1681031312382638]
	TIME [epoch: 5.98 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18474138902655965		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.18474138902655965 | validation: 0.2791254753786497]
	TIME [epoch: 5.98 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1895648798987071		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.1895648798987071 | validation: 0.10722496045201187]
	TIME [epoch: 5.99 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14164117709089777		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.14164117709089777 | validation: 0.29195043659823744]
	TIME [epoch: 6 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17318843542957787		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.17318843542957787 | validation: 0.09649384034683259]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_488.pth
	Model improved!!!
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12674083814977144		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.12674083814977144 | validation: 0.1746929661728592]
	TIME [epoch: 5.98 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11901651879841595		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.11901651879841595 | validation: 0.09988135865910437]
	TIME [epoch: 5.98 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10972626212293822		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.10972626212293822 | validation: 0.1041128971780274]
	TIME [epoch: 5.98 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10489788124380595		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.10489788124380595 | validation: 0.15911448954851407]
	TIME [epoch: 5.98 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11207705351639505		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.11207705351639505 | validation: 0.09735939605412419]
	TIME [epoch: 5.98 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12143759663361996		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.12143759663361996 | validation: 0.24648973405217936]
	TIME [epoch: 5.98 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15566283547242799		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.15566283547242799 | validation: 0.1279168337983899]
	TIME [epoch: 5.99 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18452689249121243		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.18452689249121243 | validation: 0.19105288195801118]
	TIME [epoch: 5.98 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11636689672515717		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.11636689672515717 | validation: 0.11902861263061167]
	TIME [epoch: 5.99 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11285618619920804		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.11285618619920804 | validation: 0.2050351884467387]
	TIME [epoch: 5.98 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1639542947294051		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.1639542947294051 | validation: 0.18874308529522252]
	TIME [epoch: 5.98 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20256679700958757		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.20256679700958757 | validation: 0.41335548958495416]
	TIME [epoch: 5.99 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2560525674310615		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.2560525674310615 | validation: 0.10725688184034143]
	TIME [epoch: 196 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12570888216835585		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.12570888216835585 | validation: 0.10421881784632708]
	TIME [epoch: 12.7 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10682117616164165		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.10682117616164165 | validation: 0.1950694279667775]
	TIME [epoch: 12.7 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12054003275103208		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.12054003275103208 | validation: 0.09022371731735294]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_504.pth
	Model improved!!!
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11035040284231791		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.11035040284231791 | validation: 0.12322834673659384]
	TIME [epoch: 12.7 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1002377820688947		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.1002377820688947 | validation: 0.09855675829376649]
	TIME [epoch: 12.7 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09997740875792169		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.09997740875792169 | validation: 0.13220385665977918]
	TIME [epoch: 12.7 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10664783473760987		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.10664783473760987 | validation: 0.1377548249302317]
	TIME [epoch: 12.7 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14353092496269526		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.14353092496269526 | validation: 0.14880092682261464]
	TIME [epoch: 12.7 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12948920084435606		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.12948920084435606 | validation: 0.13903483098171834]
	TIME [epoch: 12.7 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11448760588369453		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.11448760588369453 | validation: 0.1030748100514021]
	TIME [epoch: 12.7 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12141918893690633		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.12141918893690633 | validation: 0.18704959663777543]
	TIME [epoch: 12.7 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11611475940153884		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.11611475940153884 | validation: 0.10641302311143903]
	TIME [epoch: 12.7 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13416635483417744		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.13416635483417744 | validation: 0.3281442790646847]
	TIME [epoch: 12.7 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2062445132479688		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.2062445132479688 | validation: 0.23703067352677254]
	TIME [epoch: 12.7 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2578068573683769		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.2578068573683769 | validation: 0.1837915316347235]
	TIME [epoch: 12.7 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13705782222706456		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.13705782222706456 | validation: 0.13104237383275827]
	TIME [epoch: 12.7 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1009353044298016		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.1009353044298016 | validation: 0.10096706337896806]
	TIME [epoch: 12.7 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10257416869424436		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.10257416869424436 | validation: 0.09923169757472855]
	TIME [epoch: 12.7 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09985930735139573		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.09985930735139573 | validation: 0.1597145805934641]
	TIME [epoch: 12.7 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10585513663150753		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.10585513663150753 | validation: 0.10025845615801943]
	TIME [epoch: 12.7 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1285520507780626		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1285520507780626 | validation: 0.23863188090091395]
	TIME [epoch: 12.7 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1376584046440981		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.1376584046440981 | validation: 0.10383922411125293]
	TIME [epoch: 12.7 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13499992907061337		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.13499992907061337 | validation: 0.14328028856345978]
	TIME [epoch: 12.7 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10598723969537747		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.10598723969537747 | validation: 0.10463568272063922]
	TIME [epoch: 12.7 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09740874056272106		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.09740874056272106 | validation: 0.09848583563643663]
	TIME [epoch: 12.7 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09349935040408036		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.09349935040408036 | validation: 0.13147092112199696]
	TIME [epoch: 12.7 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11194242261705908		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.11194242261705908 | validation: 0.19528357755075842]
	TIME [epoch: 12.7 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15207762455795695		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.15207762455795695 | validation: 0.23349859789360272]
	TIME [epoch: 12.7 sec]
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26676284625368213		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.26676284625368213 | validation: 0.37128497566882696]
	TIME [epoch: 12.7 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21382453979121865		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.21382453979121865 | validation: 0.09695847822124631]
	TIME [epoch: 12.7 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09577807268268468		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.09577807268268468 | validation: 0.08794681713948779]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_532.pth
	Model improved!!!
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11667022144492753		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.11667022144492753 | validation: 0.17128524204076967]
	TIME [epoch: 12.7 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11346120037717729		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.11346120037717729 | validation: 0.09863407742734517]
	TIME [epoch: 12.7 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12704300943119584		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.12704300943119584 | validation: 0.27306215605242584]
	TIME [epoch: 12.7 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16049557389936617		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.16049557389936617 | validation: 0.10809979786187987]
	TIME [epoch: 12.7 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12072903132549666		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.12072903132549666 | validation: 0.09379853634865709]
	TIME [epoch: 12.7 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10066888719092024		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.10066888719092024 | validation: 0.16746394039881551]
	TIME [epoch: 12.7 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10538400055255503		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.10538400055255503 | validation: 0.08551885114482304]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_539.pth
	Model improved!!!
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10971960766980904		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.10971960766980904 | validation: 0.14486140300769645]
	TIME [epoch: 12.7 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09611847020390388		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.09611847020390388 | validation: 0.09868479709792409]
	TIME [epoch: 12.7 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10392931947860604		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.10392931947860604 | validation: 0.1644468465580604]
	TIME [epoch: 12.7 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14268614107709252		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.14268614107709252 | validation: 0.17736594220315846]
	TIME [epoch: 12.7 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17939074213285217		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.17939074213285217 | validation: 0.15287677889672044]
	TIME [epoch: 12.7 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10956588304684305		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.10956588304684305 | validation: 0.08646649414660294]
	TIME [epoch: 12.7 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09320395327551736		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.09320395327551736 | validation: 0.13902380635022213]
	TIME [epoch: 12.7 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10568571922270942		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.10568571922270942 | validation: 0.07832521748197317]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_547.pth
	Model improved!!!
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11103114275528317		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.11103114275528317 | validation: 0.1529437083606204]
	TIME [epoch: 12.6 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10209472840947		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.10209472840947 | validation: 0.09007825847630269]
	TIME [epoch: 12.6 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10548480543465724		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.10548480543465724 | validation: 0.17787886452714555]
	TIME [epoch: 12.7 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10502908047313318		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.10502908047313318 | validation: 0.09833256896541061]
	TIME [epoch: 12.8 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1356278705622951		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1356278705622951 | validation: 0.2854856970640394]
	TIME [epoch: 12.7 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1618234248054021		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.1618234248054021 | validation: 0.17955795892071316]
	TIME [epoch: 12.7 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1904145245474955		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.1904145245474955 | validation: 0.09907491850695624]
	TIME [epoch: 12.7 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11270042999132712		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.11270042999132712 | validation: 0.1846597515965864]
	TIME [epoch: 12.7 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10768005849581268		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.10768005849581268 | validation: 0.11841952687256115]
	TIME [epoch: 12.7 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12974984133919873		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.12974984133919873 | validation: 0.14907075560058203]
	TIME [epoch: 12.7 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12066934171046283		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.12066934171046283 | validation: 0.10876521382040241]
	TIME [epoch: 12.7 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1025737953519246		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1025737953519246 | validation: 0.11629320776168717]
	TIME [epoch: 12.7 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09774395812078596		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.09774395812078596 | validation: 0.09233498078587331]
	TIME [epoch: 12.7 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0954728982759987		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.0954728982759987 | validation: 0.12575578199890117]
	TIME [epoch: 12.7 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596017115448092		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.09596017115448092 | validation: 0.08128738887468856]
	TIME [epoch: 12.7 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09324749532549025		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.09324749532549025 | validation: 0.14357349533343586]
	TIME [epoch: 12.7 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1038986386077862		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.1038986386077862 | validation: 0.09604749380360321]
	TIME [epoch: 12.7 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1295398678759349		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.1295398678759349 | validation: 0.25665566934020084]
	TIME [epoch: 12.7 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15718245258920985		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.15718245258920985 | validation: 0.2181801240327412]
	TIME [epoch: 12.7 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23777121874134544		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.23777121874134544 | validation: 0.0965299764077962]
	TIME [epoch: 12.7 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0896181370675193		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.0896181370675193 | validation: 0.22354034311691057]
	TIME [epoch: 12.7 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1284986358073671		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.1284986358073671 | validation: 0.130392747957582]
	TIME [epoch: 12.7 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13426428570769494		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.13426428570769494 | validation: 0.12280901328077198]
	TIME [epoch: 12.7 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09448001115577365		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.09448001115577365 | validation: 0.13024788867592954]
	TIME [epoch: 12.7 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09467718672044714		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.09467718672044714 | validation: 0.09478962597533637]
	TIME [epoch: 12.7 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12241078734945741		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.12241078734945741 | validation: 0.24588544783189228]
	TIME [epoch: 12.7 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13317491405339696		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.13317491405339696 | validation: 0.08425488011301058]
	TIME [epoch: 12.7 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10050000223962531		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.10050000223962531 | validation: 0.09578309584421062]
	TIME [epoch: 12.7 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10570488709300391		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.10570488709300391 | validation: 0.19628057934573043]
	TIME [epoch: 12.7 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12463407600935698		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.12463407600935698 | validation: 0.09076892539751606]
	TIME [epoch: 12.7 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10071035140303788		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.10071035140303788 | validation: 0.10826001471222749]
	TIME [epoch: 12.7 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09731639165165692		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.09731639165165692 | validation: 0.09699361585556562]
	TIME [epoch: 12.7 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0918257009051166		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.0918257009051166 | validation: 0.09307706663523056]
	TIME [epoch: 12.7 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08663937187861033		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.08663937187861033 | validation: 0.12488359404655314]
	TIME [epoch: 12.7 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08695223698464064		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.08695223698464064 | validation: 0.08146341151493655]
	TIME [epoch: 12.7 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09131808592641423		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.09131808592641423 | validation: 0.15050841740883622]
	TIME [epoch: 12.7 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1038706576045296		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.1038706576045296 | validation: 0.08991710904237393]
	TIME [epoch: 12.7 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13295012695129704		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.13295012695129704 | validation: 0.20403213037127418]
	TIME [epoch: 12.7 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12893204894382826		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.12893204894382826 | validation: 0.1633389095644552]
	TIME [epoch: 12.7 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17164310211565148		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.17164310211565148 | validation: 0.12485672576338405]
	TIME [epoch: 12.7 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11843823392349691		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.11843823392349691 | validation: 0.10489203429304624]
	TIME [epoch: 12.7 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09314394736974904		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.09314394736974904 | validation: 0.10592446060521726]
	TIME [epoch: 12.7 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08590249219629223		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.08590249219629223 | validation: 0.07900828706002529]
	TIME [epoch: 12.7 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0858266823624435		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.0858266823624435 | validation: 0.10435054960952131]
	TIME [epoch: 12.7 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08744287625278344		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.08744287625278344 | validation: 0.11331044944817102]
	TIME [epoch: 12.7 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09516987279662381		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.09516987279662381 | validation: 0.12815724299594922]
	TIME [epoch: 12.7 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11750037938507975		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.11750037938507975 | validation: 0.17849148092962266]
	TIME [epoch: 12.7 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13033859646532853		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.13033859646532853 | validation: 0.1380084912709754]
	TIME [epoch: 12.7 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1643939172606381		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.1643939172606381 | validation: 0.21478821058879238]
	TIME [epoch: 12.7 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12148944051003913		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.12148944051003913 | validation: 0.07785658311208576]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_597.pth
	Model improved!!!
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09978730990605678		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.09978730990605678 | validation: 0.09272899510320698]
	TIME [epoch: 12.7 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08826330965703288		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.08826330965703288 | validation: 0.11342774343232505]
	TIME [epoch: 12.7 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08604755389476339		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.08604755389476339 | validation: 0.07463696106264549]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_600.pth
	Model improved!!!
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08539681295780142		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.08539681295780142 | validation: 0.10854339816071407]
	TIME [epoch: 12.6 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08646852576903773		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.08646852576903773 | validation: 0.06902273742789662]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_602.pth
	Model improved!!!
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09552412573729767		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.09552412573729767 | validation: 0.20678951147019475]
	TIME [epoch: 12.7 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1230985190242912		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.1230985190242912 | validation: 0.0872135731385904]
	TIME [epoch: 12.7 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11038584108837986		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.11038584108837986 | validation: 0.16596629858022638]
	TIME [epoch: 12.7 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11302861484114513		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.11302861484114513 | validation: 0.19154762235509082]
	TIME [epoch: 12.7 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19921074184737733		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.19921074184737733 | validation: 0.1133353539390438]
	TIME [epoch: 12.7 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09875043196127792		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.09875043196127792 | validation: 0.10456466018162103]
	TIME [epoch: 12.7 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08362547802773002		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.08362547802773002 | validation: 0.11020093030971752]
	TIME [epoch: 12.7 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1374536229315151		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.1374536229315151 | validation: 0.13693893797749943]
	TIME [epoch: 12.7 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09141436394213172		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.09141436394213172 | validation: 0.10575348684567812]
	TIME [epoch: 12.7 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08878046168851939		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.08878046168851939 | validation: 0.09141002740607396]
	TIME [epoch: 12.7 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1111198022351839		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.1111198022351839 | validation: 0.15899288494237773]
	TIME [epoch: 12.7 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10477887672032861		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.10477887672032861 | validation: 0.06404143261903876]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_614.pth
	Model improved!!!
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08765424321290045		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.08765424321290045 | validation: 0.09888561741746102]
	TIME [epoch: 12.7 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08416710877266181		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.08416710877266181 | validation: 0.08453436269147681]
	TIME [epoch: 12.7 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08513342090618775		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.08513342090618775 | validation: 0.12176981461917634]
	TIME [epoch: 12.7 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10030762589554776		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.10030762589554776 | validation: 0.16341680200450423]
	TIME [epoch: 12.7 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14939177116184926		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.14939177116184926 | validation: 0.22165516129552856]
	TIME [epoch: 12.7 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2323543039308332		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.2323543039308332 | validation: 0.21109262218032876]
	TIME [epoch: 12.7 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14907200090678202		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.14907200090678202 | validation: 0.14566760865100176]
	TIME [epoch: 12.7 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11994286576977575		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.11994286576977575 | validation: 0.07109556336517578]
	TIME [epoch: 12.7 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09287370240527641		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.09287370240527641 | validation: 0.10721580279728711]
	TIME [epoch: 12.7 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08947346915791331		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.08947346915791331 | validation: 0.07133939345067329]
	TIME [epoch: 12.7 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271875857465232		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.08271875857465232 | validation: 0.0910521994710023]
	TIME [epoch: 12.7 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08808475460091651		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.08808475460091651 | validation: 0.10635387187201394]
	TIME [epoch: 12.7 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08487220816216735		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.08487220816216735 | validation: 0.07546754599678418]
	TIME [epoch: 12.7 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07724199595767733		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.07724199595767733 | validation: 0.09014354605102232]
	TIME [epoch: 12.7 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07756110550500438		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.07756110550500438 | validation: 0.1000386831375003]
	TIME [epoch: 12.7 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07955973964260701		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.07955973964260701 | validation: 0.07386163314189371]
	TIME [epoch: 12.7 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07801282957770049		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.07801282957770049 | validation: 0.07659333006236219]
	TIME [epoch: 12.7 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835205086110428		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.07835205086110428 | validation: 0.0980990601210387]
	TIME [epoch: 12.7 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.081517710798525		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.081517710798525 | validation: 0.06890922260501196]
	TIME [epoch: 12.7 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0852617226664073		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.0852617226664073 | validation: 0.19017978461535262]
	TIME [epoch: 12.7 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1143429058575424		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.1143429058575424 | validation: 0.07898293295691207]
	TIME [epoch: 12.7 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11781559682810043		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.11781559682810043 | validation: 0.23342276035677312]
	TIME [epoch: 12.7 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1536020333305656		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.1536020333305656 | validation: 0.17145407844549612]
	TIME [epoch: 12.7 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19093449579905084		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.19093449579905084 | validation: 0.08620123494153716]
	TIME [epoch: 12.7 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08016930398080899		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.08016930398080899 | validation: 0.1621472640788605]
	TIME [epoch: 12.7 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11647636172469047		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.11647636172469047 | validation: 0.08419971539522475]
	TIME [epoch: 12.7 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09153364249267303		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.09153364249267303 | validation: 0.10460654180013954]
	TIME [epoch: 12.7 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0817969621559036		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.0817969621559036 | validation: 0.06942695335418006]
	TIME [epoch: 12.7 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08887043911531739		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.08887043911531739 | validation: 0.1397008573727043]
	TIME [epoch: 12.7 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09108556017734991		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.09108556017734991 | validation: 0.07060116578889256]
	TIME [epoch: 12.7 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08147266828931318		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.08147266828931318 | validation: 0.11661417224213491]
	TIME [epoch: 12.7 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08403357428724896		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.08403357428724896 | validation: 0.07779929138684995]
	TIME [epoch: 12.7 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08779055964082912		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.08779055964082912 | validation: 0.13599343787219015]
	TIME [epoch: 12.7 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09800541322368533		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.09800541322368533 | validation: 0.0862064098127947]
	TIME [epoch: 12.7 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11473030934748678		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.11473030934748678 | validation: 0.1328783877387274]
	TIME [epoch: 12.7 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816722782987102		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.08816722782987102 | validation: 0.07921730331634201]
	TIME [epoch: 12.7 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08302039902081461		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.08302039902081461 | validation: 0.1055553899443428]
	TIME [epoch: 12.7 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0952643123293047		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.0952643123293047 | validation: 0.09516198900450278]
	TIME [epoch: 12.7 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08204304702911872		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.08204304702911872 | validation: 0.0717671856541445]
	TIME [epoch: 12.7 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0791397948408063		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.0791397948408063 | validation: 0.1385836024884867]
	TIME [epoch: 12.7 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08623671310352565		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.08623671310352565 | validation: 0.07261416007862517]
	TIME [epoch: 12.7 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.110549149673889		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.110549149673889 | validation: 0.21765068810930802]
	TIME [epoch: 12.7 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11213977187108423		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.11213977187108423 | validation: 0.07188347085742339]
	TIME [epoch: 12.7 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08301040199312679		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.08301040199312679 | validation: 0.07332863222867798]
	TIME [epoch: 12.7 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08120862243725468		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.08120862243725468 | validation: 0.08635722273907064]
	TIME [epoch: 12.7 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08462410129785289		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.08462410129785289 | validation: 0.10153196889917725]
	TIME [epoch: 12.7 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0926566074188602		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.0926566074188602 | validation: 0.09450177904701636]
	TIME [epoch: 12.7 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09912366718250773		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.09912366718250773 | validation: 0.13244853033715218]
	TIME [epoch: 12.8 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09003739031511114		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.09003739031511114 | validation: 0.10704398403024969]
	TIME [epoch: 12.7 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12571931901983588		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.12571931901983588 | validation: 0.15373066862802862]
	TIME [epoch: 12.7 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09475195852932496		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.09475195852932496 | validation: 0.06645013964803635]
	TIME [epoch: 12.7 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07930218629146746		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.07930218629146746 | validation: 0.38091371923914114]
	TIME [epoch: 12.7 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42693274794369646		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.42693274794369646 | validation: 0.07838585926066553]
	TIME [epoch: 12.7 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07898654395301793		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.07898654395301793 | validation: 0.2908292603313085]
	TIME [epoch: 12.8 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1755630275827682		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.1755630275827682 | validation: 0.10714182605669764]
	TIME [epoch: 12.8 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841080711491252		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.0841080711491252 | validation: 0.07536958496180768]
	TIME [epoch: 12.7 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09259605051146434		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.09259605051146434 | validation: 0.07815223051652742]
	TIME [epoch: 12.8 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07569851571017956		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.07569851571017956 | validation: 0.07399023409216215]
	TIME [epoch: 12.8 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07694201335478063		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.07694201335478063 | validation: 0.0831582743864504]
	TIME [epoch: 12.7 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07652290101292501		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.07652290101292501 | validation: 0.08848427322697595]
	TIME [epoch: 12.7 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09827383216722463		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.09827383216722463 | validation: 0.06828519056233753]
	TIME [epoch: 12.8 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08085503349829215		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.08085503349829215 | validation: 0.07701550591175353]
	TIME [epoch: 12.8 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07418107398549487		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.07418107398549487 | validation: 0.34008343724406725]
	TIME [epoch: 12.8 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3458892447776802		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.3458892447776802 | validation: 0.15359683118107217]
	TIME [epoch: 12.7 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16902615507330598		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.16902615507330598 | validation: 0.16548601932072657]
	TIME [epoch: 12.8 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1174320882054584		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.1174320882054584 | validation: 0.11600867416367391]
	TIME [epoch: 12.8 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08792996431458393		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.08792996431458393 | validation: 0.08599901118469469]
	TIME [epoch: 12.8 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08716323156103727		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.08716323156103727 | validation: 0.07722406305407203]
	TIME [epoch: 12.7 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07770563231599638		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.07770563231599638 | validation: 0.0944141118421658]
	TIME [epoch: 12.7 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07893816809901137		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.07893816809901137 | validation: 0.07477090450350463]
	TIME [epoch: 12.8 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07618153566571334		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.07618153566571334 | validation: 0.07731674053251202]
	TIME [epoch: 12.7 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08074536400258611		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.08074536400258611 | validation: 0.07962464644100203]
	TIME [epoch: 12.8 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07498956803719128		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.07498956803719128 | validation: 0.08134845608625035]
	TIME [epoch: 12.7 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07502016042099952		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.07502016042099952 | validation: 0.07774147629725178]
	TIME [epoch: 12.7 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07100561875784457		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.07100561875784457 | validation: 0.07737849852325109]
	TIME [epoch: 12.7 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07542423568137484		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.07542423568137484 | validation: 0.06507002739718656]
	TIME [epoch: 12.7 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07597718523333456		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.07597718523333456 | validation: 0.07793653658357137]
	TIME [epoch: 12.7 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07698559234181722		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.07698559234181722 | validation: 0.10518116119846886]
	TIME [epoch: 12.7 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0833247219770944		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.0833247219770944 | validation: 0.08095915993519763]
	TIME [epoch: 12.7 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09240958921599862		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.09240958921599862 | validation: 0.10006813217951467]
	TIME [epoch: 12.7 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09320840026620589		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.09320840026620589 | validation: 0.10827543004192618]
	TIME [epoch: 12.7 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892562297156988		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.0892562297156988 | validation: 0.08824580585059656]
	TIME [epoch: 12.7 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09017407476364855		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.09017407476364855 | validation: 0.0918210705815517]
	TIME [epoch: 12.7 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0736163836400351		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.0736163836400351 | validation: 0.07517354476392397]
	TIME [epoch: 12.7 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07835145150478214		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.07835145150478214 | validation: 0.18847523144170256]
	TIME [epoch: 12.7 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1244680354643445		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.1244680354643445 | validation: 0.16354358819170742]
	TIME [epoch: 12.7 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17634730190135176		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.17634730190135176 | validation: 0.08494713613976346]
	TIME [epoch: 12.7 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07203052590929365		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.07203052590929365 | validation: 0.15172876701950397]
	TIME [epoch: 12.7 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10747225931473882		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.10747225931473882 | validation: 0.11320145725590805]
	TIME [epoch: 12.7 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12277978673715913		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.12277978673715913 | validation: 0.07384123571835918]
	TIME [epoch: 12.7 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09102450825715262		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09102450825715262 | validation: 0.13884335773748968]
	TIME [epoch: 12.7 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10499228496707391		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.10499228496707391 | validation: 0.0955548329804562]
	TIME [epoch: 12.7 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07201236864700278		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.07201236864700278 | validation: 0.06942191456801126]
	TIME [epoch: 12.7 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08503764356507003		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.08503764356507003 | validation: 0.14181544674672955]
	TIME [epoch: 12.7 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08637808423471519		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.08637808423471519 | validation: 0.07857487266566703]
	TIME [epoch: 12.7 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182942244440718		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.07182942244440718 | validation: 0.06465133564373703]
	TIME [epoch: 12.7 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157879031142712		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.07157879031142712 | validation: 0.08869882068885229]
	TIME [epoch: 12.7 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07435291528535891		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.07435291528535891 | validation: 0.06941894975348975]
	TIME [epoch: 12.7 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07484847358671763		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.07484847358671763 | validation: 0.09430861966798698]
	TIME [epoch: 12.7 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.074049760775683		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.074049760775683 | validation: 0.0688393996146691]
	TIME [epoch: 12.7 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08395121386399858		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.08395121386399858 | validation: 0.14213238222622845]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd1_20241105_150837/states/model_phi1_4c_v_mmd1_715.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 5727.837 seconds.
