Args:
Namespace(name='model_phi1_4b_v_mmd1', outdir='out/model_training/model_phi1_4b_v_mmd1', training_data='data/training_data/data_phi1_4b/training', validation_data='data/training_data/data_phi1_4b/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='exponential_decay', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=1e-05, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 2250477865

Training model...

Saving initial model state to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.741790769081515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.741790769081515 | validation: 5.842841194746656]
	TIME [epoch: 175 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 6.323422907929223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 6.323422907929223 | validation: 4.777048189753182]
	TIME [epoch: 1.49 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.888194604747828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.888194604747828 | validation: 4.4820943465758765]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.673288196533664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.673288196533664 | validation: 4.242328901710417]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_4.pth
	Model improved!!!
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.438596472436633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.438596472436633 | validation: 4.092934253900966]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.3545585240395965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.3545585240395965 | validation: 4.377758977566354]
	TIME [epoch: 1.41 sec]
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.468749713355188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.468749713355188 | validation: 4.083517898928071]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.346943183480735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.346943183480735 | validation: 3.9252694497373555]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.125481539858518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.125481539858518 | validation: 4.0336691732741405]
	TIME [epoch: 1.41 sec]
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.121502482765297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.121502482765297 | validation: 3.9543008391708803]
	TIME [epoch: 1.41 sec]
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.227148173276848		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.227148173276848 | validation: 3.8541835002290146]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9770275934938866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9770275934938866 | validation: 3.796040500942317]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_12.pth
	Model improved!!!
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.925994971838727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.925994971838727 | validation: 3.7065106057993717]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_13.pth
	Model improved!!!
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.9089765150865214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.9089765150865214 | validation: 3.881187854271341]
	TIME [epoch: 1.42 sec]
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.940383027671651		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.940383027671651 | validation: 3.768478187379731]
	TIME [epoch: 1.46 sec]
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.138589088439857		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.138589088439857 | validation: 3.65653045661732]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7905079612535184		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7905079612535184 | validation: 3.690651542846707]
	TIME [epoch: 1.41 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.7824245044462224		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.7824245044462224 | validation: 3.550492086796016]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.858251519249175		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.858251519249175 | validation: 3.673644846048283]
	TIME [epoch: 1.41 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.779195188737733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.779195188737733 | validation: 3.4704858059776544]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.737419048065648		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.737419048065648 | validation: 3.567291386138452]
	TIME [epoch: 1.41 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6782222438940044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6782222438940044 | validation: 3.413680053555959]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_22.pth
	Model improved!!!
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6677643769127695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6677643769127695 | validation: 3.5298269492036414]
	TIME [epoch: 1.41 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.643022897555227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.643022897555227 | validation: 3.3651183288817665]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_24.pth
	Model improved!!!
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.652856454211367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.652856454211367 | validation: 3.51812597279139]
	TIME [epoch: 1.41 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6276438963432818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6276438963432818 | validation: 3.3354902128048773]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.6295615807014827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.6295615807014827 | validation: 3.4314397136759256]
	TIME [epoch: 1.41 sec]
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5597062595845728		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5597062595845728 | validation: 3.288900368325735]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_28.pth
	Model improved!!!
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5313367472512227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5313367472512227 | validation: 3.3593137042708023]
	TIME [epoch: 1.41 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4989673396598056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4989673396598056 | validation: 3.249072445368849]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_30.pth
	Model improved!!!
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.491585950686275		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.491585950686275 | validation: 3.356866401479161]
	TIME [epoch: 1.41 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4817967621393633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4817967621393633 | validation: 3.2201901074123946]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5059060867352323		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5059060867352323 | validation: 3.356926645132984]
	TIME [epoch: 1.41 sec]
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.479763462647494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.479763462647494 | validation: 3.1902807453534665]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_34.pth
	Model improved!!!
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4634222160383668		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4634222160383668 | validation: 3.2592131695166557]
	TIME [epoch: 1.41 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.4056395020701973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.4056395020701973 | validation: 3.1308422825521327]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_36.pth
	Model improved!!!
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.379841229856335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.379841229856335 | validation: 3.2019030167854043]
	TIME [epoch: 1.42 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3566401880364687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3566401880364687 | validation: 3.0928964536744106]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_38.pth
	Model improved!!!
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.349616875882676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.349616875882676 | validation: 3.1859516347585832]
	TIME [epoch: 1.41 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.34311718216375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.34311718216375 | validation: 3.078508043663083]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_40.pth
	Model improved!!!
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.35362508568189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.35362508568189 | validation: 3.1906532457443273]
	TIME [epoch: 1.41 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.338499295273008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.338499295273008 | validation: 3.0440939265274776]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.3362879742079703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.3362879742079703 | validation: 3.1286894338510316]
	TIME [epoch: 1.41 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2842801520814597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2842801520814597 | validation: 3.016135717659383]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_44.pth
	Model improved!!!
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.261730683578418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.261730683578418 | validation: 3.056165022605599]
	TIME [epoch: 1.41 sec]
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.240952720706463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.240952720706463 | validation: 2.9822104274426966]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2314796717051673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2314796717051673 | validation: 3.048035027478365]
	TIME [epoch: 1.41 sec]
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.221892514595107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.221892514595107 | validation: 2.9768718932991636]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_48.pth
	Model improved!!!
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.231304577712226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.231304577712226 | validation: 3.0491937430240683]
	TIME [epoch: 1.41 sec]
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.232279476183478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.232279476183478 | validation: 2.95218620006869]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2412481932458235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2412481932458235 | validation: 3.0053335334309357]
	TIME [epoch: 1.41 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.194610822559689		[learning rate: 0.0099646]
	Learning Rate: 0.00996464
	LOSS [training: 3.194610822559689 | validation: 2.9210407404188086]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_52.pth
	Model improved!!!
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.16893076612233		[learning rate: 0.0099294]
	Learning Rate: 0.0099294
	LOSS [training: 3.16893076612233 | validation: 2.9549217134192465]
	TIME [epoch: 1.41 sec]
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.15174483611625		[learning rate: 0.0098943]
	Learning Rate: 0.00989429
	LOSS [training: 3.15174483611625 | validation: 2.9100450722437294]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_54.pth
	Model improved!!!
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.141343849338806		[learning rate: 0.0098593]
	Learning Rate: 0.0098593
	LOSS [training: 3.141343849338806 | validation: 2.895333899144891]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_55.pth
	Model improved!!!
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1233682724498864		[learning rate: 0.0098244]
	Learning Rate: 0.00982444
	LOSS [training: 3.1233682724498864 | validation: 2.9080897965916925]
	TIME [epoch: 1.41 sec]
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.120374004804964		[learning rate: 0.0097897]
	Learning Rate: 0.0097897
	LOSS [training: 3.120374004804964 | validation: 2.8729122790728496]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_57.pth
	Model improved!!!
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1137578585715593		[learning rate: 0.0097551]
	Learning Rate: 0.00975508
	LOSS [training: 3.1137578585715593 | validation: 2.914568653750697]
	TIME [epoch: 1.41 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.118128262360962		[learning rate: 0.0097206]
	Learning Rate: 0.00972058
	LOSS [training: 3.118128262360962 | validation: 2.8686536043935327]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_59.pth
	Model improved!!!
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1402278345386674		[learning rate: 0.0096862]
	Learning Rate: 0.00968621
	LOSS [training: 3.1402278345386674 | validation: 2.942184551891229]
	TIME [epoch: 1.41 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1217954568333925		[learning rate: 0.009652]
	Learning Rate: 0.00965196
	LOSS [training: 3.1217954568333925 | validation: 2.8365023148409607]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_61.pth
	Model improved!!!
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.10508068461255		[learning rate: 0.0096178]
	Learning Rate: 0.00961783
	LOSS [training: 3.10508068461255 | validation: 2.854825874991891]
	TIME [epoch: 1.41 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.064084029637936		[learning rate: 0.0095838]
	Learning Rate: 0.00958382
	LOSS [training: 3.064084029637936 | validation: 2.818669915410713]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.050620691021032		[learning rate: 0.0095499]
	Learning Rate: 0.00954993
	LOSS [training: 3.050620691021032 | validation: 2.8167348405873134]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_64.pth
	Model improved!!!
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.031488249427675		[learning rate: 0.0095162]
	Learning Rate: 0.00951616
	LOSS [training: 3.031488249427675 | validation: 2.7936672561184777]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_65.pth
	Model improved!!!
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.02573290183255		[learning rate: 0.0094825]
	Learning Rate: 0.0094825
	LOSS [training: 3.02573290183255 | validation: 2.781460268164866]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_66.pth
	Model improved!!!
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.01508466401027		[learning rate: 0.009449]
	Learning Rate: 0.00944897
	LOSS [training: 3.01508466401027 | validation: 2.787229381348423]
	TIME [epoch: 1.41 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.007832376170658		[learning rate: 0.0094156]
	Learning Rate: 0.00941556
	LOSS [training: 3.007832376170658 | validation: 2.768652819193992]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0299738873786466		[learning rate: 0.0093823]
	Learning Rate: 0.00938226
	LOSS [training: 3.0299738873786466 | validation: 2.9364109102704017]
	TIME [epoch: 1.41 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1645804007427785		[learning rate: 0.0093491]
	Learning Rate: 0.00934909
	LOSS [training: 3.1645804007427785 | validation: 2.7554071968777305]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.050949558010541		[learning rate: 0.009316]
	Learning Rate: 0.00931603
	LOSS [training: 3.050949558010541 | validation: 2.772253329795043]
	TIME [epoch: 1.42 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.0002279734102433		[learning rate: 0.0092831]
	Learning Rate: 0.00928308
	LOSS [training: 3.0002279734102433 | validation: 2.7640712488934316]
	TIME [epoch: 1.41 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.998069614566269		[learning rate: 0.0092503]
	Learning Rate: 0.00925026
	LOSS [training: 2.998069614566269 | validation: 2.7437533113429313]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_73.pth
	Model improved!!!
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9764195882992834		[learning rate: 0.0092175]
	Learning Rate: 0.00921755
	LOSS [training: 2.9764195882992834 | validation: 2.74336983617245]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_74.pth
	Model improved!!!
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.979866927855028		[learning rate: 0.009185]
	Learning Rate: 0.00918495
	LOSS [training: 2.979866927855028 | validation: 2.72544578159504]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_75.pth
	Model improved!!!
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.968306292845117		[learning rate: 0.0091525]
	Learning Rate: 0.00915247
	LOSS [training: 2.968306292845117 | validation: 2.7229419249497804]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_76.pth
	Model improved!!!
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.963118035552045		[learning rate: 0.0091201]
	Learning Rate: 0.00912011
	LOSS [training: 2.963118035552045 | validation: 2.7122556637340414]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_77.pth
	Model improved!!!
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9517089240360974		[learning rate: 0.0090879]
	Learning Rate: 0.00908786
	LOSS [training: 2.9517089240360974 | validation: 2.7107957327899386]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_78.pth
	Model improved!!!
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9465407905980747		[learning rate: 0.0090557]
	Learning Rate: 0.00905572
	LOSS [training: 2.9465407905980747 | validation: 2.6918690414010107]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.938241400543179		[learning rate: 0.0090237]
	Learning Rate: 0.0090237
	LOSS [training: 2.938241400543179 | validation: 2.6877595231289213]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_80.pth
	Model improved!!!
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9323844380652617		[learning rate: 0.0089918]
	Learning Rate: 0.00899179
	LOSS [training: 2.9323844380652617 | validation: 2.678960922665808]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_81.pth
	Model improved!!!
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.935192771299703		[learning rate: 0.00896]
	Learning Rate: 0.00895999
	LOSS [training: 2.935192771299703 | validation: 2.7282662276180893]
	TIME [epoch: 1.41 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9735029385249128		[learning rate: 0.0089283]
	Learning Rate: 0.00892831
	LOSS [training: 2.9735029385249128 | validation: 2.783851418596427]
	TIME [epoch: 1.41 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.1213336278069246		[learning rate: 0.0088967]
	Learning Rate: 0.00889674
	LOSS [training: 3.1213336278069246 | validation: 2.721135952956299]
	TIME [epoch: 1.42 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.963846110567364		[learning rate: 0.0088653]
	Learning Rate: 0.00886528
	LOSS [training: 2.963846110567364 | validation: 2.6842951826278836]
	TIME [epoch: 1.41 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9322234382317562		[learning rate: 0.0088339]
	Learning Rate: 0.00883393
	LOSS [training: 2.9322234382317562 | validation: 2.656877101274265]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_86.pth
	Model improved!!!
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.93949916141891		[learning rate: 0.0088027]
	Learning Rate: 0.00880269
	LOSS [training: 2.93949916141891 | validation: 2.6605529782968795]
	TIME [epoch: 1.4 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9078765984809003		[learning rate: 0.0087716]
	Learning Rate: 0.00877156
	LOSS [training: 2.9078765984809003 | validation: 2.644750864055614]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_88.pth
	Model improved!!!
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.902652159854481		[learning rate: 0.0087405]
	Learning Rate: 0.00874054
	LOSS [training: 2.902652159854481 | validation: 2.653918195784489]
	TIME [epoch: 1.49 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8980024888610627		[learning rate: 0.0087096]
	Learning Rate: 0.00870964
	LOSS [training: 2.8980024888610627 | validation: 2.629054552028318]
	TIME [epoch: 1.4 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_90.pth
	Model improved!!!
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8944227038043353		[learning rate: 0.0086788]
	Learning Rate: 0.00867884
	LOSS [training: 2.8944227038043353 | validation: 2.638830662781656]
	TIME [epoch: 1.41 sec]
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8915035285193644		[learning rate: 0.0086481]
	Learning Rate: 0.00864815
	LOSS [training: 2.8915035285193644 | validation: 2.6294466646741057]
	TIME [epoch: 1.41 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8839225106098034		[learning rate: 0.0086176]
	Learning Rate: 0.00861757
	LOSS [training: 2.8839225106098034 | validation: 2.634643547314603]
	TIME [epoch: 1.41 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8869203623529804		[learning rate: 0.0085871]
	Learning Rate: 0.00858709
	LOSS [training: 2.8869203623529804 | validation: 2.645558948527528]
	TIME [epoch: 1.41 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8794487187138214		[learning rate: 0.0085567]
	Learning Rate: 0.00855673
	LOSS [training: 2.8794487187138214 | validation: 2.6151749854689355]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_95.pth
	Model improved!!!
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8798940257991807		[learning rate: 0.0085265]
	Learning Rate: 0.00852647
	LOSS [training: 2.8798940257991807 | validation: 2.6841698929340283]
	TIME [epoch: 1.42 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.909202312827134		[learning rate: 0.0084963]
	Learning Rate: 0.00849632
	LOSS [training: 2.909202312827134 | validation: 2.6681447633536592]
	TIME [epoch: 1.41 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9588172332296696		[learning rate: 0.0084663]
	Learning Rate: 0.00846627
	LOSS [training: 2.9588172332296696 | validation: 2.6656460131703748]
	TIME [epoch: 1.41 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.904815023711301		[learning rate: 0.0084363]
	Learning Rate: 0.00843634
	LOSS [training: 2.904815023711301 | validation: 2.6073811913029683]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_99.pth
	Model improved!!!
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.869085573943562		[learning rate: 0.0084065]
	Learning Rate: 0.0084065
	LOSS [training: 2.869085573943562 | validation: 2.6063476476410523]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_100.pth
	Model improved!!!
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8593614507245726		[learning rate: 0.0083768]
	Learning Rate: 0.00837678
	LOSS [training: 2.8593614507245726 | validation: 2.6031343751569618]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_101.pth
	Model improved!!!
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8639931374170597		[learning rate: 0.0083472]
	Learning Rate: 0.00834715
	LOSS [training: 2.8639931374170597 | validation: 2.585557010315347]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_102.pth
	Model improved!!!
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8505308364041633		[learning rate: 0.0083176]
	Learning Rate: 0.00831764
	LOSS [training: 2.8505308364041633 | validation: 2.5902096806429094]
	TIME [epoch: 1.42 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8517639031546502		[learning rate: 0.0082882]
	Learning Rate: 0.00828823
	LOSS [training: 2.8517639031546502 | validation: 2.5963843609938544]
	TIME [epoch: 1.42 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.838832463855141		[learning rate: 0.0082589]
	Learning Rate: 0.00825892
	LOSS [training: 2.838832463855141 | validation: 2.58604999340155]
	TIME [epoch: 1.42 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.835365170188859		[learning rate: 0.0082297]
	Learning Rate: 0.00822971
	LOSS [training: 2.835365170188859 | validation: 2.587234217035407]
	TIME [epoch: 1.42 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8367292621410263		[learning rate: 0.0082006]
	Learning Rate: 0.00820061
	LOSS [training: 2.8367292621410263 | validation: 2.628220937541723]
	TIME [epoch: 1.42 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8583797935405686		[learning rate: 0.0081716]
	Learning Rate: 0.00817161
	LOSS [training: 2.8583797935405686 | validation: 2.622645912548083]
	TIME [epoch: 1.42 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9133862590407147		[learning rate: 0.0081427]
	Learning Rate: 0.00814272
	LOSS [training: 2.9133862590407147 | validation: 2.6146571499238345]
	TIME [epoch: 1.42 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.843279340045427		[learning rate: 0.0081139]
	Learning Rate: 0.00811392
	LOSS [training: 2.843279340045427 | validation: 2.579909783067125]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_110.pth
	Model improved!!!
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8134652659946084		[learning rate: 0.0080852]
	Learning Rate: 0.00808523
	LOSS [training: 2.8134652659946084 | validation: 2.5658120650554554]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_111.pth
	Model improved!!!
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8131879807554365		[learning rate: 0.0080566]
	Learning Rate: 0.00805664
	LOSS [training: 2.8131879807554365 | validation: 2.5892483240311885]
	TIME [epoch: 1.42 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.8051785185568816		[learning rate: 0.0080281]
	Learning Rate: 0.00802815
	LOSS [training: 2.8051785185568816 | validation: 2.573299949316273]
	TIME [epoch: 1.42 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.786060813071174		[learning rate: 0.0079998]
	Learning Rate: 0.00799976
	LOSS [training: 2.786060813071174 | validation: 2.549212557588392]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_114.pth
	Model improved!!!
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.768674018919605		[learning rate: 0.0079715]
	Learning Rate: 0.00797147
	LOSS [training: 2.768674018919605 | validation: 2.529795843001241]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_115.pth
	Model improved!!!
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.7177095390941446		[learning rate: 0.0079433]
	Learning Rate: 0.00794328
	LOSS [training: 2.7177095390941446 | validation: 2.4995104673415494]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_116.pth
	Model improved!!!
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.5537663399308843		[learning rate: 0.0079152]
	Learning Rate: 0.00791519
	LOSS [training: 2.5537663399308843 | validation: 2.1521158727825056]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_117.pth
	Model improved!!!
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2147747694552637		[learning rate: 0.0078872]
	Learning Rate: 0.0078872
	LOSS [training: 2.2147747694552637 | validation: 1.807133915230632]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_118.pth
	Model improved!!!
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6942867482553141		[learning rate: 0.0078593]
	Learning Rate: 0.00785931
	LOSS [training: 1.6942867482553141 | validation: 1.716068901873907]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5080540343829065		[learning rate: 0.0078315]
	Learning Rate: 0.00783152
	LOSS [training: 1.5080540343829065 | validation: 1.762873154043806]
	TIME [epoch: 1.42 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.836102067047811		[learning rate: 0.0078038]
	Learning Rate: 0.00780383
	LOSS [training: 1.836102067047811 | validation: 1.5774588874337834]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_121.pth
	Model improved!!!
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.65994570011109		[learning rate: 0.0077762]
	Learning Rate: 0.00777623
	LOSS [training: 1.65994570011109 | validation: 1.400847302480767]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_122.pth
	Model improved!!!
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2485809069566978		[learning rate: 0.0077487]
	Learning Rate: 0.00774873
	LOSS [training: 1.2485809069566978 | validation: 1.1756820057149433]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_123.pth
	Model improved!!!
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1644035709479164		[learning rate: 0.0077213]
	Learning Rate: 0.00772133
	LOSS [training: 1.1644035709479164 | validation: 1.0983408859629027]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_124.pth
	Model improved!!!
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0743758475848881		[learning rate: 0.007694]
	Learning Rate: 0.00769403
	LOSS [training: 1.0743758475848881 | validation: 1.1529505563979332]
	TIME [epoch: 1.41 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.055932139302914		[learning rate: 0.0076668]
	Learning Rate: 0.00766682
	LOSS [training: 1.055932139302914 | validation: 1.1062113369145499]
	TIME [epoch: 1.41 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0008342476443464		[learning rate: 0.0076397]
	Learning Rate: 0.00763971
	LOSS [training: 1.0008342476443464 | validation: 1.035265558085231]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_127.pth
	Model improved!!!
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9944481338177488		[learning rate: 0.0076127]
	Learning Rate: 0.0076127
	LOSS [training: 0.9944481338177488 | validation: 1.0532465661699686]
	TIME [epoch: 1.42 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9726075750960262		[learning rate: 0.0075858]
	Learning Rate: 0.00758578
	LOSS [training: 0.9726075750960262 | validation: 1.0556089613883468]
	TIME [epoch: 1.41 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9504818472122665		[learning rate: 0.007559]
	Learning Rate: 0.00755895
	LOSS [training: 0.9504818472122665 | validation: 0.978392523855732]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_130.pth
	Model improved!!!
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9402411475097155		[learning rate: 0.0075322]
	Learning Rate: 0.00753222
	LOSS [training: 0.9402411475097155 | validation: 1.0529698332563135]
	TIME [epoch: 1.41 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9294288077533116		[learning rate: 0.0075056]
	Learning Rate: 0.00750559
	LOSS [training: 0.9294288077533116 | validation: 1.005056875495687]
	TIME [epoch: 1.42 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9166159523235072		[learning rate: 0.007479]
	Learning Rate: 0.00747905
	LOSS [training: 0.9166159523235072 | validation: 1.0570489167135535]
	TIME [epoch: 1.41 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9111031203839421		[learning rate: 0.0074526]
	Learning Rate: 0.0074526
	LOSS [training: 0.9111031203839421 | validation: 0.9708364345031832]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_134.pth
	Model improved!!!
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9045974705271442		[learning rate: 0.0074262]
	Learning Rate: 0.00742624
	LOSS [training: 0.9045974705271442 | validation: 1.1020966151583353]
	TIME [epoch: 1.41 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9069342529940577		[learning rate: 0.0074]
	Learning Rate: 0.00739998
	LOSS [training: 0.9069342529940577 | validation: 0.9291834269925165]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_136.pth
	Model improved!!!
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.900325218896208		[learning rate: 0.0073738]
	Learning Rate: 0.00737382
	LOSS [training: 0.900325218896208 | validation: 1.060182966423559]
	TIME [epoch: 1.42 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9066232634311554		[learning rate: 0.0073477]
	Learning Rate: 0.00734774
	LOSS [training: 0.9066232634311554 | validation: 0.9707863468541635]
	TIME [epoch: 1.42 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8938794803113982		[learning rate: 0.0073218]
	Learning Rate: 0.00732176
	LOSS [training: 0.8938794803113982 | validation: 1.0477853289539567]
	TIME [epoch: 1.42 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8880867948159046		[learning rate: 0.0072959]
	Learning Rate: 0.00729587
	LOSS [training: 0.8880867948159046 | validation: 0.9186805288317589]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_140.pth
	Model improved!!!
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.879916153637076		[learning rate: 0.0072701]
	Learning Rate: 0.00727007
	LOSS [training: 0.879916153637076 | validation: 1.0351402890963841]
	TIME [epoch: 1.42 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8675619689967782		[learning rate: 0.0072444]
	Learning Rate: 0.00724436
	LOSS [training: 0.8675619689967782 | validation: 0.9571248248981601]
	TIME [epoch: 1.42 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8445682308797574		[learning rate: 0.0072187]
	Learning Rate: 0.00721874
	LOSS [training: 0.8445682308797574 | validation: 0.9136937718237772]
	TIME [epoch: 1.42 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_143.pth
	Model improved!!!
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8471501431735817		[learning rate: 0.0071932]
	Learning Rate: 0.00719322
	LOSS [training: 0.8471501431735817 | validation: 0.9845315026869801]
	TIME [epoch: 1.41 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8329252868686391		[learning rate: 0.0071678]
	Learning Rate: 0.00716778
	LOSS [training: 0.8329252868686391 | validation: 0.897028908615596]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_145.pth
	Model improved!!!
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8381725079355999		[learning rate: 0.0071424]
	Learning Rate: 0.00714243
	LOSS [training: 0.8381725079355999 | validation: 0.9331362276584305]
	TIME [epoch: 1.41 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8237359291177441		[learning rate: 0.0071172]
	Learning Rate: 0.00711718
	LOSS [training: 0.8237359291177441 | validation: 0.9409377201317511]
	TIME [epoch: 1.41 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8373833848059441		[learning rate: 0.007092]
	Learning Rate: 0.00709201
	LOSS [training: 0.8373833848059441 | validation: 0.9543358044417105]
	TIME [epoch: 1.41 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8735380262348917		[learning rate: 0.0070669]
	Learning Rate: 0.00706693
	LOSS [training: 0.8735380262348917 | validation: 1.0855697718800308]
	TIME [epoch: 1.41 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9277633124695888		[learning rate: 0.0070419]
	Learning Rate: 0.00704194
	LOSS [training: 0.9277633124695888 | validation: 0.9362621674059084]
	TIME [epoch: 1.41 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8290327484621368		[learning rate: 0.007017]
	Learning Rate: 0.00701704
	LOSS [training: 0.8290327484621368 | validation: 0.8782587299750058]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_151.pth
	Model improved!!!
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8199171012272686		[learning rate: 0.0069922]
	Learning Rate: 0.00699223
	LOSS [training: 0.8199171012272686 | validation: 1.0349002702397074]
	TIME [epoch: 1.41 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8373141058719668		[learning rate: 0.0069675]
	Learning Rate: 0.0069675
	LOSS [training: 0.8373141058719668 | validation: 0.8747904458432413]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_153.pth
	Model improved!!!
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8474357832944158		[learning rate: 0.0069429]
	Learning Rate: 0.00694286
	LOSS [training: 0.8474357832944158 | validation: 1.0120711888220293]
	TIME [epoch: 1.41 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8257931692771477		[learning rate: 0.0069183]
	Learning Rate: 0.00691831
	LOSS [training: 0.8257931692771477 | validation: 0.8918140978036635]
	TIME [epoch: 1.41 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7972080437462005		[learning rate: 0.0068938]
	Learning Rate: 0.00689385
	LOSS [training: 0.7972080437462005 | validation: 0.8706672366524457]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_156.pth
	Model improved!!!
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8065895138355674		[learning rate: 0.0068695]
	Learning Rate: 0.00686947
	LOSS [training: 0.8065895138355674 | validation: 0.9521825782895412]
	TIME [epoch: 1.41 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8048686914021965		[learning rate: 0.0068452]
	Learning Rate: 0.00684518
	LOSS [training: 0.8048686914021965 | validation: 0.8938016398835945]
	TIME [epoch: 1.41 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7886135731786338		[learning rate: 0.006821]
	Learning Rate: 0.00682097
	LOSS [training: 0.7886135731786338 | validation: 0.8969336469877325]
	TIME [epoch: 1.41 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8023454518829303		[learning rate: 0.0067968]
	Learning Rate: 0.00679685
	LOSS [training: 0.8023454518829303 | validation: 0.9752594771286104]
	TIME [epoch: 1.41 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8292435611623296		[learning rate: 0.0067728]
	Learning Rate: 0.00677282
	LOSS [training: 0.8292435611623296 | validation: 0.9344143468499646]
	TIME [epoch: 1.41 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.842090437229576		[learning rate: 0.0067489]
	Learning Rate: 0.00674886
	LOSS [training: 0.842090437229576 | validation: 0.8868475521699705]
	TIME [epoch: 1.41 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8004031373457017		[learning rate: 0.006725]
	Learning Rate: 0.006725
	LOSS [training: 0.8004031373457017 | validation: 0.9714891799939007]
	TIME [epoch: 1.41 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7942015857434909		[learning rate: 0.0067012]
	Learning Rate: 0.00670122
	LOSS [training: 0.7942015857434909 | validation: 0.8627839010347261]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_164.pth
	Model improved!!!
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7842495464517336		[learning rate: 0.0066775]
	Learning Rate: 0.00667752
	LOSS [training: 0.7842495464517336 | validation: 0.8823301354723494]
	TIME [epoch: 1.41 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7768643859934155		[learning rate: 0.0066539]
	Learning Rate: 0.00665391
	LOSS [training: 0.7768643859934155 | validation: 0.9374147926768807]
	TIME [epoch: 1.41 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7850806136935558		[learning rate: 0.0066304]
	Learning Rate: 0.00663038
	LOSS [training: 0.7850806136935558 | validation: 0.8206487160960217]
	TIME [epoch: 1.41 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_167.pth
	Model improved!!!
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8056602975949915		[learning rate: 0.0066069]
	Learning Rate: 0.00660693
	LOSS [training: 0.8056602975949915 | validation: 1.0507054022466584]
	TIME [epoch: 1.41 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8404811739780372		[learning rate: 0.0065836]
	Learning Rate: 0.00658357
	LOSS [training: 0.8404811739780372 | validation: 0.8883050096479073]
	TIME [epoch: 1.42 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7770513627373908		[learning rate: 0.0065603]
	Learning Rate: 0.00656029
	LOSS [training: 0.7770513627373908 | validation: 0.8625394237587948]
	TIME [epoch: 1.42 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7840048232889427		[learning rate: 0.0065371]
	Learning Rate: 0.00653709
	LOSS [training: 0.7840048232889427 | validation: 1.0161783408968963]
	TIME [epoch: 1.42 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8038976313621393		[learning rate: 0.006514]
	Learning Rate: 0.00651398
	LOSS [training: 0.8038976313621393 | validation: 0.8666817273046107]
	TIME [epoch: 1.42 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7966640118611413		[learning rate: 0.0064909]
	Learning Rate: 0.00649094
	LOSS [training: 0.7966640118611413 | validation: 0.8710380645105819]
	TIME [epoch: 1.41 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7903143753072894		[learning rate: 0.006468]
	Learning Rate: 0.00646799
	LOSS [training: 0.7903143753072894 | validation: 0.9710907069705567]
	TIME [epoch: 1.42 sec]
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7990052032591891		[learning rate: 0.0064451]
	Learning Rate: 0.00644512
	LOSS [training: 0.7990052032591891 | validation: 0.8769227685790757]
	TIME [epoch: 1.41 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7748975632749364		[learning rate: 0.0064223]
	Learning Rate: 0.00642233
	LOSS [training: 0.7748975632749364 | validation: 0.8702828893694765]
	TIME [epoch: 1.41 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7632145527239701		[learning rate: 0.0063996]
	Learning Rate: 0.00639961
	LOSS [training: 0.7632145527239701 | validation: 0.871625507574526]
	TIME [epoch: 1.41 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7615065257506269		[learning rate: 0.006377]
	Learning Rate: 0.00637698
	LOSS [training: 0.7615065257506269 | validation: 0.895349125828161]
	TIME [epoch: 1.41 sec]
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7577777585744215		[learning rate: 0.0063544]
	Learning Rate: 0.00635443
	LOSS [training: 0.7577777585744215 | validation: 0.8413828008025667]
	TIME [epoch: 1.4 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7546196263591739		[learning rate: 0.006332]
	Learning Rate: 0.00633196
	LOSS [training: 0.7546196263591739 | validation: 0.9157535244226663]
	TIME [epoch: 1.41 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7572966166372683		[learning rate: 0.0063096]
	Learning Rate: 0.00630957
	LOSS [training: 0.7572966166372683 | validation: 0.8816007280980195]
	TIME [epoch: 1.4 sec]
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7912056799186784		[learning rate: 0.0062873]
	Learning Rate: 0.00628726
	LOSS [training: 0.7912056799186784 | validation: 0.9616961580146886]
	TIME [epoch: 1.41 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8487311109026002		[learning rate: 0.006265]
	Learning Rate: 0.00626503
	LOSS [training: 0.8487311109026002 | validation: 0.97245024835171]
	TIME [epoch: 1.41 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8096895269226153		[learning rate: 0.0062429]
	Learning Rate: 0.00624287
	LOSS [training: 0.8096895269226153 | validation: 0.8626722853055846]
	TIME [epoch: 1.41 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.765696849420423		[learning rate: 0.0062208]
	Learning Rate: 0.0062208
	LOSS [training: 0.765696849420423 | validation: 0.8641178002400971]
	TIME [epoch: 1.41 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486783134273375		[learning rate: 0.0061988]
	Learning Rate: 0.0061988
	LOSS [training: 0.7486783134273375 | validation: 0.952588468610832]
	TIME [epoch: 1.41 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7736192328012714		[learning rate: 0.0061769]
	Learning Rate: 0.00617688
	LOSS [training: 0.7736192328012714 | validation: 0.8518034779450787]
	TIME [epoch: 1.4 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.762853939476008		[learning rate: 0.006155]
	Learning Rate: 0.00615504
	LOSS [training: 0.762853939476008 | validation: 0.8873606021327287]
	TIME [epoch: 1.41 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492176971696956		[learning rate: 0.0061333]
	Learning Rate: 0.00613327
	LOSS [training: 0.7492176971696956 | validation: 0.9206356208419962]
	TIME [epoch: 1.41 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7515716878234988		[learning rate: 0.0061116]
	Learning Rate: 0.00611158
	LOSS [training: 0.7515716878234988 | validation: 0.8395679520492675]
	TIME [epoch: 1.4 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7566040131378862		[learning rate: 0.00609]
	Learning Rate: 0.00608997
	LOSS [training: 0.7566040131378862 | validation: 0.9056987359217598]
	TIME [epoch: 1.41 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7636485648187538		[learning rate: 0.0060684]
	Learning Rate: 0.00606844
	LOSS [training: 0.7636485648187538 | validation: 0.9216901220687657]
	TIME [epoch: 1.41 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7860323369947847		[learning rate: 0.006047]
	Learning Rate: 0.00604698
	LOSS [training: 0.7860323369947847 | validation: 0.9230401188786752]
	TIME [epoch: 1.41 sec]
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8097586018122247		[learning rate: 0.0060256]
	Learning Rate: 0.0060256
	LOSS [training: 0.8097586018122247 | validation: 0.8670301222682139]
	TIME [epoch: 1.41 sec]
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.767666369244612		[learning rate: 0.0060043]
	Learning Rate: 0.00600429
	LOSS [training: 0.767666369244612 | validation: 0.9185842828985191]
	TIME [epoch: 1.41 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7464949907471441		[learning rate: 0.0059831]
	Learning Rate: 0.00598306
	LOSS [training: 0.7464949907471441 | validation: 0.8357510214286542]
	TIME [epoch: 1.41 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7419497415821797		[learning rate: 0.0059619]
	Learning Rate: 0.0059619
	LOSS [training: 0.7419497415821797 | validation: 0.8588448843109711]
	TIME [epoch: 1.41 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406516427673364		[learning rate: 0.0059408]
	Learning Rate: 0.00594082
	LOSS [training: 0.7406516427673364 | validation: 0.8690809555226635]
	TIME [epoch: 1.41 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7368491716345658		[learning rate: 0.0059198]
	Learning Rate: 0.00591981
	LOSS [training: 0.7368491716345658 | validation: 0.8497432371273057]
	TIME [epoch: 1.41 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7477110890982022		[learning rate: 0.0058989]
	Learning Rate: 0.00589888
	LOSS [training: 0.7477110890982022 | validation: 0.9269591579721784]
	TIME [epoch: 1.41 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.771077498781533		[learning rate: 0.005878]
	Learning Rate: 0.00587802
	LOSS [training: 0.771077498781533 | validation: 0.872945612232009]
	TIME [epoch: 184 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8124696876226797		[learning rate: 0.0058572]
	Learning Rate: 0.00585723
	LOSS [training: 0.8124696876226797 | validation: 0.9635599824900659]
	TIME [epoch: 2.8 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8085363980783793		[learning rate: 0.0058365]
	Learning Rate: 0.00583652
	LOSS [training: 0.8085363980783793 | validation: 0.9324392148418124]
	TIME [epoch: 2.78 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7492919635060931		[learning rate: 0.0058159]
	Learning Rate: 0.00581588
	LOSS [training: 0.7492919635060931 | validation: 0.8184633653377724]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_204.pth
	Model improved!!!
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7432102699237476		[learning rate: 0.0057953]
	Learning Rate: 0.00579531
	LOSS [training: 0.7432102699237476 | validation: 0.927427650564562]
	TIME [epoch: 2.78 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7460019193961631		[learning rate: 0.0057748]
	Learning Rate: 0.00577482
	LOSS [training: 0.7460019193961631 | validation: 0.8658499975112751]
	TIME [epoch: 2.78 sec]
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.740590580350875		[learning rate: 0.0057544]
	Learning Rate: 0.0057544
	LOSS [training: 0.740590580350875 | validation: 0.8525501203167217]
	TIME [epoch: 2.78 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7406779559246242		[learning rate: 0.0057341]
	Learning Rate: 0.00573405
	LOSS [training: 0.7406779559246242 | validation: 0.9338627230186444]
	TIME [epoch: 2.78 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341143878868596		[learning rate: 0.0057138]
	Learning Rate: 0.00571377
	LOSS [training: 0.7341143878868596 | validation: 0.8163238392830832]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_209.pth
	Model improved!!!
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401689875611466		[learning rate: 0.0056936]
	Learning Rate: 0.00569357
	LOSS [training: 0.7401689875611466 | validation: 0.90004486917138]
	TIME [epoch: 2.78 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7256991763244943		[learning rate: 0.0056734]
	Learning Rate: 0.00567344
	LOSS [training: 0.7256991763244943 | validation: 0.8450814482438909]
	TIME [epoch: 2.78 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233464669665589		[learning rate: 0.0056534]
	Learning Rate: 0.00565337
	LOSS [training: 0.7233464669665589 | validation: 0.8108613483263902]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_212.pth
	Model improved!!!
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7291581441002216		[learning rate: 0.0056334]
	Learning Rate: 0.00563338
	LOSS [training: 0.7291581441002216 | validation: 0.9189277156696237]
	TIME [epoch: 2.78 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.733189516781846		[learning rate: 0.0056135]
	Learning Rate: 0.00561346
	LOSS [training: 0.733189516781846 | validation: 0.7950087637058805]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.735884056952966		[learning rate: 0.0055936]
	Learning Rate: 0.00559361
	LOSS [training: 0.735884056952966 | validation: 0.9389445407499619]
	TIME [epoch: 2.8 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7528124713273441		[learning rate: 0.0055738]
	Learning Rate: 0.00557383
	LOSS [training: 0.7528124713273441 | validation: 0.9553912875240536]
	TIME [epoch: 2.79 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9237540176194216		[learning rate: 0.0055541]
	Learning Rate: 0.00555412
	LOSS [training: 0.9237540176194216 | validation: 1.028984041147213]
	TIME [epoch: 2.8 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9142228643741146		[learning rate: 0.0055345]
	Learning Rate: 0.00553448
	LOSS [training: 0.9142228643741146 | validation: 0.899335754273256]
	TIME [epoch: 2.79 sec]
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7233354091710862		[learning rate: 0.0055149]
	Learning Rate: 0.00551491
	LOSS [training: 0.7233354091710862 | validation: 0.9375918701219711]
	TIME [epoch: 2.79 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7694323393153171		[learning rate: 0.0054954]
	Learning Rate: 0.00549541
	LOSS [training: 0.7694323393153171 | validation: 0.8821507843660842]
	TIME [epoch: 2.79 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7401137192603383		[learning rate: 0.005476]
	Learning Rate: 0.00547598
	LOSS [training: 0.7401137192603383 | validation: 0.8702512230234856]
	TIME [epoch: 2.79 sec]
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7242468773786953		[learning rate: 0.0054566]
	Learning Rate: 0.00545661
	LOSS [training: 0.7242468773786953 | validation: 0.8526189785052547]
	TIME [epoch: 2.79 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7365456684834355		[learning rate: 0.0054373]
	Learning Rate: 0.00543732
	LOSS [training: 0.7365456684834355 | validation: 0.8781225076169916]
	TIME [epoch: 2.82 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7273256039809911		[learning rate: 0.0054181]
	Learning Rate: 0.00541809
	LOSS [training: 0.7273256039809911 | validation: 0.862698448144347]
	TIME [epoch: 2.79 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7158057672665182		[learning rate: 0.0053989]
	Learning Rate: 0.00539893
	LOSS [training: 0.7158057672665182 | validation: 0.8372488615345202]
	TIME [epoch: 2.79 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7203980775454669		[learning rate: 0.0053798]
	Learning Rate: 0.00537984
	LOSS [training: 0.7203980775454669 | validation: 0.8469157255647721]
	TIME [epoch: 2.79 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7194509578494583		[learning rate: 0.0053608]
	Learning Rate: 0.00536081
	LOSS [training: 0.7194509578494583 | validation: 0.8524551172570863]
	TIME [epoch: 2.79 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7132650011496128		[learning rate: 0.0053419]
	Learning Rate: 0.00534186
	LOSS [training: 0.7132650011496128 | validation: 0.8401410596336403]
	TIME [epoch: 2.79 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7071939183835516		[learning rate: 0.005323]
	Learning Rate: 0.00532297
	LOSS [training: 0.7071939183835516 | validation: 0.8319270574671705]
	TIME [epoch: 2.79 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7147920098147407		[learning rate: 0.0053041]
	Learning Rate: 0.00530415
	LOSS [training: 0.7147920098147407 | validation: 0.8754696005420339]
	TIME [epoch: 2.79 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7586913161987943		[learning rate: 0.0052854]
	Learning Rate: 0.00528539
	LOSS [training: 0.7586913161987943 | validation: 1.0141811017958962]
	TIME [epoch: 2.79 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.915022903238679		[learning rate: 0.0052667]
	Learning Rate: 0.0052667
	LOSS [training: 0.915022903238679 | validation: 0.9555594079031149]
	TIME [epoch: 2.79 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7888767712996066		[learning rate: 0.0052481]
	Learning Rate: 0.00524807
	LOSS [training: 0.7888767712996066 | validation: 0.8576103977144481]
	TIME [epoch: 2.79 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7169457918231754		[learning rate: 0.0052295]
	Learning Rate: 0.00522952
	LOSS [training: 0.7169457918231754 | validation: 0.8500222468005397]
	TIME [epoch: 2.79 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7423795488124203		[learning rate: 0.005211]
	Learning Rate: 0.00521102
	LOSS [training: 0.7423795488124203 | validation: 0.8884156553103892]
	TIME [epoch: 2.79 sec]
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7341655987842433		[learning rate: 0.0051926]
	Learning Rate: 0.0051926
	LOSS [training: 0.7341655987842433 | validation: 0.8445377765617007]
	TIME [epoch: 2.79 sec]
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7182637673034133		[learning rate: 0.0051742]
	Learning Rate: 0.00517423
	LOSS [training: 0.7182637673034133 | validation: 0.8114182363777742]
	TIME [epoch: 2.79 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7215643955138107		[learning rate: 0.0051559]
	Learning Rate: 0.00515594
	LOSS [training: 0.7215643955138107 | validation: 0.8640793937026209]
	TIME [epoch: 2.79 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7046065586511869		[learning rate: 0.0051377]
	Learning Rate: 0.00513771
	LOSS [training: 0.7046065586511869 | validation: 0.8283713677225081]
	TIME [epoch: 2.79 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7077594858374636		[learning rate: 0.0051195]
	Learning Rate: 0.00511954
	LOSS [training: 0.7077594858374636 | validation: 0.8130984021374265]
	TIME [epoch: 2.79 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038179112099218		[learning rate: 0.0051014]
	Learning Rate: 0.00510143
	LOSS [training: 0.7038179112099218 | validation: 0.8971933532660217]
	TIME [epoch: 2.8 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7104104081439059		[learning rate: 0.0050834]
	Learning Rate: 0.0050834
	LOSS [training: 0.7104104081439059 | validation: 0.7918531413270551]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_242.pth
	Model improved!!!
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7143355904170449		[learning rate: 0.0050654]
	Learning Rate: 0.00506542
	LOSS [training: 0.7143355904170449 | validation: 0.9311853004498591]
	TIME [epoch: 2.8 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7308317287734241		[learning rate: 0.0050475]
	Learning Rate: 0.00504751
	LOSS [training: 0.7308317287734241 | validation: 0.9013460980898431]
	TIME [epoch: 2.8 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.803413161570933		[learning rate: 0.0050297]
	Learning Rate: 0.00502966
	LOSS [training: 0.803413161570933 | validation: 0.9135900225289302]
	TIME [epoch: 2.8 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9139863402849565		[learning rate: 0.0050119]
	Learning Rate: 0.00501187
	LOSS [training: 0.9139863402849565 | validation: 0.9374506611137385]
	TIME [epoch: 2.8 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7265274179341888		[learning rate: 0.0049941]
	Learning Rate: 0.00499415
	LOSS [training: 0.7265274179341888 | validation: 0.9070907982900526]
	TIME [epoch: 2.8 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7266051769115017		[learning rate: 0.0049765]
	Learning Rate: 0.00497649
	LOSS [training: 0.7266051769115017 | validation: 0.8612886488960163]
	TIME [epoch: 2.79 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7299520688021771		[learning rate: 0.0049589]
	Learning Rate: 0.00495889
	LOSS [training: 0.7299520688021771 | validation: 0.8495089508568491]
	TIME [epoch: 2.8 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7041003129717496		[learning rate: 0.0049414]
	Learning Rate: 0.00494136
	LOSS [training: 0.7041003129717496 | validation: 0.8436822848410478]
	TIME [epoch: 2.8 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.706299881168417		[learning rate: 0.0049239]
	Learning Rate: 0.00492388
	LOSS [training: 0.706299881168417 | validation: 0.8404227490474352]
	TIME [epoch: 2.8 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710427024746808		[learning rate: 0.0049065]
	Learning Rate: 0.00490647
	LOSS [training: 0.710427024746808 | validation: 0.8160069468055614]
	TIME [epoch: 2.8 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7027936939648615		[learning rate: 0.0048891]
	Learning Rate: 0.00488912
	LOSS [training: 0.7027936939648615 | validation: 0.8745228931484686]
	TIME [epoch: 2.8 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7037968845846301		[learning rate: 0.0048718]
	Learning Rate: 0.00487183
	LOSS [training: 0.7037968845846301 | validation: 0.815189620341577]
	TIME [epoch: 2.8 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7043345494554243		[learning rate: 0.0048546]
	Learning Rate: 0.0048546
	LOSS [training: 0.7043345494554243 | validation: 0.8300676960699083]
	TIME [epoch: 2.8 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7014507847831525		[learning rate: 0.0048374]
	Learning Rate: 0.00483744
	LOSS [training: 0.7014507847831525 | validation: 0.8854129725550278]
	TIME [epoch: 2.8 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.71615883339381		[learning rate: 0.0048203]
	Learning Rate: 0.00482033
	LOSS [training: 0.71615883339381 | validation: 0.9236024743277987]
	TIME [epoch: 2.8 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7916762736678433		[learning rate: 0.0048033]
	Learning Rate: 0.00480329
	LOSS [training: 0.7916762736678433 | validation: 0.9796364093108814]
	TIME [epoch: 2.8 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8375389593365261		[learning rate: 0.0047863]
	Learning Rate: 0.0047863
	LOSS [training: 0.8375389593365261 | validation: 0.854694411136126]
	TIME [epoch: 2.8 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.714605176088713		[learning rate: 0.0047694]
	Learning Rate: 0.00476938
	LOSS [training: 0.714605176088713 | validation: 0.8777554411189243]
	TIME [epoch: 2.8 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7010253082634764		[learning rate: 0.0047525]
	Learning Rate: 0.00475251
	LOSS [training: 0.7010253082634764 | validation: 0.8752273414647758]
	TIME [epoch: 2.8 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.710287928350156		[learning rate: 0.0047357]
	Learning Rate: 0.0047357
	LOSS [training: 0.710287928350156 | validation: 0.8132261328653396]
	TIME [epoch: 2.79 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7090314663099399		[learning rate: 0.004719]
	Learning Rate: 0.00471896
	LOSS [training: 0.7090314663099399 | validation: 0.8862620927556952]
	TIME [epoch: 2.8 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6976422980315008		[learning rate: 0.0047023]
	Learning Rate: 0.00470227
	LOSS [training: 0.6976422980315008 | validation: 0.8452466181535248]
	TIME [epoch: 2.8 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6975912467648575		[learning rate: 0.0046856]
	Learning Rate: 0.00468564
	LOSS [training: 0.6975912467648575 | validation: 0.8213710416617265]
	TIME [epoch: 2.8 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695797424029905		[learning rate: 0.0046691]
	Learning Rate: 0.00466907
	LOSS [training: 0.695797424029905 | validation: 0.8482834316880808]
	TIME [epoch: 2.8 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6899700670314166		[learning rate: 0.0046526]
	Learning Rate: 0.00465256
	LOSS [training: 0.6899700670314166 | validation: 0.8392905258923258]
	TIME [epoch: 2.8 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6962534623479282		[learning rate: 0.0046361]
	Learning Rate: 0.00463611
	LOSS [training: 0.6962534623479282 | validation: 0.864192443897493]
	TIME [epoch: 2.8 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7200743618001707		[learning rate: 0.0046197]
	Learning Rate: 0.00461972
	LOSS [training: 0.7200743618001707 | validation: 0.9258548479259082]
	TIME [epoch: 2.8 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8163009589222872		[learning rate: 0.0046034]
	Learning Rate: 0.00460338
	LOSS [training: 0.8163009589222872 | validation: 0.9397493300363191]
	TIME [epoch: 2.8 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7754026886039926		[learning rate: 0.0045871]
	Learning Rate: 0.0045871
	LOSS [training: 0.7754026886039926 | validation: 0.8656257776599493]
	TIME [epoch: 2.8 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6934374077901464		[learning rate: 0.0045709]
	Learning Rate: 0.00457088
	LOSS [training: 0.6934374077901464 | validation: 0.8546452995628663]
	TIME [epoch: 2.8 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6922436147268479		[learning rate: 0.0045547]
	Learning Rate: 0.00455472
	LOSS [training: 0.6922436147268479 | validation: 0.8505254236035532]
	TIME [epoch: 2.8 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6951486290980092		[learning rate: 0.0045386]
	Learning Rate: 0.00453861
	LOSS [training: 0.6951486290980092 | validation: 0.851378521513431]
	TIME [epoch: 2.8 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.691799801985365		[learning rate: 0.0045226]
	Learning Rate: 0.00452256
	LOSS [training: 0.691799801985365 | validation: 0.8589074037322899]
	TIME [epoch: 2.8 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6905742205528079		[learning rate: 0.0045066]
	Learning Rate: 0.00450657
	LOSS [training: 0.6905742205528079 | validation: 0.8203727886779618]
	TIME [epoch: 2.8 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.695099843164006		[learning rate: 0.0044906]
	Learning Rate: 0.00449063
	LOSS [training: 0.695099843164006 | validation: 0.8684855593734242]
	TIME [epoch: 2.8 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6813543870006824		[learning rate: 0.0044748]
	Learning Rate: 0.00447475
	LOSS [training: 0.6813543870006824 | validation: 0.7971658766942458]
	TIME [epoch: 2.8 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6924169604156285		[learning rate: 0.0044589]
	Learning Rate: 0.00445893
	LOSS [training: 0.6924169604156285 | validation: 0.870613381998081]
	TIME [epoch: 2.8 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7038319537266509		[learning rate: 0.0044432]
	Learning Rate: 0.00444316
	LOSS [training: 0.7038319537266509 | validation: 0.8782276521468815]
	TIME [epoch: 2.8 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7428828688436551		[learning rate: 0.0044275]
	Learning Rate: 0.00442745
	LOSS [training: 0.7428828688436551 | validation: 0.9507804673084325]
	TIME [epoch: 2.8 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8206964784856452		[learning rate: 0.0044118]
	Learning Rate: 0.0044118
	LOSS [training: 0.8206964784856452 | validation: 0.8909691218046659]
	TIME [epoch: 2.8 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7231780600187804		[learning rate: 0.0043962]
	Learning Rate: 0.00439619
	LOSS [training: 0.7231780600187804 | validation: 0.8473824839442523]
	TIME [epoch: 2.8 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6832199554665996		[learning rate: 0.0043806]
	Learning Rate: 0.00438065
	LOSS [training: 0.6832199554665996 | validation: 0.8703752371705819]
	TIME [epoch: 2.79 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.69156141885003		[learning rate: 0.0043652]
	Learning Rate: 0.00436516
	LOSS [training: 0.69156141885003 | validation: 0.8337919686450125]
	TIME [epoch: 2.8 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6853829158789629		[learning rate: 0.0043497]
	Learning Rate: 0.00434972
	LOSS [training: 0.6853829158789629 | validation: 0.8499915817550563]
	TIME [epoch: 2.79 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6866896584355158		[learning rate: 0.0043343]
	Learning Rate: 0.00433434
	LOSS [training: 0.6866896584355158 | validation: 0.8119360249866965]
	TIME [epoch: 2.8 sec]
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6760782664496056		[learning rate: 0.004319]
	Learning Rate: 0.00431901
	LOSS [training: 0.6760782664496056 | validation: 0.8227692001151414]
	TIME [epoch: 2.79 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6790410570170434		[learning rate: 0.0043037]
	Learning Rate: 0.00430374
	LOSS [training: 0.6790410570170434 | validation: 0.8062651279524214]
	TIME [epoch: 2.8 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795938177343989		[learning rate: 0.0042885]
	Learning Rate: 0.00428852
	LOSS [training: 0.6795938177343989 | validation: 0.8416950627840605]
	TIME [epoch: 2.8 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974047549905117		[learning rate: 0.0042734]
	Learning Rate: 0.00427336
	LOSS [training: 0.6974047549905117 | validation: 0.8994361917632215]
	TIME [epoch: 2.8 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7486337219891971		[learning rate: 0.0042582]
	Learning Rate: 0.00425825
	LOSS [training: 0.7486337219891971 | validation: 0.9452326842999151]
	TIME [epoch: 2.8 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.786088547414487		[learning rate: 0.0042432]
	Learning Rate: 0.00424319
	LOSS [training: 0.786088547414487 | validation: 0.8543320408789342]
	TIME [epoch: 2.8 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6957481890986484		[learning rate: 0.0042282]
	Learning Rate: 0.00422818
	LOSS [training: 0.6957481890986484 | validation: 0.830468931729778]
	TIME [epoch: 2.8 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6737717611582988		[learning rate: 0.0042132]
	Learning Rate: 0.00421323
	LOSS [training: 0.6737717611582988 | validation: 0.8239689488175452]
	TIME [epoch: 2.8 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6816062747308596		[learning rate: 0.0041983]
	Learning Rate: 0.00419833
	LOSS [training: 0.6816062747308596 | validation: 0.822542902593805]
	TIME [epoch: 2.79 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6830438171939294		[learning rate: 0.0041835]
	Learning Rate: 0.00418349
	LOSS [training: 0.6830438171939294 | validation: 0.8418677420586431]
	TIME [epoch: 2.8 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6795990287231652		[learning rate: 0.0041687]
	Learning Rate: 0.00416869
	LOSS [training: 0.6795990287231652 | validation: 0.7900962341497195]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_298.pth
	Model improved!!!
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714695808338307		[learning rate: 0.004154]
	Learning Rate: 0.00415395
	LOSS [training: 0.6714695808338307 | validation: 0.8356649107482123]
	TIME [epoch: 2.78 sec]
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6640736052153398		[learning rate: 0.0041393]
	Learning Rate: 0.00413926
	LOSS [training: 0.6640736052153398 | validation: 0.784225860638771]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_300.pth
	Model improved!!!
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6646961452899857		[learning rate: 0.0041246]
	Learning Rate: 0.00412463
	LOSS [training: 0.6646961452899857 | validation: 0.8172832924553408]
	TIME [epoch: 2.77 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6637813441097896		[learning rate: 0.00411]
	Learning Rate: 0.00411004
	LOSS [training: 0.6637813441097896 | validation: 0.7799572223588058]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_302.pth
	Model improved!!!
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6699174764029653		[learning rate: 0.0040955]
	Learning Rate: 0.00409551
	LOSS [training: 0.6699174764029653 | validation: 0.8613732292702263]
	TIME [epoch: 2.78 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6974001075153572		[learning rate: 0.004081]
	Learning Rate: 0.00408102
	LOSS [training: 0.6974001075153572 | validation: 0.9388470587543861]
	TIME [epoch: 2.78 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7986311051634906		[learning rate: 0.0040666]
	Learning Rate: 0.00406659
	LOSS [training: 0.7986311051634906 | validation: 0.864403965428968]
	TIME [epoch: 2.77 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7533971003728145		[learning rate: 0.0040522]
	Learning Rate: 0.00405221
	LOSS [training: 0.7533971003728145 | validation: 0.8638989478782793]
	TIME [epoch: 2.78 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690410763502876		[learning rate: 0.0040379]
	Learning Rate: 0.00403788
	LOSS [training: 0.6690410763502876 | validation: 0.8484479378517675]
	TIME [epoch: 2.78 sec]
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6721754886655411		[learning rate: 0.0040236]
	Learning Rate: 0.00402361
	LOSS [training: 0.6721754886655411 | validation: 0.8223196348695225]
	TIME [epoch: 2.77 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6764862074621797		[learning rate: 0.0040094]
	Learning Rate: 0.00400938
	LOSS [training: 0.6764862074621797 | validation: 0.8415558393567079]
	TIME [epoch: 2.77 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6628651555457199		[learning rate: 0.0039952]
	Learning Rate: 0.0039952
	LOSS [training: 0.6628651555457199 | validation: 0.8190647064430583]
	TIME [epoch: 2.77 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6625345205398937		[learning rate: 0.0039811]
	Learning Rate: 0.00398107
	LOSS [training: 0.6625345205398937 | validation: 0.8063674220238446]
	TIME [epoch: 2.77 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6491154853126503		[learning rate: 0.003967]
	Learning Rate: 0.00396699
	LOSS [training: 0.6491154853126503 | validation: 0.7824968249114965]
	TIME [epoch: 2.77 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6571293569237276		[learning rate: 0.003953]
	Learning Rate: 0.00395297
	LOSS [training: 0.6571293569237276 | validation: 0.8270620783961266]
	TIME [epoch: 2.77 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6602610365226238		[learning rate: 0.003939]
	Learning Rate: 0.00393899
	LOSS [training: 0.6602610365226238 | validation: 0.8026388815403284]
	TIME [epoch: 2.77 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6645526325046083		[learning rate: 0.0039251]
	Learning Rate: 0.00392506
	LOSS [training: 0.6645526325046083 | validation: 0.8719074923802421]
	TIME [epoch: 2.77 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6937468742987956		[learning rate: 0.0039112]
	Learning Rate: 0.00391118
	LOSS [training: 0.6937468742987956 | validation: 0.8913044747601024]
	TIME [epoch: 2.78 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7274065166192131		[learning rate: 0.0038973]
	Learning Rate: 0.00389735
	LOSS [training: 0.7274065166192131 | validation: 0.8236375006606593]
	TIME [epoch: 2.77 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7085486961387608		[learning rate: 0.0038836]
	Learning Rate: 0.00388357
	LOSS [training: 0.7085486961387608 | validation: 0.8628377931832895]
	TIME [epoch: 2.77 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6590723368509089		[learning rate: 0.0038698]
	Learning Rate: 0.00386983
	LOSS [training: 0.6590723368509089 | validation: 0.8102096503048206]
	TIME [epoch: 2.77 sec]
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6521074653041776		[learning rate: 0.0038561]
	Learning Rate: 0.00385615
	LOSS [training: 0.6521074653041776 | validation: 0.7892687433538994]
	TIME [epoch: 2.77 sec]
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6575127304558827		[learning rate: 0.0038425]
	Learning Rate: 0.00384251
	LOSS [training: 0.6575127304558827 | validation: 0.8422667347988354]
	TIME [epoch: 2.77 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6499448668704628		[learning rate: 0.0038289]
	Learning Rate: 0.00382893
	LOSS [training: 0.6499448668704628 | validation: 0.7950763784315878]
	TIME [epoch: 2.77 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6510099161973572		[learning rate: 0.0038154]
	Learning Rate: 0.00381539
	LOSS [training: 0.6510099161973572 | validation: 0.8037123722072252]
	TIME [epoch: 2.77 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6428310879814852		[learning rate: 0.0038019]
	Learning Rate: 0.00380189
	LOSS [training: 0.6428310879814852 | validation: 0.7972566937368397]
	TIME [epoch: 2.77 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6417407423652955		[learning rate: 0.0037885]
	Learning Rate: 0.00378845
	LOSS [training: 0.6417407423652955 | validation: 0.7836081461824341]
	TIME [epoch: 2.77 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6459809159958166		[learning rate: 0.0037751]
	Learning Rate: 0.00377505
	LOSS [training: 0.6459809159958166 | validation: 0.8141580545440654]
	TIME [epoch: 2.77 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6711740548539575		[learning rate: 0.0037617]
	Learning Rate: 0.0037617
	LOSS [training: 0.6711740548539575 | validation: 0.8581436140383869]
	TIME [epoch: 2.77 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7307744965357741		[learning rate: 0.0037484]
	Learning Rate: 0.0037484
	LOSS [training: 0.7307744965357741 | validation: 0.8762012252140903]
	TIME [epoch: 2.78 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.692751956486729		[learning rate: 0.0037351]
	Learning Rate: 0.00373515
	LOSS [training: 0.692751956486729 | validation: 0.8473614775027798]
	TIME [epoch: 2.78 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6431646580737711		[learning rate: 0.0037219]
	Learning Rate: 0.00372194
	LOSS [training: 0.6431646580737711 | validation: 0.7918011798763789]
	TIME [epoch: 2.78 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6404567218796319		[learning rate: 0.0037088]
	Learning Rate: 0.00370878
	LOSS [training: 0.6404567218796319 | validation: 0.8387365311793824]
	TIME [epoch: 2.78 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.633986271204176		[learning rate: 0.0036957]
	Learning Rate: 0.00369566
	LOSS [training: 0.633986271204176 | validation: 0.792836684208194]
	TIME [epoch: 2.77 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6330797454140936		[learning rate: 0.0036826]
	Learning Rate: 0.00368259
	LOSS [training: 0.6330797454140936 | validation: 0.78977319991765]
	TIME [epoch: 2.78 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6352634042236478		[learning rate: 0.0036696]
	Learning Rate: 0.00366957
	LOSS [training: 0.6352634042236478 | validation: 0.7861569943197506]
	TIME [epoch: 2.78 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.627301253349392		[learning rate: 0.0036566]
	Learning Rate: 0.0036566
	LOSS [training: 0.627301253349392 | validation: 0.7780063768878707]
	TIME [epoch: 2.77 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_335.pth
	Model improved!!!
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6319486883114356		[learning rate: 0.0036437]
	Learning Rate: 0.00364367
	LOSS [training: 0.6319486883114356 | validation: 0.8146064441374211]
	TIME [epoch: 2.8 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6303953713590447		[learning rate: 0.0036308]
	Learning Rate: 0.00363078
	LOSS [training: 0.6303953713590447 | validation: 0.7818605632523465]
	TIME [epoch: 2.8 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6559239356655943		[learning rate: 0.0036179]
	Learning Rate: 0.00361794
	LOSS [training: 0.6559239356655943 | validation: 0.8572444565463574]
	TIME [epoch: 2.8 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6778002681368546		[learning rate: 0.0036051]
	Learning Rate: 0.00360515
	LOSS [training: 0.6778002681368546 | validation: 0.8161116391211497]
	TIME [epoch: 2.8 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.666353780408509		[learning rate: 0.0035924]
	Learning Rate: 0.0035924
	LOSS [training: 0.666353780408509 | validation: 0.7830438564810658]
	TIME [epoch: 2.8 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6242231816409042		[learning rate: 0.0035797]
	Learning Rate: 0.0035797
	LOSS [training: 0.6242231816409042 | validation: 0.7850105833379271]
	TIME [epoch: 2.8 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6174441210543102		[learning rate: 0.003567]
	Learning Rate: 0.00356704
	LOSS [training: 0.6174441210543102 | validation: 0.806498551925733]
	TIME [epoch: 2.82 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6147207182387717		[learning rate: 0.0035544]
	Learning Rate: 0.00355442
	LOSS [training: 0.6147207182387717 | validation: 0.7678454593705741]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_343.pth
	Model improved!!!
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6160779498446085		[learning rate: 0.0035419]
	Learning Rate: 0.00354185
	LOSS [training: 0.6160779498446085 | validation: 0.8000674628201783]
	TIME [epoch: 2.8 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6154946758845293		[learning rate: 0.0035293]
	Learning Rate: 0.00352933
	LOSS [training: 0.6154946758845293 | validation: 0.773825284352665]
	TIME [epoch: 2.8 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6167592919510816		[learning rate: 0.0035169]
	Learning Rate: 0.00351685
	LOSS [training: 0.6167592919510816 | validation: 0.8410868084532949]
	TIME [epoch: 2.8 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6280038676746691		[learning rate: 0.0035044]
	Learning Rate: 0.00350441
	LOSS [training: 0.6280038676746691 | validation: 0.753604052650731]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_347.pth
	Model improved!!!
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6198951388474019		[learning rate: 0.003492]
	Learning Rate: 0.00349202
	LOSS [training: 0.6198951388474019 | validation: 0.7708966556510246]
	TIME [epoch: 2.8 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6079581662071641		[learning rate: 0.0034797]
	Learning Rate: 0.00347967
	LOSS [training: 0.6079581662071641 | validation: 0.8507017307628115]
	TIME [epoch: 2.8 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6154464737272387		[learning rate: 0.0034674]
	Learning Rate: 0.00346737
	LOSS [training: 0.6154464737272387 | validation: 0.7093904414334853]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_350.pth
	Model improved!!!
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6244187260855474		[learning rate: 0.0034551]
	Learning Rate: 0.00345511
	LOSS [training: 0.6244187260855474 | validation: 0.7802082046410385]
	TIME [epoch: 2.8 sec]
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5964899363582767		[learning rate: 0.0034429]
	Learning Rate: 0.00344289
	LOSS [training: 0.5964899363582767 | validation: 0.7826149799244662]
	TIME [epoch: 2.8 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5926668699942156		[learning rate: 0.0034307]
	Learning Rate: 0.00343071
	LOSS [training: 0.5926668699942156 | validation: 0.7126979643004004]
	TIME [epoch: 2.8 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.595022958882456		[learning rate: 0.0034186]
	Learning Rate: 0.00341858
	LOSS [training: 0.595022958882456 | validation: 0.8287800287372082]
	TIME [epoch: 2.8 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.593290353217555		[learning rate: 0.0034065]
	Learning Rate: 0.00340649
	LOSS [training: 0.593290353217555 | validation: 0.7348638871360453]
	TIME [epoch: 2.8 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5908062840717113		[learning rate: 0.0033944]
	Learning Rate: 0.00339445
	LOSS [training: 0.5908062840717113 | validation: 0.7808723997873654]
	TIME [epoch: 2.8 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5927672341846322		[learning rate: 0.0033824]
	Learning Rate: 0.00338245
	LOSS [training: 0.5927672341846322 | validation: 0.766780662719272]
	TIME [epoch: 2.8 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6119709529549769		[learning rate: 0.0033705]
	Learning Rate: 0.00337048
	LOSS [training: 0.6119709529549769 | validation: 0.774669398031808]
	TIME [epoch: 2.8 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6196818660150973		[learning rate: 0.0033586]
	Learning Rate: 0.00335857
	LOSS [training: 0.6196818660150973 | validation: 0.8018396553862183]
	TIME [epoch: 2.8 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5970841648912562		[learning rate: 0.0033467]
	Learning Rate: 0.00334669
	LOSS [training: 0.5970841648912562 | validation: 0.7224840372978021]
	TIME [epoch: 2.8 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.571975331956717		[learning rate: 0.0033349]
	Learning Rate: 0.00333485
	LOSS [training: 0.571975331956717 | validation: 0.7366846658494228]
	TIME [epoch: 2.8 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5615361316079848		[learning rate: 0.0033231]
	Learning Rate: 0.00332306
	LOSS [training: 0.5615361316079848 | validation: 0.7804383574911209]
	TIME [epoch: 2.8 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5683367899358346		[learning rate: 0.0033113]
	Learning Rate: 0.00331131
	LOSS [training: 0.5683367899358346 | validation: 0.7097753993862416]
	TIME [epoch: 2.8 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5662946463124315		[learning rate: 0.0032996]
	Learning Rate: 0.0032996
	LOSS [training: 0.5662946463124315 | validation: 0.7462913767416572]
	TIME [epoch: 2.79 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5539187464731443		[learning rate: 0.0032879]
	Learning Rate: 0.00328793
	LOSS [training: 0.5539187464731443 | validation: 0.7096932780515379]
	TIME [epoch: 2.8 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5510608633480104		[learning rate: 0.0032763]
	Learning Rate: 0.00327631
	LOSS [training: 0.5510608633480104 | validation: 0.7481031015509415]
	TIME [epoch: 2.8 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5412715138868047		[learning rate: 0.0032647]
	Learning Rate: 0.00326472
	LOSS [training: 0.5412715138868047 | validation: 0.6911603637454448]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_367.pth
	Model improved!!!
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5441100620706136		[learning rate: 0.0032532]
	Learning Rate: 0.00325318
	LOSS [training: 0.5441100620706136 | validation: 0.8083484367528332]
	TIME [epoch: 2.8 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5496882417466387		[learning rate: 0.0032417]
	Learning Rate: 0.00324167
	LOSS [training: 0.5496882417466387 | validation: 0.6946678307484366]
	TIME [epoch: 2.8 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5925386854768049		[learning rate: 0.0032302]
	Learning Rate: 0.00323021
	LOSS [training: 0.5925386854768049 | validation: 0.869051838936782]
	TIME [epoch: 2.8 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.579363673446048		[learning rate: 0.0032188]
	Learning Rate: 0.00321879
	LOSS [training: 0.579363673446048 | validation: 0.7059891271499601]
	TIME [epoch: 2.8 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5452819974178336		[learning rate: 0.0032074]
	Learning Rate: 0.00320741
	LOSS [training: 0.5452819974178336 | validation: 0.6963675788579518]
	TIME [epoch: 2.8 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5279090341726925		[learning rate: 0.0031961]
	Learning Rate: 0.00319606
	LOSS [training: 0.5279090341726925 | validation: 0.7780203996549178]
	TIME [epoch: 2.8 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417526212196443		[learning rate: 0.0031848]
	Learning Rate: 0.00318476
	LOSS [training: 0.5417526212196443 | validation: 0.6660071102812051]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_374.pth
	Model improved!!!
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5542686558218399		[learning rate: 0.0031735]
	Learning Rate: 0.0031735
	LOSS [training: 0.5542686558218399 | validation: 0.74737876217588]
	TIME [epoch: 2.8 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5326137185079705		[learning rate: 0.0031623]
	Learning Rate: 0.00316228
	LOSS [training: 0.5326137185079705 | validation: 0.7478317959983682]
	TIME [epoch: 2.8 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5145817144101763		[learning rate: 0.0031511]
	Learning Rate: 0.0031511
	LOSS [training: 0.5145817144101763 | validation: 0.6545742825927865]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_377.pth
	Model improved!!!
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5173359386495735		[learning rate: 0.00314]
	Learning Rate: 0.00313995
	LOSS [training: 0.5173359386495735 | validation: 0.7595179345107598]
	TIME [epoch: 2.8 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5047367951581243		[learning rate: 0.0031288]
	Learning Rate: 0.00312885
	LOSS [training: 0.5047367951581243 | validation: 0.6581508458090563]
	TIME [epoch: 2.8 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4941248342397465		[learning rate: 0.0031178]
	Learning Rate: 0.00311779
	LOSS [training: 0.4941248342397465 | validation: 0.7036079365614776]
	TIME [epoch: 2.79 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4917739537735433		[learning rate: 0.0031068]
	Learning Rate: 0.00310676
	LOSS [training: 0.4917739537735433 | validation: 0.6837467411019191]
	TIME [epoch: 2.8 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4977009086158684		[learning rate: 0.0030958]
	Learning Rate: 0.00309577
	LOSS [training: 0.4977009086158684 | validation: 0.7122085710724579]
	TIME [epoch: 2.8 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5143544662927445		[learning rate: 0.0030848]
	Learning Rate: 0.00308483
	LOSS [training: 0.5143544662927445 | validation: 0.7196409611954159]
	TIME [epoch: 2.8 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.512274591909257		[learning rate: 0.0030739]
	Learning Rate: 0.00307392
	LOSS [training: 0.512274591909257 | validation: 0.6771778494756919]
	TIME [epoch: 2.8 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48918697118837473		[learning rate: 0.003063]
	Learning Rate: 0.00306305
	LOSS [training: 0.48918697118837473 | validation: 0.6926685977798341]
	TIME [epoch: 2.8 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.461608543112971		[learning rate: 0.0030522]
	Learning Rate: 0.00305222
	LOSS [training: 0.461608543112971 | validation: 0.6333154828909762]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_386.pth
	Model improved!!!
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46158868313084567		[learning rate: 0.0030414]
	Learning Rate: 0.00304142
	LOSS [training: 0.46158868313084567 | validation: 0.7885786952493578]
	TIME [epoch: 2.8 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4929819589260624		[learning rate: 0.0030307]
	Learning Rate: 0.00303067
	LOSS [training: 0.4929819589260624 | validation: 0.5705815651702466]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_388.pth
	Model improved!!!
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5397793868564139		[learning rate: 0.00302]
	Learning Rate: 0.00301995
	LOSS [training: 0.5397793868564139 | validation: 0.7004447588280658]
	TIME [epoch: 2.8 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45397967436802433		[learning rate: 0.0030093]
	Learning Rate: 0.00300927
	LOSS [training: 0.45397967436802433 | validation: 0.6595764798921787]
	TIME [epoch: 2.8 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4394522449983843		[learning rate: 0.0029986]
	Learning Rate: 0.00299863
	LOSS [training: 0.4394522449983843 | validation: 0.6103430469868742]
	TIME [epoch: 2.8 sec]
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44696860427032803		[learning rate: 0.002988]
	Learning Rate: 0.00298803
	LOSS [training: 0.44696860427032803 | validation: 0.6800382491614126]
	TIME [epoch: 2.8 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4408743961131707		[learning rate: 0.0029775]
	Learning Rate: 0.00297746
	LOSS [training: 0.4408743961131707 | validation: 0.5976292900415631]
	TIME [epoch: 2.8 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4452849993307484		[learning rate: 0.0029669]
	Learning Rate: 0.00296693
	LOSS [training: 0.4452849993307484 | validation: 0.675772033701159]
	TIME [epoch: 2.8 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4393605339865533		[learning rate: 0.0029564]
	Learning Rate: 0.00295644
	LOSS [training: 0.4393605339865533 | validation: 0.6463169357009269]
	TIME [epoch: 2.8 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44232573756418203		[learning rate: 0.002946]
	Learning Rate: 0.00294599
	LOSS [training: 0.44232573756418203 | validation: 0.6271783212108675]
	TIME [epoch: 2.79 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4303300782607278		[learning rate: 0.0029356]
	Learning Rate: 0.00293557
	LOSS [training: 0.4303300782607278 | validation: 0.6301582788114266]
	TIME [epoch: 2.8 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.414236289422348		[learning rate: 0.0029252]
	Learning Rate: 0.00292519
	LOSS [training: 0.414236289422348 | validation: 0.6054751902499232]
	TIME [epoch: 2.8 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3946775640830879		[learning rate: 0.0029148]
	Learning Rate: 0.00291484
	LOSS [training: 0.3946775640830879 | validation: 0.596790445397721]
	TIME [epoch: 2.8 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3861606381444553		[learning rate: 0.0029045]
	Learning Rate: 0.00290454
	LOSS [training: 0.3861606381444553 | validation: 0.6358237780906573]
	TIME [epoch: 2.79 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39162957820617583		[learning rate: 0.0028943]
	Learning Rate: 0.00289427
	LOSS [training: 0.39162957820617583 | validation: 0.5019505706968986]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.525930171890549		[learning rate: 0.002884]
	Learning Rate: 0.00288403
	LOSS [training: 0.525930171890549 | validation: 0.9226514154086903]
	TIME [epoch: 2.79 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5417101638219027		[learning rate: 0.0028738]
	Learning Rate: 0.00287383
	LOSS [training: 0.5417101638219027 | validation: 0.5403358303192551]
	TIME [epoch: 2.79 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38024442921054386		[learning rate: 0.0028637]
	Learning Rate: 0.00286367
	LOSS [training: 0.38024442921054386 | validation: 0.523815544900885]
	TIME [epoch: 2.79 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.391359837571383		[learning rate: 0.0028535]
	Learning Rate: 0.00285354
	LOSS [training: 0.391359837571383 | validation: 0.6394415064478883]
	TIME [epoch: 2.79 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3894988368219178		[learning rate: 0.0028435]
	Learning Rate: 0.00284345
	LOSS [training: 0.3894988368219178 | validation: 0.5371306609014003]
	TIME [epoch: 2.79 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3688461738015689		[learning rate: 0.0028334]
	Learning Rate: 0.0028334
	LOSS [training: 0.3688461738015689 | validation: 0.5818301713145434]
	TIME [epoch: 2.79 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3587122915680473		[learning rate: 0.0028234]
	Learning Rate: 0.00282338
	LOSS [training: 0.3587122915680473 | validation: 0.5645691197941214]
	TIME [epoch: 2.79 sec]
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3509802517410736		[learning rate: 0.0028134]
	Learning Rate: 0.0028134
	LOSS [training: 0.3509802517410736 | validation: 0.5578666297155656]
	TIME [epoch: 2.79 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3492256775371903		[learning rate: 0.0028034]
	Learning Rate: 0.00280345
	LOSS [training: 0.3492256775371903 | validation: 0.603124556611539]
	TIME [epoch: 2.78 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3490152136857584		[learning rate: 0.0027935]
	Learning Rate: 0.00279353
	LOSS [training: 0.3490152136857584 | validation: 0.5311625156199219]
	TIME [epoch: 2.78 sec]
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3680270329461385		[learning rate: 0.0027837]
	Learning Rate: 0.00278365
	LOSS [training: 0.3680270329461385 | validation: 0.6461079432046442]
	TIME [epoch: 2.78 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3735812661356782		[learning rate: 0.0027738]
	Learning Rate: 0.00277381
	LOSS [training: 0.3735812661356782 | validation: 0.5082802816410661]
	TIME [epoch: 2.79 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3361773803755979		[learning rate: 0.002764]
	Learning Rate: 0.002764
	LOSS [training: 0.3361773803755979 | validation: 0.6643438690636474]
	TIME [epoch: 2.78 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3701240483311441		[learning rate: 0.0027542]
	Learning Rate: 0.00275423
	LOSS [training: 0.3701240483311441 | validation: 0.43725218902337304]
	TIME [epoch: 2.78 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_415.pth
	Model improved!!!
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.47435516335082617		[learning rate: 0.0027445]
	Learning Rate: 0.00274449
	LOSS [training: 0.47435516335082617 | validation: 0.6913517275719355]
	TIME [epoch: 2.79 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38703691472411295		[learning rate: 0.0027348]
	Learning Rate: 0.00273478
	LOSS [training: 0.38703691472411295 | validation: 0.4881277903996004]
	TIME [epoch: 2.78 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31758370016683646		[learning rate: 0.0027251]
	Learning Rate: 0.00272511
	LOSS [training: 0.31758370016683646 | validation: 0.5088155361074298]
	TIME [epoch: 2.79 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3334230257799466		[learning rate: 0.0027155]
	Learning Rate: 0.00271548
	LOSS [training: 0.3334230257799466 | validation: 0.5756823712608762]
	TIME [epoch: 2.78 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3267363550233729		[learning rate: 0.0027059]
	Learning Rate: 0.00270588
	LOSS [training: 0.3267363550233729 | validation: 0.47109167128928864]
	TIME [epoch: 2.79 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31289334207183545		[learning rate: 0.0026963]
	Learning Rate: 0.00269631
	LOSS [training: 0.31289334207183545 | validation: 0.5859750258758708]
	TIME [epoch: 2.79 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3175678162310758		[learning rate: 0.0026868]
	Learning Rate: 0.00268677
	LOSS [training: 0.3175678162310758 | validation: 0.44515158118298603]
	TIME [epoch: 2.78 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35207568598046624		[learning rate: 0.0026773]
	Learning Rate: 0.00267727
	LOSS [training: 0.35207568598046624 | validation: 0.7009231987188765]
	TIME [epoch: 2.79 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3767134299404532		[learning rate: 0.0026678]
	Learning Rate: 0.0026678
	LOSS [training: 0.3767134299404532 | validation: 0.4433893514728599]
	TIME [epoch: 2.79 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3391427037289358		[learning rate: 0.0026584]
	Learning Rate: 0.00265837
	LOSS [training: 0.3391427037289358 | validation: 0.5395032395883455]
	TIME [epoch: 2.79 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30212395785878815		[learning rate: 0.002649]
	Learning Rate: 0.00264897
	LOSS [training: 0.30212395785878815 | validation: 0.46603968041349386]
	TIME [epoch: 2.79 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.298180455515753		[learning rate: 0.0026396]
	Learning Rate: 0.0026396
	LOSS [training: 0.298180455515753 | validation: 0.5131698352845918]
	TIME [epoch: 2.79 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2949026155985154		[learning rate: 0.0026303]
	Learning Rate: 0.00263027
	LOSS [training: 0.2949026155985154 | validation: 0.45873640428326035]
	TIME [epoch: 2.79 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2903420794877707		[learning rate: 0.002621]
	Learning Rate: 0.00262097
	LOSS [training: 0.2903420794877707 | validation: 0.5489773998505727]
	TIME [epoch: 2.79 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30168157666350415		[learning rate: 0.0026117]
	Learning Rate: 0.0026117
	LOSS [training: 0.30168157666350415 | validation: 0.3966607317271559]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_430.pth
	Model improved!!!
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3668940169670525		[learning rate: 0.0026025]
	Learning Rate: 0.00260246
	LOSS [training: 0.3668940169670525 | validation: 0.7224021141391885]
	TIME [epoch: 2.79 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3952648991907738		[learning rate: 0.0025933]
	Learning Rate: 0.00259326
	LOSS [training: 0.3952648991907738 | validation: 0.4625454533991872]
	TIME [epoch: 2.79 sec]
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31378672574490396		[learning rate: 0.0025841]
	Learning Rate: 0.00258409
	LOSS [training: 0.31378672574490396 | validation: 0.5293063964366242]
	TIME [epoch: 2.79 sec]
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35871971224111193		[learning rate: 0.002575]
	Learning Rate: 0.00257495
	LOSS [training: 0.35871971224111193 | validation: 0.560874210486101]
	TIME [epoch: 2.78 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3006073869692687		[learning rate: 0.0025658]
	Learning Rate: 0.00256585
	LOSS [training: 0.3006073869692687 | validation: 0.44036133350263534]
	TIME [epoch: 2.79 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29528951392515196		[learning rate: 0.0025568]
	Learning Rate: 0.00255677
	LOSS [training: 0.29528951392515196 | validation: 0.5353034064701178]
	TIME [epoch: 2.79 sec]
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.292489001931059		[learning rate: 0.0025477]
	Learning Rate: 0.00254773
	LOSS [training: 0.292489001931059 | validation: 0.4264535452769951]
	TIME [epoch: 2.79 sec]
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2669682253390175		[learning rate: 0.0025387]
	Learning Rate: 0.00253872
	LOSS [training: 0.2669682253390175 | validation: 0.49171114078081646]
	TIME [epoch: 2.79 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27037026530458913		[learning rate: 0.0025297]
	Learning Rate: 0.00252975
	LOSS [training: 0.27037026530458913 | validation: 0.40793028214616645]
	TIME [epoch: 2.79 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2643164260344294		[learning rate: 0.0025208]
	Learning Rate: 0.0025208
	LOSS [training: 0.2643164260344294 | validation: 0.5013873767843926]
	TIME [epoch: 2.79 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2647206744867343		[learning rate: 0.0025119]
	Learning Rate: 0.00251189
	LOSS [training: 0.2647206744867343 | validation: 0.3751081765678375]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_441.pth
	Model improved!!!
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2962902297202627		[learning rate: 0.002503]
	Learning Rate: 0.002503
	LOSS [training: 0.2962902297202627 | validation: 0.6956224629326465]
	TIME [epoch: 2.8 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3688154656783379		[learning rate: 0.0024942]
	Learning Rate: 0.00249415
	LOSS [training: 0.3688154656783379 | validation: 0.3893536471815886]
	TIME [epoch: 2.79 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.31515004507673594		[learning rate: 0.0024853]
	Learning Rate: 0.00248533
	LOSS [training: 0.31515004507673594 | validation: 0.5340671554950033]
	TIME [epoch: 2.79 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26866901648626906		[learning rate: 0.0024765]
	Learning Rate: 0.00247654
	LOSS [training: 0.26866901648626906 | validation: 0.401642076147367]
	TIME [epoch: 2.79 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24495343211620665		[learning rate: 0.0024678]
	Learning Rate: 0.00246779
	LOSS [training: 0.24495343211620665 | validation: 0.424861739556583]
	TIME [epoch: 2.79 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23564248248132325		[learning rate: 0.0024591]
	Learning Rate: 0.00245906
	LOSS [training: 0.23564248248132325 | validation: 0.43448290080171736]
	TIME [epoch: 2.79 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23300760826159214		[learning rate: 0.0024504]
	Learning Rate: 0.00245037
	LOSS [training: 0.23300760826159214 | validation: 0.41282559974944616]
	TIME [epoch: 2.79 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23526571416080377		[learning rate: 0.0024417]
	Learning Rate: 0.0024417
	LOSS [training: 0.23526571416080377 | validation: 0.4255856056303703]
	TIME [epoch: 2.79 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23187224284541733		[learning rate: 0.0024331]
	Learning Rate: 0.00243307
	LOSS [training: 0.23187224284541733 | validation: 0.37805999534969636]
	TIME [epoch: 2.79 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2323189866953063		[learning rate: 0.0024245]
	Learning Rate: 0.00242446
	LOSS [training: 0.2323189866953063 | validation: 0.49163980430363713]
	TIME [epoch: 2.8 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24271655116956645		[learning rate: 0.0024159]
	Learning Rate: 0.00241589
	LOSS [training: 0.24271655116956645 | validation: 0.35017844541243015]
	TIME [epoch: 2.79 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_452.pth
	Model improved!!!
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30890997580223184		[learning rate: 0.0024073]
	Learning Rate: 0.00240735
	LOSS [training: 0.30890997580223184 | validation: 0.8548086534854836]
	TIME [epoch: 2.8 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5087025889211899		[learning rate: 0.0023988]
	Learning Rate: 0.00239883
	LOSS [training: 0.5087025889211899 | validation: 0.37659763180628747]
	TIME [epoch: 2.8 sec]
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2672907654280081		[learning rate: 0.0023904]
	Learning Rate: 0.00239035
	LOSS [training: 0.2672907654280081 | validation: 0.4034081134886984]
	TIME [epoch: 2.8 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24907536408465525		[learning rate: 0.0023819]
	Learning Rate: 0.0023819
	LOSS [training: 0.24907536408465525 | validation: 0.503506433861569]
	TIME [epoch: 2.8 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27978308384656003		[learning rate: 0.0023735]
	Learning Rate: 0.00237347
	LOSS [training: 0.27978308384656003 | validation: 0.35004975607190825]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_457.pth
	Model improved!!!
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24904325597248836		[learning rate: 0.0023651]
	Learning Rate: 0.00236508
	LOSS [training: 0.24904325597248836 | validation: 0.4366630106356768]
	TIME [epoch: 2.8 sec]
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22403270502897799		[learning rate: 0.0023567]
	Learning Rate: 0.00235672
	LOSS [training: 0.22403270502897799 | validation: 0.39118548497743877]
	TIME [epoch: 2.8 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21939207570983693		[learning rate: 0.0023484]
	Learning Rate: 0.00234838
	LOSS [training: 0.21939207570983693 | validation: 0.43536018946144756]
	TIME [epoch: 2.8 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22300959927080619		[learning rate: 0.0023401]
	Learning Rate: 0.00234008
	LOSS [training: 0.22300959927080619 | validation: 0.3665712290233785]
	TIME [epoch: 2.8 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22818827611385661		[learning rate: 0.0023318]
	Learning Rate: 0.00233181
	LOSS [training: 0.22818827611385661 | validation: 0.5098241008819948]
	TIME [epoch: 2.8 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24679864705079663		[learning rate: 0.0023236]
	Learning Rate: 0.00232356
	LOSS [training: 0.24679864705079663 | validation: 0.3396948765173713]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_463.pth
	Model improved!!!
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25704167038047965		[learning rate: 0.0023153]
	Learning Rate: 0.00231534
	LOSS [training: 0.25704167038047965 | validation: 0.5965254171150237]
	TIME [epoch: 2.8 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29405511580016863		[learning rate: 0.0023072]
	Learning Rate: 0.00230716
	LOSS [training: 0.29405511580016863 | validation: 0.3197008098934118]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_465.pth
	Model improved!!!
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2992587746905887		[learning rate: 0.002299]
	Learning Rate: 0.002299
	LOSS [training: 0.2992587746905887 | validation: 0.5395006048643165]
	TIME [epoch: 2.8 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28329669923031436		[learning rate: 0.0022909]
	Learning Rate: 0.00229087
	LOSS [training: 0.28329669923031436 | validation: 0.3338752624328942]
	TIME [epoch: 2.8 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24228625198487372		[learning rate: 0.0022828]
	Learning Rate: 0.00228277
	LOSS [training: 0.24228625198487372 | validation: 0.44872842438279437]
	TIME [epoch: 2.81 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23036896511384564		[learning rate: 0.0022747]
	Learning Rate: 0.00227469
	LOSS [training: 0.23036896511384564 | validation: 0.3571215779875694]
	TIME [epoch: 2.8 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21365029673880037		[learning rate: 0.0022667]
	Learning Rate: 0.00226665
	LOSS [training: 0.21365029673880037 | validation: 0.3812655929963069]
	TIME [epoch: 2.8 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20295109069953765		[learning rate: 0.0022586]
	Learning Rate: 0.00225864
	LOSS [training: 0.20295109069953765 | validation: 0.40279898441671524]
	TIME [epoch: 2.8 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2019534761361703		[learning rate: 0.0022506]
	Learning Rate: 0.00225065
	LOSS [training: 0.2019534761361703 | validation: 0.3434577732137858]
	TIME [epoch: 2.8 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.210733787136621		[learning rate: 0.0022427]
	Learning Rate: 0.00224269
	LOSS [training: 0.210733787136621 | validation: 0.49147769751383863]
	TIME [epoch: 2.8 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2296302961913075		[learning rate: 0.0022348]
	Learning Rate: 0.00223476
	LOSS [training: 0.2296302961913075 | validation: 0.3175481165483627]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_474.pth
	Model improved!!!
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3025752424983282		[learning rate: 0.0022269]
	Learning Rate: 0.00222686
	LOSS [training: 0.3025752424983282 | validation: 0.670623549472697]
	TIME [epoch: 2.8 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33618548844892104		[learning rate: 0.002219]
	Learning Rate: 0.00221898
	LOSS [training: 0.33618548844892104 | validation: 0.34263156763139246]
	TIME [epoch: 2.8 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20329005520766205		[learning rate: 0.0022111]
	Learning Rate: 0.00221114
	LOSS [training: 0.20329005520766205 | validation: 0.34286414402690624]
	TIME [epoch: 2.8 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1971415537788259		[learning rate: 0.0022033]
	Learning Rate: 0.00220332
	LOSS [training: 0.1971415537788259 | validation: 0.42895501283881254]
	TIME [epoch: 2.8 sec]
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2032611936921928		[learning rate: 0.0021955]
	Learning Rate: 0.00219553
	LOSS [training: 0.2032611936921928 | validation: 0.3234160359323971]
	TIME [epoch: 2.8 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21869776708988714		[learning rate: 0.0021878]
	Learning Rate: 0.00218776
	LOSS [training: 0.21869776708988714 | validation: 0.49860909761434286]
	TIME [epoch: 2.8 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24565478021491408		[learning rate: 0.00218]
	Learning Rate: 0.00218003
	LOSS [training: 0.24565478021491408 | validation: 0.321887098437468]
	TIME [epoch: 2.8 sec]
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2135306714808372		[learning rate: 0.0021723]
	Learning Rate: 0.00217232
	LOSS [training: 0.2135306714808372 | validation: 0.49533360500640244]
	TIME [epoch: 2.8 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2408849398566968		[learning rate: 0.0021646]
	Learning Rate: 0.00216463
	LOSS [training: 0.2408849398566968 | validation: 0.35495953113017803]
	TIME [epoch: 2.8 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20627996361786718		[learning rate: 0.002157]
	Learning Rate: 0.00215698
	LOSS [training: 0.20627996361786718 | validation: 0.394336129537806]
	TIME [epoch: 2.8 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19449516591303473		[learning rate: 0.0021494]
	Learning Rate: 0.00214935
	LOSS [training: 0.19449516591303473 | validation: 0.3397617983854173]
	TIME [epoch: 2.8 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19197434345827616		[learning rate: 0.0021418]
	Learning Rate: 0.00214175
	LOSS [training: 0.19197434345827616 | validation: 0.4713573673781659]
	TIME [epoch: 2.8 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23416089342323876		[learning rate: 0.0021342]
	Learning Rate: 0.00213418
	LOSS [training: 0.23416089342323876 | validation: 0.2885729546237898]
	TIME [epoch: 2.8 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_487.pth
	Model improved!!!
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27844428758279166		[learning rate: 0.0021266]
	Learning Rate: 0.00212663
	LOSS [training: 0.27844428758279166 | validation: 0.6389756619041578]
	TIME [epoch: 2.8 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3101847785062765		[learning rate: 0.0021191]
	Learning Rate: 0.00211911
	LOSS [training: 0.3101847785062765 | validation: 0.34460166906528045]
	TIME [epoch: 2.8 sec]
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2213935167969281		[learning rate: 0.0021116]
	Learning Rate: 0.00211162
	LOSS [training: 0.2213935167969281 | validation: 0.39883965183296044]
	TIME [epoch: 2.8 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23827754660216172		[learning rate: 0.0021042]
	Learning Rate: 0.00210415
	LOSS [training: 0.23827754660216172 | validation: 0.4164744368789025]
	TIME [epoch: 2.8 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1956055989453467		[learning rate: 0.0020967]
	Learning Rate: 0.00209671
	LOSS [training: 0.1956055989453467 | validation: 0.31345258191550135]
	TIME [epoch: 2.8 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21471933091373444		[learning rate: 0.0020893]
	Learning Rate: 0.0020893
	LOSS [training: 0.21471933091373444 | validation: 0.4784597484342372]
	TIME [epoch: 2.8 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21658302970061422		[learning rate: 0.0020819]
	Learning Rate: 0.00208191
	LOSS [training: 0.21658302970061422 | validation: 0.3247549630213333]
	TIME [epoch: 2.8 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19157843031307947		[learning rate: 0.0020745]
	Learning Rate: 0.00207455
	LOSS [training: 0.19157843031307947 | validation: 0.3933825643761559]
	TIME [epoch: 2.8 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19702135006454552		[learning rate: 0.0020672]
	Learning Rate: 0.00206721
	LOSS [training: 0.19702135006454552 | validation: 0.33730066727818964]
	TIME [epoch: 2.8 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18049051664804247		[learning rate: 0.0020599]
	Learning Rate: 0.0020599
	LOSS [training: 0.18049051664804247 | validation: 0.35680794860582216]
	TIME [epoch: 2.8 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1709267828999608		[learning rate: 0.0020526]
	Learning Rate: 0.00205262
	LOSS [training: 0.1709267828999608 | validation: 0.37126912127009465]
	TIME [epoch: 2.8 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17519544146416613		[learning rate: 0.0020454]
	Learning Rate: 0.00204536
	LOSS [training: 0.17519544146416613 | validation: 0.30384698305822716]
	TIME [epoch: 2.8 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18024186894479372		[learning rate: 0.0020381]
	Learning Rate: 0.00203812
	LOSS [training: 0.18024186894479372 | validation: 0.5063528370306759]
	TIME [epoch: 2.8 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.221953411415477		[learning rate: 0.0020309]
	Learning Rate: 0.00203092
	LOSS [training: 0.221953411415477 | validation: 0.2942294361010377]
	TIME [epoch: 192 sec]
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28529911849309897		[learning rate: 0.0020237]
	Learning Rate: 0.00202374
	LOSS [training: 0.28529911849309897 | validation: 0.7082381682079223]
	TIME [epoch: 5.97 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38869372321727397		[learning rate: 0.0020166]
	Learning Rate: 0.00201658
	LOSS [training: 0.38869372321727397 | validation: 0.33670718251002474]
	TIME [epoch: 5.95 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17758123764211037		[learning rate: 0.0020094]
	Learning Rate: 0.00200945
	LOSS [training: 0.17758123764211037 | validation: 0.30679265902967834]
	TIME [epoch: 5.95 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26391546289440704		[learning rate: 0.0020023]
	Learning Rate: 0.00200234
	LOSS [training: 0.26391546289440704 | validation: 0.511681126476119]
	TIME [epoch: 5.96 sec]
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23963501069093762		[learning rate: 0.0019953]
	Learning Rate: 0.00199526
	LOSS [training: 0.23963501069093762 | validation: 0.339583445001015]
	TIME [epoch: 5.95 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17989093652789415		[learning rate: 0.0019882]
	Learning Rate: 0.00198821
	LOSS [training: 0.17989093652789415 | validation: 0.3526574261217273]
	TIME [epoch: 5.96 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18759423193052047		[learning rate: 0.0019812]
	Learning Rate: 0.00198118
	LOSS [training: 0.18759423193052047 | validation: 0.365535621664018]
	TIME [epoch: 5.95 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17981672601717325		[learning rate: 0.0019742]
	Learning Rate: 0.00197417
	LOSS [training: 0.17981672601717325 | validation: 0.3439236337809902]
	TIME [epoch: 5.96 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16464039923979268		[learning rate: 0.0019672]
	Learning Rate: 0.00196719
	LOSS [training: 0.16464039923979268 | validation: 0.34786200385700844]
	TIME [epoch: 5.95 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17137099491904656		[learning rate: 0.0019602]
	Learning Rate: 0.00196023
	LOSS [training: 0.17137099491904656 | validation: 0.35219531217578937]
	TIME [epoch: 5.96 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17481326245480414		[learning rate: 0.0019533]
	Learning Rate: 0.0019533
	LOSS [training: 0.17481326245480414 | validation: 0.3225380672273907]
	TIME [epoch: 5.95 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16518079272775388		[learning rate: 0.0019464]
	Learning Rate: 0.00194639
	LOSS [training: 0.16518079272775388 | validation: 0.41355229420058337]
	TIME [epoch: 5.96 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17375746875975231		[learning rate: 0.0019395]
	Learning Rate: 0.00193951
	LOSS [training: 0.17375746875975231 | validation: 0.2957601046932112]
	TIME [epoch: 5.95 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22776270797849285		[learning rate: 0.0019327]
	Learning Rate: 0.00193265
	LOSS [training: 0.22776270797849285 | validation: 0.6686930821917827]
	TIME [epoch: 5.96 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3062203814225439		[learning rate: 0.0019258]
	Learning Rate: 0.00192582
	LOSS [training: 0.3062203814225439 | validation: 0.2946867637694753]
	TIME [epoch: 5.95 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2064572296925264		[learning rate: 0.001919]
	Learning Rate: 0.00191901
	LOSS [training: 0.2064572296925264 | validation: 0.3702500961251464]
	TIME [epoch: 5.96 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.193883288731178		[learning rate: 0.0019122]
	Learning Rate: 0.00191222
	LOSS [training: 0.193883288731178 | validation: 0.40645698909025063]
	TIME [epoch: 5.95 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16853315079487363		[learning rate: 0.0019055]
	Learning Rate: 0.00190546
	LOSS [training: 0.16853315079487363 | validation: 0.2944792082706578]
	TIME [epoch: 5.95 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18759407908018474		[learning rate: 0.0018987]
	Learning Rate: 0.00189872
	LOSS [training: 0.18759407908018474 | validation: 0.4561840643937203]
	TIME [epoch: 6 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20482240898859577		[learning rate: 0.001892]
	Learning Rate: 0.00189201
	LOSS [training: 0.20482240898859577 | validation: 0.29232959885834914]
	TIME [epoch: 5.94 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1910791616493902		[learning rate: 0.0018853]
	Learning Rate: 0.00188532
	LOSS [training: 0.1910791616493902 | validation: 0.5035613434239137]
	TIME [epoch: 5.95 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20169868543864486		[learning rate: 0.0018787]
	Learning Rate: 0.00187865
	LOSS [training: 0.20169868543864486 | validation: 0.29333623581859314]
	TIME [epoch: 5.95 sec]
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1859077938849702		[learning rate: 0.001872]
	Learning Rate: 0.00187201
	LOSS [training: 0.1859077938849702 | validation: 0.43816794280688287]
	TIME [epoch: 5.95 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19733557816564312		[learning rate: 0.0018654]
	Learning Rate: 0.00186539
	LOSS [training: 0.19733557816564312 | validation: 0.2958486049063322]
	TIME [epoch: 5.95 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17129863842554927		[learning rate: 0.0018588]
	Learning Rate: 0.00185879
	LOSS [training: 0.17129863842554927 | validation: 0.39285525647926595]
	TIME [epoch: 5.94 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16084891494883483		[learning rate: 0.0018522]
	Learning Rate: 0.00185222
	LOSS [training: 0.16084891494883483 | validation: 0.3084341859012032]
	TIME [epoch: 5.94 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1546553774032661		[learning rate: 0.0018457]
	Learning Rate: 0.00184567
	LOSS [training: 0.1546553774032661 | validation: 0.3740865361984985]
	TIME [epoch: 5.94 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15698898804444367		[learning rate: 0.0018391]
	Learning Rate: 0.00183914
	LOSS [training: 0.15698898804444367 | validation: 0.28102929484762784]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17117611660620088		[learning rate: 0.0018326]
	Learning Rate: 0.00183264
	LOSS [training: 0.17117611660620088 | validation: 0.5400717624455392]
	TIME [epoch: 5.94 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23255067523220357		[learning rate: 0.0018262]
	Learning Rate: 0.00182616
	LOSS [training: 0.23255067523220357 | validation: 0.2750799682450665]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_531.pth
	Model improved!!!
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2311671789864245		[learning rate: 0.0018197]
	Learning Rate: 0.0018197
	LOSS [training: 0.2311671789864245 | validation: 0.5237652931013296]
	TIME [epoch: 5.94 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2170237241929597		[learning rate: 0.0018133]
	Learning Rate: 0.00181327
	LOSS [training: 0.2170237241929597 | validation: 0.2863905779823941]
	TIME [epoch: 5.94 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1773495718625366		[learning rate: 0.0018069]
	Learning Rate: 0.00180685
	LOSS [training: 0.1773495718625366 | validation: 0.38502071060724885]
	TIME [epoch: 5.94 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17603451120909214		[learning rate: 0.0018005]
	Learning Rate: 0.00180046
	LOSS [training: 0.17603451120909214 | validation: 0.3759434982314005]
	TIME [epoch: 5.95 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15392136457892533		[learning rate: 0.0017941]
	Learning Rate: 0.0017941
	LOSS [training: 0.15392136457892533 | validation: 0.2855028358793036]
	TIME [epoch: 5.94 sec]
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15705779777997791		[learning rate: 0.0017878]
	Learning Rate: 0.00178775
	LOSS [training: 0.15705779777997791 | validation: 0.42481359665728435]
	TIME [epoch: 5.94 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16997925314133505		[learning rate: 0.0017814]
	Learning Rate: 0.00178143
	LOSS [training: 0.16997925314133505 | validation: 0.27680821284199547]
	TIME [epoch: 5.94 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16780089864148914		[learning rate: 0.0017751]
	Learning Rate: 0.00177513
	LOSS [training: 0.16780089864148914 | validation: 0.4580438229549395]
	TIME [epoch: 5.94 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17621101589415467		[learning rate: 0.0017689]
	Learning Rate: 0.00176886
	LOSS [training: 0.17621101589415467 | validation: 0.27464190075969824]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_540.pth
	Model improved!!!
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.167245007438108		[learning rate: 0.0017626]
	Learning Rate: 0.0017626
	LOSS [training: 0.167245007438108 | validation: 0.4321369543532087]
	TIME [epoch: 5.94 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17515243378113682		[learning rate: 0.0017564]
	Learning Rate: 0.00175637
	LOSS [training: 0.17515243378113682 | validation: 0.28603646763449747]
	TIME [epoch: 5.95 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15350067215480023		[learning rate: 0.0017502]
	Learning Rate: 0.00175016
	LOSS [training: 0.15350067215480023 | validation: 0.3763946391820607]
	TIME [epoch: 5.94 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15058486568610818		[learning rate: 0.001744]
	Learning Rate: 0.00174397
	LOSS [training: 0.15058486568610818 | validation: 0.29371396384428833]
	TIME [epoch: 5.94 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15583075506192426		[learning rate: 0.0017378]
	Learning Rate: 0.0017378
	LOSS [training: 0.15583075506192426 | validation: 0.4731439186838313]
	TIME [epoch: 5.94 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1768873078509207		[learning rate: 0.0017317]
	Learning Rate: 0.00173166
	LOSS [training: 0.1768873078509207 | validation: 0.2771877602997656]
	TIME [epoch: 5.95 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1576164276857929		[learning rate: 0.0017255]
	Learning Rate: 0.00172553
	LOSS [training: 0.1576164276857929 | validation: 0.41433280650682613]
	TIME [epoch: 5.94 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15811299865367376		[learning rate: 0.0017194]
	Learning Rate: 0.00171943
	LOSS [training: 0.15811299865367376 | validation: 0.2658650577041639]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_548.pth
	Model improved!!!
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16926428450689585		[learning rate: 0.0017134]
	Learning Rate: 0.00171335
	LOSS [training: 0.16926428450689585 | validation: 0.514071216179511]
	TIME [epoch: 5.94 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24448200092700553		[learning rate: 0.0017073]
	Learning Rate: 0.00170729
	LOSS [training: 0.24448200092700553 | validation: 0.28901522925648687]
	TIME [epoch: 5.94 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1520044360858613		[learning rate: 0.0017013]
	Learning Rate: 0.00170125
	LOSS [training: 0.1520044360858613 | validation: 0.3435714890610584]
	TIME [epoch: 5.94 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1372481528604308		[learning rate: 0.0016952]
	Learning Rate: 0.00169524
	LOSS [training: 0.1372481528604308 | validation: 0.3146734948065666]
	TIME [epoch: 5.94 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13190146400841923		[learning rate: 0.0016892]
	Learning Rate: 0.00168924
	LOSS [training: 0.13190146400841923 | validation: 0.32380977946162526]
	TIME [epoch: 5.94 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402149926133466		[learning rate: 0.0016833]
	Learning Rate: 0.00168327
	LOSS [training: 0.1402149926133466 | validation: 0.35590555050557815]
	TIME [epoch: 5.94 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14489723006334582		[learning rate: 0.0016773]
	Learning Rate: 0.00167732
	LOSS [training: 0.14489723006334582 | validation: 0.28791318494781637]
	TIME [epoch: 5.94 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14565352081029662		[learning rate: 0.0016714]
	Learning Rate: 0.00167139
	LOSS [training: 0.14565352081029662 | validation: 0.4085714560329914]
	TIME [epoch: 5.94 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1605064775365179		[learning rate: 0.0016655]
	Learning Rate: 0.00166548
	LOSS [training: 0.1605064775365179 | validation: 0.2730561921820414]
	TIME [epoch: 5.94 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15388906618470555		[learning rate: 0.0016596]
	Learning Rate: 0.00165959
	LOSS [training: 0.15388906618470555 | validation: 0.5106883603966651]
	TIME [epoch: 5.94 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1946188422682251		[learning rate: 0.0016537]
	Learning Rate: 0.00165372
	LOSS [training: 0.1946188422682251 | validation: 0.2770453895616967]
	TIME [epoch: 5.94 sec]
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.250736233753336		[learning rate: 0.0016479]
	Learning Rate: 0.00164787
	LOSS [training: 0.250736233753336 | validation: 0.5191485436298852]
	TIME [epoch: 5.94 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1994418736009679		[learning rate: 0.001642]
	Learning Rate: 0.00164204
	LOSS [training: 0.1994418736009679 | validation: 0.31372035035610574]
	TIME [epoch: 5.94 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13010234776125099		[learning rate: 0.0016362]
	Learning Rate: 0.00163624
	LOSS [training: 0.13010234776125099 | validation: 0.29642404592336924]
	TIME [epoch: 5.94 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1563009165888703		[learning rate: 0.0016305]
	Learning Rate: 0.00163045
	LOSS [training: 0.1563009165888703 | validation: 0.4247603225347402]
	TIME [epoch: 5.94 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15850375672440487		[learning rate: 0.0016247]
	Learning Rate: 0.00162469
	LOSS [training: 0.15850375672440487 | validation: 0.3143586651455707]
	TIME [epoch: 5.94 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12925932827073092		[learning rate: 0.0016189]
	Learning Rate: 0.00161894
	LOSS [training: 0.12925932827073092 | validation: 0.29319506830235625]
	TIME [epoch: 5.94 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1419914155613062		[learning rate: 0.0016132]
	Learning Rate: 0.00161322
	LOSS [training: 0.1419914155613062 | validation: 0.42649974377091504]
	TIME [epoch: 5.94 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1500631642643654		[learning rate: 0.0016075]
	Learning Rate: 0.00160751
	LOSS [training: 0.1500631642643654 | validation: 0.26514564173827165]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_567.pth
	Model improved!!!
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13929537351017257		[learning rate: 0.0016018]
	Learning Rate: 0.00160183
	LOSS [training: 0.13929537351017257 | validation: 0.41778966867666345]
	TIME [epoch: 5.98 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1618150118350592		[learning rate: 0.0015962]
	Learning Rate: 0.00159616
	LOSS [training: 0.1618150118350592 | validation: 0.2688104672261816]
	TIME [epoch: 5.98 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1534748734592015		[learning rate: 0.0015905]
	Learning Rate: 0.00159052
	LOSS [training: 0.1534748734592015 | validation: 0.4056154399927735]
	TIME [epoch: 5.94 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16120991571722817		[learning rate: 0.0015849]
	Learning Rate: 0.00158489
	LOSS [training: 0.16120991571722817 | validation: 0.27053053187470866]
	TIME [epoch: 5.93 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14230534228879624		[learning rate: 0.0015793]
	Learning Rate: 0.00157929
	LOSS [training: 0.14230534228879624 | validation: 0.41775512008426485]
	TIME [epoch: 5.94 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14221531782068375		[learning rate: 0.0015737]
	Learning Rate: 0.0015737
	LOSS [training: 0.14221531782068375 | validation: 0.2650774249150006]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_573.pth
	Model improved!!!
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13834674781344497		[learning rate: 0.0015681]
	Learning Rate: 0.00156814
	LOSS [training: 0.13834674781344497 | validation: 0.41437285830496884]
	TIME [epoch: 5.97 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16343563876154776		[learning rate: 0.0015626]
	Learning Rate: 0.00156259
	LOSS [training: 0.16343563876154776 | validation: 0.27915948410564945]
	TIME [epoch: 5.98 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13504027277406272		[learning rate: 0.0015571]
	Learning Rate: 0.00155707
	LOSS [training: 0.13504027277406272 | validation: 0.3965322927147645]
	TIME [epoch: 5.98 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1461572578588977		[learning rate: 0.0015516]
	Learning Rate: 0.00155156
	LOSS [training: 0.1461572578588977 | validation: 0.2727904803884794]
	TIME [epoch: 5.98 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13942138534227125		[learning rate: 0.0015461]
	Learning Rate: 0.00154608
	LOSS [training: 0.13942138534227125 | validation: 0.4506414322238156]
	TIME [epoch: 5.97 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15510968339969539		[learning rate: 0.0015406]
	Learning Rate: 0.00154061
	LOSS [training: 0.15510968339969539 | validation: 0.25314577759934404]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_579.pth
	Model improved!!!
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.181327178907738		[learning rate: 0.0015352]
	Learning Rate: 0.00153516
	LOSS [training: 0.181327178907738 | validation: 0.5155844671853987]
	TIME [epoch: 5.98 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22482005739888763		[learning rate: 0.0015297]
	Learning Rate: 0.00152973
	LOSS [training: 0.22482005739888763 | validation: 0.30160598242003567]
	TIME [epoch: 5.98 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1243632955219265		[learning rate: 0.0015243]
	Learning Rate: 0.00152432
	LOSS [training: 0.1243632955219265 | validation: 0.2890586442362012]
	TIME [epoch: 5.98 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15373995533405807		[learning rate: 0.0015189]
	Learning Rate: 0.00151893
	LOSS [training: 0.15373995533405807 | validation: 0.3770196401400465]
	TIME [epoch: 5.98 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14947898783732325		[learning rate: 0.0015136]
	Learning Rate: 0.00151356
	LOSS [training: 0.14947898783732325 | validation: 0.30047200679815317]
	TIME [epoch: 5.99 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12123079358215705		[learning rate: 0.0015082]
	Learning Rate: 0.00150821
	LOSS [training: 0.12123079358215705 | validation: 0.3147591551233965]
	TIME [epoch: 5.98 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13859487677854446		[learning rate: 0.0015029]
	Learning Rate: 0.00150288
	LOSS [training: 0.13859487677854446 | validation: 0.31018675781799204]
	TIME [epoch: 5.98 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12752736872219136		[learning rate: 0.0014976]
	Learning Rate: 0.00149756
	LOSS [training: 0.12752736872219136 | validation: 0.2843295367229299]
	TIME [epoch: 5.98 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11123035484719365		[learning rate: 0.0014923]
	Learning Rate: 0.00149227
	LOSS [training: 0.11123035484719365 | validation: 0.3115632332562089]
	TIME [epoch: 5.99 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.113376757453693		[learning rate: 0.001487]
	Learning Rate: 0.00148699
	LOSS [training: 0.113376757453693 | validation: 0.2996012148038739]
	TIME [epoch: 5.99 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11512994727705297		[learning rate: 0.0014817]
	Learning Rate: 0.00148173
	LOSS [training: 0.11512994727705297 | validation: 0.2920342710550781]
	TIME [epoch: 5.99 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11492694999337051		[learning rate: 0.0014765]
	Learning Rate: 0.00147649
	LOSS [training: 0.11492694999337051 | validation: 0.3436709254016766]
	TIME [epoch: 5.98 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11515313631578526		[learning rate: 0.0014713]
	Learning Rate: 0.00147127
	LOSS [training: 0.11515313631578526 | validation: 0.2601147087027824]
	TIME [epoch: 5.99 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15609904230264685		[learning rate: 0.0014661]
	Learning Rate: 0.00146607
	LOSS [training: 0.15609904230264685 | validation: 0.6686119951949435]
	TIME [epoch: 5.98 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28345995109557964		[learning rate: 0.0014609]
	Learning Rate: 0.00146088
	LOSS [training: 0.28345995109557964 | validation: 0.2560503748593813]
	TIME [epoch: 5.98 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13418462618456026		[learning rate: 0.0014557]
	Learning Rate: 0.00145572
	LOSS [training: 0.13418462618456026 | validation: 0.3023821457725006]
	TIME [epoch: 5.99 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12620254879351572		[learning rate: 0.0014506]
	Learning Rate: 0.00145057
	LOSS [training: 0.12620254879351572 | validation: 0.37468047013466377]
	TIME [epoch: 5.99 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1271581649933154		[learning rate: 0.0014454]
	Learning Rate: 0.00144544
	LOSS [training: 0.1271581649933154 | validation: 0.27339155945449434]
	TIME [epoch: 5.98 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1145501581435691		[learning rate: 0.0014403]
	Learning Rate: 0.00144033
	LOSS [training: 0.1145501581435691 | validation: 0.3372706234015804]
	TIME [epoch: 5.99 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12144806889372887		[learning rate: 0.0014352]
	Learning Rate: 0.00143524
	LOSS [training: 0.12144806889372887 | validation: 0.27043382971706126]
	TIME [epoch: 5.98 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14381245559206754		[learning rate: 0.0014302]
	Learning Rate: 0.00143016
	LOSS [training: 0.14381245559206754 | validation: 0.4262282496984526]
	TIME [epoch: 5.98 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17007642052015282		[learning rate: 0.0014251]
	Learning Rate: 0.0014251
	LOSS [training: 0.17007642052015282 | validation: 0.2575463702687754]
	TIME [epoch: 5.94 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13597435874849054		[learning rate: 0.0014201]
	Learning Rate: 0.00142006
	LOSS [training: 0.13597435874849054 | validation: 0.44116833432413644]
	TIME [epoch: 5.94 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14686657252556745		[learning rate: 0.001415]
	Learning Rate: 0.00141504
	LOSS [training: 0.14686657252556745 | validation: 0.2669430864894354]
	TIME [epoch: 5.96 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11997950201552003		[learning rate: 0.00141]
	Learning Rate: 0.00141004
	LOSS [training: 0.11997950201552003 | validation: 0.3058497413876374]
	TIME [epoch: 5.98 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11143721213875404		[learning rate: 0.0014051]
	Learning Rate: 0.00140505
	LOSS [training: 0.11143721213875404 | validation: 0.29965690019445906]
	TIME [epoch: 5.98 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10838076432452848		[learning rate: 0.0014001]
	Learning Rate: 0.00140008
	LOSS [training: 0.10838076432452848 | validation: 0.30126020518540725]
	TIME [epoch: 5.97 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10242030033124039		[learning rate: 0.0013951]
	Learning Rate: 0.00139513
	LOSS [training: 0.10242030033124039 | validation: 0.30500661443190186]
	TIME [epoch: 5.97 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1038525155122331		[learning rate: 0.0013902]
	Learning Rate: 0.0013902
	LOSS [training: 0.1038525155122331 | validation: 0.29726904946997995]
	TIME [epoch: 5.98 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10587675982281937		[learning rate: 0.0013853]
	Learning Rate: 0.00138528
	LOSS [training: 0.10587675982281937 | validation: 0.30805100003183744]
	TIME [epoch: 5.99 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10749833594773503		[learning rate: 0.0013804]
	Learning Rate: 0.00138038
	LOSS [training: 0.10749833594773503 | validation: 0.3094661843053692]
	TIME [epoch: 5.99 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10663495174426818		[learning rate: 0.0013755]
	Learning Rate: 0.0013755
	LOSS [training: 0.10663495174426818 | validation: 0.28008397077210445]
	TIME [epoch: 5.99 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11277454080954033		[learning rate: 0.0013706]
	Learning Rate: 0.00137064
	LOSS [training: 0.11277454080954033 | validation: 0.3910088326949656]
	TIME [epoch: 5.99 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16372352544464158		[learning rate: 0.0013658]
	Learning Rate: 0.00136579
	LOSS [training: 0.16372352544464158 | validation: 0.2557637442667975]
	TIME [epoch: 5.99 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12012604955956967		[learning rate: 0.001361]
	Learning Rate: 0.00136096
	LOSS [training: 0.12012604955956967 | validation: 0.5648202730135762]
	TIME [epoch: 5.98 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1929592051821573		[learning rate: 0.0013561]
	Learning Rate: 0.00135615
	LOSS [training: 0.1929592051821573 | validation: 0.23601955898635413]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_615.pth
	Model improved!!!
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1343769836798358		[learning rate: 0.0013514]
	Learning Rate: 0.00135135
	LOSS [training: 0.1343769836798358 | validation: 0.4250568205335906]
	TIME [epoch: 5.97 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16266701189178634		[learning rate: 0.0013466]
	Learning Rate: 0.00134658
	LOSS [training: 0.16266701189178634 | validation: 0.2856415655383104]
	TIME [epoch: 5.97 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11163992194875427		[learning rate: 0.0013418]
	Learning Rate: 0.00134181
	LOSS [training: 0.11163992194875427 | validation: 0.3081012593848744]
	TIME [epoch: 5.97 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12480631655994011		[learning rate: 0.0013371]
	Learning Rate: 0.00133707
	LOSS [training: 0.12480631655994011 | validation: 0.29610510652368055]
	TIME [epoch: 5.98 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10553965466236408		[learning rate: 0.0013323]
	Learning Rate: 0.00133234
	LOSS [training: 0.10553965466236408 | validation: 0.26144881958836547]
	TIME [epoch: 5.97 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10113623713427378		[learning rate: 0.0013276]
	Learning Rate: 0.00132763
	LOSS [training: 0.10113623713427378 | validation: 0.3894588229471607]
	TIME [epoch: 5.97 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12229514855662911		[learning rate: 0.0013229]
	Learning Rate: 0.00132293
	LOSS [training: 0.12229514855662911 | validation: 0.24852335525649172]
	TIME [epoch: 5.97 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14666558109494252		[learning rate: 0.0013183]
	Learning Rate: 0.00131826
	LOSS [training: 0.14666558109494252 | validation: 0.4802412361708786]
	TIME [epoch: 5.97 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16832663406280787		[learning rate: 0.0013136]
	Learning Rate: 0.0013136
	LOSS [training: 0.16832663406280787 | validation: 0.2532864105247654]
	TIME [epoch: 5.97 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11627678590893208		[learning rate: 0.001309]
	Learning Rate: 0.00130895
	LOSS [training: 0.11627678590893208 | validation: 0.37330857772879655]
	TIME [epoch: 5.97 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11184977542086834		[learning rate: 0.0013043]
	Learning Rate: 0.00130432
	LOSS [training: 0.11184977542086834 | validation: 0.2934900816887723]
	TIME [epoch: 5.97 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10145571723870317		[learning rate: 0.0012997]
	Learning Rate: 0.00129971
	LOSS [training: 0.10145571723870317 | validation: 0.2517876795661104]
	TIME [epoch: 5.97 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10749077580662075		[learning rate: 0.0012951]
	Learning Rate: 0.00129511
	LOSS [training: 0.10749077580662075 | validation: 0.4530759241698409]
	TIME [epoch: 5.97 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1355339453423587		[learning rate: 0.0012905]
	Learning Rate: 0.00129053
	LOSS [training: 0.1355339453423587 | validation: 0.24737862524457613]
	TIME [epoch: 5.97 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11083031053656273		[learning rate: 0.001286]
	Learning Rate: 0.00128597
	LOSS [training: 0.11083031053656273 | validation: 0.3795724738976157]
	TIME [epoch: 5.98 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11531813598322253		[learning rate: 0.0012814]
	Learning Rate: 0.00128142
	LOSS [training: 0.11531813598322253 | validation: 0.2502424621435941]
	TIME [epoch: 5.97 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1216588875516808		[learning rate: 0.0012769]
	Learning Rate: 0.00127689
	LOSS [training: 0.1216588875516808 | validation: 0.35325948381269767]
	TIME [epoch: 5.97 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1167459346404364		[learning rate: 0.0012724]
	Learning Rate: 0.00127238
	LOSS [training: 0.1167459346404364 | validation: 0.27135358902469]
	TIME [epoch: 5.97 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0987296989262239		[learning rate: 0.0012679]
	Learning Rate: 0.00126788
	LOSS [training: 0.0987296989262239 | validation: 0.32422685288492037]
	TIME [epoch: 5.98 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1067369596881494		[learning rate: 0.0012634]
	Learning Rate: 0.00126339
	LOSS [training: 0.1067369596881494 | validation: 0.28597724039988626]
	TIME [epoch: 5.97 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1057341627146882		[learning rate: 0.0012589]
	Learning Rate: 0.00125893
	LOSS [training: 0.1057341627146882 | validation: 0.2957358410678938]
	TIME [epoch: 5.97 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10040074005764989		[learning rate: 0.0012545]
	Learning Rate: 0.00125447
	LOSS [training: 0.10040074005764989 | validation: 0.29449443629461236]
	TIME [epoch: 5.98 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09236041677373404		[learning rate: 0.00125]
	Learning Rate: 0.00125004
	LOSS [training: 0.09236041677373404 | validation: 0.2710505472117162]
	TIME [epoch: 5.98 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09528349541852324		[learning rate: 0.0012456]
	Learning Rate: 0.00124562
	LOSS [training: 0.09528349541852324 | validation: 0.40933048643103653]
	TIME [epoch: 5.98 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12695577443831407		[learning rate: 0.0012412]
	Learning Rate: 0.00124121
	LOSS [training: 0.12695577443831407 | validation: 0.25138068192571145]
	TIME [epoch: 5.97 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20011524433149908		[learning rate: 0.0012368]
	Learning Rate: 0.00123682
	LOSS [training: 0.20011524433149908 | validation: 0.5895586896166127]
	TIME [epoch: 5.97 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2058660391785997		[learning rate: 0.0012324]
	Learning Rate: 0.00123245
	LOSS [training: 0.2058660391785997 | validation: 0.3038379796534189]
	TIME [epoch: 5.97 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0955759670768364		[learning rate: 0.0012281]
	Learning Rate: 0.00122809
	LOSS [training: 0.0955759670768364 | validation: 0.24189607674542632]
	TIME [epoch: 5.96 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.148054787196176		[learning rate: 0.0012237]
	Learning Rate: 0.00122375
	LOSS [training: 0.148054787196176 | validation: 0.4452691314864625]
	TIME [epoch: 5.97 sec]
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14340801212599788		[learning rate: 0.0012194]
	Learning Rate: 0.00121942
	LOSS [training: 0.14340801212599788 | validation: 0.3152963410620453]
	TIME [epoch: 5.97 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.099120664814425		[learning rate: 0.0012151]
	Learning Rate: 0.00121511
	LOSS [training: 0.099120664814425 | validation: 0.23465599420249933]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_646.pth
	Model improved!!!
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11223369268979307		[learning rate: 0.0012108]
	Learning Rate: 0.00121081
	LOSS [training: 0.11223369268979307 | validation: 0.44077863087725383]
	TIME [epoch: 5.98 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21033317963070416		[learning rate: 0.0012065]
	Learning Rate: 0.00120653
	LOSS [training: 0.21033317963070416 | validation: 0.34227608259364084]
	TIME [epoch: 5.99 sec]
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11636153818320366		[learning rate: 0.0012023]
	Learning Rate: 0.00120226
	LOSS [training: 0.11636153818320366 | validation: 0.2507873185015202]
	TIME [epoch: 5.98 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13219909903371493		[learning rate: 0.001198]
	Learning Rate: 0.00119801
	LOSS [training: 0.13219909903371493 | validation: 0.34125019020179015]
	TIME [epoch: 5.98 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11130500323471904		[learning rate: 0.0011938]
	Learning Rate: 0.00119378
	LOSS [training: 0.11130500323471904 | validation: 0.28166176299581974]
	TIME [epoch: 5.98 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09315952665667648		[learning rate: 0.0011896]
	Learning Rate: 0.00118956
	LOSS [training: 0.09315952665667648 | validation: 0.2750551483896239]
	TIME [epoch: 5.98 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10198538107353666		[learning rate: 0.0011853]
	Learning Rate: 0.00118535
	LOSS [training: 0.10198538107353666 | validation: 0.307041149800224]
	TIME [epoch: 5.98 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09635658793709943		[learning rate: 0.0011812]
	Learning Rate: 0.00118116
	LOSS [training: 0.09635658793709943 | validation: 0.25677427951283466]
	TIME [epoch: 5.97 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09259781745882233		[learning rate: 0.001177]
	Learning Rate: 0.00117698
	LOSS [training: 0.09259781745882233 | validation: 0.2770610983017558]
	TIME [epoch: 5.98 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1132420756212757		[learning rate: 0.0011728]
	Learning Rate: 0.00117282
	LOSS [training: 0.1132420756212757 | validation: 0.2964303356471157]
	TIME [epoch: 5.98 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09326349478392584		[learning rate: 0.0011687]
	Learning Rate: 0.00116867
	LOSS [training: 0.09326349478392584 | validation: 0.2934369443595976]
	TIME [epoch: 5.98 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0871443103516713		[learning rate: 0.0011645]
	Learning Rate: 0.00116454
	LOSS [training: 0.0871443103516713 | validation: 0.24363285881745564]
	TIME [epoch: 5.98 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11975093761509101		[learning rate: 0.0011604]
	Learning Rate: 0.00116042
	LOSS [training: 0.11975093761509101 | validation: 0.4617694397384245]
	TIME [epoch: 5.97 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15204946419272522		[learning rate: 0.0011563]
	Learning Rate: 0.00115632
	LOSS [training: 0.15204946419272522 | validation: 0.26473479002182637]
	TIME [epoch: 5.97 sec]
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10579713765544106		[learning rate: 0.0011522]
	Learning Rate: 0.00115223
	LOSS [training: 0.10579713765544106 | validation: 0.32561248085304667]
	TIME [epoch: 5.98 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10817973399564881		[learning rate: 0.0011482]
	Learning Rate: 0.00114815
	LOSS [training: 0.10817973399564881 | validation: 0.27248362414125094]
	TIME [epoch: 5.98 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09066310297897096		[learning rate: 0.0011441]
	Learning Rate: 0.00114409
	LOSS [training: 0.09066310297897096 | validation: 0.266671374226856]
	TIME [epoch: 5.98 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0890456520500859		[learning rate: 0.00114]
	Learning Rate: 0.00114005
	LOSS [training: 0.0890456520500859 | validation: 0.2979017123481273]
	TIME [epoch: 6.01 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0935073668970696		[learning rate: 0.001136]
	Learning Rate: 0.00113602
	LOSS [training: 0.0935073668970696 | validation: 0.2848627534576787]
	TIME [epoch: 5.98 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09180356744788278		[learning rate: 0.001132]
	Learning Rate: 0.001132
	LOSS [training: 0.09180356744788278 | validation: 0.24738509374846895]
	TIME [epoch: 5.98 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09124462136314217		[learning rate: 0.001128]
	Learning Rate: 0.001128
	LOSS [training: 0.09124462136314217 | validation: 0.40687404186529774]
	TIME [epoch: 5.98 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11638373154957725		[learning rate: 0.001124]
	Learning Rate: 0.00112401
	LOSS [training: 0.11638373154957725 | validation: 0.24100311923889384]
	TIME [epoch: 5.98 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14262946028121806		[learning rate: 0.00112]
	Learning Rate: 0.00112003
	LOSS [training: 0.14262946028121806 | validation: 0.4848157659053783]
	TIME [epoch: 5.97 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1536381474088714		[learning rate: 0.0011161]
	Learning Rate: 0.00111607
	LOSS [training: 0.1536381474088714 | validation: 0.2853168832201608]
	TIME [epoch: 5.97 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0865986190968426		[learning rate: 0.0011121]
	Learning Rate: 0.00111213
	LOSS [training: 0.0865986190968426 | validation: 0.23191940951986392]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_671.pth
	Model improved!!!
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0959905572940636		[learning rate: 0.0011082]
	Learning Rate: 0.00110819
	LOSS [training: 0.0959905572940636 | validation: 0.3700727317571686]
	TIME [epoch: 5.98 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10084210027122724		[learning rate: 0.0011043]
	Learning Rate: 0.00110427
	LOSS [training: 0.10084210027122724 | validation: 0.24482214868343874]
	TIME [epoch: 5.98 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08911724549430432		[learning rate: 0.0011004]
	Learning Rate: 0.00110037
	LOSS [training: 0.08911724549430432 | validation: 0.286149841890551]
	TIME [epoch: 5.98 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09078124547416608		[learning rate: 0.0010965]
	Learning Rate: 0.00109648
	LOSS [training: 0.09078124547416608 | validation: 0.2857380613489667]
	TIME [epoch: 5.98 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15641499677220652		[learning rate: 0.0010926]
	Learning Rate: 0.0010926
	LOSS [training: 0.15641499677220652 | validation: 0.41401820554726004]
	TIME [epoch: 5.98 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12531405630551487		[learning rate: 0.0010887]
	Learning Rate: 0.00108874
	LOSS [training: 0.12531405630551487 | validation: 0.26767346264955844]
	TIME [epoch: 5.97 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11605329593023012		[learning rate: 0.0010849]
	Learning Rate: 0.00108489
	LOSS [training: 0.11605329593023012 | validation: 0.32662492091016554]
	TIME [epoch: 5.98 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09646820581217526		[learning rate: 0.0010811]
	Learning Rate: 0.00108105
	LOSS [training: 0.09646820581217526 | validation: 0.2711934305404324]
	TIME [epoch: 5.98 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08620210213032888		[learning rate: 0.0010772]
	Learning Rate: 0.00107723
	LOSS [training: 0.08620210213032888 | validation: 0.30230920027297836]
	TIME [epoch: 5.98 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09803876693654492		[learning rate: 0.0010734]
	Learning Rate: 0.00107342
	LOSS [training: 0.09803876693654492 | validation: 0.26034389210131104]
	TIME [epoch: 5.97 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09438099062118208		[learning rate: 0.0010696]
	Learning Rate: 0.00106962
	LOSS [training: 0.09438099062118208 | validation: 0.4120294960833816]
	TIME [epoch: 5.98 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11416169173531547		[learning rate: 0.0010658]
	Learning Rate: 0.00106584
	LOSS [training: 0.11416169173531547 | validation: 0.2519841293873955]
	TIME [epoch: 5.98 sec]
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867089530800822		[learning rate: 0.0010621]
	Learning Rate: 0.00106207
	LOSS [training: 0.08867089530800822 | validation: 0.2833534649942146]
	TIME [epoch: 5.99 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08133196963812285		[learning rate: 0.0010583]
	Learning Rate: 0.00105832
	LOSS [training: 0.08133196963812285 | validation: 0.2667137839836859]
	TIME [epoch: 5.97 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08252273836284442		[learning rate: 0.0010546]
	Learning Rate: 0.00105457
	LOSS [training: 0.08252273836284442 | validation: 0.27024365014415336]
	TIME [epoch: 5.98 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0804067112954186		[learning rate: 0.0010508]
	Learning Rate: 0.00105084
	LOSS [training: 0.0804067112954186 | validation: 0.2603295495694348]
	TIME [epoch: 5.97 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08736679324270079		[learning rate: 0.0010471]
	Learning Rate: 0.00104713
	LOSS [training: 0.08736679324270079 | validation: 0.3266955135929206]
	TIME [epoch: 5.97 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1089235600564179		[learning rate: 0.0010434]
	Learning Rate: 0.00104343
	LOSS [training: 0.1089235600564179 | validation: 0.25274039321047986]
	TIME [epoch: 5.97 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886423254246758		[learning rate: 0.0010397]
	Learning Rate: 0.00103974
	LOSS [training: 0.09886423254246758 | validation: 0.528776730636553]
	TIME [epoch: 5.99 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14698546279577276		[learning rate: 0.0010361]
	Learning Rate: 0.00103606
	LOSS [training: 0.14698546279577276 | validation: 0.24104808833112293]
	TIME [epoch: 5.97 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10443519267736584		[learning rate: 0.0010324]
	Learning Rate: 0.0010324
	LOSS [training: 0.10443519267736584 | validation: 0.30863217289880573]
	TIME [epoch: 5.98 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970738091196569		[learning rate: 0.0010287]
	Learning Rate: 0.00102874
	LOSS [training: 0.0970738091196569 | validation: 0.27909145263996304]
	TIME [epoch: 5.98 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08038759988252381		[learning rate: 0.0010251]
	Learning Rate: 0.00102511
	LOSS [training: 0.08038759988252381 | validation: 0.2737993355317298]
	TIME [epoch: 5.98 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07976359670380959		[learning rate: 0.0010215]
	Learning Rate: 0.00102148
	LOSS [training: 0.07976359670380959 | validation: 0.2867657274507769]
	TIME [epoch: 5.98 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08445348842546657		[learning rate: 0.0010179]
	Learning Rate: 0.00101787
	LOSS [training: 0.08445348842546657 | validation: 0.2503948065269844]
	TIME [epoch: 5.98 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08733925255811784		[learning rate: 0.0010143]
	Learning Rate: 0.00101427
	LOSS [training: 0.08733925255811784 | validation: 0.3764664973720687]
	TIME [epoch: 5.98 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1046263183625898		[learning rate: 0.0010107]
	Learning Rate: 0.00101068
	LOSS [training: 0.1046263183625898 | validation: 0.22798258140373645]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_698.pth
	Model improved!!!
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13921757665418155		[learning rate: 0.0010071]
	Learning Rate: 0.00100711
	LOSS [training: 0.13921757665418155 | validation: 0.48638976427833414]
	TIME [epoch: 5.97 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1477040631513897		[learning rate: 0.0010035]
	Learning Rate: 0.00100355
	LOSS [training: 0.1477040631513897 | validation: 0.26967400071835196]
	TIME [epoch: 5.98 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08867952665294235		[learning rate: 0.001]
	Learning Rate: 0.001
	LOSS [training: 0.08867952665294235 | validation: 0.23706841959194316]
	TIME [epoch: 5.94 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12739580615526855		[learning rate: 0.00099646]
	Learning Rate: 0.000996464
	LOSS [training: 0.12739580615526855 | validation: 0.4360336645087083]
	TIME [epoch: 5.96 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1304342763716637		[learning rate: 0.00099294]
	Learning Rate: 0.00099294
	LOSS [training: 0.1304342763716637 | validation: 0.2689683348021549]
	TIME [epoch: 5.96 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08271721645698467		[learning rate: 0.00098943]
	Learning Rate: 0.000989429
	LOSS [training: 0.08271721645698467 | validation: 0.23475438651187527]
	TIME [epoch: 5.95 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09354525062213892		[learning rate: 0.00098593]
	Learning Rate: 0.00098593
	LOSS [training: 0.09354525062213892 | validation: 0.354863883247086]
	TIME [epoch: 5.96 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0966426137497746		[learning rate: 0.00098244]
	Learning Rate: 0.000982444
	LOSS [training: 0.0966426137497746 | validation: 0.2670959674337133]
	TIME [epoch: 5.95 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08020628730314147		[learning rate: 0.00097897]
	Learning Rate: 0.00097897
	LOSS [training: 0.08020628730314147 | validation: 0.25063015026152297]
	TIME [epoch: 5.95 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692602877383596		[learning rate: 0.00097551]
	Learning Rate: 0.000975508
	LOSS [training: 0.08692602877383596 | validation: 0.3092908369090519]
	TIME [epoch: 5.96 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0815483248944933		[learning rate: 0.00097206]
	Learning Rate: 0.000972058
	LOSS [training: 0.0815483248944933 | validation: 0.23550199933815286]
	TIME [epoch: 5.95 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0857750423280788		[learning rate: 0.00096862]
	Learning Rate: 0.000968621
	LOSS [training: 0.0857750423280788 | validation: 0.355045564410705]
	TIME [epoch: 5.96 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09566682277933325		[learning rate: 0.0009652]
	Learning Rate: 0.000965196
	LOSS [training: 0.09566682277933325 | validation: 0.24751985075636718]
	TIME [epoch: 5.96 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09349173579256757		[learning rate: 0.00096178]
	Learning Rate: 0.000961783
	LOSS [training: 0.09349173579256757 | validation: 0.3605401788284887]
	TIME [epoch: 5.96 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09757048973667722		[learning rate: 0.00095838]
	Learning Rate: 0.000958382
	LOSS [training: 0.09757048973667722 | validation: 0.26416474607115936]
	TIME [epoch: 5.96 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07507978466571487		[learning rate: 0.00095499]
	Learning Rate: 0.000954993
	LOSS [training: 0.07507978466571487 | validation: 0.25272819491784976]
	TIME [epoch: 5.96 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07792335419274299		[learning rate: 0.00095162]
	Learning Rate: 0.000951616
	LOSS [training: 0.07792335419274299 | validation: 0.29947587748754945]
	TIME [epoch: 5.96 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08116526427339955		[learning rate: 0.00094825]
	Learning Rate: 0.000948251
	LOSS [training: 0.08116526427339955 | validation: 0.24039809475160717]
	TIME [epoch: 5.95 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09699880757307346		[learning rate: 0.0009449]
	Learning Rate: 0.000944897
	LOSS [training: 0.09699880757307346 | validation: 0.44048575808078055]
	TIME [epoch: 5.95 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11983353822222645		[learning rate: 0.00094156]
	Learning Rate: 0.000941556
	LOSS [training: 0.11983353822222645 | validation: 0.2454744832438301]
	TIME [epoch: 5.96 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09324008518070813		[learning rate: 0.00093823]
	Learning Rate: 0.000938227
	LOSS [training: 0.09324008518070813 | validation: 0.32312539959042064]
	TIME [epoch: 5.96 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10751218212673164		[learning rate: 0.00093491]
	Learning Rate: 0.000934909
	LOSS [training: 0.10751218212673164 | validation: 0.25646137213641035]
	TIME [epoch: 5.96 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07522807875482267		[learning rate: 0.0009316]
	Learning Rate: 0.000931603
	LOSS [training: 0.07522807875482267 | validation: 0.25929095764948584]
	TIME [epoch: 5.96 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08692092068572387		[learning rate: 0.00092831]
	Learning Rate: 0.000928309
	LOSS [training: 0.08692092068572387 | validation: 0.3202544361543511]
	TIME [epoch: 5.96 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09584642938089992		[learning rate: 0.00092503]
	Learning Rate: 0.000925026
	LOSS [training: 0.09584642938089992 | validation: 0.22736199950336822]
	TIME [epoch: 5.96 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_723.pth
	Model improved!!!
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09103071302577917		[learning rate: 0.00092175]
	Learning Rate: 0.000921755
	LOSS [training: 0.09103071302577917 | validation: 0.31852943988225924]
	TIME [epoch: 6 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08639730809439801		[learning rate: 0.0009185]
	Learning Rate: 0.000918495
	LOSS [training: 0.08639730809439801 | validation: 0.23209355047589886]
	TIME [epoch: 6 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08329943940392812		[learning rate: 0.00091525]
	Learning Rate: 0.000915247
	LOSS [training: 0.08329943940392812 | validation: 0.337305145554373]
	TIME [epoch: 6 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08494239388260523		[learning rate: 0.00091201]
	Learning Rate: 0.000912011
	LOSS [training: 0.08494239388260523 | validation: 0.26443293622695724]
	TIME [epoch: 6.01 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07822221901165216		[learning rate: 0.00090879]
	Learning Rate: 0.000908786
	LOSS [training: 0.07822221901165216 | validation: 0.283042166464567]
	TIME [epoch: 6 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07965795687447325		[learning rate: 0.00090557]
	Learning Rate: 0.000905572
	LOSS [training: 0.07965795687447325 | validation: 0.2519171115138857]
	TIME [epoch: 5.95 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.077695661149821		[learning rate: 0.00090237]
	Learning Rate: 0.00090237
	LOSS [training: 0.077695661149821 | validation: 0.33508011335230253]
	TIME [epoch: 5.95 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0979705582042305		[learning rate: 0.00089918]
	Learning Rate: 0.000899179
	LOSS [training: 0.0979705582042305 | validation: 0.2088558991589145]
	TIME [epoch: 5.95 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_731.pth
	Model improved!!!
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1384163507644336		[learning rate: 0.000896]
	Learning Rate: 0.000895999
	LOSS [training: 0.1384163507644336 | validation: 0.47406199377450736]
	TIME [epoch: 5.99 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13732673412831062		[learning rate: 0.00089283]
	Learning Rate: 0.000892831
	LOSS [training: 0.13732673412831062 | validation: 0.2821988430350042]
	TIME [epoch: 5.98 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09088408083455297		[learning rate: 0.00088967]
	Learning Rate: 0.000889674
	LOSS [training: 0.09088408083455297 | validation: 0.21866922266614752]
	TIME [epoch: 5.98 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10415943105954738		[learning rate: 0.00088653]
	Learning Rate: 0.000886528
	LOSS [training: 0.10415943105954738 | validation: 0.3837873648712737]
	TIME [epoch: 5.98 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040249975048104		[learning rate: 0.00088339]
	Learning Rate: 0.000883393
	LOSS [training: 0.1040249975048104 | validation: 0.26427335215096237]
	TIME [epoch: 5.98 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07791790711945608		[learning rate: 0.00088027]
	Learning Rate: 0.000880269
	LOSS [training: 0.07791790711945608 | validation: 0.2198911756360011]
	TIME [epoch: 5.98 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09045871420833106		[learning rate: 0.00087716]
	Learning Rate: 0.000877156
	LOSS [training: 0.09045871420833106 | validation: 0.30602414903291664]
	TIME [epoch: 5.99 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0895229226894643		[learning rate: 0.00087405]
	Learning Rate: 0.000874055
	LOSS [training: 0.0895229226894643 | validation: 0.23517496445646602]
	TIME [epoch: 5.99 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08009321277571925		[learning rate: 0.00087096]
	Learning Rate: 0.000870964
	LOSS [training: 0.08009321277571925 | validation: 0.3074969721106531]
	TIME [epoch: 5.98 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0787924617587983		[learning rate: 0.00086788]
	Learning Rate: 0.000867884
	LOSS [training: 0.0787924617587983 | validation: 0.25515653259010834]
	TIME [epoch: 5.98 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07318721405186837		[learning rate: 0.00086481]
	Learning Rate: 0.000864815
	LOSS [training: 0.07318721405186837 | validation: 0.24718089832891557]
	TIME [epoch: 5.99 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07682784669749725		[learning rate: 0.00086176]
	Learning Rate: 0.000861757
	LOSS [training: 0.07682784669749725 | validation: 0.2770066581367467]
	TIME [epoch: 5.97 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07433319214735527		[learning rate: 0.00085871]
	Learning Rate: 0.000858709
	LOSS [training: 0.07433319214735527 | validation: 0.2595407188607323]
	TIME [epoch: 5.98 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06964379655491414		[learning rate: 0.00085567]
	Learning Rate: 0.000855673
	LOSS [training: 0.06964379655491414 | validation: 0.26108370184323]
	TIME [epoch: 5.98 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07103106327259835		[learning rate: 0.00085265]
	Learning Rate: 0.000852647
	LOSS [training: 0.07103106327259835 | validation: 0.23984956645450875]
	TIME [epoch: 5.99 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0744136663444775		[learning rate: 0.00084963]
	Learning Rate: 0.000849632
	LOSS [training: 0.0744136663444775 | validation: 0.3030844433367527]
	TIME [epoch: 5.98 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838272567572235		[learning rate: 0.00084663]
	Learning Rate: 0.000846627
	LOSS [training: 0.0838272567572235 | validation: 0.22982287123317913]
	TIME [epoch: 5.99 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09776884714655527		[learning rate: 0.00084363]
	Learning Rate: 0.000843634
	LOSS [training: 0.09776884714655527 | validation: 0.45247372510347256]
	TIME [epoch: 5.98 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11487705538780951		[learning rate: 0.00084065]
	Learning Rate: 0.00084065
	LOSS [training: 0.11487705538780951 | validation: 0.21815179411066926]
	TIME [epoch: 5.98 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0880049781694407		[learning rate: 0.00083768]
	Learning Rate: 0.000837678
	LOSS [training: 0.0880049781694407 | validation: 0.2694087497111663]
	TIME [epoch: 5.98 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07742696966596509		[learning rate: 0.00083472]
	Learning Rate: 0.000834715
	LOSS [training: 0.07742696966596509 | validation: 0.2681414813751718]
	TIME [epoch: 5.98 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07527176173222447		[learning rate: 0.00083176]
	Learning Rate: 0.000831764
	LOSS [training: 0.07527176173222447 | validation: 0.26367417639421]
	TIME [epoch: 5.98 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0754124939943305		[learning rate: 0.00082882]
	Learning Rate: 0.000828823
	LOSS [training: 0.0754124939943305 | validation: 0.2490274400995407]
	TIME [epoch: 5.99 sec]
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0726098497862515		[learning rate: 0.00082589]
	Learning Rate: 0.000825892
	LOSS [training: 0.0726098497862515 | validation: 0.29894536249456166]
	TIME [epoch: 5.98 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07620281142356661		[learning rate: 0.00082297]
	Learning Rate: 0.000822971
	LOSS [training: 0.07620281142356661 | validation: 0.22818284378851256]
	TIME [epoch: 5.99 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09104485494896729		[learning rate: 0.00082006]
	Learning Rate: 0.000820061
	LOSS [training: 0.09104485494896729 | validation: 0.38058252926624975]
	TIME [epoch: 5.98 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11932300449943978		[learning rate: 0.00081716]
	Learning Rate: 0.000817161
	LOSS [training: 0.11932300449943978 | validation: 0.22786169759807048]
	TIME [epoch: 5.98 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07790095056016076		[learning rate: 0.00081427]
	Learning Rate: 0.000814272
	LOSS [training: 0.07790095056016076 | validation: 0.28390006099750603]
	TIME [epoch: 5.98 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0751817827268741		[learning rate: 0.00081139]
	Learning Rate: 0.000811392
	LOSS [training: 0.0751817827268741 | validation: 0.25958384615158214]
	TIME [epoch: 5.98 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07363387042857208		[learning rate: 0.00080852]
	Learning Rate: 0.000808523
	LOSS [training: 0.07363387042857208 | validation: 0.25880559486092947]
	TIME [epoch: 5.98 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07276312871907009		[learning rate: 0.00080566]
	Learning Rate: 0.000805664
	LOSS [training: 0.07276312871907009 | validation: 0.2451468891039914]
	TIME [epoch: 5.99 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07083981990562833		[learning rate: 0.00080281]
	Learning Rate: 0.000802815
	LOSS [training: 0.07083981990562833 | validation: 0.3076835787901609]
	TIME [epoch: 5.98 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07604853184187696		[learning rate: 0.00079998]
	Learning Rate: 0.000799976
	LOSS [training: 0.07604853184187696 | validation: 0.23233278211671218]
	TIME [epoch: 5.99 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09596527730906736		[learning rate: 0.00079715]
	Learning Rate: 0.000797147
	LOSS [training: 0.09596527730906736 | validation: 0.4150467478092809]
	TIME [epoch: 5.99 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11957682564770954		[learning rate: 0.00079433]
	Learning Rate: 0.000794328
	LOSS [training: 0.11957682564770954 | validation: 0.2454422935241261]
	TIME [epoch: 5.99 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07093416227853407		[learning rate: 0.00079152]
	Learning Rate: 0.000791519
	LOSS [training: 0.07093416227853407 | validation: 0.23282741659411502]
	TIME [epoch: 5.98 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10606483164441384		[learning rate: 0.00078872]
	Learning Rate: 0.00078872
	LOSS [training: 0.10606483164441384 | validation: 0.40842801033436243]
	TIME [epoch: 5.98 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12391008849433054		[learning rate: 0.00078593]
	Learning Rate: 0.000785931
	LOSS [training: 0.12391008849433054 | validation: 0.25654536512890064]
	TIME [epoch: 5.98 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07603911749935957		[learning rate: 0.00078315]
	Learning Rate: 0.000783152
	LOSS [training: 0.07603911749935957 | validation: 0.21732939363032266]
	TIME [epoch: 5.98 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11107976788388067		[learning rate: 0.00078038]
	Learning Rate: 0.000780383
	LOSS [training: 0.11107976788388067 | validation: 0.38555988207660263]
	TIME [epoch: 5.99 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10576966033426785		[learning rate: 0.00077762]
	Learning Rate: 0.000777623
	LOSS [training: 0.10576966033426785 | validation: 0.2680261466092801]
	TIME [epoch: 5.99 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08311455216834808		[learning rate: 0.00077487]
	Learning Rate: 0.000774873
	LOSS [training: 0.08311455216834808 | validation: 0.21521915611101067]
	TIME [epoch: 5.99 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08794844054878531		[learning rate: 0.00077213]
	Learning Rate: 0.000772134
	LOSS [training: 0.08794844054878531 | validation: 0.3433487755424578]
	TIME [epoch: 5.99 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08306113445402577		[learning rate: 0.0007694]
	Learning Rate: 0.000769403
	LOSS [training: 0.08306113445402577 | validation: 0.26250270819080485]
	TIME [epoch: 5.98 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07012999496072006		[learning rate: 0.00076668]
	Learning Rate: 0.000766682
	LOSS [training: 0.07012999496072006 | validation: 0.2248577632210793]
	TIME [epoch: 5.98 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0742456930448676		[learning rate: 0.00076397]
	Learning Rate: 0.000763971
	LOSS [training: 0.0742456930448676 | validation: 0.3010329633818716]
	TIME [epoch: 5.98 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532224714450678		[learning rate: 0.00076127]
	Learning Rate: 0.00076127
	LOSS [training: 0.07532224714450678 | validation: 0.22119087373705965]
	TIME [epoch: 5.99 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07479634231459281		[learning rate: 0.00075858]
	Learning Rate: 0.000758578
	LOSS [training: 0.07479634231459281 | validation: 0.307885919007952]
	TIME [epoch: 5.98 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07958210427861767		[learning rate: 0.0007559]
	Learning Rate: 0.000755895
	LOSS [training: 0.07958210427861767 | validation: 0.21911220726280367]
	TIME [epoch: 5.99 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07096100274559902		[learning rate: 0.00075322]
	Learning Rate: 0.000753222
	LOSS [training: 0.07096100274559902 | validation: 0.26360786545316345]
	TIME [epoch: 5.99 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06525963813092674		[learning rate: 0.00075056]
	Learning Rate: 0.000750559
	LOSS [training: 0.06525963813092674 | validation: 0.2580803944329806]
	TIME [epoch: 5.99 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06784863715192248		[learning rate: 0.0007479]
	Learning Rate: 0.000747905
	LOSS [training: 0.06784863715192248 | validation: 0.2432838541638679]
	TIME [epoch: 5.99 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766180824624791		[learning rate: 0.00074526]
	Learning Rate: 0.00074526
	LOSS [training: 0.06766180824624791 | validation: 0.2645674759045605]
	TIME [epoch: 6 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07155443929889878		[learning rate: 0.00074262]
	Learning Rate: 0.000742624
	LOSS [training: 0.07155443929889878 | validation: 0.21699239272432913]
	TIME [epoch: 5.98 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07460025700562964		[learning rate: 0.00074]
	Learning Rate: 0.000739998
	LOSS [training: 0.07460025700562964 | validation: 0.33942681359167326]
	TIME [epoch: 5.98 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09608916886853607		[learning rate: 0.00073738]
	Learning Rate: 0.000737382
	LOSS [training: 0.09608916886853607 | validation: 0.2165832171372983]
	TIME [epoch: 5.98 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07190158265274843		[learning rate: 0.00073477]
	Learning Rate: 0.000734774
	LOSS [training: 0.07190158265274843 | validation: 0.26934537259357433]
	TIME [epoch: 5.98 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07182571967777948		[learning rate: 0.00073218]
	Learning Rate: 0.000732176
	LOSS [training: 0.07182571967777948 | validation: 0.22434524129156821]
	TIME [epoch: 6.03 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07352366416033196		[learning rate: 0.00072959]
	Learning Rate: 0.000729587
	LOSS [training: 0.07352366416033196 | validation: 0.3302145161486847]
	TIME [epoch: 5.99 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07363993338546346		[learning rate: 0.00072701]
	Learning Rate: 0.000727007
	LOSS [training: 0.07363993338546346 | validation: 0.22051471228862385]
	TIME [epoch: 5.99 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06324061306581187		[learning rate: 0.00072444]
	Learning Rate: 0.000724436
	LOSS [training: 0.06324061306581187 | validation: 0.24027602043859045]
	TIME [epoch: 5.98 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06804757192839757		[learning rate: 0.00072187]
	Learning Rate: 0.000721874
	LOSS [training: 0.06804757192839757 | validation: 0.36686314941798004]
	TIME [epoch: 5.98 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08894369742710413		[learning rate: 0.00071932]
	Learning Rate: 0.000719322
	LOSS [training: 0.08894369742710413 | validation: 0.20848242317575327]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_794.pth
	Model improved!!!
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09045975798401507		[learning rate: 0.00071678]
	Learning Rate: 0.000716778
	LOSS [training: 0.09045975798401507 | validation: 0.3247489328519946]
	TIME [epoch: 5.99 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09513071247239711		[learning rate: 0.00071424]
	Learning Rate: 0.000714243
	LOSS [training: 0.09513071247239711 | validation: 0.22201959843624117]
	TIME [epoch: 5.98 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06965813529447348		[learning rate: 0.00071172]
	Learning Rate: 0.000711718
	LOSS [training: 0.06965813529447348 | validation: 0.2704692109324253]
	TIME [epoch: 5.98 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07063504734179973		[learning rate: 0.0007092]
	Learning Rate: 0.000709201
	LOSS [training: 0.07063504734179973 | validation: 0.23128156812228903]
	TIME [epoch: 5.99 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07287860899862995		[learning rate: 0.00070669]
	Learning Rate: 0.000706693
	LOSS [training: 0.07287860899862995 | validation: 0.29272936995944593]
	TIME [epoch: 5.99 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07370159929923928		[learning rate: 0.00070419]
	Learning Rate: 0.000704194
	LOSS [training: 0.07370159929923928 | validation: 0.22985368391854186]
	TIME [epoch: 5.99 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06655738889225077		[learning rate: 0.0007017]
	Learning Rate: 0.000701704
	LOSS [training: 0.06655738889225077 | validation: 0.23242632022971704]
	TIME [epoch: 5.99 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06475518436091807		[learning rate: 0.00069922]
	Learning Rate: 0.000699222
	LOSS [training: 0.06475518436091807 | validation: 0.2807075129601576]
	TIME [epoch: 5.98 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07458793317288386		[learning rate: 0.00069675]
	Learning Rate: 0.00069675
	LOSS [training: 0.07458793317288386 | validation: 0.2242596098998917]
	TIME [epoch: 5.98 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07624520873329108		[learning rate: 0.00069429]
	Learning Rate: 0.000694286
	LOSS [training: 0.07624520873329108 | validation: 0.2931385012656264]
	TIME [epoch: 5.99 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06957225188760628		[learning rate: 0.00069183]
	Learning Rate: 0.000691831
	LOSS [training: 0.06957225188760628 | validation: 0.20453343499165333]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_805.pth
	Model improved!!!
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08961199854524128		[learning rate: 0.00068938]
	Learning Rate: 0.000689385
	LOSS [training: 0.08961199854524128 | validation: 0.4273868079775624]
	TIME [epoch: 5.98 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12898984799086258		[learning rate: 0.00068695]
	Learning Rate: 0.000686947
	LOSS [training: 0.12898984799086258 | validation: 0.2503133971242096]
	TIME [epoch: 5.98 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06308401941035083		[learning rate: 0.00068452]
	Learning Rate: 0.000684518
	LOSS [training: 0.06308401941035083 | validation: 0.2164183149082856]
	TIME [epoch: 5.99 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09482414402814325		[learning rate: 0.0006821]
	Learning Rate: 0.000682097
	LOSS [training: 0.09482414402814325 | validation: 0.34118058046139704]
	TIME [epoch: 5.99 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09548713531297029		[learning rate: 0.00067969]
	Learning Rate: 0.000679685
	LOSS [training: 0.09548713531297029 | validation: 0.2350934912657542]
	TIME [epoch: 5.99 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06736170191943021		[learning rate: 0.00067728]
	Learning Rate: 0.000677282
	LOSS [training: 0.06736170191943021 | validation: 0.23022220180831768]
	TIME [epoch: 5.99 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07977897504713724		[learning rate: 0.00067489]
	Learning Rate: 0.000674887
	LOSS [training: 0.07977897504713724 | validation: 0.331737787977294]
	TIME [epoch: 5.99 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.084156063541678		[learning rate: 0.0006725]
	Learning Rate: 0.0006725
	LOSS [training: 0.084156063541678 | validation: 0.23008438773215692]
	TIME [epoch: 5.98 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06688031460881284		[learning rate: 0.00067012]
	Learning Rate: 0.000670122
	LOSS [training: 0.06688031460881284 | validation: 0.2314767750702084]
	TIME [epoch: 5.99 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06788644321894237		[learning rate: 0.00066775]
	Learning Rate: 0.000667752
	LOSS [training: 0.06788644321894237 | validation: 0.25027607861240114]
	TIME [epoch: 5.99 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06810831435808125		[learning rate: 0.00066539]
	Learning Rate: 0.000665391
	LOSS [training: 0.06810831435808125 | validation: 0.23770694937023393]
	TIME [epoch: 5.99 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06498579491552552		[learning rate: 0.00066304]
	Learning Rate: 0.000663038
	LOSS [training: 0.06498579491552552 | validation: 0.24871183500525837]
	TIME [epoch: 5.99 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06528335987641933		[learning rate: 0.00066069]
	Learning Rate: 0.000660694
	LOSS [training: 0.06528335987641933 | validation: 0.25142575672027834]
	TIME [epoch: 5.99 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0653985782321507		[learning rate: 0.00065836]
	Learning Rate: 0.000658357
	LOSS [training: 0.0653985782321507 | validation: 0.23822598608377646]
	TIME [epoch: 5.99 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06422867616943555		[learning rate: 0.00065603]
	Learning Rate: 0.000656029
	LOSS [training: 0.06422867616943555 | validation: 0.2793697303555899]
	TIME [epoch: 5.99 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07497965010880052		[learning rate: 0.00065371]
	Learning Rate: 0.000653709
	LOSS [training: 0.07497965010880052 | validation: 0.2128844360091975]
	TIME [epoch: 5.99 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07993860303319383		[learning rate: 0.0006514]
	Learning Rate: 0.000651398
	LOSS [training: 0.07993860303319383 | validation: 0.3399244302332194]
	TIME [epoch: 5.99 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08912010021555171		[learning rate: 0.00064909]
	Learning Rate: 0.000649094
	LOSS [training: 0.08912010021555171 | validation: 0.2238134702050359]
	TIME [epoch: 5.99 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06399090217798674		[learning rate: 0.0006468]
	Learning Rate: 0.000646799
	LOSS [training: 0.06399090217798674 | validation: 0.22946491770026212]
	TIME [epoch: 5.99 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07590601652993863		[learning rate: 0.00064451]
	Learning Rate: 0.000644512
	LOSS [training: 0.07590601652993863 | validation: 0.3087300770024436]
	TIME [epoch: 5.99 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07339657126752618		[learning rate: 0.00064223]
	Learning Rate: 0.000642233
	LOSS [training: 0.07339657126752618 | validation: 0.20460600990378236]
	TIME [epoch: 6 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0833142112727538		[learning rate: 0.00063996]
	Learning Rate: 0.000639962
	LOSS [training: 0.0833142112727538 | validation: 0.29933277916670187]
	TIME [epoch: 6 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09511232263809342		[learning rate: 0.0006377]
	Learning Rate: 0.000637699
	LOSS [training: 0.09511232263809342 | validation: 0.22156825181157852]
	TIME [epoch: 6 sec]
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0700597832623016		[learning rate: 0.00063544]
	Learning Rate: 0.000635443
	LOSS [training: 0.0700597832623016 | validation: 0.2808483461069586]
	TIME [epoch: 5.99 sec]
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705665490607623		[learning rate: 0.0006332]
	Learning Rate: 0.000633196
	LOSS [training: 0.0705665490607623 | validation: 0.2364116631282509]
	TIME [epoch: 5.99 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06528389689733244		[learning rate: 0.00063096]
	Learning Rate: 0.000630957
	LOSS [training: 0.06528389689733244 | validation: 0.24946946782234952]
	TIME [epoch: 5.98 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06648730829727542		[learning rate: 0.00062873]
	Learning Rate: 0.000628726
	LOSS [training: 0.06648730829727542 | validation: 0.257107330320921]
	TIME [epoch: 5.99 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06279933204801384		[learning rate: 0.0006265]
	Learning Rate: 0.000626503
	LOSS [training: 0.06279933204801384 | validation: 0.24040332362539574]
	TIME [epoch: 5.98 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06328462766746232		[learning rate: 0.00062429]
	Learning Rate: 0.000624287
	LOSS [training: 0.06328462766746232 | validation: 0.2397585101853923]
	TIME [epoch: 5.99 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06021685279177513		[learning rate: 0.00062208]
	Learning Rate: 0.00062208
	LOSS [training: 0.06021685279177513 | validation: 0.23644325162995142]
	TIME [epoch: 5.99 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059915967173465125		[learning rate: 0.00061988]
	Learning Rate: 0.00061988
	LOSS [training: 0.059915967173465125 | validation: 0.25315467491891613]
	TIME [epoch: 5.99 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06627856718683928		[learning rate: 0.00061769]
	Learning Rate: 0.000617688
	LOSS [training: 0.06627856718683928 | validation: 0.23251520048578775]
	TIME [epoch: 5.98 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06591722675856568		[learning rate: 0.0006155]
	Learning Rate: 0.000615504
	LOSS [training: 0.06591722675856568 | validation: 0.25846755722929354]
	TIME [epoch: 5.98 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07607496447741313		[learning rate: 0.00061333]
	Learning Rate: 0.000613327
	LOSS [training: 0.07607496447741313 | validation: 0.20390330384968217]
	TIME [epoch: 5.97 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_839.pth
	Model improved!!!
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07249947087674993		[learning rate: 0.00061116]
	Learning Rate: 0.000611158
	LOSS [training: 0.07249947087674993 | validation: 0.2886637071617232]
	TIME [epoch: 5.97 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07148641266010299		[learning rate: 0.000609]
	Learning Rate: 0.000608997
	LOSS [training: 0.07148641266010299 | validation: 0.20940839519210996]
	TIME [epoch: 5.98 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07656739118749119		[learning rate: 0.00060684]
	Learning Rate: 0.000606844
	LOSS [training: 0.07656739118749119 | validation: 0.34683687614099884]
	TIME [epoch: 5.98 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08967679168822083		[learning rate: 0.0006047]
	Learning Rate: 0.000604698
	LOSS [training: 0.08967679168822083 | validation: 0.22361598951137848]
	TIME [epoch: 5.98 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06961678354289985		[learning rate: 0.00060256]
	Learning Rate: 0.00060256
	LOSS [training: 0.06961678354289985 | validation: 0.23505259831091468]
	TIME [epoch: 5.99 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07376805216559086		[learning rate: 0.00060043]
	Learning Rate: 0.000600429
	LOSS [training: 0.07376805216559086 | validation: 0.29610755049299226]
	TIME [epoch: 5.98 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06905046592913144		[learning rate: 0.00059831]
	Learning Rate: 0.000598306
	LOSS [training: 0.06905046592913144 | validation: 0.21222646307906504]
	TIME [epoch: 5.98 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06665700483892195		[learning rate: 0.00059619]
	Learning Rate: 0.00059619
	LOSS [training: 0.06665700483892195 | validation: 0.2744971313748836]
	TIME [epoch: 5.98 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06884229113224995		[learning rate: 0.00059408]
	Learning Rate: 0.000594082
	LOSS [training: 0.06884229113224995 | validation: 0.24634747141886654]
	TIME [epoch: 5.98 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06187102634703825		[learning rate: 0.00059198]
	Learning Rate: 0.000591981
	LOSS [training: 0.06187102634703825 | validation: 0.19895895538884445]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_849.pth
	Model improved!!!
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07480449001181695		[learning rate: 0.00058989]
	Learning Rate: 0.000589888
	LOSS [training: 0.07480449001181695 | validation: 0.36053163381036435]
	TIME [epoch: 5.98 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0950508395731303		[learning rate: 0.0005878]
	Learning Rate: 0.000587802
	LOSS [training: 0.0950508395731303 | validation: 0.2270252251194061]
	TIME [epoch: 5.98 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06646462288537851		[learning rate: 0.00058572]
	Learning Rate: 0.000585723
	LOSS [training: 0.06646462288537851 | validation: 0.24680795377482176]
	TIME [epoch: 5.99 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07692474366862008		[learning rate: 0.00058365]
	Learning Rate: 0.000583652
	LOSS [training: 0.07692474366862008 | validation: 0.24092891679829226]
	TIME [epoch: 5.98 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06298526897866666		[learning rate: 0.00058159]
	Learning Rate: 0.000581588
	LOSS [training: 0.06298526897866666 | validation: 0.2135581058328765]
	TIME [epoch: 5.99 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06679711996704114		[learning rate: 0.00057953]
	Learning Rate: 0.000579531
	LOSS [training: 0.06679711996704114 | validation: 0.26272288828730855]
	TIME [epoch: 5.98 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06396043597528338		[learning rate: 0.00057748]
	Learning Rate: 0.000577482
	LOSS [training: 0.06396043597528338 | validation: 0.20348458454638008]
	TIME [epoch: 5.98 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06300009608384126		[learning rate: 0.00057544]
	Learning Rate: 0.00057544
	LOSS [training: 0.06300009608384126 | validation: 0.23729778433250648]
	TIME [epoch: 5.98 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0639287323688285		[learning rate: 0.00057341]
	Learning Rate: 0.000573405
	LOSS [training: 0.0639287323688285 | validation: 0.21054454685891036]
	TIME [epoch: 5.98 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06353316808873313		[learning rate: 0.00057138]
	Learning Rate: 0.000571377
	LOSS [training: 0.06353316808873313 | validation: 0.25099115153601465]
	TIME [epoch: 5.97 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642465311410696		[learning rate: 0.00056936]
	Learning Rate: 0.000569357
	LOSS [training: 0.0642465311410696 | validation: 0.21761388598763398]
	TIME [epoch: 5.98 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06987684149903452		[learning rate: 0.00056734]
	Learning Rate: 0.000567344
	LOSS [training: 0.06987684149903452 | validation: 0.31769798662247206]
	TIME [epoch: 5.98 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0717473350423072		[learning rate: 0.00056534]
	Learning Rate: 0.000565337
	LOSS [training: 0.0717473350423072 | validation: 0.21882214943531816]
	TIME [epoch: 5.98 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06546421956514442		[learning rate: 0.00056334]
	Learning Rate: 0.000563338
	LOSS [training: 0.06546421956514442 | validation: 0.22588742434509862]
	TIME [epoch: 5.98 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218212620133061		[learning rate: 0.00056135]
	Learning Rate: 0.000561346
	LOSS [training: 0.06218212620133061 | validation: 0.2642030998530291]
	TIME [epoch: 5.99 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06402525201088076		[learning rate: 0.00055936]
	Learning Rate: 0.000559361
	LOSS [training: 0.06402525201088076 | validation: 0.214957634650482]
	TIME [epoch: 5.98 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07364205666813137		[learning rate: 0.00055738]
	Learning Rate: 0.000557383
	LOSS [training: 0.07364205666813137 | validation: 0.349212059936627]
	TIME [epoch: 5.98 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08882295065838221		[learning rate: 0.00055541]
	Learning Rate: 0.000555412
	LOSS [training: 0.08882295065838221 | validation: 0.22614112922562812]
	TIME [epoch: 5.98 sec]
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05715058096730744		[learning rate: 0.00055345]
	Learning Rate: 0.000553448
	LOSS [training: 0.05715058096730744 | validation: 0.20252325212279398]
	TIME [epoch: 5.98 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07575754653952864		[learning rate: 0.00055149]
	Learning Rate: 0.000551491
	LOSS [training: 0.07575754653952864 | validation: 0.32966825095295504]
	TIME [epoch: 5.98 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08529883094295995		[learning rate: 0.00054954]
	Learning Rate: 0.000549541
	LOSS [training: 0.08529883094295995 | validation: 0.2156052682343005]
	TIME [epoch: 5.99 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05922899897888399		[learning rate: 0.0005476]
	Learning Rate: 0.000547598
	LOSS [training: 0.05922899897888399 | validation: 0.2052311831749652]
	TIME [epoch: 5.99 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06777275150454751		[learning rate: 0.00054566]
	Learning Rate: 0.000545661
	LOSS [training: 0.06777275150454751 | validation: 0.2904393047102187]
	TIME [epoch: 5.99 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09200784751095224		[learning rate: 0.00054373]
	Learning Rate: 0.000543732
	LOSS [training: 0.09200784751095224 | validation: 0.24934701035316445]
	TIME [epoch: 5.99 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0682115381867391		[learning rate: 0.00054181]
	Learning Rate: 0.000541809
	LOSS [training: 0.0682115381867391 | validation: 0.21594231698094868]
	TIME [epoch: 5.99 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07202706255468434		[learning rate: 0.00053989]
	Learning Rate: 0.000539893
	LOSS [training: 0.07202706255468434 | validation: 0.2611854089939494]
	TIME [epoch: 5.98 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05823945957412702		[learning rate: 0.00053798]
	Learning Rate: 0.000537984
	LOSS [training: 0.05823945957412702 | validation: 0.2233413880210014]
	TIME [epoch: 5.98 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06302300780896374		[learning rate: 0.00053608]
	Learning Rate: 0.000536081
	LOSS [training: 0.06302300780896374 | validation: 0.21469052812177147]
	TIME [epoch: 5.98 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060794904179674515		[learning rate: 0.00053419]
	Learning Rate: 0.000534186
	LOSS [training: 0.060794904179674515 | validation: 0.24800340242707863]
	TIME [epoch: 5.99 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060560469781514734		[learning rate: 0.0005323]
	Learning Rate: 0.000532297
	LOSS [training: 0.060560469781514734 | validation: 0.19683222890313734]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_879.pth
	Model improved!!!
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06422682456663421		[learning rate: 0.00053041]
	Learning Rate: 0.000530415
	LOSS [training: 0.06422682456663421 | validation: 0.27629391437990636]
	TIME [epoch: 5.99 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07069225301568344		[learning rate: 0.00052854]
	Learning Rate: 0.000528539
	LOSS [training: 0.07069225301568344 | validation: 0.21056199389103014]
	TIME [epoch: 5.99 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0704716407164568		[learning rate: 0.00052667]
	Learning Rate: 0.00052667
	LOSS [training: 0.0704716407164568 | validation: 0.2279757800685701]
	TIME [epoch: 5.98 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06000844315105176		[learning rate: 0.00052481]
	Learning Rate: 0.000524808
	LOSS [training: 0.06000844315105176 | validation: 0.22859579305188632]
	TIME [epoch: 5.98 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06390437868949002		[learning rate: 0.00052295]
	Learning Rate: 0.000522952
	LOSS [training: 0.06390437868949002 | validation: 0.20983874467531477]
	TIME [epoch: 5.98 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05824153225986797		[learning rate: 0.0005211]
	Learning Rate: 0.000521102
	LOSS [training: 0.05824153225986797 | validation: 0.23933255364125558]
	TIME [epoch: 5.99 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06051899730408818		[learning rate: 0.00051926]
	Learning Rate: 0.00051926
	LOSS [training: 0.06051899730408818 | validation: 0.22445528796566183]
	TIME [epoch: 5.98 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05560150651231877		[learning rate: 0.00051742]
	Learning Rate: 0.000517423
	LOSS [training: 0.05560150651231877 | validation: 0.21525128274762417]
	TIME [epoch: 5.98 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06150185440041036		[learning rate: 0.00051559]
	Learning Rate: 0.000515594
	LOSS [training: 0.06150185440041036 | validation: 0.2583836061944324]
	TIME [epoch: 5.99 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060619487210487634		[learning rate: 0.00051377]
	Learning Rate: 0.000513771
	LOSS [training: 0.060619487210487634 | validation: 0.19728022436678616]
	TIME [epoch: 5.98 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06955132222219848		[learning rate: 0.00051195]
	Learning Rate: 0.000511954
	LOSS [training: 0.06955132222219848 | validation: 0.29115510253070437]
	TIME [epoch: 5.99 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09127017611729521		[learning rate: 0.00051014]
	Learning Rate: 0.000510144
	LOSS [training: 0.09127017611729521 | validation: 0.22251006353443545]
	TIME [epoch: 5.99 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06394220661219613		[learning rate: 0.00050834]
	Learning Rate: 0.000508339
	LOSS [training: 0.06394220661219613 | validation: 0.1966185433798798]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_892.pth
	Model improved!!!
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08913850492207305		[learning rate: 0.00050654]
	Learning Rate: 0.000506542
	LOSS [training: 0.08913850492207305 | validation: 0.3831379032895358]
	TIME [epoch: 5.98 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09698433407364027		[learning rate: 0.00050475]
	Learning Rate: 0.000504751
	LOSS [training: 0.09698433407364027 | validation: 0.24688248841193927]
	TIME [epoch: 5.98 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06093465684804056		[learning rate: 0.00050297]
	Learning Rate: 0.000502966
	LOSS [training: 0.06093465684804056 | validation: 0.18742616987929359]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_895.pth
	Model improved!!!
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07561619767071026		[learning rate: 0.00050119]
	Learning Rate: 0.000501187
	LOSS [training: 0.07561619767071026 | validation: 0.2850235184683976]
	TIME [epoch: 5.98 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06844998471598367		[learning rate: 0.00049941]
	Learning Rate: 0.000499415
	LOSS [training: 0.06844998471598367 | validation: 0.22450998232181557]
	TIME [epoch: 5.99 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056272692170532185		[learning rate: 0.00049765]
	Learning Rate: 0.000497649
	LOSS [training: 0.056272692170532185 | validation: 0.19023339833626673]
	TIME [epoch: 6 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07124827125240306		[learning rate: 0.00049589]
	Learning Rate: 0.000495889
	LOSS [training: 0.07124827125240306 | validation: 0.24865901504253946]
	TIME [epoch: 5.99 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0605413305238458		[learning rate: 0.00049414]
	Learning Rate: 0.000494136
	LOSS [training: 0.0605413305238458 | validation: 0.22891712441238116]
	TIME [epoch: 6 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05637797340136544		[learning rate: 0.00049239]
	Learning Rate: 0.000492388
	LOSS [training: 0.05637797340136544 | validation: 0.21934396511445653]
	TIME [epoch: 5.99 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.058334374110236516		[learning rate: 0.00049065]
	Learning Rate: 0.000490647
	LOSS [training: 0.058334374110236516 | validation: 0.2611731193906393]
	TIME [epoch: 6 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06140912856079399		[learning rate: 0.00048891]
	Learning Rate: 0.000488912
	LOSS [training: 0.06140912856079399 | validation: 0.2163667330703496]
	TIME [epoch: 5.99 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059668756929937915		[learning rate: 0.00048718]
	Learning Rate: 0.000487183
	LOSS [training: 0.059668756929937915 | validation: 0.23730506418916067]
	TIME [epoch: 5.99 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06065933185944012		[learning rate: 0.00048546]
	Learning Rate: 0.00048546
	LOSS [training: 0.06065933185944012 | validation: 0.1986867423204164]
	TIME [epoch: 6 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056769554688510165		[learning rate: 0.00048374]
	Learning Rate: 0.000483744
	LOSS [training: 0.056769554688510165 | validation: 0.23006345169952946]
	TIME [epoch: 6 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05648719054776221		[learning rate: 0.00048203]
	Learning Rate: 0.000482033
	LOSS [training: 0.05648719054776221 | validation: 0.2277681935822609]
	TIME [epoch: 6.01 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05406931196086658		[learning rate: 0.00048033]
	Learning Rate: 0.000480329
	LOSS [training: 0.05406931196086658 | validation: 0.19616147915122115]
	TIME [epoch: 6 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06008794707359316		[learning rate: 0.00047863]
	Learning Rate: 0.00047863
	LOSS [training: 0.06008794707359316 | validation: 0.2708297619644339]
	TIME [epoch: 6 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06680074962192566		[learning rate: 0.00047694]
	Learning Rate: 0.000476938
	LOSS [training: 0.06680074962192566 | validation: 0.1957724994194127]
	TIME [epoch: 6 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07355768473123946		[learning rate: 0.00047525]
	Learning Rate: 0.000475251
	LOSS [training: 0.07355768473123946 | validation: 0.3023319254962484]
	TIME [epoch: 6 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08042736166162504		[learning rate: 0.00047357]
	Learning Rate: 0.00047357
	LOSS [training: 0.08042736166162504 | validation: 0.22400230851172528]
	TIME [epoch: 6 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05737929424288429		[learning rate: 0.0004719]
	Learning Rate: 0.000471896
	LOSS [training: 0.05737929424288429 | validation: 0.19091866707884605]
	TIME [epoch: 5.99 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06707644370658404		[learning rate: 0.00047023]
	Learning Rate: 0.000470227
	LOSS [training: 0.06707644370658404 | validation: 0.30010682078124473]
	TIME [epoch: 6 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07037010366330107		[learning rate: 0.00046856]
	Learning Rate: 0.000468564
	LOSS [training: 0.07037010366330107 | validation: 0.2084431372826385]
	TIME [epoch: 6 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05802125472057609		[learning rate: 0.00046691]
	Learning Rate: 0.000466907
	LOSS [training: 0.05802125472057609 | validation: 0.21083994700911896]
	TIME [epoch: 6 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07070764448559506		[learning rate: 0.00046526]
	Learning Rate: 0.000465256
	LOSS [training: 0.07070764448559506 | validation: 0.2750919580902094]
	TIME [epoch: 6.01 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06942784783203033		[learning rate: 0.00046361]
	Learning Rate: 0.000463611
	LOSS [training: 0.06942784783203033 | validation: 0.21919460170947302]
	TIME [epoch: 6 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06128877450350653		[learning rate: 0.00046197]
	Learning Rate: 0.000461972
	LOSS [training: 0.06128877450350653 | validation: 0.2012928231967787]
	TIME [epoch: 6 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06120530866883635		[learning rate: 0.00046034]
	Learning Rate: 0.000460338
	LOSS [training: 0.06120530866883635 | validation: 0.26013309058067274]
	TIME [epoch: 5.99 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06244670303757568		[learning rate: 0.00045871]
	Learning Rate: 0.00045871
	LOSS [training: 0.06244670303757568 | validation: 0.2217307439138642]
	TIME [epoch: 6 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06227623375654719		[learning rate: 0.00045709]
	Learning Rate: 0.000457088
	LOSS [training: 0.06227623375654719 | validation: 0.20074349564594388]
	TIME [epoch: 5.99 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06057865721039399		[learning rate: 0.00045547]
	Learning Rate: 0.000455472
	LOSS [training: 0.06057865721039399 | validation: 0.24766301162262044]
	TIME [epoch: 6 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06072278552279363		[learning rate: 0.00045386]
	Learning Rate: 0.000453861
	LOSS [training: 0.06072278552279363 | validation: 0.2061897971426878]
	TIME [epoch: 6 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05704813925246063		[learning rate: 0.00045226]
	Learning Rate: 0.000452256
	LOSS [training: 0.05704813925246063 | validation: 0.1974474005315884]
	TIME [epoch: 6 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0577537737468886		[learning rate: 0.00045066]
	Learning Rate: 0.000450657
	LOSS [training: 0.0577537737468886 | validation: 0.2566884644835814]
	TIME [epoch: 6 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06028181888988721		[learning rate: 0.00044906]
	Learning Rate: 0.000449063
	LOSS [training: 0.06028181888988721 | validation: 0.19286144361080987]
	TIME [epoch: 6 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05932867792904391		[learning rate: 0.00044748]
	Learning Rate: 0.000447476
	LOSS [training: 0.05932867792904391 | validation: 0.21089916629905223]
	TIME [epoch: 6 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05580688812621733		[learning rate: 0.00044589]
	Learning Rate: 0.000445893
	LOSS [training: 0.05580688812621733 | validation: 0.2280057726140358]
	TIME [epoch: 6 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055517913858716135		[learning rate: 0.00044432]
	Learning Rate: 0.000444316
	LOSS [training: 0.055517913858716135 | validation: 0.18635539603562457]
	TIME [epoch: 6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_930.pth
	Model improved!!!
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06190572701620077		[learning rate: 0.00044275]
	Learning Rate: 0.000442745
	LOSS [training: 0.06190572701620077 | validation: 0.3053034935388874]
	TIME [epoch: 5.97 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06944036096552358		[learning rate: 0.00044118]
	Learning Rate: 0.00044118
	LOSS [training: 0.06944036096552358 | validation: 0.17998519551754155]
	TIME [epoch: 5.98 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_932.pth
	Model improved!!!
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0697230396929256		[learning rate: 0.00043962]
	Learning Rate: 0.00043962
	LOSS [training: 0.0697230396929256 | validation: 0.23011767767878155]
	TIME [epoch: 5.98 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061593538722077026		[learning rate: 0.00043806]
	Learning Rate: 0.000438065
	LOSS [training: 0.061593538722077026 | validation: 0.19123232726357658]
	TIME [epoch: 5.98 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05547207535216987		[learning rate: 0.00043652]
	Learning Rate: 0.000436516
	LOSS [training: 0.05547207535216987 | validation: 0.22161149945327707]
	TIME [epoch: 5.98 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054018300068368524		[learning rate: 0.00043497]
	Learning Rate: 0.000434972
	LOSS [training: 0.054018300068368524 | validation: 0.21996538229155088]
	TIME [epoch: 5.98 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05366159075954181		[learning rate: 0.00043343]
	Learning Rate: 0.000433434
	LOSS [training: 0.05366159075954181 | validation: 0.1833412818325585]
	TIME [epoch: 5.98 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061166998820940795		[learning rate: 0.0004319]
	Learning Rate: 0.000431901
	LOSS [training: 0.061166998820940795 | validation: 0.2729613294877852]
	TIME [epoch: 5.98 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06608292959601325		[learning rate: 0.00043037]
	Learning Rate: 0.000430374
	LOSS [training: 0.06608292959601325 | validation: 0.1985394981117652]
	TIME [epoch: 5.97 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05785808404675469		[learning rate: 0.00042885]
	Learning Rate: 0.000428852
	LOSS [training: 0.05785808404675469 | validation: 0.19024186083326688]
	TIME [epoch: 5.99 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06825732040246626		[learning rate: 0.00042734]
	Learning Rate: 0.000427336
	LOSS [training: 0.06825732040246626 | validation: 0.27281315435843906]
	TIME [epoch: 6 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07730704184837768		[learning rate: 0.00042582]
	Learning Rate: 0.000425825
	LOSS [training: 0.07730704184837768 | validation: 0.2223762615472219]
	TIME [epoch: 6 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05819132705870212		[learning rate: 0.00042432]
	Learning Rate: 0.000424319
	LOSS [training: 0.05819132705870212 | validation: 0.18892073097864615]
	TIME [epoch: 6 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07716843697413675		[learning rate: 0.00042282]
	Learning Rate: 0.000422818
	LOSS [training: 0.07716843697413675 | validation: 0.25157708513698296]
	TIME [epoch: 5.99 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.055730807393057545		[learning rate: 0.00042132]
	Learning Rate: 0.000421323
	LOSS [training: 0.055730807393057545 | validation: 0.20776973856733083]
	TIME [epoch: 6 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05813904480783488		[learning rate: 0.00041983]
	Learning Rate: 0.000419833
	LOSS [training: 0.05813904480783488 | validation: 0.22240445852897095]
	TIME [epoch: 5.99 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05636547294645048		[learning rate: 0.00041835]
	Learning Rate: 0.000418349
	LOSS [training: 0.05636547294645048 | validation: 0.20618480497490138]
	TIME [epoch: 6 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05133553409448444		[learning rate: 0.00041687]
	Learning Rate: 0.000416869
	LOSS [training: 0.05133553409448444 | validation: 0.2005759079391497]
	TIME [epoch: 5.99 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053668094246427885		[learning rate: 0.0004154]
	Learning Rate: 0.000415395
	LOSS [training: 0.053668094246427885 | validation: 0.2018068330208235]
	TIME [epoch: 6 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052249636438236935		[learning rate: 0.00041393]
	Learning Rate: 0.000413926
	LOSS [training: 0.052249636438236935 | validation: 0.19495129398652036]
	TIME [epoch: 5.99 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0547609480001346		[learning rate: 0.00041246]
	Learning Rate: 0.000412463
	LOSS [training: 0.0547609480001346 | validation: 0.19290951755299696]
	TIME [epoch: 6 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0557614163932525		[learning rate: 0.000411]
	Learning Rate: 0.000411004
	LOSS [training: 0.0557614163932525 | validation: 0.210427167632795]
	TIME [epoch: 6 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05369728064292639		[learning rate: 0.00040955]
	Learning Rate: 0.000409551
	LOSS [training: 0.05369728064292639 | validation: 0.2282893379919569]
	TIME [epoch: 6 sec]
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0563571344964015		[learning rate: 0.0004081]
	Learning Rate: 0.000408102
	LOSS [training: 0.0563571344964015 | validation: 0.18721251275912407]
	TIME [epoch: 5.99 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060881046681896846		[learning rate: 0.00040666]
	Learning Rate: 0.000406659
	LOSS [training: 0.060881046681896846 | validation: 0.3064219980628125]
	TIME [epoch: 5.99 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07428301775758787		[learning rate: 0.00040522]
	Learning Rate: 0.000405221
	LOSS [training: 0.07428301775758787 | validation: 0.1778160144999058]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_956.pth
	Model improved!!!
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05942264033669574		[learning rate: 0.00040379]
	Learning Rate: 0.000403788
	LOSS [training: 0.05942264033669574 | validation: 0.19400248373975856]
	TIME [epoch: 5.93 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05813530438172009		[learning rate: 0.00040236]
	Learning Rate: 0.000402361
	LOSS [training: 0.05813530438172009 | validation: 0.31877543165985395]
	TIME [epoch: 5.94 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07032488084648822		[learning rate: 0.00040094]
	Learning Rate: 0.000400938
	LOSS [training: 0.07032488084648822 | validation: 0.19077213392616085]
	TIME [epoch: 6 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05229974084507248		[learning rate: 0.00039952]
	Learning Rate: 0.00039952
	LOSS [training: 0.05229974084507248 | validation: 0.1704158249935169]
	TIME [epoch: 5.99 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_960.pth
	Model improved!!!
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0565241374688881		[learning rate: 0.00039811]
	Learning Rate: 0.000398107
	LOSS [training: 0.0565241374688881 | validation: 0.22275236638598953]
	TIME [epoch: 5.94 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05442368726832674		[learning rate: 0.0003967]
	Learning Rate: 0.000396699
	LOSS [training: 0.05442368726832674 | validation: 0.2182609658975248]
	TIME [epoch: 5.93 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05319062567008548		[learning rate: 0.0003953]
	Learning Rate: 0.000395297
	LOSS [training: 0.05319062567008548 | validation: 0.188702748445155]
	TIME [epoch: 5.94 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05607680225321936		[learning rate: 0.0003939]
	Learning Rate: 0.000393899
	LOSS [training: 0.05607680225321936 | validation: 0.27267494747191007]
	TIME [epoch: 5.94 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08012918300031606		[learning rate: 0.00039251]
	Learning Rate: 0.000392506
	LOSS [training: 0.08012918300031606 | validation: 0.18261136224330402]
	TIME [epoch: 5.94 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054774539704158826		[learning rate: 0.00039112]
	Learning Rate: 0.000391118
	LOSS [training: 0.054774539704158826 | validation: 0.2251590985980183]
	TIME [epoch: 5.94 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05507636862335181		[learning rate: 0.00038973]
	Learning Rate: 0.000389735
	LOSS [training: 0.05507636862335181 | validation: 0.2154240656873455]
	TIME [epoch: 5.94 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053236024373995015		[learning rate: 0.00038836]
	Learning Rate: 0.000388357
	LOSS [training: 0.053236024373995015 | validation: 0.1897641245084976]
	TIME [epoch: 5.95 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05417819336984131		[learning rate: 0.00038698]
	Learning Rate: 0.000386983
	LOSS [training: 0.05417819336984131 | validation: 0.20634316200235286]
	TIME [epoch: 5.94 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.059776321389085736		[learning rate: 0.00038561]
	Learning Rate: 0.000385615
	LOSS [training: 0.059776321389085736 | validation: 0.18479640298290329]
	TIME [epoch: 5.94 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05812916730348553		[learning rate: 0.00038425]
	Learning Rate: 0.000384251
	LOSS [training: 0.05812916730348553 | validation: 0.21636708091882004]
	TIME [epoch: 5.95 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05747239233521828		[learning rate: 0.00038289]
	Learning Rate: 0.000382893
	LOSS [training: 0.05747239233521828 | validation: 0.18787975874984764]
	TIME [epoch: 5.94 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06793611717521154		[learning rate: 0.00038154]
	Learning Rate: 0.000381539
	LOSS [training: 0.06793611717521154 | validation: 0.25396154864273196]
	TIME [epoch: 5.94 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05817022644685939		[learning rate: 0.00038019]
	Learning Rate: 0.000380189
	LOSS [training: 0.05817022644685939 | validation: 0.1998933412162921]
	TIME [epoch: 5.93 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056397959575855916		[learning rate: 0.00037885]
	Learning Rate: 0.000378845
	LOSS [training: 0.056397959575855916 | validation: 0.1920194149487661]
	TIME [epoch: 5.94 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05388533892404047		[learning rate: 0.00037751]
	Learning Rate: 0.000377505
	LOSS [training: 0.05388533892404047 | validation: 0.2141598566813531]
	TIME [epoch: 5.94 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05955948909803355		[learning rate: 0.00037617]
	Learning Rate: 0.00037617
	LOSS [training: 0.05955948909803355 | validation: 0.22086552407781854]
	TIME [epoch: 5.94 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05519721335760906		[learning rate: 0.00037484]
	Learning Rate: 0.00037484
	LOSS [training: 0.05519721335760906 | validation: 0.19483682305860703]
	TIME [epoch: 5.96 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05284061782786398		[learning rate: 0.00037351]
	Learning Rate: 0.000373515
	LOSS [training: 0.05284061782786398 | validation: 0.20626993168325064]
	TIME [epoch: 5.96 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05271041982245801		[learning rate: 0.00037219]
	Learning Rate: 0.000372194
	LOSS [training: 0.05271041982245801 | validation: 0.22234496537598575]
	TIME [epoch: 5.95 sec]
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054198062631607015		[learning rate: 0.00037088]
	Learning Rate: 0.000370878
	LOSS [training: 0.054198062631607015 | validation: 0.17492285257017878]
	TIME [epoch: 5.95 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05435455941412542		[learning rate: 0.00036957]
	Learning Rate: 0.000369566
	LOSS [training: 0.05435455941412542 | validation: 0.2165487077205552]
	TIME [epoch: 5.95 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051066629636559176		[learning rate: 0.00036826]
	Learning Rate: 0.000368259
	LOSS [training: 0.051066629636559176 | validation: 0.20123354647898797]
	TIME [epoch: 5.95 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05454701939079769		[learning rate: 0.00036696]
	Learning Rate: 0.000366957
	LOSS [training: 0.05454701939079769 | validation: 0.19292109633609134]
	TIME [epoch: 5.95 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053859158121870525		[learning rate: 0.00036566]
	Learning Rate: 0.00036566
	LOSS [training: 0.053859158121870525 | validation: 0.20896199193832718]
	TIME [epoch: 5.95 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315875526308454		[learning rate: 0.00036437]
	Learning Rate: 0.000364367
	LOSS [training: 0.05315875526308454 | validation: 0.18166692758745273]
	TIME [epoch: 5.95 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06083773895916858		[learning rate: 0.00036308]
	Learning Rate: 0.000363078
	LOSS [training: 0.06083773895916858 | validation: 0.23026357564681876]
	TIME [epoch: 5.95 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05850737431411268		[learning rate: 0.00036179]
	Learning Rate: 0.000361794
	LOSS [training: 0.05850737431411268 | validation: 0.18090134859753118]
	TIME [epoch: 5.95 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05807769603469125		[learning rate: 0.00036051]
	Learning Rate: 0.000360515
	LOSS [training: 0.05807769603469125 | validation: 0.2573852472160791]
	TIME [epoch: 5.95 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05703615077480056		[learning rate: 0.00035924]
	Learning Rate: 0.00035924
	LOSS [training: 0.05703615077480056 | validation: 0.18825509217503977]
	TIME [epoch: 5.95 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0517035693354431		[learning rate: 0.00035797]
	Learning Rate: 0.00035797
	LOSS [training: 0.0517035693354431 | validation: 0.19589585861761782]
	TIME [epoch: 5.95 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854697237138353		[learning rate: 0.0003567]
	Learning Rate: 0.000356704
	LOSS [training: 0.05854697237138353 | validation: 0.16277892779412717]
	TIME [epoch: 5.94 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_992.pth
	Model improved!!!
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061387790638138595		[learning rate: 0.00035544]
	Learning Rate: 0.000355442
	LOSS [training: 0.061387790638138595 | validation: 0.3118764755733446]
	TIME [epoch: 6.01 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0703976736218966		[learning rate: 0.00035419]
	Learning Rate: 0.000354185
	LOSS [training: 0.0703976736218966 | validation: 0.20238163240384682]
	TIME [epoch: 6 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051441502064512375		[learning rate: 0.00035293]
	Learning Rate: 0.000352933
	LOSS [training: 0.051441502064512375 | validation: 0.16508898490059132]
	TIME [epoch: 6 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06262916933474075		[learning rate: 0.00035169]
	Learning Rate: 0.000351685
	LOSS [training: 0.06262916933474075 | validation: 0.26790379543117515]
	TIME [epoch: 6.01 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06011535991573369		[learning rate: 0.00035044]
	Learning Rate: 0.000350441
	LOSS [training: 0.06011535991573369 | validation: 0.1959736482632767]
	TIME [epoch: 6 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05122558442632281		[learning rate: 0.0003492]
	Learning Rate: 0.000349202
	LOSS [training: 0.05122558442632281 | validation: 0.1884524173037167]
	TIME [epoch: 6.01 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049499075194895444		[learning rate: 0.00034797]
	Learning Rate: 0.000347967
	LOSS [training: 0.049499075194895444 | validation: 0.1875856826023959]
	TIME [epoch: 6 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05235755099479683		[learning rate: 0.00034674]
	Learning Rate: 0.000346737
	LOSS [training: 0.05235755099479683 | validation: 0.19986239594010555]
	TIME [epoch: 6.01 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050361132154983404		[learning rate: 0.00034551]
	Learning Rate: 0.000345511
	LOSS [training: 0.050361132154983404 | validation: 0.18540271418203513]
	TIME [epoch: 218 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05350005145643671		[learning rate: 0.00034429]
	Learning Rate: 0.000344289
	LOSS [training: 0.05350005145643671 | validation: 0.22606377547786582]
	TIME [epoch: 12.8 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05396393111492168		[learning rate: 0.00034307]
	Learning Rate: 0.000343072
	LOSS [training: 0.05396393111492168 | validation: 0.18576296750991395]
	TIME [epoch: 12.7 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0544508340786782		[learning rate: 0.00034186]
	Learning Rate: 0.000341858
	LOSS [training: 0.0544508340786782 | validation: 0.21721891390917794]
	TIME [epoch: 12.7 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05414918181242234		[learning rate: 0.00034065]
	Learning Rate: 0.000340649
	LOSS [training: 0.05414918181242234 | validation: 0.17441088554939815]
	TIME [epoch: 12.7 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06072345440669521		[learning rate: 0.00033944]
	Learning Rate: 0.000339445
	LOSS [training: 0.06072345440669521 | validation: 0.22906523857957373]
	TIME [epoch: 12.7 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06635408123774958		[learning rate: 0.00033824]
	Learning Rate: 0.000338245
	LOSS [training: 0.06635408123774958 | validation: 0.20782913784736184]
	TIME [epoch: 12.7 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04886042691543466		[learning rate: 0.00033705]
	Learning Rate: 0.000337048
	LOSS [training: 0.04886042691543466 | validation: 0.17490694034696086]
	TIME [epoch: 12.7 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05968415475007028		[learning rate: 0.00033586]
	Learning Rate: 0.000335857
	LOSS [training: 0.05968415475007028 | validation: 0.2393648767268072]
	TIME [epoch: 12.7 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07215105682459608		[learning rate: 0.00033467]
	Learning Rate: 0.000334669
	LOSS [training: 0.07215105682459608 | validation: 0.18665503714042242]
	TIME [epoch: 12.7 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052016988143105555		[learning rate: 0.00033349]
	Learning Rate: 0.000333486
	LOSS [training: 0.052016988143105555 | validation: 0.17818748577093982]
	TIME [epoch: 12.7 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06218472391128706		[learning rate: 0.00033231]
	Learning Rate: 0.000332306
	LOSS [training: 0.06218472391128706 | validation: 0.23126632051623128]
	TIME [epoch: 12.7 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05543708819168303		[learning rate: 0.00033113]
	Learning Rate: 0.000331131
	LOSS [training: 0.05543708819168303 | validation: 0.1913070031405517]
	TIME [epoch: 12.7 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05112544177461674		[learning rate: 0.00032996]
	Learning Rate: 0.00032996
	LOSS [training: 0.05112544177461674 | validation: 0.17361245689210564]
	TIME [epoch: 12.7 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.056489769995511814		[learning rate: 0.00032879]
	Learning Rate: 0.000328793
	LOSS [training: 0.056489769995511814 | validation: 0.2250986544664996]
	TIME [epoch: 12.7 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050169465313716		[learning rate: 0.00032763]
	Learning Rate: 0.000327631
	LOSS [training: 0.050169465313716 | validation: 0.17190743171552866]
	TIME [epoch: 12.7 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053692033192635526		[learning rate: 0.00032647]
	Learning Rate: 0.000326472
	LOSS [training: 0.053692033192635526 | validation: 0.202487579885795]
	TIME [epoch: 12.7 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050876817260824554		[learning rate: 0.00032532]
	Learning Rate: 0.000325318
	LOSS [training: 0.050876817260824554 | validation: 0.19330117099836258]
	TIME [epoch: 12.7 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052539479266407375		[learning rate: 0.00032417]
	Learning Rate: 0.000324167
	LOSS [training: 0.052539479266407375 | validation: 0.19716506873181222]
	TIME [epoch: 12.7 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04853776967073866		[learning rate: 0.00032302]
	Learning Rate: 0.000323021
	LOSS [training: 0.04853776967073866 | validation: 0.22298212908160459]
	TIME [epoch: 12.7 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05270924904206809		[learning rate: 0.00032188]
	Learning Rate: 0.000321879
	LOSS [training: 0.05270924904206809 | validation: 0.16668659882560852]
	TIME [epoch: 12.7 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05242103801541076		[learning rate: 0.00032074]
	Learning Rate: 0.000320741
	LOSS [training: 0.05242103801541076 | validation: 0.22367834551720744]
	TIME [epoch: 12.7 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05564576353007829		[learning rate: 0.00031961]
	Learning Rate: 0.000319606
	LOSS [training: 0.05564576353007829 | validation: 0.18577389677541647]
	TIME [epoch: 12.7 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057839419337247046		[learning rate: 0.00031848]
	Learning Rate: 0.000318476
	LOSS [training: 0.057839419337247046 | validation: 0.2178686584790079]
	TIME [epoch: 12.7 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057182147701158766		[learning rate: 0.00031735]
	Learning Rate: 0.00031735
	LOSS [training: 0.057182147701158766 | validation: 0.21823281789858218]
	TIME [epoch: 12.7 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0505928784974201		[learning rate: 0.00031623]
	Learning Rate: 0.000316228
	LOSS [training: 0.0505928784974201 | validation: 0.1865695673569663]
	TIME [epoch: 12.7 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05610970991979587		[learning rate: 0.00031511]
	Learning Rate: 0.00031511
	LOSS [training: 0.05610970991979587 | validation: 0.23455447136028984]
	TIME [epoch: 12.7 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05359150995879969		[learning rate: 0.000314]
	Learning Rate: 0.000313995
	LOSS [training: 0.05359150995879969 | validation: 0.1844649835876199]
	TIME [epoch: 12.7 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04675364580643709		[learning rate: 0.00031288]
	Learning Rate: 0.000312885
	LOSS [training: 0.04675364580643709 | validation: 0.17879152329262982]
	TIME [epoch: 12.7 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05093513131832923		[learning rate: 0.00031178]
	Learning Rate: 0.000311779
	LOSS [training: 0.05093513131832923 | validation: 0.23375173077564437]
	TIME [epoch: 12.7 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054480387460846005		[learning rate: 0.00031068]
	Learning Rate: 0.000310676
	LOSS [training: 0.054480387460846005 | validation: 0.18104674529898937]
	TIME [epoch: 12.7 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05176340550366862		[learning rate: 0.00030958]
	Learning Rate: 0.000309577
	LOSS [training: 0.05176340550366862 | validation: 0.19591766296859964]
	TIME [epoch: 12.7 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05043953785307933		[learning rate: 0.00030848]
	Learning Rate: 0.000308483
	LOSS [training: 0.05043953785307933 | validation: 0.2224970733226063]
	TIME [epoch: 12.7 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05587789980156301		[learning rate: 0.00030739]
	Learning Rate: 0.000307392
	LOSS [training: 0.05587789980156301 | validation: 0.18152111963969997]
	TIME [epoch: 12.7 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05174790001847594		[learning rate: 0.0003063]
	Learning Rate: 0.000306305
	LOSS [training: 0.05174790001847594 | validation: 0.22250966112712844]
	TIME [epoch: 12.7 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05408561144280252		[learning rate: 0.00030522]
	Learning Rate: 0.000305222
	LOSS [training: 0.05408561144280252 | validation: 0.19640643246409847]
	TIME [epoch: 12.7 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05250160030245076		[learning rate: 0.00030414]
	Learning Rate: 0.000304142
	LOSS [training: 0.05250160030245076 | validation: 0.18462006823675498]
	TIME [epoch: 12.7 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05384239130832435		[learning rate: 0.00030307]
	Learning Rate: 0.000303067
	LOSS [training: 0.05384239130832435 | validation: 0.2184414697258603]
	TIME [epoch: 12.7 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06107974695525755		[learning rate: 0.000302]
	Learning Rate: 0.000301995
	LOSS [training: 0.06107974695525755 | validation: 0.17770191322591233]
	TIME [epoch: 12.7 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049286081463617064		[learning rate: 0.00030093]
	Learning Rate: 0.000300927
	LOSS [training: 0.049286081463617064 | validation: 0.2126218106100579]
	TIME [epoch: 12.7 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0529884878669158		[learning rate: 0.00029986]
	Learning Rate: 0.000299863
	LOSS [training: 0.0529884878669158 | validation: 0.19050528902100788]
	TIME [epoch: 12.7 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05074782131460464		[learning rate: 0.0002988]
	Learning Rate: 0.000298803
	LOSS [training: 0.05074782131460464 | validation: 0.2048387107184201]
	TIME [epoch: 12.7 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06306784914965208		[learning rate: 0.00029775]
	Learning Rate: 0.000297746
	LOSS [training: 0.06306784914965208 | validation: 0.2127874768616506]
	TIME [epoch: 12.7 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051846307442793095		[learning rate: 0.00029669]
	Learning Rate: 0.000296693
	LOSS [training: 0.051846307442793095 | validation: 0.18486557186629726]
	TIME [epoch: 12.7 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07269617318106772		[learning rate: 0.00029564]
	Learning Rate: 0.000295644
	LOSS [training: 0.07269617318106772 | validation: 0.23524495950091354]
	TIME [epoch: 12.7 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.062130980401748144		[learning rate: 0.0002946]
	Learning Rate: 0.000294599
	LOSS [training: 0.062130980401748144 | validation: 0.19765671962346737]
	TIME [epoch: 12.7 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05608694533358602		[learning rate: 0.00029356]
	Learning Rate: 0.000293557
	LOSS [training: 0.05608694533358602 | validation: 0.1705746682603076]
	TIME [epoch: 12.7 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047806337746187595		[learning rate: 0.00029252]
	Learning Rate: 0.000292519
	LOSS [training: 0.047806337746187595 | validation: 0.203518990609869]
	TIME [epoch: 12.7 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050552145884105254		[learning rate: 0.00029148]
	Learning Rate: 0.000291484
	LOSS [training: 0.050552145884105254 | validation: 0.20312133891771028]
	TIME [epoch: 12.7 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048120464037163684		[learning rate: 0.00029045]
	Learning Rate: 0.000290454
	LOSS [training: 0.048120464037163684 | validation: 0.18120047373532244]
	TIME [epoch: 12.7 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04988397512229467		[learning rate: 0.00028943]
	Learning Rate: 0.000289427
	LOSS [training: 0.04988397512229467 | validation: 0.18318843430229576]
	TIME [epoch: 12.7 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049223801980594645		[learning rate: 0.0002884]
	Learning Rate: 0.000288403
	LOSS [training: 0.049223801980594645 | validation: 0.21848838675392487]
	TIME [epoch: 12.7 sec]
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05133313231038469		[learning rate: 0.00028738]
	Learning Rate: 0.000287383
	LOSS [training: 0.05133313231038469 | validation: 0.17540341246871355]
	TIME [epoch: 12.7 sec]
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05114932787632431		[learning rate: 0.00028637]
	Learning Rate: 0.000286367
	LOSS [training: 0.05114932787632431 | validation: 0.1863235342711531]
	TIME [epoch: 12.7 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0468922859859399		[learning rate: 0.00028535]
	Learning Rate: 0.000285354
	LOSS [training: 0.0468922859859399 | validation: 0.17639114169769415]
	TIME [epoch: 12.7 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047656288547558906		[learning rate: 0.00028435]
	Learning Rate: 0.000284345
	LOSS [training: 0.047656288547558906 | validation: 0.181184695880533]
	TIME [epoch: 12.7 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04951876339921224		[learning rate: 0.00028334]
	Learning Rate: 0.00028334
	LOSS [training: 0.04951876339921224 | validation: 0.23923821955869318]
	TIME [epoch: 12.7 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053178301182975674		[learning rate: 0.00028234]
	Learning Rate: 0.000282338
	LOSS [training: 0.053178301182975674 | validation: 0.167869662929954]
	TIME [epoch: 12.7 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0536222026260405		[learning rate: 0.00028134]
	Learning Rate: 0.00028134
	LOSS [training: 0.0536222026260405 | validation: 0.19372510707740698]
	TIME [epoch: 12.7 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051090054324685455		[learning rate: 0.00028034]
	Learning Rate: 0.000280345
	LOSS [training: 0.051090054324685455 | validation: 0.19539901442184282]
	TIME [epoch: 12.7 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04807689811371219		[learning rate: 0.00027935]
	Learning Rate: 0.000279353
	LOSS [training: 0.04807689811371219 | validation: 0.17840223181298465]
	TIME [epoch: 12.7 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046355282505155665		[learning rate: 0.00027837]
	Learning Rate: 0.000278366
	LOSS [training: 0.046355282505155665 | validation: 0.1866308652892285]
	TIME [epoch: 12.7 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04770521784407289		[learning rate: 0.00027738]
	Learning Rate: 0.000277381
	LOSS [training: 0.04770521784407289 | validation: 0.17704846682867142]
	TIME [epoch: 12.7 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047548376093004684		[learning rate: 0.0002764]
	Learning Rate: 0.0002764
	LOSS [training: 0.047548376093004684 | validation: 0.2059268732201235]
	TIME [epoch: 12.7 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049362706314371285		[learning rate: 0.00027542]
	Learning Rate: 0.000275423
	LOSS [training: 0.049362706314371285 | validation: 0.23447252604291063]
	TIME [epoch: 12.7 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05600397180154131		[learning rate: 0.00027445]
	Learning Rate: 0.000274449
	LOSS [training: 0.05600397180154131 | validation: 0.18624325553000728]
	TIME [epoch: 12.7 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0494025987071865		[learning rate: 0.00027348]
	Learning Rate: 0.000273479
	LOSS [training: 0.0494025987071865 | validation: 0.20755489347467365]
	TIME [epoch: 12.7 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049502916274243386		[learning rate: 0.00027251]
	Learning Rate: 0.000272511
	LOSS [training: 0.049502916274243386 | validation: 0.19132379484185816]
	TIME [epoch: 12.7 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0475334185973102		[learning rate: 0.00027155]
	Learning Rate: 0.000271548
	LOSS [training: 0.0475334185973102 | validation: 0.1783690053467265]
	TIME [epoch: 12.7 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0483163338496582		[learning rate: 0.00027059]
	Learning Rate: 0.000270588
	LOSS [training: 0.0483163338496582 | validation: 0.17792263161279553]
	TIME [epoch: 12.7 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049019075923856466		[learning rate: 0.00026963]
	Learning Rate: 0.000269631
	LOSS [training: 0.049019075923856466 | validation: 0.18288890807098707]
	TIME [epoch: 12.7 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046723388613138093		[learning rate: 0.00026868]
	Learning Rate: 0.000268677
	LOSS [training: 0.046723388613138093 | validation: 0.1849728039990386]
	TIME [epoch: 12.7 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141263010418125		[learning rate: 0.00026773]
	Learning Rate: 0.000267727
	LOSS [training: 0.05141263010418125 | validation: 0.2406038624533796]
	TIME [epoch: 12.7 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05611217027101972		[learning rate: 0.00026678]
	Learning Rate: 0.00026678
	LOSS [training: 0.05611217027101972 | validation: 0.17456115704970432]
	TIME [epoch: 12.7 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049806604494757545		[learning rate: 0.00026584]
	Learning Rate: 0.000265837
	LOSS [training: 0.049806604494757545 | validation: 0.1955796004386643]
	TIME [epoch: 12.7 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048208607845547764		[learning rate: 0.0002649]
	Learning Rate: 0.000264897
	LOSS [training: 0.048208607845547764 | validation: 0.17382554797832797]
	TIME [epoch: 12.7 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04844963979276676		[learning rate: 0.00026396]
	Learning Rate: 0.00026396
	LOSS [training: 0.04844963979276676 | validation: 0.18635128444631144]
	TIME [epoch: 12.7 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049065115791624654		[learning rate: 0.00026303]
	Learning Rate: 0.000263027
	LOSS [training: 0.049065115791624654 | validation: 0.20817177436660173]
	TIME [epoch: 12.7 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04948857807919319		[learning rate: 0.0002621]
	Learning Rate: 0.000262097
	LOSS [training: 0.04948857807919319 | validation: 0.16839546415217233]
	TIME [epoch: 12.7 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04930426570986958		[learning rate: 0.00026117]
	Learning Rate: 0.00026117
	LOSS [training: 0.04930426570986958 | validation: 0.2058773256406871]
	TIME [epoch: 12.7 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05273748305459863		[learning rate: 0.00026025]
	Learning Rate: 0.000260246
	LOSS [training: 0.05273748305459863 | validation: 0.18622054283652717]
	TIME [epoch: 12.7 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05155768430144246		[learning rate: 0.00025933]
	Learning Rate: 0.000259326
	LOSS [training: 0.05155768430144246 | validation: 0.23865342071107853]
	TIME [epoch: 12.7 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.060672366161101964		[learning rate: 0.00025841]
	Learning Rate: 0.000258409
	LOSS [training: 0.060672366161101964 | validation: 0.18090157493758463]
	TIME [epoch: 12.7 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05092878454393429		[learning rate: 0.0002575]
	Learning Rate: 0.000257495
	LOSS [training: 0.05092878454393429 | validation: 0.16218132708104466]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1084.pth
	Model improved!!!
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05363414201097827		[learning rate: 0.00025658]
	Learning Rate: 0.000256585
	LOSS [training: 0.05363414201097827 | validation: 0.2323951054983261]
	TIME [epoch: 12.7 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05937097520439858		[learning rate: 0.00025568]
	Learning Rate: 0.000255677
	LOSS [training: 0.05937097520439858 | validation: 0.18090024871992805]
	TIME [epoch: 12.7 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04964115461151526		[learning rate: 0.00025477]
	Learning Rate: 0.000254773
	LOSS [training: 0.04964115461151526 | validation: 0.16206061440006556]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1087.pth
	Model improved!!!
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05411445589981424		[learning rate: 0.00025387]
	Learning Rate: 0.000253872
	LOSS [training: 0.05411445589981424 | validation: 0.19120172230547441]
	TIME [epoch: 12.7 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046523895640143906		[learning rate: 0.00025297]
	Learning Rate: 0.000252975
	LOSS [training: 0.046523895640143906 | validation: 0.19279669476471506]
	TIME [epoch: 12.7 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04834712077878009		[learning rate: 0.00025208]
	Learning Rate: 0.00025208
	LOSS [training: 0.04834712077878009 | validation: 0.20041223990253831]
	TIME [epoch: 12.7 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04990568115978338		[learning rate: 0.00025119]
	Learning Rate: 0.000251189
	LOSS [training: 0.04990568115978338 | validation: 0.1577816811478474]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1091.pth
	Model improved!!!
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05246803086995062		[learning rate: 0.0002503]
	Learning Rate: 0.0002503
	LOSS [training: 0.05246803086995062 | validation: 0.20258916739310348]
	TIME [epoch: 12.7 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050389632584225796		[learning rate: 0.00024942]
	Learning Rate: 0.000249415
	LOSS [training: 0.050389632584225796 | validation: 0.17700310685024576]
	TIME [epoch: 12.7 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050625094283849224		[learning rate: 0.00024853]
	Learning Rate: 0.000248533
	LOSS [training: 0.050625094283849224 | validation: 0.1886806751676602]
	TIME [epoch: 12.7 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044215488743684674		[learning rate: 0.00024765]
	Learning Rate: 0.000247655
	LOSS [training: 0.044215488743684674 | validation: 0.1809242869119907]
	TIME [epoch: 12.7 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04882355445632799		[learning rate: 0.00024678]
	Learning Rate: 0.000246779
	LOSS [training: 0.04882355445632799 | validation: 0.1969138716288159]
	TIME [epoch: 12.7 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0444857958621304		[learning rate: 0.00024591]
	Learning Rate: 0.000245906
	LOSS [training: 0.0444857958621304 | validation: 0.183267203517266]
	TIME [epoch: 12.7 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048297584737608194		[learning rate: 0.00024504]
	Learning Rate: 0.000245037
	LOSS [training: 0.048297584737608194 | validation: 0.18617637143900187]
	TIME [epoch: 12.7 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046212711943778795		[learning rate: 0.00024417]
	Learning Rate: 0.00024417
	LOSS [training: 0.046212711943778795 | validation: 0.19354714956037014]
	TIME [epoch: 12.7 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05022635651024153		[learning rate: 0.00024331]
	Learning Rate: 0.000243307
	LOSS [training: 0.05022635651024153 | validation: 0.1615550083482561]
	TIME [epoch: 12.7 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04913313811536412		[learning rate: 0.00024245]
	Learning Rate: 0.000242446
	LOSS [training: 0.04913313811536412 | validation: 0.21621976289604056]
	TIME [epoch: 12.7 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05141511270305259		[learning rate: 0.00024159]
	Learning Rate: 0.000241589
	LOSS [training: 0.05141511270305259 | validation: 0.1817092366826868]
	TIME [epoch: 12.7 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049356841570426975		[learning rate: 0.00024073]
	Learning Rate: 0.000240735
	LOSS [training: 0.049356841570426975 | validation: 0.16582740963843487]
	TIME [epoch: 12.7 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04541342298343674		[learning rate: 0.00023988]
	Learning Rate: 0.000239883
	LOSS [training: 0.04541342298343674 | validation: 0.20613273778053648]
	TIME [epoch: 12.7 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04897386902014021		[learning rate: 0.00023904]
	Learning Rate: 0.000239035
	LOSS [training: 0.04897386902014021 | validation: 0.18794580512663317]
	TIME [epoch: 12.7 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04814466670771438		[learning rate: 0.00023819]
	Learning Rate: 0.00023819
	LOSS [training: 0.04814466670771438 | validation: 0.17464693126752223]
	TIME [epoch: 12.7 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046461329311643096		[learning rate: 0.00023735]
	Learning Rate: 0.000237348
	LOSS [training: 0.046461329311643096 | validation: 0.19109335329560628]
	TIME [epoch: 12.7 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052892417794038166		[learning rate: 0.00023651]
	Learning Rate: 0.000236508
	LOSS [training: 0.052892417794038166 | validation: 0.21585215212374997]
	TIME [epoch: 12.7 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04810679459724389		[learning rate: 0.00023567]
	Learning Rate: 0.000235672
	LOSS [training: 0.04810679459724389 | validation: 0.17439972719678992]
	TIME [epoch: 12.7 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05144584569789655		[learning rate: 0.00023484]
	Learning Rate: 0.000234838
	LOSS [training: 0.05144584569789655 | validation: 0.17133071105339837]
	TIME [epoch: 12.7 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04806131216541175		[learning rate: 0.00023401]
	Learning Rate: 0.000234008
	LOSS [training: 0.04806131216541175 | validation: 0.1942884480011011]
	TIME [epoch: 12.7 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048519960537551796		[learning rate: 0.00023318]
	Learning Rate: 0.000233181
	LOSS [training: 0.048519960537551796 | validation: 0.16622365660438063]
	TIME [epoch: 12.7 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05117945349906636		[learning rate: 0.00023236]
	Learning Rate: 0.000232356
	LOSS [training: 0.05117945349906636 | validation: 0.25059527101817797]
	TIME [epoch: 12.7 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05744368440236241		[learning rate: 0.00023153]
	Learning Rate: 0.000231534
	LOSS [training: 0.05744368440236241 | validation: 0.19572923547678506]
	TIME [epoch: 12.7 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04616350101050958		[learning rate: 0.00023072]
	Learning Rate: 0.000230716
	LOSS [training: 0.04616350101050958 | validation: 0.15788242798921948]
	TIME [epoch: 12.7 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05620140023973642		[learning rate: 0.0002299]
	Learning Rate: 0.0002299
	LOSS [training: 0.05620140023973642 | validation: 0.20407356043506955]
	TIME [epoch: 12.7 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05207922351550695		[learning rate: 0.00022909]
	Learning Rate: 0.000229087
	LOSS [training: 0.05207922351550695 | validation: 0.194343019519409]
	TIME [epoch: 12.7 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04601140466161452		[learning rate: 0.00022828]
	Learning Rate: 0.000228277
	LOSS [training: 0.04601140466161452 | validation: 0.18685391766924422]
	TIME [epoch: 12.7 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047451719956547685		[learning rate: 0.00022747]
	Learning Rate: 0.000227469
	LOSS [training: 0.047451719956547685 | validation: 0.18739497772991023]
	TIME [epoch: 12.7 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048713868139794024		[learning rate: 0.00022667]
	Learning Rate: 0.000226665
	LOSS [training: 0.048713868139794024 | validation: 0.1771548493230955]
	TIME [epoch: 12.7 sec]
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05011939000368819		[learning rate: 0.00022586]
	Learning Rate: 0.000225864
	LOSS [training: 0.05011939000368819 | validation: 0.16558740966812163]
	TIME [epoch: 12.7 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06307544678396795		[learning rate: 0.00022506]
	Learning Rate: 0.000225065
	LOSS [training: 0.06307544678396795 | validation: 0.20852795772104538]
	TIME [epoch: 12.7 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05049605216793951		[learning rate: 0.00022427]
	Learning Rate: 0.000224269
	LOSS [training: 0.05049605216793951 | validation: 0.21304897652528815]
	TIME [epoch: 12.7 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0486003496688736		[learning rate: 0.00022348]
	Learning Rate: 0.000223476
	LOSS [training: 0.0486003496688736 | validation: 0.1607102499292938]
	TIME [epoch: 12.7 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04782510314561205		[learning rate: 0.00022269]
	Learning Rate: 0.000222686
	LOSS [training: 0.04782510314561205 | validation: 0.17835566412631687]
	TIME [epoch: 12.7 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04533659129680924		[learning rate: 0.0002219]
	Learning Rate: 0.000221898
	LOSS [training: 0.04533659129680924 | validation: 0.22182382605081888]
	TIME [epoch: 12.7 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05326520176363642		[learning rate: 0.00022111]
	Learning Rate: 0.000221114
	LOSS [training: 0.05326520176363642 | validation: 0.18005365162608036]
	TIME [epoch: 12.7 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04449592806923466		[learning rate: 0.00022033]
	Learning Rate: 0.000220332
	LOSS [training: 0.04449592806923466 | validation: 0.17093464721742777]
	TIME [epoch: 12.7 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04623177503150261		[learning rate: 0.00021955]
	Learning Rate: 0.000219553
	LOSS [training: 0.04623177503150261 | validation: 0.18969940588729273]
	TIME [epoch: 12.7 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047966826781574295		[learning rate: 0.00021878]
	Learning Rate: 0.000218776
	LOSS [training: 0.047966826781574295 | validation: 0.1793458744889356]
	TIME [epoch: 12.7 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04533232156218693		[learning rate: 0.000218]
	Learning Rate: 0.000218003
	LOSS [training: 0.04533232156218693 | validation: 0.1843877319250942]
	TIME [epoch: 12.7 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04519515417092		[learning rate: 0.00021723]
	Learning Rate: 0.000217232
	LOSS [training: 0.04519515417092 | validation: 0.1656609613046166]
	TIME [epoch: 12.7 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047487579511457545		[learning rate: 0.00021646]
	Learning Rate: 0.000216463
	LOSS [training: 0.047487579511457545 | validation: 0.20594086632561692]
	TIME [epoch: 12.7 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04795323624291287		[learning rate: 0.0002157]
	Learning Rate: 0.000215698
	LOSS [training: 0.04795323624291287 | validation: 0.17715125533626241]
	TIME [epoch: 12.7 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0456333547323643		[learning rate: 0.00021494]
	Learning Rate: 0.000214935
	LOSS [training: 0.0456333547323643 | validation: 0.16846424690895404]
	TIME [epoch: 12.7 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04635050754513743		[learning rate: 0.00021418]
	Learning Rate: 0.000214175
	LOSS [training: 0.04635050754513743 | validation: 0.1940138465611303]
	TIME [epoch: 12.7 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04494673730433344		[learning rate: 0.00021342]
	Learning Rate: 0.000213418
	LOSS [training: 0.04494673730433344 | validation: 0.2035848808183162]
	TIME [epoch: 12.7 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050151219909045794		[learning rate: 0.00021266]
	Learning Rate: 0.000212663
	LOSS [training: 0.050151219909045794 | validation: 0.16138946730600387]
	TIME [epoch: 12.7 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05359188600170595		[learning rate: 0.00021191]
	Learning Rate: 0.000211911
	LOSS [training: 0.05359188600170595 | validation: 0.1845447923519965]
	TIME [epoch: 12.7 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045213663794195585		[learning rate: 0.00021116]
	Learning Rate: 0.000211162
	LOSS [training: 0.045213663794195585 | validation: 0.20969619248300087]
	TIME [epoch: 12.7 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04560793466898108		[learning rate: 0.00021042]
	Learning Rate: 0.000210415
	LOSS [training: 0.04560793466898108 | validation: 0.15005504306180317]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1141.pth
	Model improved!!!
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05128040839656899		[learning rate: 0.00020967]
	Learning Rate: 0.000209671
	LOSS [training: 0.05128040839656899 | validation: 0.17009980756030668]
	TIME [epoch: 12.7 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043617483419461145		[learning rate: 0.00020893]
	Learning Rate: 0.00020893
	LOSS [training: 0.043617483419461145 | validation: 0.18322550244174565]
	TIME [epoch: 12.7 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048855821782839204		[learning rate: 0.00020819]
	Learning Rate: 0.000208191
	LOSS [training: 0.048855821782839204 | validation: 0.16897807625648703]
	TIME [epoch: 12.7 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044226977485487866		[learning rate: 0.00020745]
	Learning Rate: 0.000207455
	LOSS [training: 0.044226977485487866 | validation: 0.1709324765840238]
	TIME [epoch: 12.7 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042563783793820045		[learning rate: 0.00020672]
	Learning Rate: 0.000206721
	LOSS [training: 0.042563783793820045 | validation: 0.17771847083118752]
	TIME [epoch: 12.7 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04237954852747446		[learning rate: 0.00020599]
	Learning Rate: 0.00020599
	LOSS [training: 0.04237954852747446 | validation: 0.17873069200299652]
	TIME [epoch: 12.7 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04783470734209901		[learning rate: 0.00020526]
	Learning Rate: 0.000205262
	LOSS [training: 0.04783470734209901 | validation: 0.16934958784145412]
	TIME [epoch: 12.7 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041866336629929606		[learning rate: 0.00020454]
	Learning Rate: 0.000204536
	LOSS [training: 0.041866336629929606 | validation: 0.18193897549333346]
	TIME [epoch: 12.7 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04416580696189657		[learning rate: 0.00020381]
	Learning Rate: 0.000203812
	LOSS [training: 0.04416580696189657 | validation: 0.2108251415708633]
	TIME [epoch: 12.7 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05108551023492368		[learning rate: 0.00020309]
	Learning Rate: 0.000203092
	LOSS [training: 0.05108551023492368 | validation: 0.16026767733192987]
	TIME [epoch: 12.7 sec]
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04709754220403757		[learning rate: 0.00020237]
	Learning Rate: 0.000202374
	LOSS [training: 0.04709754220403757 | validation: 0.16958830005412864]
	TIME [epoch: 12.7 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04708056006425629		[learning rate: 0.00020166]
	Learning Rate: 0.000201658
	LOSS [training: 0.04708056006425629 | validation: 0.16618853360718028]
	TIME [epoch: 12.7 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042661910397999436		[learning rate: 0.00020094]
	Learning Rate: 0.000200945
	LOSS [training: 0.042661910397999436 | validation: 0.18601998487664778]
	TIME [epoch: 12.7 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04666509405289088		[learning rate: 0.00020023]
	Learning Rate: 0.000200234
	LOSS [training: 0.04666509405289088 | validation: 0.16905478764878037]
	TIME [epoch: 12.7 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048771065173299814		[learning rate: 0.00019953]
	Learning Rate: 0.000199526
	LOSS [training: 0.048771065173299814 | validation: 0.19659833214420386]
	TIME [epoch: 12.7 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046886291275444594		[learning rate: 0.00019882]
	Learning Rate: 0.000198821
	LOSS [training: 0.046886291275444594 | validation: 0.17428467740897166]
	TIME [epoch: 12.7 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04485172087339942		[learning rate: 0.00019812]
	Learning Rate: 0.000198118
	LOSS [training: 0.04485172087339942 | validation: 0.16195560298030803]
	TIME [epoch: 12.7 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04757036454964087		[learning rate: 0.00019742]
	Learning Rate: 0.000197417
	LOSS [training: 0.04757036454964087 | validation: 0.17974793672089956]
	TIME [epoch: 12.7 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0468409891299741		[learning rate: 0.00019672]
	Learning Rate: 0.000196719
	LOSS [training: 0.0468409891299741 | validation: 0.19005281597402449]
	TIME [epoch: 12.7 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046915564379870885		[learning rate: 0.00019602]
	Learning Rate: 0.000196023
	LOSS [training: 0.046915564379870885 | validation: 0.1787355015419559]
	TIME [epoch: 12.7 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04635984325082362		[learning rate: 0.00019533]
	Learning Rate: 0.00019533
	LOSS [training: 0.04635984325082362 | validation: 0.17475681952988031]
	TIME [epoch: 12.7 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04305523881737363		[learning rate: 0.00019464]
	Learning Rate: 0.000194639
	LOSS [training: 0.04305523881737363 | validation: 0.17264420271010833]
	TIME [epoch: 12.7 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04622562276084524		[learning rate: 0.00019395]
	Learning Rate: 0.000193951
	LOSS [training: 0.04622562276084524 | validation: 0.1926413281934433]
	TIME [epoch: 12.7 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04788577184947676		[learning rate: 0.00019327]
	Learning Rate: 0.000193265
	LOSS [training: 0.04788577184947676 | validation: 0.16438871481685227]
	TIME [epoch: 12.7 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05586671820618735		[learning rate: 0.00019258]
	Learning Rate: 0.000192582
	LOSS [training: 0.05586671820618735 | validation: 0.18071699742518588]
	TIME [epoch: 12.7 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.044241905036986634		[learning rate: 0.0001919]
	Learning Rate: 0.000191901
	LOSS [training: 0.044241905036986634 | validation: 0.1558834618574697]
	TIME [epoch: 12.7 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04855691868781019		[learning rate: 0.00019122]
	Learning Rate: 0.000191222
	LOSS [training: 0.04855691868781019 | validation: 0.17872343323983594]
	TIME [epoch: 12.7 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04432698425303789		[learning rate: 0.00019055]
	Learning Rate: 0.000190546
	LOSS [training: 0.04432698425303789 | validation: 0.15969785049546636]
	TIME [epoch: 12.7 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0453991525693195		[learning rate: 0.00018987]
	Learning Rate: 0.000189872
	LOSS [training: 0.0453991525693195 | validation: 0.1548301314608297]
	TIME [epoch: 12.7 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0447716150175561		[learning rate: 0.0001892]
	Learning Rate: 0.000189201
	LOSS [training: 0.0447716150175561 | validation: 0.19081257358028136]
	TIME [epoch: 12.7 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043396172049868814		[learning rate: 0.00018853]
	Learning Rate: 0.000188532
	LOSS [training: 0.043396172049868814 | validation: 0.17302216562169961]
	TIME [epoch: 12.7 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04640317355460221		[learning rate: 0.00018787]
	Learning Rate: 0.000187865
	LOSS [training: 0.04640317355460221 | validation: 0.16685131400783088]
	TIME [epoch: 12.7 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04710174694201273		[learning rate: 0.0001872]
	Learning Rate: 0.000187201
	LOSS [training: 0.04710174694201273 | validation: 0.21454518288872892]
	TIME [epoch: 12.7 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04424598634043374		[learning rate: 0.00018654]
	Learning Rate: 0.000186539
	LOSS [training: 0.04424598634043374 | validation: 0.1682836805365489]
	TIME [epoch: 12.7 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04381562846062202		[learning rate: 0.00018588]
	Learning Rate: 0.000185879
	LOSS [training: 0.04381562846062202 | validation: 0.1614196945388847]
	TIME [epoch: 12.7 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04729563225850982		[learning rate: 0.00018522]
	Learning Rate: 0.000185222
	LOSS [training: 0.04729563225850982 | validation: 0.1964475187262931]
	TIME [epoch: 12.7 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05241790618407818		[learning rate: 0.00018457]
	Learning Rate: 0.000184567
	LOSS [training: 0.05241790618407818 | validation: 0.16927030376334803]
	TIME [epoch: 12.7 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041917761173293495		[learning rate: 0.00018391]
	Learning Rate: 0.000183914
	LOSS [training: 0.041917761173293495 | validation: 0.17265295035505618]
	TIME [epoch: 12.7 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04149205200488327		[learning rate: 0.00018326]
	Learning Rate: 0.000183264
	LOSS [training: 0.04149205200488327 | validation: 0.16600251891772222]
	TIME [epoch: 12.7 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04587044432203177		[learning rate: 0.00018262]
	Learning Rate: 0.000182616
	LOSS [training: 0.04587044432203177 | validation: 0.21395514472572402]
	TIME [epoch: 12.7 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04704454352424563		[learning rate: 0.00018197]
	Learning Rate: 0.00018197
	LOSS [training: 0.04704454352424563 | validation: 0.18333887920484165]
	TIME [epoch: 12.7 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04587931665155388		[learning rate: 0.00018133]
	Learning Rate: 0.000181327
	LOSS [training: 0.04587931665155388 | validation: 0.16107186114152322]
	TIME [epoch: 12.7 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04589715046784077		[learning rate: 0.00018069]
	Learning Rate: 0.000180685
	LOSS [training: 0.04589715046784077 | validation: 0.17850143728457238]
	TIME [epoch: 12.7 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04446163707194101		[learning rate: 0.00018005]
	Learning Rate: 0.000180046
	LOSS [training: 0.04446163707194101 | validation: 0.2247488039168192]
	TIME [epoch: 12.7 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05283014996412685		[learning rate: 0.00017941]
	Learning Rate: 0.00017941
	LOSS [training: 0.05283014996412685 | validation: 0.1743059840418266]
	TIME [epoch: 12.7 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04264297279608689		[learning rate: 0.00017878]
	Learning Rate: 0.000178775
	LOSS [training: 0.04264297279608689 | validation: 0.16144239910727862]
	TIME [epoch: 12.7 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04819130016261492		[learning rate: 0.00017814]
	Learning Rate: 0.000178143
	LOSS [training: 0.04819130016261492 | validation: 0.18657208128013392]
	TIME [epoch: 12.7 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04481933326435411		[learning rate: 0.00017751]
	Learning Rate: 0.000177513
	LOSS [training: 0.04481933326435411 | validation: 0.1709678251493839]
	TIME [epoch: 12.7 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041928588328044844		[learning rate: 0.00017689]
	Learning Rate: 0.000176886
	LOSS [training: 0.041928588328044844 | validation: 0.16925520272423222]
	TIME [epoch: 12.7 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042331420111365176		[learning rate: 0.00017626]
	Learning Rate: 0.00017626
	LOSS [training: 0.042331420111365176 | validation: 0.16413341632267817]
	TIME [epoch: 12.7 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046847276488105834		[learning rate: 0.00017564]
	Learning Rate: 0.000175637
	LOSS [training: 0.046847276488105834 | validation: 0.17031362386064833]
	TIME [epoch: 12.7 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043573769096002246		[learning rate: 0.00017502]
	Learning Rate: 0.000175016
	LOSS [training: 0.043573769096002246 | validation: 0.17688929853896793]
	TIME [epoch: 12.7 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04981881936264377		[learning rate: 0.0001744]
	Learning Rate: 0.000174397
	LOSS [training: 0.04981881936264377 | validation: 0.19748338336313798]
	TIME [epoch: 12.7 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04450401597850253		[learning rate: 0.00017378]
	Learning Rate: 0.00017378
	LOSS [training: 0.04450401597850253 | validation: 0.1809528324799136]
	TIME [epoch: 12.7 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04265711764246548		[learning rate: 0.00017317]
	Learning Rate: 0.000173166
	LOSS [training: 0.04265711764246548 | validation: 0.1637845428576332]
	TIME [epoch: 12.7 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045356987737349266		[learning rate: 0.00017255]
	Learning Rate: 0.000172553
	LOSS [training: 0.045356987737349266 | validation: 0.1800589833969454]
	TIME [epoch: 12.7 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04426289441452078		[learning rate: 0.00017194]
	Learning Rate: 0.000171943
	LOSS [training: 0.04426289441452078 | validation: 0.20150075942587825]
	TIME [epoch: 12.7 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04530222267426098		[learning rate: 0.00017134]
	Learning Rate: 0.000171335
	LOSS [training: 0.04530222267426098 | validation: 0.14979804756048845]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1199.pth
	Model improved!!!
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579719358695673		[learning rate: 0.00017073]
	Learning Rate: 0.000170729
	LOSS [training: 0.04579719358695673 | validation: 0.15639752310092467]
	TIME [epoch: 12.7 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04311294991004178		[learning rate: 0.00017013]
	Learning Rate: 0.000170125
	LOSS [training: 0.04311294991004178 | validation: 0.21469486515738995]
	TIME [epoch: 12.7 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0489660527318142		[learning rate: 0.00016952]
	Learning Rate: 0.000169524
	LOSS [training: 0.0489660527318142 | validation: 0.18859984467457933]
	TIME [epoch: 12.7 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04598135035771069		[learning rate: 0.00016892]
	Learning Rate: 0.000168924
	LOSS [training: 0.04598135035771069 | validation: 0.15381099518096383]
	TIME [epoch: 12.7 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04905303927431438		[learning rate: 0.00016833]
	Learning Rate: 0.000168327
	LOSS [training: 0.04905303927431438 | validation: 0.16796645394803078]
	TIME [epoch: 12.7 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04263790261926762		[learning rate: 0.00016773]
	Learning Rate: 0.000167732
	LOSS [training: 0.04263790261926762 | validation: 0.20339687937209672]
	TIME [epoch: 12.7 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04335340346071523		[learning rate: 0.00016714]
	Learning Rate: 0.000167139
	LOSS [training: 0.04335340346071523 | validation: 0.17597365911174234]
	TIME [epoch: 12.7 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04312510355095412		[learning rate: 0.00016655]
	Learning Rate: 0.000166548
	LOSS [training: 0.04312510355095412 | validation: 0.17244475906383175]
	TIME [epoch: 12.7 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0451772250514928		[learning rate: 0.00016596]
	Learning Rate: 0.000165959
	LOSS [training: 0.0451772250514928 | validation: 0.1824157296630404]
	TIME [epoch: 12.7 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04444441472144637		[learning rate: 0.00016537]
	Learning Rate: 0.000165372
	LOSS [training: 0.04444441472144637 | validation: 0.17996107521294036]
	TIME [epoch: 12.7 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042521447781564914		[learning rate: 0.00016479]
	Learning Rate: 0.000164787
	LOSS [training: 0.042521447781564914 | validation: 0.15878679880515165]
	TIME [epoch: 12.7 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04412997296794992		[learning rate: 0.0001642]
	Learning Rate: 0.000164204
	LOSS [training: 0.04412997296794992 | validation: 0.15391708832097473]
	TIME [epoch: 12.7 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0420580125565121		[learning rate: 0.00016362]
	Learning Rate: 0.000163624
	LOSS [training: 0.0420580125565121 | validation: 0.1597385772352172]
	TIME [epoch: 12.7 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047792636155335626		[learning rate: 0.00016305]
	Learning Rate: 0.000163045
	LOSS [training: 0.047792636155335626 | validation: 0.2018517095000167]
	TIME [epoch: 12.7 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04781752171883452		[learning rate: 0.00016247]
	Learning Rate: 0.000162469
	LOSS [training: 0.04781752171883452 | validation: 0.1711416118284464]
	TIME [epoch: 12.7 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045060536670179		[learning rate: 0.00016189]
	Learning Rate: 0.000161894
	LOSS [training: 0.045060536670179 | validation: 0.16084694142748526]
	TIME [epoch: 12.7 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.048329608440734716		[learning rate: 0.00016132]
	Learning Rate: 0.000161322
	LOSS [training: 0.048329608440734716 | validation: 0.1682451300984346]
	TIME [epoch: 12.7 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.043830081533600074		[learning rate: 0.00016075]
	Learning Rate: 0.000160751
	LOSS [training: 0.043830081533600074 | validation: 0.18018489883282207]
	TIME [epoch: 12.7 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04074246836304251		[learning rate: 0.00016018]
	Learning Rate: 0.000160183
	LOSS [training: 0.04074246836304251 | validation: 0.15468232092901274]
	TIME [epoch: 12.7 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04338404301055071		[learning rate: 0.00015962]
	Learning Rate: 0.000159616
	LOSS [training: 0.04338404301055071 | validation: 0.1653828716110806]
	TIME [epoch: 12.7 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04140255267219216		[learning rate: 0.00015905]
	Learning Rate: 0.000159052
	LOSS [training: 0.04140255267219216 | validation: 0.18108575891867976]
	TIME [epoch: 12.7 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04319632019749312		[learning rate: 0.00015849]
	Learning Rate: 0.000158489
	LOSS [training: 0.04319632019749312 | validation: 0.16640679049036633]
	TIME [epoch: 12.7 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084044725256218		[learning rate: 0.00015793]
	Learning Rate: 0.000157929
	LOSS [training: 0.04084044725256218 | validation: 0.16663068453093088]
	TIME [epoch: 12.7 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042361737754943576		[learning rate: 0.00015737]
	Learning Rate: 0.00015737
	LOSS [training: 0.042361737754943576 | validation: 0.16882534548280898]
	TIME [epoch: 12.7 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04492631647857549		[learning rate: 0.00015681]
	Learning Rate: 0.000156814
	LOSS [training: 0.04492631647857549 | validation: 0.1601006356740151]
	TIME [epoch: 12.7 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042306915696534705		[learning rate: 0.00015626]
	Learning Rate: 0.000156259
	LOSS [training: 0.042306915696534705 | validation: 0.1993457829961236]
	TIME [epoch: 12.7 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0485043357926591		[learning rate: 0.00015571]
	Learning Rate: 0.000155707
	LOSS [training: 0.0485043357926591 | validation: 0.16707612947752973]
	TIME [epoch: 12.7 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04272964108942423		[learning rate: 0.00015516]
	Learning Rate: 0.000155156
	LOSS [training: 0.04272964108942423 | validation: 0.16321897338632352]
	TIME [epoch: 12.7 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04746548782839209		[learning rate: 0.00015461]
	Learning Rate: 0.000154608
	LOSS [training: 0.04746548782839209 | validation: 0.1824629046019279]
	TIME [epoch: 12.7 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04655242432866046		[learning rate: 0.00015406]
	Learning Rate: 0.000154061
	LOSS [training: 0.04655242432866046 | validation: 0.17725296977982166]
	TIME [epoch: 12.7 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04201708022839253		[learning rate: 0.00015352]
	Learning Rate: 0.000153516
	LOSS [training: 0.04201708022839253 | validation: 0.1542319834270199]
	TIME [epoch: 12.7 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04793312117545842		[learning rate: 0.00015297]
	Learning Rate: 0.000152973
	LOSS [training: 0.04793312117545842 | validation: 0.17751302385397835]
	TIME [epoch: 12.7 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04285564231403689		[learning rate: 0.00015243]
	Learning Rate: 0.000152432
	LOSS [training: 0.04285564231403689 | validation: 0.17391611568870338]
	TIME [epoch: 12.7 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04016848120884103		[learning rate: 0.00015189]
	Learning Rate: 0.000151893
	LOSS [training: 0.04016848120884103 | validation: 0.17017639532790813]
	TIME [epoch: 12.7 sec]
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0432814327406192		[learning rate: 0.00015136]
	Learning Rate: 0.000151356
	LOSS [training: 0.0432814327406192 | validation: 0.17398681134361182]
	TIME [epoch: 12.7 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041722428814458125		[learning rate: 0.00015082]
	Learning Rate: 0.000150821
	LOSS [training: 0.041722428814458125 | validation: 0.18771459608127153]
	TIME [epoch: 12.7 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046969628619800086		[learning rate: 0.00015029]
	Learning Rate: 0.000150288
	LOSS [training: 0.046969628619800086 | validation: 0.16564365598266453]
	TIME [epoch: 12.7 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04237996611692384		[learning rate: 0.00014976]
	Learning Rate: 0.000149756
	LOSS [training: 0.04237996611692384 | validation: 0.15856029660356075]
	TIME [epoch: 12.7 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04286375050713078		[learning rate: 0.00014923]
	Learning Rate: 0.000149227
	LOSS [training: 0.04286375050713078 | validation: 0.18739321714659593]
	TIME [epoch: 12.7 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04517605831755069		[learning rate: 0.0001487]
	Learning Rate: 0.000148699
	LOSS [training: 0.04517605831755069 | validation: 0.17207060837727614]
	TIME [epoch: 12.7 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041170640367678146		[learning rate: 0.00014817]
	Learning Rate: 0.000148173
	LOSS [training: 0.041170640367678146 | validation: 0.16557570309457073]
	TIME [epoch: 12.7 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042202451943887116		[learning rate: 0.00014765]
	Learning Rate: 0.000147649
	LOSS [training: 0.042202451943887116 | validation: 0.1635259104030585]
	TIME [epoch: 12.7 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04509946446425598		[learning rate: 0.00014713]
	Learning Rate: 0.000147127
	LOSS [training: 0.04509946446425598 | validation: 0.16923948520929752]
	TIME [epoch: 12.7 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042607747257318744		[learning rate: 0.00014661]
	Learning Rate: 0.000146607
	LOSS [training: 0.042607747257318744 | validation: 0.1644047589763526]
	TIME [epoch: 12.7 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04240794860370663		[learning rate: 0.00014609]
	Learning Rate: 0.000146088
	LOSS [training: 0.04240794860370663 | validation: 0.17996653178543234]
	TIME [epoch: 12.7 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04185955931549208		[learning rate: 0.00014557]
	Learning Rate: 0.000145572
	LOSS [training: 0.04185955931549208 | validation: 0.1703301622361469]
	TIME [epoch: 12.7 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04177863642562222		[learning rate: 0.00014506]
	Learning Rate: 0.000145057
	LOSS [training: 0.04177863642562222 | validation: 0.15018624122937396]
	TIME [epoch: 12.7 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04231357543696914		[learning rate: 0.00014454]
	Learning Rate: 0.000144544
	LOSS [training: 0.04231357543696914 | validation: 0.17914267992851984]
	TIME [epoch: 12.7 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04251286884456385		[learning rate: 0.00014403]
	Learning Rate: 0.000144033
	LOSS [training: 0.04251286884456385 | validation: 0.17474253160764133]
	TIME [epoch: 12.7 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042284768984895874		[learning rate: 0.00014352]
	Learning Rate: 0.000143524
	LOSS [training: 0.042284768984895874 | validation: 0.16703365121262703]
	TIME [epoch: 12.7 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045064364227484346		[learning rate: 0.00014302]
	Learning Rate: 0.000143016
	LOSS [training: 0.045064364227484346 | validation: 0.15806256171306454]
	TIME [epoch: 12.7 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042491244472677034		[learning rate: 0.00014251]
	Learning Rate: 0.00014251
	LOSS [training: 0.042491244472677034 | validation: 0.18727294591477117]
	TIME [epoch: 12.7 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04379443821550089		[learning rate: 0.00014201]
	Learning Rate: 0.000142006
	LOSS [training: 0.04379443821550089 | validation: 0.16905996986583893]
	TIME [epoch: 12.7 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04422158868322077		[learning rate: 0.0001415]
	Learning Rate: 0.000141504
	LOSS [training: 0.04422158868322077 | validation: 0.15708880394881744]
	TIME [epoch: 12.7 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04242180841137427		[learning rate: 0.000141]
	Learning Rate: 0.000141004
	LOSS [training: 0.04242180841137427 | validation: 0.151744740792446]
	TIME [epoch: 12.7 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04267884490976995		[learning rate: 0.00014051]
	Learning Rate: 0.000140505
	LOSS [training: 0.04267884490976995 | validation: 0.16442441146786743]
	TIME [epoch: 12.7 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04120484726838644		[learning rate: 0.00014001]
	Learning Rate: 0.000140008
	LOSS [training: 0.04120484726838644 | validation: 0.1392782434353988]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1256.pth
	Model improved!!!
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045074254959677436		[learning rate: 0.00013951]
	Learning Rate: 0.000139513
	LOSS [training: 0.045074254959677436 | validation: 0.1673083589044672]
	TIME [epoch: 12.7 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0420500221133457		[learning rate: 0.00013902]
	Learning Rate: 0.00013902
	LOSS [training: 0.0420500221133457 | validation: 0.1658339247488376]
	TIME [epoch: 12.7 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04092332233944033		[learning rate: 0.00013853]
	Learning Rate: 0.000138528
	LOSS [training: 0.04092332233944033 | validation: 0.16509105281739148]
	TIME [epoch: 12.7 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04162561109321923		[learning rate: 0.00013804]
	Learning Rate: 0.000138038
	LOSS [training: 0.04162561109321923 | validation: 0.15962422451938993]
	TIME [epoch: 12.7 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04284597094297078		[learning rate: 0.00013755]
	Learning Rate: 0.00013755
	LOSS [training: 0.04284597094297078 | validation: 0.14312424859717937]
	TIME [epoch: 12.7 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04547755337449852		[learning rate: 0.00013706]
	Learning Rate: 0.000137064
	LOSS [training: 0.04547755337449852 | validation: 0.1620453995865963]
	TIME [epoch: 12.7 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039329774597679744		[learning rate: 0.00013658]
	Learning Rate: 0.000136579
	LOSS [training: 0.039329774597679744 | validation: 0.18712704470091185]
	TIME [epoch: 12.7 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0402894119324355		[learning rate: 0.0001361]
	Learning Rate: 0.000136096
	LOSS [training: 0.0402894119324355 | validation: 0.16943400305523004]
	TIME [epoch: 12.7 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0420652588487335		[learning rate: 0.00013562]
	Learning Rate: 0.000135615
	LOSS [training: 0.0420652588487335 | validation: 0.17730843296166665]
	TIME [epoch: 12.7 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042974222643651576		[learning rate: 0.00013514]
	Learning Rate: 0.000135135
	LOSS [training: 0.042974222643651576 | validation: 0.16375373612643523]
	TIME [epoch: 12.7 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04309133497491642		[learning rate: 0.00013466]
	Learning Rate: 0.000134658
	LOSS [training: 0.04309133497491642 | validation: 0.19694531885498]
	TIME [epoch: 12.7 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04324447718332652		[learning rate: 0.00013418]
	Learning Rate: 0.000134181
	LOSS [training: 0.04324447718332652 | validation: 0.17692728724341253]
	TIME [epoch: 12.7 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04109719839721979		[learning rate: 0.00013371]
	Learning Rate: 0.000133707
	LOSS [training: 0.04109719839721979 | validation: 0.15668355617844268]
	TIME [epoch: 12.7 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04077243850922806		[learning rate: 0.00013323]
	Learning Rate: 0.000133234
	LOSS [training: 0.04077243850922806 | validation: 0.14872428821877892]
	TIME [epoch: 12.7 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041215493287324795		[learning rate: 0.00013276]
	Learning Rate: 0.000132763
	LOSS [training: 0.041215493287324795 | validation: 0.1664558501834236]
	TIME [epoch: 12.7 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04056601004290427		[learning rate: 0.00013229]
	Learning Rate: 0.000132293
	LOSS [training: 0.04056601004290427 | validation: 0.16209358886742276]
	TIME [epoch: 12.7 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04340512211794596		[learning rate: 0.00013183]
	Learning Rate: 0.000131826
	LOSS [training: 0.04340512211794596 | validation: 0.1538901734111322]
	TIME [epoch: 12.7 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04194428903078296		[learning rate: 0.00013136]
	Learning Rate: 0.00013136
	LOSS [training: 0.04194428903078296 | validation: 0.18229676141936624]
	TIME [epoch: 12.7 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0413822273241956		[learning rate: 0.0001309]
	Learning Rate: 0.000130895
	LOSS [training: 0.0413822273241956 | validation: 0.1559129102278603]
	TIME [epoch: 12.7 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042335755236823244		[learning rate: 0.00013043]
	Learning Rate: 0.000130432
	LOSS [training: 0.042335755236823244 | validation: 0.17711839959674314]
	TIME [epoch: 12.7 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03951001283209804		[learning rate: 0.00012997]
	Learning Rate: 0.000129971
	LOSS [training: 0.03951001283209804 | validation: 0.1525991213628892]
	TIME [epoch: 12.7 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04388725271349373		[learning rate: 0.00012951]
	Learning Rate: 0.000129511
	LOSS [training: 0.04388725271349373 | validation: 0.1577361338085088]
	TIME [epoch: 12.7 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04044227082319532		[learning rate: 0.00012905]
	Learning Rate: 0.000129053
	LOSS [training: 0.04044227082319532 | validation: 0.1638257629589667]
	TIME [epoch: 12.7 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045655320199651545		[learning rate: 0.0001286]
	Learning Rate: 0.000128597
	LOSS [training: 0.045655320199651545 | validation: 0.15081014772588724]
	TIME [epoch: 12.7 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04271506915329718		[learning rate: 0.00012814]
	Learning Rate: 0.000128142
	LOSS [training: 0.04271506915329718 | validation: 0.18577808263878526]
	TIME [epoch: 12.7 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04172838018108576		[learning rate: 0.00012769]
	Learning Rate: 0.000127689
	LOSS [training: 0.04172838018108576 | validation: 0.17460990820161817]
	TIME [epoch: 12.7 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04567147021796735		[learning rate: 0.00012724]
	Learning Rate: 0.000127238
	LOSS [training: 0.04567147021796735 | validation: 0.16781926257004978]
	TIME [epoch: 12.7 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04560965863761581		[learning rate: 0.00012679]
	Learning Rate: 0.000126788
	LOSS [training: 0.04560965863761581 | validation: 0.1424505458059636]
	TIME [epoch: 12.7 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04052655417983642		[learning rate: 0.00012634]
	Learning Rate: 0.000126339
	LOSS [training: 0.04052655417983642 | validation: 0.1838988849684758]
	TIME [epoch: 12.7 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106303435665961		[learning rate: 0.00012589]
	Learning Rate: 0.000125893
	LOSS [training: 0.04106303435665961 | validation: 0.17919948446506156]
	TIME [epoch: 12.7 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03923016528389682		[learning rate: 0.00012545]
	Learning Rate: 0.000125447
	LOSS [training: 0.03923016528389682 | validation: 0.1543539920051769]
	TIME [epoch: 12.7 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04131047130464888		[learning rate: 0.000125]
	Learning Rate: 0.000125004
	LOSS [training: 0.04131047130464888 | validation: 0.1501931774843066]
	TIME [epoch: 12.7 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040829345564979164		[learning rate: 0.00012456]
	Learning Rate: 0.000124562
	LOSS [training: 0.040829345564979164 | validation: 0.1727723100545128]
	TIME [epoch: 12.7 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04045521134664161		[learning rate: 0.00012412]
	Learning Rate: 0.000124121
	LOSS [training: 0.04045521134664161 | validation: 0.15676685748577138]
	TIME [epoch: 12.7 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04339589510309115		[learning rate: 0.00012368]
	Learning Rate: 0.000123682
	LOSS [training: 0.04339589510309115 | validation: 0.19037410252601128]
	TIME [epoch: 12.7 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040869265690507056		[learning rate: 0.00012325]
	Learning Rate: 0.000123245
	LOSS [training: 0.040869265690507056 | validation: 0.15797763531535722]
	TIME [epoch: 12.7 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860373886451366		[learning rate: 0.00012281]
	Learning Rate: 0.000122809
	LOSS [training: 0.03860373886451366 | validation: 0.15997220160125517]
	TIME [epoch: 12.7 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041909660605921654		[learning rate: 0.00012237]
	Learning Rate: 0.000122375
	LOSS [training: 0.041909660605921654 | validation: 0.17440009141828872]
	TIME [epoch: 12.7 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04109335079263071		[learning rate: 0.00012194]
	Learning Rate: 0.000121942
	LOSS [training: 0.04109335079263071 | validation: 0.16328362000470983]
	TIME [epoch: 12.7 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04463660848920152		[learning rate: 0.00012151]
	Learning Rate: 0.000121511
	LOSS [training: 0.04463660848920152 | validation: 0.1598211196590656]
	TIME [epoch: 12.7 sec]
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04209219984178145		[learning rate: 0.00012108]
	Learning Rate: 0.000121081
	LOSS [training: 0.04209219984178145 | validation: 0.18693124269779068]
	TIME [epoch: 12.7 sec]
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04346352457437672		[learning rate: 0.00012065]
	Learning Rate: 0.000120653
	LOSS [training: 0.04346352457437672 | validation: 0.1604361309593892]
	TIME [epoch: 12.7 sec]
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039335584735055044		[learning rate: 0.00012023]
	Learning Rate: 0.000120226
	LOSS [training: 0.039335584735055044 | validation: 0.14739313353197242]
	TIME [epoch: 12.7 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04215438820678684		[learning rate: 0.0001198]
	Learning Rate: 0.000119801
	LOSS [training: 0.04215438820678684 | validation: 0.1576451939199969]
	TIME [epoch: 12.7 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04014178917421989		[learning rate: 0.00011938]
	Learning Rate: 0.000119378
	LOSS [training: 0.04014178917421989 | validation: 0.16777769408068866]
	TIME [epoch: 12.7 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04106700374819471		[learning rate: 0.00011896]
	Learning Rate: 0.000118956
	LOSS [training: 0.04106700374819471 | validation: 0.15371468616161005]
	TIME [epoch: 12.7 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04225241040202149		[learning rate: 0.00011853]
	Learning Rate: 0.000118535
	LOSS [training: 0.04225241040202149 | validation: 0.15092516668538455]
	TIME [epoch: 12.7 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040798695536732835		[learning rate: 0.00011812]
	Learning Rate: 0.000118116
	LOSS [training: 0.040798695536732835 | validation: 0.15484987378476134]
	TIME [epoch: 12.7 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03920401707813424		[learning rate: 0.0001177]
	Learning Rate: 0.000117698
	LOSS [training: 0.03920401707813424 | validation: 0.13537751105861592]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1305.pth
	Model improved!!!
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03902376111203069		[learning rate: 0.00011728]
	Learning Rate: 0.000117282
	LOSS [training: 0.03902376111203069 | validation: 0.16000657387131825]
	TIME [epoch: 12.7 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03957691456684363		[learning rate: 0.00011687]
	Learning Rate: 0.000116867
	LOSS [training: 0.03957691456684363 | validation: 0.17673579718884863]
	TIME [epoch: 12.7 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04212339559749288		[learning rate: 0.00011645]
	Learning Rate: 0.000116454
	LOSS [training: 0.04212339559749288 | validation: 0.15672128879849656]
	TIME [epoch: 12.7 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.045738121455118616		[learning rate: 0.00011604]
	Learning Rate: 0.000116042
	LOSS [training: 0.045738121455118616 | validation: 0.15079289125315753]
	TIME [epoch: 12.7 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04579490519765452		[learning rate: 0.00011563]
	Learning Rate: 0.000115632
	LOSS [training: 0.04579490519765452 | validation: 0.17612154638206148]
	TIME [epoch: 12.7 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04218361469572697		[learning rate: 0.00011522]
	Learning Rate: 0.000115223
	LOSS [training: 0.04218361469572697 | validation: 0.19742118069864056]
	TIME [epoch: 12.7 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046023854209896464		[learning rate: 0.00011482]
	Learning Rate: 0.000114815
	LOSS [training: 0.046023854209896464 | validation: 0.14744676732076878]
	TIME [epoch: 12.7 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0387472829071922		[learning rate: 0.00011441]
	Learning Rate: 0.000114409
	LOSS [training: 0.0387472829071922 | validation: 0.14021477822058623]
	TIME [epoch: 12.6 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041060752192869025		[learning rate: 0.000114]
	Learning Rate: 0.000114005
	LOSS [training: 0.041060752192869025 | validation: 0.1554677253162995]
	TIME [epoch: 12.7 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04022209138764974		[learning rate: 0.0001136]
	Learning Rate: 0.000113602
	LOSS [training: 0.04022209138764974 | validation: 0.1735468338078662]
	TIME [epoch: 12.6 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04170602139302327		[learning rate: 0.0001132]
	Learning Rate: 0.0001132
	LOSS [training: 0.04170602139302327 | validation: 0.16610009989319488]
	TIME [epoch: 12.7 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04169634231675949		[learning rate: 0.0001128]
	Learning Rate: 0.0001128
	LOSS [training: 0.04169634231675949 | validation: 0.1554571101919685]
	TIME [epoch: 12.6 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041545900213798295		[learning rate: 0.0001124]
	Learning Rate: 0.000112401
	LOSS [training: 0.041545900213798295 | validation: 0.15610501175120017]
	TIME [epoch: 12.7 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038950780721999555		[learning rate: 0.000112]
	Learning Rate: 0.000112003
	LOSS [training: 0.038950780721999555 | validation: 0.1561641239070679]
	TIME [epoch: 12.7 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040007333818534045		[learning rate: 0.00011161]
	Learning Rate: 0.000111607
	LOSS [training: 0.040007333818534045 | validation: 0.19034509677435044]
	TIME [epoch: 12.7 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041111826749010476		[learning rate: 0.00011121]
	Learning Rate: 0.000111213
	LOSS [training: 0.041111826749010476 | validation: 0.15798520397797616]
	TIME [epoch: 12.7 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0415094502146668		[learning rate: 0.00011082]
	Learning Rate: 0.000110819
	LOSS [training: 0.0415094502146668 | validation: 0.15716208133882914]
	TIME [epoch: 12.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04151374259670854		[learning rate: 0.00011043]
	Learning Rate: 0.000110427
	LOSS [training: 0.04151374259670854 | validation: 0.16082024121157412]
	TIME [epoch: 12.7 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0382891937912573		[learning rate: 0.00011004]
	Learning Rate: 0.000110037
	LOSS [training: 0.0382891937912573 | validation: 0.15824979918687349]
	TIME [epoch: 12.7 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04280236667619459		[learning rate: 0.00010965]
	Learning Rate: 0.000109648
	LOSS [training: 0.04280236667619459 | validation: 0.14853034968479328]
	TIME [epoch: 12.7 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040589369055420516		[learning rate: 0.00010926]
	Learning Rate: 0.00010926
	LOSS [training: 0.040589369055420516 | validation: 0.16607319038155197]
	TIME [epoch: 12.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04009869079267337		[learning rate: 0.00010887]
	Learning Rate: 0.000108874
	LOSS [training: 0.04009869079267337 | validation: 0.1392266020972703]
	TIME [epoch: 12.7 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040097041707905866		[learning rate: 0.00010849]
	Learning Rate: 0.000108489
	LOSS [training: 0.040097041707905866 | validation: 0.14454663222098377]
	TIME [epoch: 12.7 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04084140160519916		[learning rate: 0.00010811]
	Learning Rate: 0.000108105
	LOSS [training: 0.04084140160519916 | validation: 0.17406375339748625]
	TIME [epoch: 12.7 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040115953288029624		[learning rate: 0.00010772]
	Learning Rate: 0.000107723
	LOSS [training: 0.040115953288029624 | validation: 0.1659588726384886]
	TIME [epoch: 12.7 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020832295792489		[learning rate: 0.00010734]
	Learning Rate: 0.000107342
	LOSS [training: 0.04020832295792489 | validation: 0.14903537983219853]
	TIME [epoch: 12.7 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040653552365804244		[learning rate: 0.00010696]
	Learning Rate: 0.000106962
	LOSS [training: 0.040653552365804244 | validation: 0.1472403667149134]
	TIME [epoch: 12.7 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039908325648546586		[learning rate: 0.00010658]
	Learning Rate: 0.000106584
	LOSS [training: 0.039908325648546586 | validation: 0.16999503292065682]
	TIME [epoch: 12.7 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0405027155318244		[learning rate: 0.00010621]
	Learning Rate: 0.000106207
	LOSS [training: 0.0405027155318244 | validation: 0.16461835329048524]
	TIME [epoch: 12.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054497313883961		[learning rate: 0.00010583]
	Learning Rate: 0.000105832
	LOSS [training: 0.04054497313883961 | validation: 0.1528067053696935]
	TIME [epoch: 12.7 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039507023032915		[learning rate: 0.00010546]
	Learning Rate: 0.000105457
	LOSS [training: 0.039507023032915 | validation: 0.1529585470319767]
	TIME [epoch: 12.7 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04049795438250769		[learning rate: 0.00010508]
	Learning Rate: 0.000105084
	LOSS [training: 0.04049795438250769 | validation: 0.16459060579259954]
	TIME [epoch: 12.7 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03966952580208341		[learning rate: 0.00010471]
	Learning Rate: 0.000104713
	LOSS [training: 0.03966952580208341 | validation: 0.1734681963583622]
	TIME [epoch: 12.7 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04097032334529489		[learning rate: 0.00010434]
	Learning Rate: 0.000104343
	LOSS [training: 0.04097032334529489 | validation: 0.14321708944199646]
	TIME [epoch: 12.7 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04353835577882047		[learning rate: 0.00010397]
	Learning Rate: 0.000103974
	LOSS [training: 0.04353835577882047 | validation: 0.1542887766286281]
	TIME [epoch: 12.7 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03926484105035772		[learning rate: 0.00010361]
	Learning Rate: 0.000103606
	LOSS [training: 0.03926484105035772 | validation: 0.16051589124154678]
	TIME [epoch: 12.7 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038831793379648975		[learning rate: 0.00010324]
	Learning Rate: 0.00010324
	LOSS [training: 0.038831793379648975 | validation: 0.14317308851724717]
	TIME [epoch: 12.7 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04441091975136863		[learning rate: 0.00010287]
	Learning Rate: 0.000102874
	LOSS [training: 0.04441091975136863 | validation: 0.17995660021348403]
	TIME [epoch: 12.6 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04194633177731573		[learning rate: 0.00010251]
	Learning Rate: 0.000102511
	LOSS [training: 0.04194633177731573 | validation: 0.1563643052355192]
	TIME [epoch: 12.7 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04198435790149234		[learning rate: 0.00010215]
	Learning Rate: 0.000102148
	LOSS [training: 0.04198435790149234 | validation: 0.14907394361717446]
	TIME [epoch: 12.7 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040968846160888486		[learning rate: 0.00010179]
	Learning Rate: 0.000101787
	LOSS [training: 0.040968846160888486 | validation: 0.1554803595413733]
	TIME [epoch: 12.7 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04006750074989098		[learning rate: 0.00010143]
	Learning Rate: 0.000101427
	LOSS [training: 0.04006750074989098 | validation: 0.16832784607192564]
	TIME [epoch: 12.6 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04551152121008348		[learning rate: 0.00010107]
	Learning Rate: 0.000101068
	LOSS [training: 0.04551152121008348 | validation: 0.16300762174155148]
	TIME [epoch: 12.7 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040277902420520276		[learning rate: 0.00010071]
	Learning Rate: 0.000100711
	LOSS [training: 0.040277902420520276 | validation: 0.15839278166324366]
	TIME [epoch: 12.7 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03835793428060533		[learning rate: 0.00010035]
	Learning Rate: 0.000100355
	LOSS [training: 0.03835793428060533 | validation: 0.14841624287684393]
	TIME [epoch: 12.7 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03959838087317959		[learning rate: 0.0001]
	Learning Rate: 0.0001
	LOSS [training: 0.03959838087317959 | validation: 0.16302366365146223]
	TIME [epoch: 12.7 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04088399707621887		[learning rate: 9.9646e-05]
	Learning Rate: 9.96464e-05
	LOSS [training: 0.04088399707621887 | validation: 0.17356648158372256]
	TIME [epoch: 12.7 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061129080378526		[learning rate: 9.9294e-05]
	Learning Rate: 9.9294e-05
	LOSS [training: 0.04061129080378526 | validation: 0.15333134767563605]
	TIME [epoch: 12.7 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04125244350614898		[learning rate: 9.8943e-05]
	Learning Rate: 9.89429e-05
	LOSS [training: 0.04125244350614898 | validation: 0.1626574938138986]
	TIME [epoch: 12.7 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03819719264930508		[learning rate: 9.8593e-05]
	Learning Rate: 9.8593e-05
	LOSS [training: 0.03819719264930508 | validation: 0.1472711658689534]
	TIME [epoch: 12.7 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04092099378448186		[learning rate: 9.8244e-05]
	Learning Rate: 9.82444e-05
	LOSS [training: 0.04092099378448186 | validation: 0.16739964123407544]
	TIME [epoch: 12.7 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03893423877908613		[learning rate: 9.7897e-05]
	Learning Rate: 9.7897e-05
	LOSS [training: 0.03893423877908613 | validation: 0.16632911238320133]
	TIME [epoch: 12.7 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039006495181642224		[learning rate: 9.7551e-05]
	Learning Rate: 9.75508e-05
	LOSS [training: 0.039006495181642224 | validation: 0.14707536744076166]
	TIME [epoch: 12.7 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602293978527716		[learning rate: 9.7206e-05]
	Learning Rate: 9.72058e-05
	LOSS [training: 0.03602293978527716 | validation: 0.1586715342826746]
	TIME [epoch: 12.7 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905074790689847		[learning rate: 9.6862e-05]
	Learning Rate: 9.68621e-05
	LOSS [training: 0.03905074790689847 | validation: 0.1543550961583333]
	TIME [epoch: 12.7 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03830756041058239		[learning rate: 9.652e-05]
	Learning Rate: 9.65196e-05
	LOSS [training: 0.03830756041058239 | validation: 0.1698490602883832]
	TIME [epoch: 12.7 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04177180827127182		[learning rate: 9.6178e-05]
	Learning Rate: 9.61783e-05
	LOSS [training: 0.04177180827127182 | validation: 0.15948324439483816]
	TIME [epoch: 12.7 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04068196414403418		[learning rate: 9.5838e-05]
	Learning Rate: 9.58382e-05
	LOSS [training: 0.04068196414403418 | validation: 0.14511560226784223]
	TIME [epoch: 12.7 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873391108183519		[learning rate: 9.5499e-05]
	Learning Rate: 9.54993e-05
	LOSS [training: 0.03873391108183519 | validation: 0.15147699697837141]
	TIME [epoch: 12.7 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04073770219192122		[learning rate: 9.5162e-05]
	Learning Rate: 9.51616e-05
	LOSS [training: 0.04073770219192122 | validation: 0.16030701538151224]
	TIME [epoch: 12.7 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03824702341922111		[learning rate: 9.4825e-05]
	Learning Rate: 9.48251e-05
	LOSS [training: 0.03824702341922111 | validation: 0.13870193038786757]
	TIME [epoch: 12.7 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04054007347758648		[learning rate: 9.449e-05]
	Learning Rate: 9.44898e-05
	LOSS [training: 0.04054007347758648 | validation: 0.14946680824294117]
	TIME [epoch: 12.7 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04042080299176314		[learning rate: 9.4156e-05]
	Learning Rate: 9.41556e-05
	LOSS [training: 0.04042080299176314 | validation: 0.1452914793213443]
	TIME [epoch: 12.7 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03815020139463807		[learning rate: 9.3823e-05]
	Learning Rate: 9.38227e-05
	LOSS [training: 0.03815020139463807 | validation: 0.1430541913280858]
	TIME [epoch: 12.7 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04141112258460408		[learning rate: 9.3491e-05]
	Learning Rate: 9.34909e-05
	LOSS [training: 0.04141112258460408 | validation: 0.14653729519735623]
	TIME [epoch: 12.7 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03646028213769393		[learning rate: 9.316e-05]
	Learning Rate: 9.31603e-05
	LOSS [training: 0.03646028213769393 | validation: 0.1510912374589949]
	TIME [epoch: 12.7 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04044093442831817		[learning rate: 9.2831e-05]
	Learning Rate: 9.28309e-05
	LOSS [training: 0.04044093442831817 | validation: 0.14609819175090313]
	TIME [epoch: 12.7 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04028689636580567		[learning rate: 9.2503e-05]
	Learning Rate: 9.25026e-05
	LOSS [training: 0.04028689636580567 | validation: 0.1652210928179262]
	TIME [epoch: 12.7 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04492069206435186		[learning rate: 9.2176e-05]
	Learning Rate: 9.21755e-05
	LOSS [training: 0.04492069206435186 | validation: 0.14526402046036546]
	TIME [epoch: 12.7 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03905543783482379		[learning rate: 9.185e-05]
	Learning Rate: 9.18495e-05
	LOSS [training: 0.03905543783482379 | validation: 0.15245959699269537]
	TIME [epoch: 12.7 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040127615082614726		[learning rate: 9.1525e-05]
	Learning Rate: 9.15247e-05
	LOSS [training: 0.040127615082614726 | validation: 0.18169678985715226]
	TIME [epoch: 12.7 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04125054213147099		[learning rate: 9.1201e-05]
	Learning Rate: 9.12011e-05
	LOSS [training: 0.04125054213147099 | validation: 0.16477359443342576]
	TIME [epoch: 12.7 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036317017487381115		[learning rate: 9.0879e-05]
	Learning Rate: 9.08786e-05
	LOSS [training: 0.036317017487381115 | validation: 0.1762643433043584]
	TIME [epoch: 12.7 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04001351039740376		[learning rate: 9.0557e-05]
	Learning Rate: 9.05572e-05
	LOSS [training: 0.04001351039740376 | validation: 0.13250861966070407]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1379.pth
	Model improved!!!
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04085567692984947		[learning rate: 9.0237e-05]
	Learning Rate: 9.0237e-05
	LOSS [training: 0.04085567692984947 | validation: 0.15263671675752044]
	TIME [epoch: 12.7 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04280633265543136		[learning rate: 8.9918e-05]
	Learning Rate: 8.99179e-05
	LOSS [training: 0.04280633265543136 | validation: 0.1483765089017718]
	TIME [epoch: 12.7 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041238788015754826		[learning rate: 8.96e-05]
	Learning Rate: 8.96e-05
	LOSS [training: 0.041238788015754826 | validation: 0.163007897040173]
	TIME [epoch: 12.7 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03946042328307762		[learning rate: 8.9283e-05]
	Learning Rate: 8.92831e-05
	LOSS [training: 0.03946042328307762 | validation: 0.1517260994375239]
	TIME [epoch: 12.7 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04065547035158234		[learning rate: 8.8967e-05]
	Learning Rate: 8.89674e-05
	LOSS [training: 0.04065547035158234 | validation: 0.15040887301239977]
	TIME [epoch: 12.7 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036912359139822624		[learning rate: 8.8653e-05]
	Learning Rate: 8.86528e-05
	LOSS [training: 0.036912359139822624 | validation: 0.15749427773427238]
	TIME [epoch: 12.7 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038997018201350875		[learning rate: 8.8339e-05]
	Learning Rate: 8.83393e-05
	LOSS [training: 0.038997018201350875 | validation: 0.15720801513775073]
	TIME [epoch: 12.7 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041827030312959756		[learning rate: 8.8027e-05]
	Learning Rate: 8.80269e-05
	LOSS [training: 0.041827030312959756 | validation: 0.14408122589804512]
	TIME [epoch: 12.7 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041712473915743085		[learning rate: 8.7716e-05]
	Learning Rate: 8.77156e-05
	LOSS [training: 0.041712473915743085 | validation: 0.16091950631887203]
	TIME [epoch: 12.7 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03954056774280284		[learning rate: 8.7405e-05]
	Learning Rate: 8.74055e-05
	LOSS [training: 0.03954056774280284 | validation: 0.16599206580932654]
	TIME [epoch: 12.7 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897328301165531		[learning rate: 8.7096e-05]
	Learning Rate: 8.70964e-05
	LOSS [training: 0.03897328301165531 | validation: 0.14176154279160422]
	TIME [epoch: 12.7 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038093092065479325		[learning rate: 8.6788e-05]
	Learning Rate: 8.67884e-05
	LOSS [training: 0.038093092065479325 | validation: 0.15034670184743104]
	TIME [epoch: 12.7 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04020251043342757		[learning rate: 8.6481e-05]
	Learning Rate: 8.64815e-05
	LOSS [training: 0.04020251043342757 | validation: 0.14215047085828833]
	TIME [epoch: 12.7 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03783619941089227		[learning rate: 8.6176e-05]
	Learning Rate: 8.61757e-05
	LOSS [training: 0.03783619941089227 | validation: 0.15860658421569915]
	TIME [epoch: 12.7 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03881000705139912		[learning rate: 8.5871e-05]
	Learning Rate: 8.5871e-05
	LOSS [training: 0.03881000705139912 | validation: 0.154152350296807]
	TIME [epoch: 12.7 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03865113789556232		[learning rate: 8.5567e-05]
	Learning Rate: 8.55673e-05
	LOSS [training: 0.03865113789556232 | validation: 0.16163443881221856]
	TIME [epoch: 12.7 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03811876517050891		[learning rate: 8.5265e-05]
	Learning Rate: 8.52647e-05
	LOSS [training: 0.03811876517050891 | validation: 0.16571963521394797]
	TIME [epoch: 12.7 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041166788950812716		[learning rate: 8.4963e-05]
	Learning Rate: 8.49632e-05
	LOSS [training: 0.041166788950812716 | validation: 0.13505285626446786]
	TIME [epoch: 12.7 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676034896035638		[learning rate: 8.4663e-05]
	Learning Rate: 8.46628e-05
	LOSS [training: 0.03676034896035638 | validation: 0.14293818166333055]
	TIME [epoch: 12.7 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03979832893404634		[learning rate: 8.4363e-05]
	Learning Rate: 8.43634e-05
	LOSS [training: 0.03979832893404634 | validation: 0.15131567157022252]
	TIME [epoch: 12.7 sec]
EPOCH 1400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038532145116936284		[learning rate: 8.4065e-05]
	Learning Rate: 8.4065e-05
	LOSS [training: 0.038532145116936284 | validation: 0.14592629974568327]
	TIME [epoch: 12.7 sec]
EPOCH 1401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040195280852494346		[learning rate: 8.3768e-05]
	Learning Rate: 8.37678e-05
	LOSS [training: 0.040195280852494346 | validation: 0.1483517451212514]
	TIME [epoch: 12.6 sec]
EPOCH 1402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036217864694584344		[learning rate: 8.3472e-05]
	Learning Rate: 8.34716e-05
	LOSS [training: 0.036217864694584344 | validation: 0.17207977580245315]
	TIME [epoch: 12.6 sec]
EPOCH 1403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.041255744983376134		[learning rate: 8.3176e-05]
	Learning Rate: 8.31764e-05
	LOSS [training: 0.041255744983376134 | validation: 0.15844708494517093]
	TIME [epoch: 12.6 sec]
EPOCH 1404/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882867792644839		[learning rate: 8.2882e-05]
	Learning Rate: 8.28823e-05
	LOSS [training: 0.03882867792644839 | validation: 0.15013737416403672]
	TIME [epoch: 12.6 sec]
EPOCH 1405/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04008690931958369		[learning rate: 8.2589e-05]
	Learning Rate: 8.25892e-05
	LOSS [training: 0.04008690931958369 | validation: 0.14493606465066097]
	TIME [epoch: 12.6 sec]
EPOCH 1406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03893651157569429		[learning rate: 8.2297e-05]
	Learning Rate: 8.22971e-05
	LOSS [training: 0.03893651157569429 | validation: 0.17452767940013955]
	TIME [epoch: 12.6 sec]
EPOCH 1407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039319578349103275		[learning rate: 8.2006e-05]
	Learning Rate: 8.20061e-05
	LOSS [training: 0.039319578349103275 | validation: 0.15516812359466392]
	TIME [epoch: 12.6 sec]
EPOCH 1408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03734818016757024		[learning rate: 8.1716e-05]
	Learning Rate: 8.17161e-05
	LOSS [training: 0.03734818016757024 | validation: 0.13739650165708275]
	TIME [epoch: 12.6 sec]
EPOCH 1409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03670285160824943		[learning rate: 8.1427e-05]
	Learning Rate: 8.14272e-05
	LOSS [training: 0.03670285160824943 | validation: 0.14616607255306843]
	TIME [epoch: 12.6 sec]
EPOCH 1410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04037739615659605		[learning rate: 8.1139e-05]
	Learning Rate: 8.11392e-05
	LOSS [training: 0.04037739615659605 | validation: 0.14752777222532895]
	TIME [epoch: 12.6 sec]
EPOCH 1411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037409303096919685		[learning rate: 8.0852e-05]
	Learning Rate: 8.08523e-05
	LOSS [training: 0.037409303096919685 | validation: 0.1539669133617768]
	TIME [epoch: 12.7 sec]
EPOCH 1412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040000000326995376		[learning rate: 8.0566e-05]
	Learning Rate: 8.05664e-05
	LOSS [training: 0.040000000326995376 | validation: 0.15836504525445916]
	TIME [epoch: 12.6 sec]
EPOCH 1413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038335176782296976		[learning rate: 8.0281e-05]
	Learning Rate: 8.02815e-05
	LOSS [training: 0.038335176782296976 | validation: 0.13641641249161324]
	TIME [epoch: 12.6 sec]
EPOCH 1414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04308013983444658		[learning rate: 7.9998e-05]
	Learning Rate: 7.99976e-05
	LOSS [training: 0.04308013983444658 | validation: 0.14568558945635607]
	TIME [epoch: 12.6 sec]
EPOCH 1415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03860854137275239		[learning rate: 7.9715e-05]
	Learning Rate: 7.97147e-05
	LOSS [training: 0.03860854137275239 | validation: 0.16364009258349066]
	TIME [epoch: 12.6 sec]
EPOCH 1416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036508654731221404		[learning rate: 7.9433e-05]
	Learning Rate: 7.94328e-05
	LOSS [training: 0.036508654731221404 | validation: 0.1529546741472664]
	TIME [epoch: 12.6 sec]
EPOCH 1417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04067893090894401		[learning rate: 7.9152e-05]
	Learning Rate: 7.9152e-05
	LOSS [training: 0.04067893090894401 | validation: 0.13666076990015624]
	TIME [epoch: 12.6 sec]
EPOCH 1418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03807045991563494		[learning rate: 7.8872e-05]
	Learning Rate: 7.88721e-05
	LOSS [training: 0.03807045991563494 | validation: 0.14608801648010475]
	TIME [epoch: 12.6 sec]
EPOCH 1419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03717084160453256		[learning rate: 7.8593e-05]
	Learning Rate: 7.85931e-05
	LOSS [training: 0.03717084160453256 | validation: 0.1552963051461337]
	TIME [epoch: 12.6 sec]
EPOCH 1420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04061188494233862		[learning rate: 7.8315e-05]
	Learning Rate: 7.83152e-05
	LOSS [training: 0.04061188494233862 | validation: 0.16136363441311466]
	TIME [epoch: 12.6 sec]
EPOCH 1421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03864132179374721		[learning rate: 7.8038e-05]
	Learning Rate: 7.80383e-05
	LOSS [training: 0.03864132179374721 | validation: 0.133123624235379]
	TIME [epoch: 12.7 sec]
EPOCH 1422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038197684654080535		[learning rate: 7.7762e-05]
	Learning Rate: 7.77623e-05
	LOSS [training: 0.038197684654080535 | validation: 0.15237194469652585]
	TIME [epoch: 12.6 sec]
EPOCH 1423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03777006871181832		[learning rate: 7.7487e-05]
	Learning Rate: 7.74873e-05
	LOSS [training: 0.03777006871181832 | validation: 0.15708864716716353]
	TIME [epoch: 12.7 sec]
EPOCH 1424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0378269812052881		[learning rate: 7.7213e-05]
	Learning Rate: 7.72134e-05
	LOSS [training: 0.0378269812052881 | validation: 0.1619323076056814]
	TIME [epoch: 12.6 sec]
EPOCH 1425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037786897956130244		[learning rate: 7.694e-05]
	Learning Rate: 7.69403e-05
	LOSS [training: 0.037786897956130244 | validation: 0.12566464458338875]
	TIME [epoch: 12.6 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1425.pth
	Model improved!!!
EPOCH 1426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039234671969380505		[learning rate: 7.6668e-05]
	Learning Rate: 7.66682e-05
	LOSS [training: 0.039234671969380505 | validation: 0.12586847979648935]
	TIME [epoch: 12.7 sec]
EPOCH 1427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04065080614419785		[learning rate: 7.6397e-05]
	Learning Rate: 7.63971e-05
	LOSS [training: 0.04065080614419785 | validation: 0.15186611753323798]
	TIME [epoch: 12.7 sec]
EPOCH 1428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460238008662948		[learning rate: 7.6127e-05]
	Learning Rate: 7.6127e-05
	LOSS [training: 0.03460238008662948 | validation: 0.16072937096475795]
	TIME [epoch: 12.7 sec]
EPOCH 1429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03880520950284766		[learning rate: 7.5858e-05]
	Learning Rate: 7.58578e-05
	LOSS [training: 0.03880520950284766 | validation: 0.1419133208244046]
	TIME [epoch: 12.7 sec]
EPOCH 1430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03867156840176555		[learning rate: 7.559e-05]
	Learning Rate: 7.55895e-05
	LOSS [training: 0.03867156840176555 | validation: 0.14436673442392906]
	TIME [epoch: 12.7 sec]
EPOCH 1431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036750494827108245		[learning rate: 7.5322e-05]
	Learning Rate: 7.53222e-05
	LOSS [training: 0.036750494827108245 | validation: 0.15090621010941976]
	TIME [epoch: 12.7 sec]
EPOCH 1432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03828128711810464		[learning rate: 7.5056e-05]
	Learning Rate: 7.50559e-05
	LOSS [training: 0.03828128711810464 | validation: 0.16461187632767674]
	TIME [epoch: 12.7 sec]
EPOCH 1433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03992262777443834		[learning rate: 7.479e-05]
	Learning Rate: 7.47905e-05
	LOSS [training: 0.03992262777443834 | validation: 0.16516270874212408]
	TIME [epoch: 12.6 sec]
EPOCH 1434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03909200696697476		[learning rate: 7.4526e-05]
	Learning Rate: 7.4526e-05
	LOSS [training: 0.03909200696697476 | validation: 0.14558535327278924]
	TIME [epoch: 12.7 sec]
EPOCH 1435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039234280469892406		[learning rate: 7.4262e-05]
	Learning Rate: 7.42625e-05
	LOSS [training: 0.039234280469892406 | validation: 0.139752506137173]
	TIME [epoch: 12.7 sec]
EPOCH 1436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03818106499001309		[learning rate: 7.4e-05]
	Learning Rate: 7.39998e-05
	LOSS [training: 0.03818106499001309 | validation: 0.15588869417926854]
	TIME [epoch: 12.7 sec]
EPOCH 1437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757319743228103		[learning rate: 7.3738e-05]
	Learning Rate: 7.37382e-05
	LOSS [training: 0.03757319743228103 | validation: 0.14542427861555587]
	TIME [epoch: 12.6 sec]
EPOCH 1438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03800216682348826		[learning rate: 7.3477e-05]
	Learning Rate: 7.34774e-05
	LOSS [training: 0.03800216682348826 | validation: 0.16513963113752675]
	TIME [epoch: 12.7 sec]
EPOCH 1439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03786417718674862		[learning rate: 7.3218e-05]
	Learning Rate: 7.32176e-05
	LOSS [training: 0.03786417718674862 | validation: 0.1352725732089581]
	TIME [epoch: 12.7 sec]
EPOCH 1440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03708687487433173		[learning rate: 7.2959e-05]
	Learning Rate: 7.29587e-05
	LOSS [training: 0.03708687487433173 | validation: 0.14186692916401047]
	TIME [epoch: 12.7 sec]
EPOCH 1441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03742822330411994		[learning rate: 7.2701e-05]
	Learning Rate: 7.27007e-05
	LOSS [training: 0.03742822330411994 | validation: 0.15821744976932495]
	TIME [epoch: 12.7 sec]
EPOCH 1442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037937060451687064		[learning rate: 7.2444e-05]
	Learning Rate: 7.24436e-05
	LOSS [training: 0.037937060451687064 | validation: 0.16117635050592444]
	TIME [epoch: 12.6 sec]
EPOCH 1443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03816713657566061		[learning rate: 7.2187e-05]
	Learning Rate: 7.21874e-05
	LOSS [training: 0.03816713657566061 | validation: 0.14059232542851974]
	TIME [epoch: 12.6 sec]
EPOCH 1444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03720102292594153		[learning rate: 7.1932e-05]
	Learning Rate: 7.19322e-05
	LOSS [training: 0.03720102292594153 | validation: 0.13973229376037097]
	TIME [epoch: 12.6 sec]
EPOCH 1445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03897906680070262		[learning rate: 7.1678e-05]
	Learning Rate: 7.16778e-05
	LOSS [training: 0.03897906680070262 | validation: 0.15751931955165524]
	TIME [epoch: 12.6 sec]
EPOCH 1446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764104255645624		[learning rate: 7.1424e-05]
	Learning Rate: 7.14243e-05
	LOSS [training: 0.03764104255645624 | validation: 0.1589951924034998]
	TIME [epoch: 12.6 sec]
EPOCH 1447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037008493028875024		[learning rate: 7.1172e-05]
	Learning Rate: 7.11718e-05
	LOSS [training: 0.037008493028875024 | validation: 0.14505202271566534]
	TIME [epoch: 12.6 sec]
EPOCH 1448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038466990186498365		[learning rate: 7.092e-05]
	Learning Rate: 7.09201e-05
	LOSS [training: 0.038466990186498365 | validation: 0.15679682715182539]
	TIME [epoch: 12.6 sec]
EPOCH 1449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03783938129601638		[learning rate: 7.0669e-05]
	Learning Rate: 7.06693e-05
	LOSS [training: 0.03783938129601638 | validation: 0.1465934164480022]
	TIME [epoch: 12.5 sec]
EPOCH 1450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842715878300376		[learning rate: 7.0419e-05]
	Learning Rate: 7.04194e-05
	LOSS [training: 0.03842715878300376 | validation: 0.1469748983888803]
	TIME [epoch: 12.6 sec]
EPOCH 1451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03753235631757326		[learning rate: 7.017e-05]
	Learning Rate: 7.01704e-05
	LOSS [training: 0.03753235631757326 | validation: 0.15710254718377178]
	TIME [epoch: 12.6 sec]
EPOCH 1452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04451495671760202		[learning rate: 6.9922e-05]
	Learning Rate: 6.99223e-05
	LOSS [training: 0.04451495671760202 | validation: 0.1413629686635898]
	TIME [epoch: 12.6 sec]
EPOCH 1453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042054226550526305		[learning rate: 6.9675e-05]
	Learning Rate: 6.9675e-05
	LOSS [training: 0.042054226550526305 | validation: 0.1456448830428599]
	TIME [epoch: 12.6 sec]
EPOCH 1454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037882139265158446		[learning rate: 6.9429e-05]
	Learning Rate: 6.94286e-05
	LOSS [training: 0.037882139265158446 | validation: 0.15667955144191753]
	TIME [epoch: 12.6 sec]
EPOCH 1455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035414928413082095		[learning rate: 6.9183e-05]
	Learning Rate: 6.91831e-05
	LOSS [training: 0.035414928413082095 | validation: 0.14316362612783204]
	TIME [epoch: 12.7 sec]
EPOCH 1456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03740048516738609		[learning rate: 6.8938e-05]
	Learning Rate: 6.89385e-05
	LOSS [training: 0.03740048516738609 | validation: 0.13787581342334435]
	TIME [epoch: 12.6 sec]
EPOCH 1457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03604711467811689		[learning rate: 6.8695e-05]
	Learning Rate: 6.86947e-05
	LOSS [training: 0.03604711467811689 | validation: 0.13466236719674604]
	TIME [epoch: 12.7 sec]
EPOCH 1458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035117220054015984		[learning rate: 6.8452e-05]
	Learning Rate: 6.84518e-05
	LOSS [training: 0.035117220054015984 | validation: 0.14169530852597131]
	TIME [epoch: 12.6 sec]
EPOCH 1459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03671854995029472		[learning rate: 6.821e-05]
	Learning Rate: 6.82097e-05
	LOSS [training: 0.03671854995029472 | validation: 0.14416163532148496]
	TIME [epoch: 12.7 sec]
EPOCH 1460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03788951387466904		[learning rate: 6.7969e-05]
	Learning Rate: 6.79685e-05
	LOSS [training: 0.03788951387466904 | validation: 0.1610968415984841]
	TIME [epoch: 12.7 sec]
EPOCH 1461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039537698581037097		[learning rate: 6.7728e-05]
	Learning Rate: 6.77282e-05
	LOSS [training: 0.039537698581037097 | validation: 0.1652051809007733]
	TIME [epoch: 12.6 sec]
EPOCH 1462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035865596538963554		[learning rate: 6.7489e-05]
	Learning Rate: 6.74887e-05
	LOSS [training: 0.035865596538963554 | validation: 0.15442877342213573]
	TIME [epoch: 12.6 sec]
EPOCH 1463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03719898932588981		[learning rate: 6.725e-05]
	Learning Rate: 6.725e-05
	LOSS [training: 0.03719898932588981 | validation: 0.15425687515425587]
	TIME [epoch: 12.6 sec]
EPOCH 1464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03765838375604192		[learning rate: 6.7012e-05]
	Learning Rate: 6.70122e-05
	LOSS [training: 0.03765838375604192 | validation: 0.1536004047219496]
	TIME [epoch: 12.6 sec]
EPOCH 1465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03700083834732379		[learning rate: 6.6775e-05]
	Learning Rate: 6.67752e-05
	LOSS [training: 0.03700083834732379 | validation: 0.14247928021746598]
	TIME [epoch: 12.6 sec]
EPOCH 1466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03706060833103756		[learning rate: 6.6539e-05]
	Learning Rate: 6.65391e-05
	LOSS [training: 0.03706060833103756 | validation: 0.1435381590152353]
	TIME [epoch: 12.6 sec]
EPOCH 1467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392495356822298		[learning rate: 6.6304e-05]
	Learning Rate: 6.63038e-05
	LOSS [training: 0.0392495356822298 | validation: 0.14851443049664376]
	TIME [epoch: 12.6 sec]
EPOCH 1468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03621216798811672		[learning rate: 6.6069e-05]
	Learning Rate: 6.60694e-05
	LOSS [training: 0.03621216798811672 | validation: 0.1377805718011247]
	TIME [epoch: 12.6 sec]
EPOCH 1469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03553694465270458		[learning rate: 6.5836e-05]
	Learning Rate: 6.58357e-05
	LOSS [training: 0.03553694465270458 | validation: 0.1440929718870405]
	TIME [epoch: 12.6 sec]
EPOCH 1470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038534887830504766		[learning rate: 6.5603e-05]
	Learning Rate: 6.56029e-05
	LOSS [training: 0.038534887830504766 | validation: 0.1567033583435006]
	TIME [epoch: 12.6 sec]
EPOCH 1471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03895467383219328		[learning rate: 6.5371e-05]
	Learning Rate: 6.53709e-05
	LOSS [training: 0.03895467383219328 | validation: 0.15286933952776943]
	TIME [epoch: 12.6 sec]
EPOCH 1472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04112829144893902		[learning rate: 6.514e-05]
	Learning Rate: 6.51398e-05
	LOSS [training: 0.04112829144893902 | validation: 0.15221995694831436]
	TIME [epoch: 12.6 sec]
EPOCH 1473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03691348481082056		[learning rate: 6.4909e-05]
	Learning Rate: 6.49094e-05
	LOSS [training: 0.03691348481082056 | validation: 0.14071389845810042]
	TIME [epoch: 12.6 sec]
EPOCH 1474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791713604348797		[learning rate: 6.468e-05]
	Learning Rate: 6.46799e-05
	LOSS [training: 0.03791713604348797 | validation: 0.13822683351480097]
	TIME [epoch: 12.6 sec]
EPOCH 1475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036754714705819966		[learning rate: 6.4451e-05]
	Learning Rate: 6.44512e-05
	LOSS [training: 0.036754714705819966 | validation: 0.14727008838200517]
	TIME [epoch: 12.6 sec]
EPOCH 1476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04129228698769772		[learning rate: 6.4223e-05]
	Learning Rate: 6.42233e-05
	LOSS [training: 0.04129228698769772 | validation: 0.15170389136774826]
	TIME [epoch: 12.6 sec]
EPOCH 1477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03642236743992489		[learning rate: 6.3996e-05]
	Learning Rate: 6.39962e-05
	LOSS [training: 0.03642236743992489 | validation: 0.14685923564795625]
	TIME [epoch: 12.7 sec]
EPOCH 1478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614253055813095		[learning rate: 6.377e-05]
	Learning Rate: 6.37699e-05
	LOSS [training: 0.03614253055813095 | validation: 0.15233301157434254]
	TIME [epoch: 12.6 sec]
EPOCH 1479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03834068832518528		[learning rate: 6.3544e-05]
	Learning Rate: 6.35444e-05
	LOSS [training: 0.03834068832518528 | validation: 0.14960043953342383]
	TIME [epoch: 12.7 sec]
EPOCH 1480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03540385436843984		[learning rate: 6.332e-05]
	Learning Rate: 6.33196e-05
	LOSS [training: 0.03540385436843984 | validation: 0.1275787482465308]
	TIME [epoch: 12.6 sec]
EPOCH 1481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04023894854858788		[learning rate: 6.3096e-05]
	Learning Rate: 6.30958e-05
	LOSS [training: 0.04023894854858788 | validation: 0.14329221263410324]
	TIME [epoch: 12.6 sec]
EPOCH 1482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564580352845718		[learning rate: 6.2873e-05]
	Learning Rate: 6.28726e-05
	LOSS [training: 0.03564580352845718 | validation: 0.16218887197847592]
	TIME [epoch: 12.6 sec]
EPOCH 1483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03811759833121466		[learning rate: 6.265e-05]
	Learning Rate: 6.26503e-05
	LOSS [training: 0.03811759833121466 | validation: 0.15453380455368204]
	TIME [epoch: 12.7 sec]
EPOCH 1484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0370849459717363		[learning rate: 6.2429e-05]
	Learning Rate: 6.24288e-05
	LOSS [training: 0.0370849459717363 | validation: 0.14839313354262523]
	TIME [epoch: 12.6 sec]
EPOCH 1485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03806058528584705		[learning rate: 6.2208e-05]
	Learning Rate: 6.2208e-05
	LOSS [training: 0.03806058528584705 | validation: 0.14525686254679146]
	TIME [epoch: 12.6 sec]
EPOCH 1486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038109938434972505		[learning rate: 6.1988e-05]
	Learning Rate: 6.1988e-05
	LOSS [training: 0.038109938434972505 | validation: 0.14825189830135327]
	TIME [epoch: 12.6 sec]
EPOCH 1487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03852392346718307		[learning rate: 6.1769e-05]
	Learning Rate: 6.17688e-05
	LOSS [training: 0.03852392346718307 | validation: 0.13268063317143836]
	TIME [epoch: 12.7 sec]
EPOCH 1488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03684094015048212		[learning rate: 6.155e-05]
	Learning Rate: 6.15504e-05
	LOSS [training: 0.03684094015048212 | validation: 0.13549947183084066]
	TIME [epoch: 12.7 sec]
EPOCH 1489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036118780249640915		[learning rate: 6.1333e-05]
	Learning Rate: 6.13327e-05
	LOSS [training: 0.036118780249640915 | validation: 0.152755250097579]
	TIME [epoch: 12.7 sec]
EPOCH 1490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039939619893127755		[learning rate: 6.1116e-05]
	Learning Rate: 6.11158e-05
	LOSS [training: 0.039939619893127755 | validation: 0.15608238854765905]
	TIME [epoch: 12.7 sec]
EPOCH 1491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03761028415433148		[learning rate: 6.09e-05]
	Learning Rate: 6.08998e-05
	LOSS [training: 0.03761028415433148 | validation: 0.14098524325765038]
	TIME [epoch: 12.7 sec]
EPOCH 1492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036494802817160894		[learning rate: 6.0684e-05]
	Learning Rate: 6.06844e-05
	LOSS [training: 0.036494802817160894 | validation: 0.14735403564422392]
	TIME [epoch: 12.7 sec]
EPOCH 1493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03991888948198391		[learning rate: 6.047e-05]
	Learning Rate: 6.04698e-05
	LOSS [training: 0.03991888948198391 | validation: 0.14799657449620449]
	TIME [epoch: 12.7 sec]
EPOCH 1494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037703172317031124		[learning rate: 6.0256e-05]
	Learning Rate: 6.0256e-05
	LOSS [training: 0.037703172317031124 | validation: 0.14846790745857474]
	TIME [epoch: 12.7 sec]
EPOCH 1495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03747013098405277		[learning rate: 6.0043e-05]
	Learning Rate: 6.00429e-05
	LOSS [training: 0.03747013098405277 | validation: 0.1361533733398778]
	TIME [epoch: 12.7 sec]
EPOCH 1496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03791851505860571		[learning rate: 5.9831e-05]
	Learning Rate: 5.98306e-05
	LOSS [training: 0.03791851505860571 | validation: 0.16045468911379945]
	TIME [epoch: 12.7 sec]
EPOCH 1497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040873595241011085		[learning rate: 5.9619e-05]
	Learning Rate: 5.9619e-05
	LOSS [training: 0.040873595241011085 | validation: 0.14631749461546814]
	TIME [epoch: 12.7 sec]
EPOCH 1498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0376339481931615		[learning rate: 5.9408e-05]
	Learning Rate: 5.94082e-05
	LOSS [training: 0.0376339481931615 | validation: 0.158690755948491]
	TIME [epoch: 12.6 sec]
EPOCH 1499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0369409466775363		[learning rate: 5.9198e-05]
	Learning Rate: 5.91981e-05
	LOSS [training: 0.0369409466775363 | validation: 0.13263366139325652]
	TIME [epoch: 12.6 sec]
EPOCH 1500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512995345350036		[learning rate: 5.8989e-05]
	Learning Rate: 5.89888e-05
	LOSS [training: 0.03512995345350036 | validation: 0.14778194957460863]
	TIME [epoch: 12.6 sec]
EPOCH 1501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038276186094603135		[learning rate: 5.878e-05]
	Learning Rate: 5.87802e-05
	LOSS [training: 0.038276186094603135 | validation: 0.14563869099122498]
	TIME [epoch: 12.6 sec]
EPOCH 1502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03939801164899686		[learning rate: 5.8572e-05]
	Learning Rate: 5.85723e-05
	LOSS [training: 0.03939801164899686 | validation: 0.15246911581191505]
	TIME [epoch: 12.6 sec]
EPOCH 1503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035329857080391276		[learning rate: 5.8365e-05]
	Learning Rate: 5.83652e-05
	LOSS [training: 0.035329857080391276 | validation: 0.14415196983592155]
	TIME [epoch: 12.6 sec]
EPOCH 1504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0384990677034083		[learning rate: 5.8159e-05]
	Learning Rate: 5.81588e-05
	LOSS [training: 0.0384990677034083 | validation: 0.15158788632329057]
	TIME [epoch: 12.6 sec]
EPOCH 1505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03882715363939875		[learning rate: 5.7953e-05]
	Learning Rate: 5.79531e-05
	LOSS [training: 0.03882715363939875 | validation: 0.14104865712200415]
	TIME [epoch: 13.7 sec]
EPOCH 1506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552390970647849		[learning rate: 5.7748e-05]
	Learning Rate: 5.77482e-05
	LOSS [training: 0.03552390970647849 | validation: 0.13944130540069707]
	TIME [epoch: 12.6 sec]
EPOCH 1507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03741574924396235		[learning rate: 5.7544e-05]
	Learning Rate: 5.7544e-05
	LOSS [training: 0.03741574924396235 | validation: 0.12354305484385189]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1507.pth
	Model improved!!!
EPOCH 1508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036886567069479174		[learning rate: 5.7341e-05]
	Learning Rate: 5.73405e-05
	LOSS [training: 0.036886567069479174 | validation: 0.1580135123228817]
	TIME [epoch: 12.7 sec]
EPOCH 1509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036520619961541975		[learning rate: 5.7138e-05]
	Learning Rate: 5.71378e-05
	LOSS [training: 0.036520619961541975 | validation: 0.15654516546762418]
	TIME [epoch: 12.7 sec]
EPOCH 1510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036535788398651604		[learning rate: 5.6936e-05]
	Learning Rate: 5.69357e-05
	LOSS [training: 0.036535788398651604 | validation: 0.13710783118789024]
	TIME [epoch: 12.7 sec]
EPOCH 1511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03702531002478894		[learning rate: 5.6734e-05]
	Learning Rate: 5.67344e-05
	LOSS [training: 0.03702531002478894 | validation: 0.1410238881226432]
	TIME [epoch: 12.7 sec]
EPOCH 1512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03516493655702799		[learning rate: 5.6534e-05]
	Learning Rate: 5.65337e-05
	LOSS [training: 0.03516493655702799 | validation: 0.14656453012320791]
	TIME [epoch: 12.7 sec]
EPOCH 1513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03895638082595596		[learning rate: 5.6334e-05]
	Learning Rate: 5.63338e-05
	LOSS [training: 0.03895638082595596 | validation: 0.14019327665506492]
	TIME [epoch: 12.7 sec]
EPOCH 1514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03735658432825213		[learning rate: 5.6135e-05]
	Learning Rate: 5.61346e-05
	LOSS [training: 0.03735658432825213 | validation: 0.1515035611068712]
	TIME [epoch: 12.7 sec]
EPOCH 1515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04130576773679522		[learning rate: 5.5936e-05]
	Learning Rate: 5.59361e-05
	LOSS [training: 0.04130576773679522 | validation: 0.14032455573577632]
	TIME [epoch: 12.7 sec]
EPOCH 1516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037886438061571256		[learning rate: 5.5738e-05]
	Learning Rate: 5.57383e-05
	LOSS [training: 0.037886438061571256 | validation: 0.12408182143133523]
	TIME [epoch: 12.7 sec]
EPOCH 1517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03857208894451389		[learning rate: 5.5541e-05]
	Learning Rate: 5.55412e-05
	LOSS [training: 0.03857208894451389 | validation: 0.1432444441808899]
	TIME [epoch: 12.7 sec]
EPOCH 1518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03743931619354077		[learning rate: 5.5345e-05]
	Learning Rate: 5.53448e-05
	LOSS [training: 0.03743931619354077 | validation: 0.15417970571888184]
	TIME [epoch: 12.7 sec]
EPOCH 1519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03566885227431673		[learning rate: 5.5149e-05]
	Learning Rate: 5.51491e-05
	LOSS [training: 0.03566885227431673 | validation: 0.1531470285953131]
	TIME [epoch: 12.7 sec]
EPOCH 1520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035585369854429214		[learning rate: 5.4954e-05]
	Learning Rate: 5.49541e-05
	LOSS [training: 0.035585369854429214 | validation: 0.15144005411947084]
	TIME [epoch: 12.7 sec]
EPOCH 1521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03507381245269722		[learning rate: 5.476e-05]
	Learning Rate: 5.47598e-05
	LOSS [training: 0.03507381245269722 | validation: 0.12977841576182078]
	TIME [epoch: 12.7 sec]
EPOCH 1522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03548812903473208		[learning rate: 5.4566e-05]
	Learning Rate: 5.45661e-05
	LOSS [training: 0.03548812903473208 | validation: 0.15223955584069657]
	TIME [epoch: 12.7 sec]
EPOCH 1523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03590069669281735		[learning rate: 5.4373e-05]
	Learning Rate: 5.43732e-05
	LOSS [training: 0.03590069669281735 | validation: 0.14198794594903866]
	TIME [epoch: 12.7 sec]
EPOCH 1524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037976848878513825		[learning rate: 5.4181e-05]
	Learning Rate: 5.41809e-05
	LOSS [training: 0.037976848878513825 | validation: 0.14330590208303226]
	TIME [epoch: 12.7 sec]
EPOCH 1525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038465057005059325		[learning rate: 5.3989e-05]
	Learning Rate: 5.39893e-05
	LOSS [training: 0.038465057005059325 | validation: 0.13338323682660272]
	TIME [epoch: 12.7 sec]
EPOCH 1526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03764624121131652		[learning rate: 5.3798e-05]
	Learning Rate: 5.37984e-05
	LOSS [training: 0.03764624121131652 | validation: 0.15455080410332445]
	TIME [epoch: 12.7 sec]
EPOCH 1527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037858796528627725		[learning rate: 5.3608e-05]
	Learning Rate: 5.36082e-05
	LOSS [training: 0.037858796528627725 | validation: 0.16571917033814398]
	TIME [epoch: 12.7 sec]
EPOCH 1528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038504820515969124		[learning rate: 5.3419e-05]
	Learning Rate: 5.34186e-05
	LOSS [training: 0.038504820515969124 | validation: 0.14730643133708332]
	TIME [epoch: 12.7 sec]
EPOCH 1529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03517277034094041		[learning rate: 5.323e-05]
	Learning Rate: 5.32297e-05
	LOSS [training: 0.03517277034094041 | validation: 0.1500062049632622]
	TIME [epoch: 12.7 sec]
EPOCH 1530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676821499498829		[learning rate: 5.3041e-05]
	Learning Rate: 5.30415e-05
	LOSS [training: 0.03676821499498829 | validation: 0.1459263115425586]
	TIME [epoch: 12.7 sec]
EPOCH 1531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03673475143267199		[learning rate: 5.2854e-05]
	Learning Rate: 5.28539e-05
	LOSS [training: 0.03673475143267199 | validation: 0.13221456413256227]
	TIME [epoch: 12.7 sec]
EPOCH 1532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03484980529766732		[learning rate: 5.2667e-05]
	Learning Rate: 5.2667e-05
	LOSS [training: 0.03484980529766732 | validation: 0.14798660467214023]
	TIME [epoch: 12.7 sec]
EPOCH 1533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03756233324577039		[learning rate: 5.2481e-05]
	Learning Rate: 5.24807e-05
	LOSS [training: 0.03756233324577039 | validation: 0.15649295136093744]
	TIME [epoch: 12.7 sec]
EPOCH 1534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037708708498179096		[learning rate: 5.2295e-05]
	Learning Rate: 5.22952e-05
	LOSS [training: 0.037708708498179096 | validation: 0.1540578941485888]
	TIME [epoch: 12.7 sec]
EPOCH 1535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03603291018037372		[learning rate: 5.211e-05]
	Learning Rate: 5.21103e-05
	LOSS [training: 0.03603291018037372 | validation: 0.13478114785600423]
	TIME [epoch: 12.7 sec]
EPOCH 1536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03787048414102841		[learning rate: 5.1926e-05]
	Learning Rate: 5.1926e-05
	LOSS [training: 0.03787048414102841 | validation: 0.13835192027642826]
	TIME [epoch: 12.7 sec]
EPOCH 1537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03535869372446625		[learning rate: 5.1742e-05]
	Learning Rate: 5.17424e-05
	LOSS [training: 0.03535869372446625 | validation: 0.13438637598034786]
	TIME [epoch: 12.7 sec]
EPOCH 1538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03815369251646444		[learning rate: 5.1559e-05]
	Learning Rate: 5.15594e-05
	LOSS [training: 0.03815369251646444 | validation: 0.1371070629313876]
	TIME [epoch: 12.7 sec]
EPOCH 1539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036397357709943685		[learning rate: 5.1377e-05]
	Learning Rate: 5.13771e-05
	LOSS [training: 0.036397357709943685 | validation: 0.14354299191147565]
	TIME [epoch: 12.7 sec]
EPOCH 1540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035676196971583224		[learning rate: 5.1195e-05]
	Learning Rate: 5.11954e-05
	LOSS [training: 0.035676196971583224 | validation: 0.1615497566108527]
	TIME [epoch: 12.7 sec]
EPOCH 1541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037109187918817836		[learning rate: 5.1014e-05]
	Learning Rate: 5.10144e-05
	LOSS [training: 0.037109187918817836 | validation: 0.14853968318875618]
	TIME [epoch: 12.7 sec]
EPOCH 1542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03653066148093088		[learning rate: 5.0834e-05]
	Learning Rate: 5.0834e-05
	LOSS [training: 0.03653066148093088 | validation: 0.13612317731111362]
	TIME [epoch: 12.7 sec]
EPOCH 1543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03593403785761542		[learning rate: 5.0654e-05]
	Learning Rate: 5.06542e-05
	LOSS [training: 0.03593403785761542 | validation: 0.14268148741128314]
	TIME [epoch: 12.7 sec]
EPOCH 1544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03542524821636244		[learning rate: 5.0475e-05]
	Learning Rate: 5.04751e-05
	LOSS [training: 0.03542524821636244 | validation: 0.14166217702767095]
	TIME [epoch: 12.7 sec]
EPOCH 1545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039111593773502006		[learning rate: 5.0297e-05]
	Learning Rate: 5.02966e-05
	LOSS [training: 0.039111593773502006 | validation: 0.15805212003761598]
	TIME [epoch: 12.7 sec]
EPOCH 1546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03798825234619307		[learning rate: 5.0119e-05]
	Learning Rate: 5.01187e-05
	LOSS [training: 0.03798825234619307 | validation: 0.14427125052719456]
	TIME [epoch: 12.7 sec]
EPOCH 1547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03500751804320399		[learning rate: 4.9942e-05]
	Learning Rate: 4.99415e-05
	LOSS [training: 0.03500751804320399 | validation: 0.13915200484217782]
	TIME [epoch: 12.7 sec]
EPOCH 1548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03906110304246357		[learning rate: 4.9765e-05]
	Learning Rate: 4.97649e-05
	LOSS [training: 0.03906110304246357 | validation: 0.1573101534877663]
	TIME [epoch: 12.7 sec]
EPOCH 1549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03819907892215249		[learning rate: 4.9589e-05]
	Learning Rate: 4.95889e-05
	LOSS [training: 0.03819907892215249 | validation: 0.15701851329801333]
	TIME [epoch: 12.7 sec]
EPOCH 1550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03820380746698787		[learning rate: 4.9414e-05]
	Learning Rate: 4.94136e-05
	LOSS [training: 0.03820380746698787 | validation: 0.16221328867377496]
	TIME [epoch: 12.7 sec]
EPOCH 1551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757881261700747		[learning rate: 4.9239e-05]
	Learning Rate: 4.92388e-05
	LOSS [training: 0.03757881261700747 | validation: 0.13070627585638078]
	TIME [epoch: 12.7 sec]
EPOCH 1552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03871057937021252		[learning rate: 4.9065e-05]
	Learning Rate: 4.90647e-05
	LOSS [training: 0.03871057937021252 | validation: 0.13647407388971874]
	TIME [epoch: 12.7 sec]
EPOCH 1553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03671621461887874		[learning rate: 4.8891e-05]
	Learning Rate: 4.88912e-05
	LOSS [training: 0.03671621461887874 | validation: 0.1484269652148221]
	TIME [epoch: 12.7 sec]
EPOCH 1554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038774447658242936		[learning rate: 4.8718e-05]
	Learning Rate: 4.87183e-05
	LOSS [training: 0.038774447658242936 | validation: 0.1616366903639176]
	TIME [epoch: 12.7 sec]
EPOCH 1555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03832751203911526		[learning rate: 4.8546e-05]
	Learning Rate: 4.85461e-05
	LOSS [training: 0.03832751203911526 | validation: 0.15947958551139454]
	TIME [epoch: 12.7 sec]
EPOCH 1556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03460177149193709		[learning rate: 4.8374e-05]
	Learning Rate: 4.83744e-05
	LOSS [training: 0.03460177149193709 | validation: 0.13572474546966593]
	TIME [epoch: 12.7 sec]
EPOCH 1557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03518418638166967		[learning rate: 4.8203e-05]
	Learning Rate: 4.82033e-05
	LOSS [training: 0.03518418638166967 | validation: 0.1454485723548927]
	TIME [epoch: 12.7 sec]
EPOCH 1558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03526860239513252		[learning rate: 4.8033e-05]
	Learning Rate: 4.80329e-05
	LOSS [training: 0.03526860239513252 | validation: 0.1484944799011191]
	TIME [epoch: 12.7 sec]
EPOCH 1559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03636872303539413		[learning rate: 4.7863e-05]
	Learning Rate: 4.7863e-05
	LOSS [training: 0.03636872303539413 | validation: 0.13449656780667155]
	TIME [epoch: 12.7 sec]
EPOCH 1560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040710256048946965		[learning rate: 4.7694e-05]
	Learning Rate: 4.76938e-05
	LOSS [training: 0.040710256048946965 | validation: 0.15000815514467852]
	TIME [epoch: 12.7 sec]
EPOCH 1561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03408093153897177		[learning rate: 4.7525e-05]
	Learning Rate: 4.75251e-05
	LOSS [training: 0.03408093153897177 | validation: 0.1675062569067779]
	TIME [epoch: 12.7 sec]
EPOCH 1562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.040799207293719005		[learning rate: 4.7357e-05]
	Learning Rate: 4.73571e-05
	LOSS [training: 0.040799207293719005 | validation: 0.14948266475326674]
	TIME [epoch: 12.7 sec]
EPOCH 1563/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03757204675581506		[learning rate: 4.719e-05]
	Learning Rate: 4.71896e-05
	LOSS [training: 0.03757204675581506 | validation: 0.12672148434505323]
	TIME [epoch: 12.7 sec]
EPOCH 1564/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035876072411834554		[learning rate: 4.7023e-05]
	Learning Rate: 4.70227e-05
	LOSS [training: 0.035876072411834554 | validation: 0.13221850181873646]
	TIME [epoch: 12.7 sec]
EPOCH 1565/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03583837794250393		[learning rate: 4.6856e-05]
	Learning Rate: 4.68564e-05
	LOSS [training: 0.03583837794250393 | validation: 0.13350653154258552]
	TIME [epoch: 12.7 sec]
EPOCH 1566/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03838541583709087		[learning rate: 4.6691e-05]
	Learning Rate: 4.66907e-05
	LOSS [training: 0.03838541583709087 | validation: 0.1486285299566729]
	TIME [epoch: 12.7 sec]
EPOCH 1567/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03594810702449655		[learning rate: 4.6526e-05]
	Learning Rate: 4.65257e-05
	LOSS [training: 0.03594810702449655 | validation: 0.12924509760327094]
	TIME [epoch: 12.7 sec]
EPOCH 1568/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036017785930004396		[learning rate: 4.6361e-05]
	Learning Rate: 4.63611e-05
	LOSS [training: 0.036017785930004396 | validation: 0.13166267902804477]
	TIME [epoch: 12.7 sec]
EPOCH 1569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035691688895259394		[learning rate: 4.6197e-05]
	Learning Rate: 4.61972e-05
	LOSS [training: 0.035691688895259394 | validation: 0.14658892636463391]
	TIME [epoch: 12.7 sec]
EPOCH 1570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.038191352776293035		[learning rate: 4.6034e-05]
	Learning Rate: 4.60338e-05
	LOSS [training: 0.038191352776293035 | validation: 0.14474208194870375]
	TIME [epoch: 12.7 sec]
EPOCH 1571/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037043621069810406		[learning rate: 4.5871e-05]
	Learning Rate: 4.5871e-05
	LOSS [training: 0.037043621069810406 | validation: 0.1318715101680877]
	TIME [epoch: 12.7 sec]
EPOCH 1572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03883235699153153		[learning rate: 4.5709e-05]
	Learning Rate: 4.57088e-05
	LOSS [training: 0.03883235699153153 | validation: 0.1552047430723342]
	TIME [epoch: 12.7 sec]
EPOCH 1573/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037623320748700825		[learning rate: 4.5547e-05]
	Learning Rate: 4.55472e-05
	LOSS [training: 0.037623320748700825 | validation: 0.14000132657597178]
	TIME [epoch: 12.7 sec]
EPOCH 1574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036872878467531976		[learning rate: 4.5386e-05]
	Learning Rate: 4.53861e-05
	LOSS [training: 0.036872878467531976 | validation: 0.15822512111031786]
	TIME [epoch: 12.7 sec]
EPOCH 1575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036253429965588606		[learning rate: 4.5226e-05]
	Learning Rate: 4.52256e-05
	LOSS [training: 0.036253429965588606 | validation: 0.1451952422970216]
	TIME [epoch: 12.7 sec]
EPOCH 1576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03643080493027729		[learning rate: 4.5066e-05]
	Learning Rate: 4.50657e-05
	LOSS [training: 0.03643080493027729 | validation: 0.13953877986889035]
	TIME [epoch: 12.7 sec]
EPOCH 1577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03795795631417101		[learning rate: 4.4906e-05]
	Learning Rate: 4.49064e-05
	LOSS [training: 0.03795795631417101 | validation: 0.1393718089284089]
	TIME [epoch: 12.7 sec]
EPOCH 1578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036860483119501075		[learning rate: 4.4748e-05]
	Learning Rate: 4.47476e-05
	LOSS [training: 0.036860483119501075 | validation: 0.13802787292652755]
	TIME [epoch: 12.7 sec]
EPOCH 1579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03533843545963059		[learning rate: 4.4589e-05]
	Learning Rate: 4.45893e-05
	LOSS [training: 0.03533843545963059 | validation: 0.1371575488183597]
	TIME [epoch: 12.7 sec]
EPOCH 1580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03538452422412147		[learning rate: 4.4432e-05]
	Learning Rate: 4.44316e-05
	LOSS [training: 0.03538452422412147 | validation: 0.14488867484284734]
	TIME [epoch: 12.7 sec]
EPOCH 1581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035292118751217905		[learning rate: 4.4275e-05]
	Learning Rate: 4.42745e-05
	LOSS [training: 0.035292118751217905 | validation: 0.14442909845752633]
	TIME [epoch: 12.7 sec]
EPOCH 1582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037225479550535165		[learning rate: 4.4118e-05]
	Learning Rate: 4.4118e-05
	LOSS [training: 0.037225479550535165 | validation: 0.14394293666010546]
	TIME [epoch: 12.7 sec]
EPOCH 1583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034830436857286484		[learning rate: 4.3962e-05]
	Learning Rate: 4.3962e-05
	LOSS [training: 0.034830436857286484 | validation: 0.15107673474709743]
	TIME [epoch: 12.7 sec]
EPOCH 1584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03541832093341271		[learning rate: 4.3807e-05]
	Learning Rate: 4.38065e-05
	LOSS [training: 0.03541832093341271 | validation: 0.14934120040565396]
	TIME [epoch: 12.7 sec]
EPOCH 1585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04018561669525889		[learning rate: 4.3652e-05]
	Learning Rate: 4.36516e-05
	LOSS [training: 0.04018561669525889 | validation: 0.14575364611430672]
	TIME [epoch: 12.7 sec]
EPOCH 1586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03552607680566766		[learning rate: 4.3497e-05]
	Learning Rate: 4.34972e-05
	LOSS [training: 0.03552607680566766 | validation: 0.1484056551393441]
	TIME [epoch: 12.7 sec]
EPOCH 1587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03582856258016717		[learning rate: 4.3343e-05]
	Learning Rate: 4.33434e-05
	LOSS [training: 0.03582856258016717 | validation: 0.15615465763454706]
	TIME [epoch: 12.7 sec]
EPOCH 1588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03559947769050186		[learning rate: 4.319e-05]
	Learning Rate: 4.31902e-05
	LOSS [training: 0.03559947769050186 | validation: 0.14167318532993758]
	TIME [epoch: 12.7 sec]
EPOCH 1589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03575058246404111		[learning rate: 4.3037e-05]
	Learning Rate: 4.30374e-05
	LOSS [training: 0.03575058246404111 | validation: 0.1479734297453615]
	TIME [epoch: 12.7 sec]
EPOCH 1590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035483553542280576		[learning rate: 4.2885e-05]
	Learning Rate: 4.28852e-05
	LOSS [training: 0.035483553542280576 | validation: 0.16359868272951053]
	TIME [epoch: 12.7 sec]
EPOCH 1591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03545178527717518		[learning rate: 4.2734e-05]
	Learning Rate: 4.27336e-05
	LOSS [training: 0.03545178527717518 | validation: 0.1333041559256293]
	TIME [epoch: 12.7 sec]
EPOCH 1592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03701145926147		[learning rate: 4.2582e-05]
	Learning Rate: 4.25825e-05
	LOSS [training: 0.03701145926147 | validation: 0.13116384279770404]
	TIME [epoch: 12.7 sec]
EPOCH 1593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03855501438603838		[learning rate: 4.2432e-05]
	Learning Rate: 4.24319e-05
	LOSS [training: 0.03855501438603838 | validation: 0.14245863911297219]
	TIME [epoch: 12.7 sec]
EPOCH 1594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03449808511866552		[learning rate: 4.2282e-05]
	Learning Rate: 4.22819e-05
	LOSS [training: 0.03449808511866552 | validation: 0.14848742407467003]
	TIME [epoch: 12.7 sec]
EPOCH 1595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0383563015219229		[learning rate: 4.2132e-05]
	Learning Rate: 4.21323e-05
	LOSS [training: 0.0383563015219229 | validation: 0.13999355550553835]
	TIME [epoch: 12.7 sec]
EPOCH 1596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037602397035277485		[learning rate: 4.1983e-05]
	Learning Rate: 4.19833e-05
	LOSS [training: 0.037602397035277485 | validation: 0.13364985061768325]
	TIME [epoch: 12.7 sec]
EPOCH 1597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03672196330586621		[learning rate: 4.1835e-05]
	Learning Rate: 4.18349e-05
	LOSS [training: 0.03672196330586621 | validation: 0.1371019475844357]
	TIME [epoch: 12.7 sec]
EPOCH 1598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03778892252117048		[learning rate: 4.1687e-05]
	Learning Rate: 4.1687e-05
	LOSS [training: 0.03778892252117048 | validation: 0.14340084967433836]
	TIME [epoch: 12.7 sec]
EPOCH 1599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035160369464779045		[learning rate: 4.154e-05]
	Learning Rate: 4.15395e-05
	LOSS [training: 0.035160369464779045 | validation: 0.14345970306343336]
	TIME [epoch: 12.7 sec]
EPOCH 1600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707151317534395		[learning rate: 4.1393e-05]
	Learning Rate: 4.13926e-05
	LOSS [training: 0.03707151317534395 | validation: 0.15314026195356986]
	TIME [epoch: 12.7 sec]
EPOCH 1601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03705384035040035		[learning rate: 4.1246e-05]
	Learning Rate: 4.12463e-05
	LOSS [training: 0.03705384035040035 | validation: 0.14016207021639873]
	TIME [epoch: 12.7 sec]
EPOCH 1602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036603534054737964		[learning rate: 4.11e-05]
	Learning Rate: 4.11004e-05
	LOSS [training: 0.036603534054737964 | validation: 0.13615868590294075]
	TIME [epoch: 12.7 sec]
EPOCH 1603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03588259588414681		[learning rate: 4.0955e-05]
	Learning Rate: 4.09551e-05
	LOSS [training: 0.03588259588414681 | validation: 0.14620246538305035]
	TIME [epoch: 12.7 sec]
EPOCH 1604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03601170246732885		[learning rate: 4.081e-05]
	Learning Rate: 4.08103e-05
	LOSS [training: 0.03601170246732885 | validation: 0.15623585898814474]
	TIME [epoch: 12.7 sec]
EPOCH 1605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03727440452593696		[learning rate: 4.0666e-05]
	Learning Rate: 4.06659e-05
	LOSS [training: 0.03727440452593696 | validation: 0.14944645361138137]
	TIME [epoch: 12.7 sec]
EPOCH 1606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03729635136776821		[learning rate: 4.0522e-05]
	Learning Rate: 4.05221e-05
	LOSS [training: 0.03729635136776821 | validation: 0.14501693941995755]
	TIME [epoch: 12.7 sec]
EPOCH 1607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03752701333393676		[learning rate: 4.0379e-05]
	Learning Rate: 4.03788e-05
	LOSS [training: 0.03752701333393676 | validation: 0.12316025984575776]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1607.pth
	Model improved!!!
EPOCH 1608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037308414990708985		[learning rate: 4.0236e-05]
	Learning Rate: 4.02361e-05
	LOSS [training: 0.037308414990708985 | validation: 0.14426814382866013]
	TIME [epoch: 12.7 sec]
EPOCH 1609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03821951142208556		[learning rate: 4.0094e-05]
	Learning Rate: 4.00938e-05
	LOSS [training: 0.03821951142208556 | validation: 0.13827834419566595]
	TIME [epoch: 12.7 sec]
EPOCH 1610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03956776120384484		[learning rate: 3.9952e-05]
	Learning Rate: 3.9952e-05
	LOSS [training: 0.03956776120384484 | validation: 0.14787555911844077]
	TIME [epoch: 12.7 sec]
EPOCH 1611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034774144817754434		[learning rate: 3.9811e-05]
	Learning Rate: 3.98107e-05
	LOSS [training: 0.034774144817754434 | validation: 0.14479977069696442]
	TIME [epoch: 12.7 sec]
EPOCH 1612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03497914310127066		[learning rate: 3.967e-05]
	Learning Rate: 3.967e-05
	LOSS [training: 0.03497914310127066 | validation: 0.14631243636406746]
	TIME [epoch: 12.7 sec]
EPOCH 1613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03604038436822211		[learning rate: 3.953e-05]
	Learning Rate: 3.95297e-05
	LOSS [training: 0.03604038436822211 | validation: 0.14181115126104768]
	TIME [epoch: 12.7 sec]
EPOCH 1614/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037401784292271055		[learning rate: 3.939e-05]
	Learning Rate: 3.93899e-05
	LOSS [training: 0.037401784292271055 | validation: 0.12510692669811782]
	TIME [epoch: 12.7 sec]
EPOCH 1615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03502053989458952		[learning rate: 3.9251e-05]
	Learning Rate: 3.92506e-05
	LOSS [training: 0.03502053989458952 | validation: 0.137922652732936]
	TIME [epoch: 12.7 sec]
EPOCH 1616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035382251594088886		[learning rate: 3.9112e-05]
	Learning Rate: 3.91118e-05
	LOSS [training: 0.035382251594088886 | validation: 0.14407447654476763]
	TIME [epoch: 12.7 sec]
EPOCH 1617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03664342979137479		[learning rate: 3.8973e-05]
	Learning Rate: 3.89735e-05
	LOSS [training: 0.03664342979137479 | validation: 0.13992206410572036]
	TIME [epoch: 12.7 sec]
EPOCH 1618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034518711108951335		[learning rate: 3.8836e-05]
	Learning Rate: 3.88357e-05
	LOSS [training: 0.034518711108951335 | validation: 0.14252194634535376]
	TIME [epoch: 12.7 sec]
EPOCH 1619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03652304564111425		[learning rate: 3.8698e-05]
	Learning Rate: 3.86983e-05
	LOSS [training: 0.03652304564111425 | validation: 0.1553919541036014]
	TIME [epoch: 12.7 sec]
EPOCH 1620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034065907588705126		[learning rate: 3.8561e-05]
	Learning Rate: 3.85615e-05
	LOSS [training: 0.034065907588705126 | validation: 0.140204677130006]
	TIME [epoch: 12.7 sec]
EPOCH 1621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03510667790511784		[learning rate: 3.8425e-05]
	Learning Rate: 3.84251e-05
	LOSS [training: 0.03510667790511784 | validation: 0.14143285567668792]
	TIME [epoch: 12.7 sec]
EPOCH 1622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037482621612876625		[learning rate: 3.8289e-05]
	Learning Rate: 3.82893e-05
	LOSS [training: 0.037482621612876625 | validation: 0.14589676379237712]
	TIME [epoch: 12.7 sec]
EPOCH 1623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03543869733454374		[learning rate: 3.8154e-05]
	Learning Rate: 3.81539e-05
	LOSS [training: 0.03543869733454374 | validation: 0.14116156249455822]
	TIME [epoch: 12.7 sec]
EPOCH 1624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.039204487522209876		[learning rate: 3.8019e-05]
	Learning Rate: 3.8019e-05
	LOSS [training: 0.039204487522209876 | validation: 0.1529190192940597]
	TIME [epoch: 12.7 sec]
EPOCH 1625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03842533461103181		[learning rate: 3.7885e-05]
	Learning Rate: 3.78845e-05
	LOSS [training: 0.03842533461103181 | validation: 0.16342705883417566]
	TIME [epoch: 12.7 sec]
EPOCH 1626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03677222054103577		[learning rate: 3.7751e-05]
	Learning Rate: 3.77505e-05
	LOSS [training: 0.03677222054103577 | validation: 0.13600809164495203]
	TIME [epoch: 12.7 sec]
EPOCH 1627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03345969587502766		[learning rate: 3.7617e-05]
	Learning Rate: 3.7617e-05
	LOSS [training: 0.03345969587502766 | validation: 0.12363935558863313]
	TIME [epoch: 12.7 sec]
EPOCH 1628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512114220846366		[learning rate: 3.7484e-05]
	Learning Rate: 3.7484e-05
	LOSS [training: 0.03512114220846366 | validation: 0.133076456844566]
	TIME [epoch: 12.7 sec]
EPOCH 1629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035881746380490895		[learning rate: 3.7351e-05]
	Learning Rate: 3.73515e-05
	LOSS [training: 0.035881746380490895 | validation: 0.1553915814461811]
	TIME [epoch: 12.7 sec]
EPOCH 1630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03537776930620631		[learning rate: 3.7219e-05]
	Learning Rate: 3.72194e-05
	LOSS [training: 0.03537776930620631 | validation: 0.13408648469292136]
	TIME [epoch: 12.7 sec]
EPOCH 1631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03544284498731667		[learning rate: 3.7088e-05]
	Learning Rate: 3.70878e-05
	LOSS [training: 0.03544284498731667 | validation: 0.14085588067468785]
	TIME [epoch: 12.7 sec]
EPOCH 1632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03606701707625965		[learning rate: 3.6957e-05]
	Learning Rate: 3.69566e-05
	LOSS [training: 0.03606701707625965 | validation: 0.15604033386575814]
	TIME [epoch: 12.7 sec]
EPOCH 1633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03563294900221739		[learning rate: 3.6826e-05]
	Learning Rate: 3.68259e-05
	LOSS [training: 0.03563294900221739 | validation: 0.1548465717923799]
	TIME [epoch: 12.7 sec]
EPOCH 1634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035813374357458544		[learning rate: 3.6696e-05]
	Learning Rate: 3.66957e-05
	LOSS [training: 0.035813374357458544 | validation: 0.13913606332363543]
	TIME [epoch: 12.7 sec]
EPOCH 1635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676715494096929		[learning rate: 3.6566e-05]
	Learning Rate: 3.6566e-05
	LOSS [training: 0.03676715494096929 | validation: 0.1445078811232217]
	TIME [epoch: 12.7 sec]
EPOCH 1636/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035337707854604765		[learning rate: 3.6437e-05]
	Learning Rate: 3.64367e-05
	LOSS [training: 0.035337707854604765 | validation: 0.13954536806052795]
	TIME [epoch: 12.7 sec]
EPOCH 1637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03463633197840153		[learning rate: 3.6308e-05]
	Learning Rate: 3.63078e-05
	LOSS [training: 0.03463633197840153 | validation: 0.13579094547660306]
	TIME [epoch: 12.7 sec]
EPOCH 1638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035872604749655916		[learning rate: 3.6179e-05]
	Learning Rate: 3.61794e-05
	LOSS [training: 0.035872604749655916 | validation: 0.13532180356211465]
	TIME [epoch: 12.7 sec]
EPOCH 1639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03438168065255881		[learning rate: 3.6051e-05]
	Learning Rate: 3.60515e-05
	LOSS [training: 0.03438168065255881 | validation: 0.14726668441371585]
	TIME [epoch: 12.7 sec]
EPOCH 1640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03564122905745884		[learning rate: 3.5924e-05]
	Learning Rate: 3.5924e-05
	LOSS [training: 0.03564122905745884 | validation: 0.13607905489158578]
	TIME [epoch: 12.7 sec]
EPOCH 1641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035913447045698814		[learning rate: 3.5797e-05]
	Learning Rate: 3.5797e-05
	LOSS [training: 0.035913447045698814 | validation: 0.14451159973349342]
	TIME [epoch: 12.7 sec]
EPOCH 1642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037170010265917756		[learning rate: 3.567e-05]
	Learning Rate: 3.56704e-05
	LOSS [training: 0.037170010265917756 | validation: 0.1394376130153825]
	TIME [epoch: 12.7 sec]
EPOCH 1643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03642679193663025		[learning rate: 3.5544e-05]
	Learning Rate: 3.55442e-05
	LOSS [training: 0.03642679193663025 | validation: 0.14191396985244745]
	TIME [epoch: 12.7 sec]
EPOCH 1644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03451989589025151		[learning rate: 3.5419e-05]
	Learning Rate: 3.54186e-05
	LOSS [training: 0.03451989589025151 | validation: 0.13442557104924516]
	TIME [epoch: 12.7 sec]
EPOCH 1645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578496157653782		[learning rate: 3.5293e-05]
	Learning Rate: 3.52933e-05
	LOSS [training: 0.03578496157653782 | validation: 0.14325313484809782]
	TIME [epoch: 12.7 sec]
EPOCH 1646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614658964441908		[learning rate: 3.5169e-05]
	Learning Rate: 3.51685e-05
	LOSS [training: 0.03614658964441908 | validation: 0.14698869570835393]
	TIME [epoch: 12.7 sec]
EPOCH 1647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036416152635596714		[learning rate: 3.5044e-05]
	Learning Rate: 3.50441e-05
	LOSS [training: 0.036416152635596714 | validation: 0.14490854382468357]
	TIME [epoch: 12.7 sec]
EPOCH 1648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03633994305091068		[learning rate: 3.492e-05]
	Learning Rate: 3.49202e-05
	LOSS [training: 0.03633994305091068 | validation: 0.14184621300407213]
	TIME [epoch: 12.7 sec]
EPOCH 1649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0392039253194452		[learning rate: 3.4797e-05]
	Learning Rate: 3.47967e-05
	LOSS [training: 0.0392039253194452 | validation: 0.1435561233967748]
	TIME [epoch: 12.7 sec]
EPOCH 1650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037013316601424186		[learning rate: 3.4674e-05]
	Learning Rate: 3.46737e-05
	LOSS [training: 0.037013316601424186 | validation: 0.1544715819825038]
	TIME [epoch: 12.7 sec]
EPOCH 1651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03648651568470838		[learning rate: 3.4551e-05]
	Learning Rate: 3.45511e-05
	LOSS [training: 0.03648651568470838 | validation: 0.1424262751381372]
	TIME [epoch: 12.7 sec]
EPOCH 1652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034629342596799985		[learning rate: 3.4429e-05]
	Learning Rate: 3.44289e-05
	LOSS [training: 0.034629342596799985 | validation: 0.13703166581395163]
	TIME [epoch: 12.7 sec]
EPOCH 1653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03505440486729184		[learning rate: 3.4307e-05]
	Learning Rate: 3.43072e-05
	LOSS [training: 0.03505440486729184 | validation: 0.12814697962303834]
	TIME [epoch: 12.7 sec]
EPOCH 1654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035477607540727506		[learning rate: 3.4186e-05]
	Learning Rate: 3.41858e-05
	LOSS [training: 0.035477607540727506 | validation: 0.12940601516862893]
	TIME [epoch: 12.7 sec]
EPOCH 1655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.032156125379487824		[learning rate: 3.4065e-05]
	Learning Rate: 3.4065e-05
	LOSS [training: 0.032156125379487824 | validation: 0.12638921264775096]
	TIME [epoch: 12.7 sec]
EPOCH 1656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03647111059894674		[learning rate: 3.3944e-05]
	Learning Rate: 3.39445e-05
	LOSS [training: 0.03647111059894674 | validation: 0.1507115642117121]
	TIME [epoch: 12.7 sec]
EPOCH 1657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03612314292848713		[learning rate: 3.3824e-05]
	Learning Rate: 3.38245e-05
	LOSS [training: 0.03612314292848713 | validation: 0.15069605552630616]
	TIME [epoch: 12.7 sec]
EPOCH 1658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035477459761575916		[learning rate: 3.3705e-05]
	Learning Rate: 3.37049e-05
	LOSS [training: 0.035477459761575916 | validation: 0.1616624698396946]
	TIME [epoch: 12.7 sec]
EPOCH 1659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03258308321190599		[learning rate: 3.3586e-05]
	Learning Rate: 3.35857e-05
	LOSS [training: 0.03258308321190599 | validation: 0.1475878155097714]
	TIME [epoch: 12.7 sec]
EPOCH 1660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036454902078521166		[learning rate: 3.3467e-05]
	Learning Rate: 3.34669e-05
	LOSS [training: 0.036454902078521166 | validation: 0.13112820951708323]
	TIME [epoch: 12.7 sec]
EPOCH 1661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034954494718012236		[learning rate: 3.3349e-05]
	Learning Rate: 3.33486e-05
	LOSS [training: 0.034954494718012236 | validation: 0.13277842348553734]
	TIME [epoch: 12.7 sec]
EPOCH 1662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03477466234354506		[learning rate: 3.3231e-05]
	Learning Rate: 3.32306e-05
	LOSS [training: 0.03477466234354506 | validation: 0.1338251820056441]
	TIME [epoch: 12.7 sec]
EPOCH 1663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03531172645981659		[learning rate: 3.3113e-05]
	Learning Rate: 3.31131e-05
	LOSS [training: 0.03531172645981659 | validation: 0.15609685814606694]
	TIME [epoch: 12.7 sec]
EPOCH 1664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035462575641441466		[learning rate: 3.2996e-05]
	Learning Rate: 3.2996e-05
	LOSS [training: 0.035462575641441466 | validation: 0.14953715920922167]
	TIME [epoch: 12.7 sec]
EPOCH 1665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03607982915372953		[learning rate: 3.2879e-05]
	Learning Rate: 3.28794e-05
	LOSS [training: 0.03607982915372953 | validation: 0.14208410818763387]
	TIME [epoch: 12.7 sec]
EPOCH 1666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663934611683789		[learning rate: 3.2763e-05]
	Learning Rate: 3.27631e-05
	LOSS [training: 0.03663934611683789 | validation: 0.14031006795784673]
	TIME [epoch: 12.7 sec]
EPOCH 1667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03662079787547862		[learning rate: 3.2647e-05]
	Learning Rate: 3.26472e-05
	LOSS [training: 0.03662079787547862 | validation: 0.1515533804584282]
	TIME [epoch: 12.7 sec]
EPOCH 1668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03617628006545344		[learning rate: 3.2532e-05]
	Learning Rate: 3.25318e-05
	LOSS [training: 0.03617628006545344 | validation: 0.14990134956551968]
	TIME [epoch: 12.7 sec]
EPOCH 1669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035265572248600156		[learning rate: 3.2417e-05]
	Learning Rate: 3.24167e-05
	LOSS [training: 0.035265572248600156 | validation: 0.1459286665654663]
	TIME [epoch: 12.7 sec]
EPOCH 1670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035592180906067986		[learning rate: 3.2302e-05]
	Learning Rate: 3.23021e-05
	LOSS [training: 0.035592180906067986 | validation: 0.1354436767395224]
	TIME [epoch: 12.7 sec]
EPOCH 1671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03693956514007665		[learning rate: 3.2188e-05]
	Learning Rate: 3.21879e-05
	LOSS [training: 0.03693956514007665 | validation: 0.1448249830678637]
	TIME [epoch: 12.7 sec]
EPOCH 1672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03873707781757015		[learning rate: 3.2074e-05]
	Learning Rate: 3.20741e-05
	LOSS [training: 0.03873707781757015 | validation: 0.13394632678857898]
	TIME [epoch: 12.7 sec]
EPOCH 1673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036549006704123914		[learning rate: 3.1961e-05]
	Learning Rate: 3.19606e-05
	LOSS [training: 0.036549006704123914 | validation: 0.14634352367394823]
	TIME [epoch: 12.7 sec]
EPOCH 1674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03454304752869956		[learning rate: 3.1848e-05]
	Learning Rate: 3.18476e-05
	LOSS [training: 0.03454304752869956 | validation: 0.14114840484304086]
	TIME [epoch: 12.7 sec]
EPOCH 1675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03342327952639096		[learning rate: 3.1735e-05]
	Learning Rate: 3.1735e-05
	LOSS [training: 0.03342327952639096 | validation: 0.1384279206139745]
	TIME [epoch: 12.7 sec]
EPOCH 1676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0345871249664561		[learning rate: 3.1623e-05]
	Learning Rate: 3.16228e-05
	LOSS [training: 0.0345871249664561 | validation: 0.12978491735691597]
	TIME [epoch: 12.7 sec]
EPOCH 1677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03519951993117772		[learning rate: 3.1511e-05]
	Learning Rate: 3.1511e-05
	LOSS [training: 0.03519951993117772 | validation: 0.1326177906853272]
	TIME [epoch: 12.7 sec]
EPOCH 1678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03612413142739498		[learning rate: 3.14e-05]
	Learning Rate: 3.13995e-05
	LOSS [training: 0.03612413142739498 | validation: 0.13896602902999006]
	TIME [epoch: 12.7 sec]
EPOCH 1679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0358000682096156		[learning rate: 3.1288e-05]
	Learning Rate: 3.12885e-05
	LOSS [training: 0.0358000682096156 | validation: 0.1480925769774141]
	TIME [epoch: 12.7 sec]
EPOCH 1680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03614063167904318		[learning rate: 3.1178e-05]
	Learning Rate: 3.11779e-05
	LOSS [training: 0.03614063167904318 | validation: 0.14465175535713218]
	TIME [epoch: 12.7 sec]
EPOCH 1681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035669342475547296		[learning rate: 3.1068e-05]
	Learning Rate: 3.10676e-05
	LOSS [training: 0.035669342475547296 | validation: 0.13546958618294364]
	TIME [epoch: 12.7 sec]
EPOCH 1682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036153112127219285		[learning rate: 3.0958e-05]
	Learning Rate: 3.09577e-05
	LOSS [training: 0.036153112127219285 | validation: 0.13646966336804534]
	TIME [epoch: 12.7 sec]
EPOCH 1683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035194331806765275		[learning rate: 3.0848e-05]
	Learning Rate: 3.08483e-05
	LOSS [training: 0.035194331806765275 | validation: 0.14090768040016033]
	TIME [epoch: 12.7 sec]
EPOCH 1684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03566699809137716		[learning rate: 3.0739e-05]
	Learning Rate: 3.07392e-05
	LOSS [training: 0.03566699809137716 | validation: 0.12796731207049206]
	TIME [epoch: 12.7 sec]
EPOCH 1685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03580063556186546		[learning rate: 3.063e-05]
	Learning Rate: 3.06305e-05
	LOSS [training: 0.03580063556186546 | validation: 0.14702364485899544]
	TIME [epoch: 12.7 sec]
EPOCH 1686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03707001278964051		[learning rate: 3.0522e-05]
	Learning Rate: 3.05222e-05
	LOSS [training: 0.03707001278964051 | validation: 0.1568142861327686]
	TIME [epoch: 12.7 sec]
EPOCH 1687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03676475652748618		[learning rate: 3.0414e-05]
	Learning Rate: 3.04142e-05
	LOSS [training: 0.03676475652748618 | validation: 0.14208295787593314]
	TIME [epoch: 12.7 sec]
EPOCH 1688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03663667556598922		[learning rate: 3.0307e-05]
	Learning Rate: 3.03067e-05
	LOSS [training: 0.03663667556598922 | validation: 0.1384764985474134]
	TIME [epoch: 12.7 sec]
EPOCH 1689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03315258536486024		[learning rate: 3.02e-05]
	Learning Rate: 3.01995e-05
	LOSS [training: 0.03315258536486024 | validation: 0.13427874367245618]
	TIME [epoch: 12.7 sec]
EPOCH 1690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03503352820008815		[learning rate: 3.0093e-05]
	Learning Rate: 3.00927e-05
	LOSS [training: 0.03503352820008815 | validation: 0.1259781920903561]
	TIME [epoch: 12.7 sec]
EPOCH 1691/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03480926781115749		[learning rate: 2.9986e-05]
	Learning Rate: 2.99863e-05
	LOSS [training: 0.03480926781115749 | validation: 0.13923134156137912]
	TIME [epoch: 12.7 sec]
EPOCH 1692/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03478024544471994		[learning rate: 2.988e-05]
	Learning Rate: 2.98803e-05
	LOSS [training: 0.03478024544471994 | validation: 0.1363204363273008]
	TIME [epoch: 12.7 sec]
EPOCH 1693/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035598963284406884		[learning rate: 2.9775e-05]
	Learning Rate: 2.97746e-05
	LOSS [training: 0.035598963284406884 | validation: 0.13595916175085934]
	TIME [epoch: 12.7 sec]
EPOCH 1694/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03689450393794989		[learning rate: 2.9669e-05]
	Learning Rate: 2.96693e-05
	LOSS [training: 0.03689450393794989 | validation: 0.15081748001081402]
	TIME [epoch: 12.7 sec]
EPOCH 1695/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036237364172574886		[learning rate: 2.9564e-05]
	Learning Rate: 2.95644e-05
	LOSS [training: 0.036237364172574886 | validation: 0.14273685010401863]
	TIME [epoch: 12.7 sec]
EPOCH 1696/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0370907293938463		[learning rate: 2.946e-05]
	Learning Rate: 2.94599e-05
	LOSS [training: 0.0370907293938463 | validation: 0.13783449801297434]
	TIME [epoch: 12.7 sec]
EPOCH 1697/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035237087361920126		[learning rate: 2.9356e-05]
	Learning Rate: 2.93557e-05
	LOSS [training: 0.035237087361920126 | validation: 0.15055988482389526]
	TIME [epoch: 12.7 sec]
EPOCH 1698/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03590400876969521		[learning rate: 2.9252e-05]
	Learning Rate: 2.92519e-05
	LOSS [training: 0.03590400876969521 | validation: 0.13274067077416177]
	TIME [epoch: 12.7 sec]
EPOCH 1699/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03421320663926468		[learning rate: 2.9148e-05]
	Learning Rate: 2.91485e-05
	LOSS [training: 0.03421320663926468 | validation: 0.14608988663645303]
	TIME [epoch: 12.7 sec]
EPOCH 1700/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.036560420181715056		[learning rate: 2.9045e-05]
	Learning Rate: 2.90454e-05
	LOSS [training: 0.036560420181715056 | validation: 0.14697532402267602]
	TIME [epoch: 12.7 sec]
EPOCH 1701/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03512372676864281		[learning rate: 2.8943e-05]
	Learning Rate: 2.89427e-05
	LOSS [training: 0.03512372676864281 | validation: 0.1412144673808583]
	TIME [epoch: 12.7 sec]
EPOCH 1702/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035934286324831956		[learning rate: 2.884e-05]
	Learning Rate: 2.88403e-05
	LOSS [training: 0.035934286324831956 | validation: 0.1478889744928304]
	TIME [epoch: 12.7 sec]
EPOCH 1703/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037469821062938884		[learning rate: 2.8738e-05]
	Learning Rate: 2.87383e-05
	LOSS [training: 0.037469821062938884 | validation: 0.14621391751113424]
	TIME [epoch: 12.7 sec]
EPOCH 1704/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.035441801597159296		[learning rate: 2.8637e-05]
	Learning Rate: 2.86367e-05
	LOSS [training: 0.035441801597159296 | validation: 0.12607045934682634]
	TIME [epoch: 12.7 sec]
EPOCH 1705/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0334925546541549		[learning rate: 2.8535e-05]
	Learning Rate: 2.85355e-05
	LOSS [training: 0.0334925546541549 | validation: 0.13291137861500543]
	TIME [epoch: 12.7 sec]
EPOCH 1706/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03602456259199403		[learning rate: 2.8435e-05]
	Learning Rate: 2.84345e-05
	LOSS [training: 0.03602456259199403 | validation: 0.14574426793317993]
	TIME [epoch: 12.7 sec]
EPOCH 1707/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03578682366872127		[learning rate: 2.8334e-05]
	Learning Rate: 2.8334e-05
	LOSS [training: 0.03578682366872127 | validation: 0.14697529433166343]
	TIME [epoch: 12.7 sec]
EPOCH 1708/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03353898715525008		[learning rate: 2.8234e-05]
	Learning Rate: 2.82338e-05
	LOSS [training: 0.03353898715525008 | validation: 0.14727254740341003]
	TIME [epoch: 12.7 sec]
	Saving model to: out/model_training/model_phi1_4b_v_mmd1_20241105_152741/states/model_phi1_4b_v_mmd1_1708.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 14001.272 seconds.
