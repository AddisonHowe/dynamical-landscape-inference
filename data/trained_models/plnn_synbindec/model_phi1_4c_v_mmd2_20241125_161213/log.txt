Args:
Namespace(name='model_phi1_4c_v_mmd2', outdir='out/model_training/model_phi1_4c_v_mmd2', training_data='data/training_data/data_phi1_4c/training', validation_data='data/training_data/data_phi1_4c/validation', model_type='deep_phi', nsims_training=None, nsims_validation=None, num_epochs=2000, passes_per_epoch=1, batch_size=250, patience=100, min_epochs=500, report_every=10, reduce_dt_on_nan=False, dt_reduction_factor=0.5, reduce_cf_on_nan=True, cf_reduction_factor=0.1, nan_max_attempts=4, quadratic_a=1.0, quadratic_b=1.0, ndims=2, nparams=2, nsigs=2, ncells=200, ncells_sample=0, model_do_sample=True, dt=0.1, dt_schedule='stepped', dt_schedule_bounds=[200, 500, 1000], dt_schedule_scales=[0.5, 0.5, 0.5], signal_function='sigmoid', solver='heun', confine=True, confinement_factor=0.1, phi_hidden_dims=[16, 32, 32, 16], phi_hidden_acts=['softplus'], phi_final_act='None', phi_layer_normalize=False, tilt_hidden_dims=[0], tilt_hidden_acts=['None'], tilt_final_act='None', tilt_layer_normalize=False, infer_metric=False, metric_hidden_dims=[8, 8, 8, 8], metric_hidden_acts=['softplus', 'softplus', 'softplus', 'softplus'], metric_final_act=None, metric_layer_normalize=False, fix_noise=False, sigma=0.05, init_phi_weights_method='xavier_uniform', init_phi_weights_args=[], init_phi_bias_method='constant', init_phi_bias_args=[0.0], init_tilt_weights_method='xavier_uniform', init_tilt_weights_args=[], init_tilt_bias_method='constant', init_tilt_bias_args=[0.0], init_metric_weights_method='xavier_uniform', init_metric_weights_args=[], init_metric_bias_method=None, init_metric_bias_args=None, loss='mmd', kernel='multiscale', bw_range=[0.2, 0.5, 0.9, 1.3], optimizer='rms', momentum=0.5, weight_decay=0.9, clip=1.0, lr_schedule='constant', learning_rate=0.01, nepochs_warmup=50, nepochs_decay=-1, final_learning_rate=0.01, peak_learning_rate=0.02, warmup_cosine_decay_exponent=1.0, plot_radius=4, plot=True, dtype='float64', seed=0, timestamp=True, save_all=False, enforce_gpu=True, continuation=None)

Using seed: 1721523446

Training model...

Saving initial model state to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_0.pth
EPOCH 1/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.975069943887686		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.975069943887686 | validation: 4.317973140765796]
	TIME [epoch: 181 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1.pth
	Model improved!!!
EPOCH 2/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.371098502156229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.371098502156229 | validation: 3.4148304832893164]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_2.pth
	Model improved!!!
EPOCH 3/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.476172898839113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.476172898839113 | validation: 3.3295759681729744]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_3.pth
	Model improved!!!
EPOCH 4/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.440298954553305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.440298954553305 | validation: 3.793716179199359]
	TIME [epoch: 2.64 sec]
EPOCH 5/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.032630621565386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.032630621565386 | validation: 3.2504040320444894]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_5.pth
	Model improved!!!
EPOCH 6/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2051181248832155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2051181248832155 | validation: 2.5756381040386884]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_6.pth
	Model improved!!!
EPOCH 7/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.465753937898635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.465753937898635 | validation: 2.551805962518802]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_7.pth
	Model improved!!!
EPOCH 8/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.779805841798942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.779805841798942 | validation: 2.0175162484172717]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_8.pth
	Model improved!!!
EPOCH 9/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.0047217347357233		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.0047217347357233 | validation: 1.9559363353292314]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_9.pth
	Model improved!!!
EPOCH 10/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8427637139853703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8427637139853703 | validation: 1.6188789918674769]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_10.pth
	Model improved!!!
EPOCH 11/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5343452315576687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5343452315576687 | validation: 1.5658829704809691]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_11.pth
	Model improved!!!
EPOCH 12/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3894452869176803		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3894452869176803 | validation: 1.6161192217988785]
	TIME [epoch: 2.62 sec]
EPOCH 13/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4954482735533556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4954482735533556 | validation: 1.8302442770369525]
	TIME [epoch: 2.62 sec]
EPOCH 14/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6759795478665314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6759795478665314 | validation: 1.391578203816851]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_14.pth
	Model improved!!!
EPOCH 15/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2991924702998738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2991924702998738 | validation: 1.2716769983406113]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_15.pth
	Model improved!!!
EPOCH 16/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2928972584524474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2928972584524474 | validation: 1.2241395180130539]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_16.pth
	Model improved!!!
EPOCH 17/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3280486074965832		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3280486074965832 | validation: 1.550162024073473]
	TIME [epoch: 2.63 sec]
EPOCH 18/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6103133098071418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6103133098071418 | validation: 1.0539018621159955]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_18.pth
	Model improved!!!
EPOCH 19/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.200733678176066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.200733678176066 | validation: 1.120654426507448]
	TIME [epoch: 2.63 sec]
EPOCH 20/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2899936146957365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2899936146957365 | validation: 1.000146933293438]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_20.pth
	Model improved!!!
EPOCH 21/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1538489396916902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1538489396916902 | validation: 1.0989043279127246]
	TIME [epoch: 2.63 sec]
EPOCH 22/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1570801350621438		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1570801350621438 | validation: 1.0375204461673924]
	TIME [epoch: 2.63 sec]
EPOCH 23/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0745000089973185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0745000089973185 | validation: 1.0715913545937135]
	TIME [epoch: 2.62 sec]
EPOCH 24/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.065566131667697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.065566131667697 | validation: 1.0515938936489075]
	TIME [epoch: 2.62 sec]
EPOCH 25/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0526337775055457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0526337775055457 | validation: 1.0387140606911476]
	TIME [epoch: 2.62 sec]
EPOCH 26/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0532223078709329		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0532223078709329 | validation: 0.946358766169984]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_26.pth
	Model improved!!!
EPOCH 27/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0966735251221709		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0966735251221709 | validation: 0.9386162010989282]
	TIME [epoch: 2.64 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_27.pth
	Model improved!!!
EPOCH 28/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.074945599665297		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.074945599665297 | validation: 1.0172283424418687]
	TIME [epoch: 2.64 sec]
EPOCH 29/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1374978872888204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1374978872888204 | validation: 1.1029113834528546]
	TIME [epoch: 2.63 sec]
EPOCH 30/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1730519295483635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1730519295483635 | validation: 0.9625301020670292]
	TIME [epoch: 2.63 sec]
EPOCH 31/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0909774662627112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0909774662627112 | validation: 1.0301313617438617]
	TIME [epoch: 2.63 sec]
EPOCH 32/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0390083660232188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0390083660232188 | validation: 0.9320943449261098]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_32.pth
	Model improved!!!
EPOCH 33/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0187521882440316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0187521882440316 | validation: 0.9285146949361313]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_33.pth
	Model improved!!!
EPOCH 34/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0080102140896907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0080102140896907 | validation: 0.9741654994971815]
	TIME [epoch: 2.62 sec]
EPOCH 35/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9899992592680227		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9899992592680227 | validation: 0.9287225522084537]
	TIME [epoch: 2.63 sec]
EPOCH 36/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9988094995638295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9988094995638295 | validation: 0.9672579772005121]
	TIME [epoch: 2.62 sec]
EPOCH 37/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0067738647307007		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0067738647307007 | validation: 0.971539951230237]
	TIME [epoch: 2.62 sec]
EPOCH 38/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0699989863712613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0699989863712613 | validation: 1.039853252096074]
	TIME [epoch: 2.62 sec]
EPOCH 39/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0333796063609229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0333796063609229 | validation: 0.9481156489208008]
	TIME [epoch: 2.62 sec]
EPOCH 40/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0440459108034097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0440459108034097 | validation: 1.0250548429132162]
	TIME [epoch: 2.62 sec]
EPOCH 41/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0195247685726694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0195247685726694 | validation: 0.9300099636502503]
	TIME [epoch: 2.62 sec]
EPOCH 42/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9939208602759126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9939208602759126 | validation: 0.9002894491933706]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_42.pth
	Model improved!!!
EPOCH 43/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9752560352484616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9752560352484616 | validation: 0.9348840226932777]
	TIME [epoch: 2.62 sec]
EPOCH 44/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1051424182361331		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1051424182361331 | validation: 1.0262132447829577]
	TIME [epoch: 2.61 sec]
EPOCH 45/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.08700228007721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.08700228007721 | validation: 0.8955315644460431]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_45.pth
	Model improved!!!
EPOCH 46/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9582225679489881		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9582225679489881 | validation: 0.8846308967295292]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_46.pth
	Model improved!!!
EPOCH 47/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9593788777532393		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9593788777532393 | validation: 0.8747254927637962]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_47.pth
	Model improved!!!
EPOCH 48/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9785402759898182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9785402759898182 | validation: 0.886756526229342]
	TIME [epoch: 2.63 sec]
EPOCH 49/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9710254668161173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9710254668161173 | validation: 0.8672609819763681]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_49.pth
	Model improved!!!
EPOCH 50/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9707749013657377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9707749013657377 | validation: 0.8425532583309328]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_50.pth
	Model improved!!!
EPOCH 51/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9557522609446708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9557522609446708 | validation: 0.9363993696566735]
	TIME [epoch: 2.63 sec]
EPOCH 52/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0426110370999775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0426110370999775 | validation: 0.9021175807999189]
	TIME [epoch: 2.62 sec]
EPOCH 53/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.022433514845721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.022433514845721 | validation: 0.8303897428632153]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_53.pth
	Model improved!!!
EPOCH 54/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9328737363076282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9328737363076282 | validation: 0.8440402711543528]
	TIME [epoch: 2.62 sec]
EPOCH 55/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.965677388839205		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.965677388839205 | validation: 0.8332985171096713]
	TIME [epoch: 2.62 sec]
EPOCH 56/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9433534690239925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9433534690239925 | validation: 0.806412728206982]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_56.pth
	Model improved!!!
EPOCH 57/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.918254882536063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.918254882536063 | validation: 0.8609828834513658]
	TIME [epoch: 2.63 sec]
EPOCH 58/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9191740729572229		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9191740729572229 | validation: 0.8185304371947768]
	TIME [epoch: 2.61 sec]
EPOCH 59/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9134981087330534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9134981087330534 | validation: 0.8549092355384907]
	TIME [epoch: 2.62 sec]
EPOCH 60/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9205940070153389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9205940070153389 | validation: 0.8405096621047101]
	TIME [epoch: 2.61 sec]
EPOCH 61/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.965755590302079		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.965755590302079 | validation: 0.9102626507348192]
	TIME [epoch: 2.61 sec]
EPOCH 62/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9876970626607076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9876970626607076 | validation: 0.8072461800333294]
	TIME [epoch: 2.62 sec]
EPOCH 63/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9347290830744501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9347290830744501 | validation: 0.7809378638101003]
	TIME [epoch: 2.61 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_63.pth
	Model improved!!!
EPOCH 64/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8940623957509692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8940623957509692 | validation: 0.8218088286530261]
	TIME [epoch: 2.63 sec]
EPOCH 65/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9049847395281851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9049847395281851 | validation: 0.8137486549012989]
	TIME [epoch: 2.63 sec]
EPOCH 66/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9431482118565214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9431482118565214 | validation: 1.0213469036028993]
	TIME [epoch: 2.63 sec]
EPOCH 67/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.117156273702458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.117156273702458 | validation: 0.9005279698326529]
	TIME [epoch: 2.61 sec]
EPOCH 68/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0613936045181884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0613936045181884 | validation: 0.7590714856015335]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_68.pth
	Model improved!!!
EPOCH 69/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953331710872087		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8953331710872087 | validation: 0.8536333327827189]
	TIME [epoch: 2.62 sec]
EPOCH 70/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9592070300981402		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9592070300981402 | validation: 0.7552977337846141]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_70.pth
	Model improved!!!
EPOCH 71/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8916241108133691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8916241108133691 | validation: 0.7921358105773726]
	TIME [epoch: 2.63 sec]
EPOCH 72/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9143249865634493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9143249865634493 | validation: 0.8015344279853885]
	TIME [epoch: 2.61 sec]
EPOCH 73/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9026950180208738		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9026950180208738 | validation: 0.7675672953233657]
	TIME [epoch: 2.62 sec]
EPOCH 74/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8874140108003006		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8874140108003006 | validation: 0.7675302317764499]
	TIME [epoch: 2.61 sec]
EPOCH 75/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.89909859350376		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.89909859350376 | validation: 0.8134471965550788]
	TIME [epoch: 2.63 sec]
EPOCH 76/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9515083475765576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9515083475765576 | validation: 0.7630102217076307]
	TIME [epoch: 2.61 sec]
EPOCH 77/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8815472667995843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8815472667995843 | validation: 0.7989369495378724]
	TIME [epoch: 2.62 sec]
EPOCH 78/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202600472269049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202600472269049 | validation: 0.891665088647272]
	TIME [epoch: 2.61 sec]
EPOCH 79/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9994211629251308		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9994211629251308 | validation: 0.749541192263054]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_79.pth
	Model improved!!!
EPOCH 80/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8760698938378226		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8760698938378226 | validation: 0.839060605767648]
	TIME [epoch: 2.62 sec]
EPOCH 81/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9936360028644109		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9936360028644109 | validation: 0.8180106494718258]
	TIME [epoch: 2.61 sec]
EPOCH 82/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.935461483362975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.935461483362975 | validation: 0.7930187860164839]
	TIME [epoch: 2.62 sec]
EPOCH 83/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.897255001270474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.897255001270474 | validation: 0.7951650757467666]
	TIME [epoch: 2.62 sec]
EPOCH 84/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.958124150234097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.958124150234097 | validation: 0.8246046584256487]
	TIME [epoch: 2.62 sec]
EPOCH 85/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9299817874346301		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9299817874346301 | validation: 0.7844275663512315]
	TIME [epoch: 2.62 sec]
EPOCH 86/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9202976035801703		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9202976035801703 | validation: 0.7635083741490979]
	TIME [epoch: 2.62 sec]
EPOCH 87/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8848244553822405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8848244553822405 | validation: 0.9128719380371205]
	TIME [epoch: 2.62 sec]
EPOCH 88/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0044833559651662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0044833559651662 | validation: 0.7560854692806687]
	TIME [epoch: 2.62 sec]
EPOCH 89/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.887956919469452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.887956919469452 | validation: 0.7801469443274895]
	TIME [epoch: 2.62 sec]
EPOCH 90/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9077312201823918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9077312201823918 | validation: 0.798612404444306]
	TIME [epoch: 2.62 sec]
EPOCH 91/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157568350957901		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9157568350957901 | validation: 0.7358873183839045]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_91.pth
	Model improved!!!
EPOCH 92/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8912793677923915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8912793677923915 | validation: 0.7526791880083042]
	TIME [epoch: 2.62 sec]
EPOCH 93/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8828976669590628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8828976669590628 | validation: 0.7566885181395862]
	TIME [epoch: 2.61 sec]
EPOCH 94/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8891858909792442		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8891858909792442 | validation: 0.7630331502296522]
	TIME [epoch: 2.63 sec]
EPOCH 95/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8961767665410203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8961767665410203 | validation: 0.7640331472357452]
	TIME [epoch: 2.62 sec]
EPOCH 96/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8875327054074202		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8875327054074202 | validation: 0.7697692822475903]
	TIME [epoch: 2.63 sec]
EPOCH 97/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9045671333073191		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9045671333073191 | validation: 0.877457844588389]
	TIME [epoch: 2.61 sec]
EPOCH 98/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9522555625703821		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9522555625703821 | validation: 0.8592562031305804]
	TIME [epoch: 2.61 sec]
EPOCH 99/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0412958498851417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0412958498851417 | validation: 0.7493090611820007]
	TIME [epoch: 2.6 sec]
EPOCH 100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8862261484006416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8862261484006416 | validation: 0.7644168256018928]
	TIME [epoch: 2.62 sec]
EPOCH 101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9027840431312478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9027840431312478 | validation: 0.8150838430758567]
	TIME [epoch: 2.62 sec]
EPOCH 102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9353871874342642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9353871874342642 | validation: 0.7720897116670087]
	TIME [epoch: 2.64 sec]
EPOCH 103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8908314221125195		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8908314221125195 | validation: 0.7613182665226154]
	TIME [epoch: 2.63 sec]
EPOCH 104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8891698396554967		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8891698396554967 | validation: 0.7585625865585185]
	TIME [epoch: 2.63 sec]
EPOCH 105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860638680494595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8860638680494595 | validation: 0.7720199483275244]
	TIME [epoch: 2.62 sec]
EPOCH 106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873317896934211		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873317896934211 | validation: 0.7626728022461791]
	TIME [epoch: 2.62 sec]
EPOCH 107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171222865505808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9171222865505808 | validation: 0.8716271400731851]
	TIME [epoch: 2.62 sec]
EPOCH 108/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0156238581043682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0156238581043682 | validation: 0.7483909146664148]
	TIME [epoch: 2.63 sec]
EPOCH 109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.893571526783477		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.893571526783477 | validation: 0.8175921374890134]
	TIME [epoch: 2.62 sec]
EPOCH 110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.930140661116206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.930140661116206 | validation: 0.7973483703165638]
	TIME [epoch: 2.63 sec]
EPOCH 111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9470126571856694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9470126571856694 | validation: 0.753707210422002]
	TIME [epoch: 2.63 sec]
EPOCH 112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8934330205389034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8934330205389034 | validation: 0.7868507768118347]
	TIME [epoch: 2.63 sec]
EPOCH 113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9065468290407916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9065468290407916 | validation: 0.7406135038197145]
	TIME [epoch: 2.63 sec]
EPOCH 114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960377412401923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960377412401923 | validation: 0.7542193998511361]
	TIME [epoch: 2.64 sec]
EPOCH 115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9115932175713245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9115932175713245 | validation: 0.7773955303916952]
	TIME [epoch: 2.63 sec]
EPOCH 116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9338229637057711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9338229637057711 | validation: 0.7573273461788612]
	TIME [epoch: 2.64 sec]
EPOCH 117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9040818295278209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9040818295278209 | validation: 0.7863218168148474]
	TIME [epoch: 2.63 sec]
EPOCH 118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9442700693504057		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9442700693504057 | validation: 0.7789698852604702]
	TIME [epoch: 2.64 sec]
EPOCH 119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9325228534178482		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9325228534178482 | validation: 0.721865236751539]
	TIME [epoch: 2.62 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_119.pth
	Model improved!!!
EPOCH 120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8797336591971177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8797336591971177 | validation: 0.777680436276831]
	TIME [epoch: 2.64 sec]
EPOCH 121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8952768810849253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8952768810849253 | validation: 0.7704651893729241]
	TIME [epoch: 2.64 sec]
EPOCH 122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9171731838855334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9171731838855334 | validation: 0.7298231255916647]
	TIME [epoch: 2.65 sec]
EPOCH 123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8729585440604609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8729585440604609 | validation: 0.7693149091862947]
	TIME [epoch: 2.63 sec]
EPOCH 124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.903721782438814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.903721782438814 | validation: 0.7908434790140767]
	TIME [epoch: 2.65 sec]
EPOCH 125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9488966702798186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9488966702798186 | validation: 0.7321306859822384]
	TIME [epoch: 2.64 sec]
EPOCH 126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767936302886377		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767936302886377 | validation: 0.7771484063764338]
	TIME [epoch: 2.65 sec]
EPOCH 127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9075209309339511		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9075209309339511 | validation: 0.753745085756181]
	TIME [epoch: 2.63 sec]
EPOCH 128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8767626778770045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8767626778770045 | validation: 0.8887504211261483]
	TIME [epoch: 2.64 sec]
EPOCH 129/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0468455590143757		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0468455590143757 | validation: 0.8142131130328867]
	TIME [epoch: 2.63 sec]
EPOCH 130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9861251629975784		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9861251629975784 | validation: 0.7651285059650897]
	TIME [epoch: 2.65 sec]
EPOCH 131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9057350770068299		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9057350770068299 | validation: 0.7449714456003442]
	TIME [epoch: 2.63 sec]
EPOCH 132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.892652993177957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.892652993177957 | validation: 0.7601583478048624]
	TIME [epoch: 2.65 sec]
EPOCH 133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8942347766703277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8942347766703277 | validation: 0.7609852114355578]
	TIME [epoch: 2.64 sec]
EPOCH 134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8953670197331658		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8953670197331658 | validation: 0.7271822024439883]
	TIME [epoch: 2.65 sec]
EPOCH 135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750332338870018		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750332338870018 | validation: 0.7563835178147374]
	TIME [epoch: 2.63 sec]
EPOCH 136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8873702929539136		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8873702929539136 | validation: 0.7545263922718114]
	TIME [epoch: 2.65 sec]
EPOCH 137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005344161195095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005344161195095 | validation: 0.7504632244218609]
	TIME [epoch: 2.63 sec]
EPOCH 138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8998898010226245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8998898010226245 | validation: 0.74732294951669]
	TIME [epoch: 2.64 sec]
EPOCH 139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9112739472185396		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9112739472185396 | validation: 0.7460339595336717]
	TIME [epoch: 2.64 sec]
EPOCH 140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8922342764223167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8922342764223167 | validation: 0.7384953187744318]
	TIME [epoch: 2.65 sec]
EPOCH 141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8808250262071805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8808250262071805 | validation: 0.7408630388539051]
	TIME [epoch: 2.64 sec]
EPOCH 142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8791969474090447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8791969474090447 | validation: 0.7581770661998161]
	TIME [epoch: 2.64 sec]
EPOCH 143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9010869812357934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9010869812357934 | validation: 0.7325345986301532]
	TIME [epoch: 2.64 sec]
EPOCH 144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.873783401230258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.873783401230258 | validation: 0.7620082658904459]
	TIME [epoch: 2.65 sec]
EPOCH 145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.896221126171878		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.896221126171878 | validation: 0.7417602113958955]
	TIME [epoch: 2.64 sec]
EPOCH 146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8854136279238674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8854136279238674 | validation: 0.746319194443252]
	TIME [epoch: 2.64 sec]
EPOCH 147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.861761532029625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.861761532029625 | validation: 0.7320928175883354]
	TIME [epoch: 2.64 sec]
EPOCH 148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8876041711601161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8876041711601161 | validation: 0.7688818251600041]
	TIME [epoch: 2.67 sec]
EPOCH 149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9357712896139521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9357712896139521 | validation: 0.8070480499584143]
	TIME [epoch: 2.64 sec]
EPOCH 150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9494823931389695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9494823931389695 | validation: 0.8123726229051272]
	TIME [epoch: 2.65 sec]
EPOCH 151/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0011309943445765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0011309943445765 | validation: 0.7314374797270697]
	TIME [epoch: 2.64 sec]
EPOCH 152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8713093815705392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8713093815705392 | validation: 0.7897896559472515]
	TIME [epoch: 2.65 sec]
EPOCH 153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9462136750621742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9462136750621742 | validation: 0.7297425659663205]
	TIME [epoch: 2.65 sec]
EPOCH 154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8803666553711608		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8803666553711608 | validation: 0.7955757057244957]
	TIME [epoch: 2.64 sec]
EPOCH 155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9288979095875426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9288979095875426 | validation: 0.7353745447753587]
	TIME [epoch: 2.63 sec]
EPOCH 156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8785119693010518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8785119693010518 | validation: 0.7338457963550868]
	TIME [epoch: 2.65 sec]
EPOCH 157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8845587478399739		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8845587478399739 | validation: 0.738825849773014]
	TIME [epoch: 2.64 sec]
EPOCH 158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8913779124792782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8913779124792782 | validation: 0.7444470403988626]
	TIME [epoch: 2.65 sec]
EPOCH 159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8865656286866679		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8865656286866679 | validation: 0.7306573354236102]
	TIME [epoch: 2.64 sec]
EPOCH 160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8665200484378008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8665200484378008 | validation: 0.7455767242621303]
	TIME [epoch: 2.65 sec]
EPOCH 161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8727974296704512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8727974296704512 | validation: 0.7383378248430601]
	TIME [epoch: 2.63 sec]
EPOCH 162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8860756443655362		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8860756443655362 | validation: 0.7319989610462918]
	TIME [epoch: 2.64 sec]
EPOCH 163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8688147758926439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8688147758926439 | validation: 0.7335614223079039]
	TIME [epoch: 2.63 sec]
EPOCH 164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8673677567973723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8673677567973723 | validation: 0.7249633662385456]
	TIME [epoch: 2.64 sec]
EPOCH 165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8766811926293869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8766811926293869 | validation: 0.773514082455431]
	TIME [epoch: 2.64 sec]
EPOCH 166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9050726296161917		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9050726296161917 | validation: 0.8899479930695045]
	TIME [epoch: 2.64 sec]
EPOCH 167/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0746826365215492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0746826365215492 | validation: 0.7370134378085356]
	TIME [epoch: 2.63 sec]
EPOCH 168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8715344145910412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8715344145910412 | validation: 0.7550953850681326]
	TIME [epoch: 2.64 sec]
EPOCH 169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8973361726623409		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8973361726623409 | validation: 0.742937395797293]
	TIME [epoch: 2.62 sec]
EPOCH 170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8906226131882258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8906226131882258 | validation: 0.7529228005580694]
	TIME [epoch: 2.64 sec]
EPOCH 171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8630488509262975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8630488509262975 | validation: 0.73196336282028]
	TIME [epoch: 2.64 sec]
EPOCH 172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8884884636018572		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8884884636018572 | validation: 0.757516044997593]
	TIME [epoch: 2.64 sec]
EPOCH 173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8992143645867069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8992143645867069 | validation: 0.7323893192476189]
	TIME [epoch: 2.65 sec]
EPOCH 174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8663083161298581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8663083161298581 | validation: 0.7169101468043476]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_174.pth
	Model improved!!!
EPOCH 175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638786929059525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638786929059525 | validation: 0.7258619988024634]
	TIME [epoch: 2.65 sec]
EPOCH 176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609871202219169		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609871202219169 | validation: 0.7795436187073723]
	TIME [epoch: 2.65 sec]
EPOCH 177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9507197899707114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9507197899707114 | validation: 0.7381982344552505]
	TIME [epoch: 2.64 sec]
EPOCH 178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8929441394886318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8929441394886318 | validation: 0.7153675848297859]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_178.pth
	Model improved!!!
EPOCH 179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642357306547188		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642357306547188 | validation: 0.7661692959387436]
	TIME [epoch: 2.64 sec]
EPOCH 180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.871424062065566		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.871424062065566 | validation: 0.7525717673529047]
	TIME [epoch: 2.65 sec]
EPOCH 181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925112174680657		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8925112174680657 | validation: 0.7121426478821662]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_181.pth
	Model improved!!!
EPOCH 182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8639383073374064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8639383073374064 | validation: 0.7304273230571662]
	TIME [epoch: 2.65 sec]
EPOCH 183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8555329315788606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8555329315788606 | validation: 0.7620916124314252]
	TIME [epoch: 2.63 sec]
EPOCH 184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9069925331592326		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9069925331592326 | validation: 0.8588540169660757]
	TIME [epoch: 2.64 sec]
EPOCH 185/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0506691607337029		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0506691607337029 | validation: 0.8436775987443618]
	TIME [epoch: 2.63 sec]
EPOCH 186/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0182228669269793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0182228669269793 | validation: 0.7247257844369828]
	TIME [epoch: 2.64 sec]
EPOCH 187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8935489857097193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8935489857097193 | validation: 0.8096436806418055]
	TIME [epoch: 2.64 sec]
EPOCH 188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9786130294941778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9786130294941778 | validation: 0.7139069310216204]
	TIME [epoch: 2.64 sec]
EPOCH 189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605164660901747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605164660901747 | validation: 0.7152744019926885]
	TIME [epoch: 2.64 sec]
EPOCH 190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8607701614128879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8607701614128879 | validation: 0.712779559445553]
	TIME [epoch: 2.64 sec]
EPOCH 191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8533030465553898		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8533030465553898 | validation: 0.7287660178379456]
	TIME [epoch: 2.64 sec]
EPOCH 192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8847496821335332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8847496821335332 | validation: 0.7145052569078267]
	TIME [epoch: 2.65 sec]
EPOCH 193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8565500249848171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8565500249848171 | validation: 0.7048656477190027]
	TIME [epoch: 2.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_193.pth
	Model improved!!!
EPOCH 194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506787859085817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8506787859085817 | validation: 0.7048054789454249]
	TIME [epoch: 2.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_194.pth
	Model improved!!!
EPOCH 195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8586281524532319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8586281524532319 | validation: 0.7230763021745782]
	TIME [epoch: 2.63 sec]
EPOCH 196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8465678670980455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8465678670980455 | validation: 4.470395353155045]
	TIME [epoch: 2.61 sec]
EPOCH 197/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.505254000866066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.505254000866066 | validation: 0.9901823229058988]
	TIME [epoch: 2.63 sec]
EPOCH 198/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1121881969994347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1121881969994347 | validation: 0.9591677756813799]
	TIME [epoch: 2.62 sec]
EPOCH 199/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0699980820995163		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0699980820995163 | validation: 0.871490576734619]
	TIME [epoch: 2.62 sec]
EPOCH 200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9622314825938886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9622314825938886 | validation: 0.8161987156319862]
	TIME [epoch: 2.62 sec]
EPOCH 201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9244663421820587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9244663421820587 | validation: 0.7555614440342184]
	TIME [epoch: 190 sec]
EPOCH 202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8655086491917541		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8655086491917541 | validation: 0.7264834601873613]
	TIME [epoch: 5.68 sec]
EPOCH 203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8575491590783364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8575491590783364 | validation: 0.7351541393825392]
	TIME [epoch: 5.7 sec]
EPOCH 204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724407535843176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724407535843176 | validation: 0.7098467682801615]
	TIME [epoch: 5.68 sec]
EPOCH 205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8636331293655843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8636331293655843 | validation: 0.7210169531626921]
	TIME [epoch: 5.67 sec]
EPOCH 206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8750729536921884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8750729536921884 | validation: 0.7036722046380297]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_206.pth
	Model improved!!!
EPOCH 207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8598531807587844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8598531807587844 | validation: 0.7152124251116071]
	TIME [epoch: 5.7 sec]
EPOCH 208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8499699870766203		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8499699870766203 | validation: 0.7218938456279149]
	TIME [epoch: 5.67 sec]
EPOCH 209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8659300670741805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8659300670741805 | validation: 0.7155687230376023]
	TIME [epoch: 5.69 sec]
EPOCH 210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8501455134154062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8501455134154062 | validation: 0.7106958664694111]
	TIME [epoch: 5.68 sec]
EPOCH 211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8645240394496186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8645240394496186 | validation: 0.7163105666472669]
	TIME [epoch: 5.67 sec]
EPOCH 212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8685730011794439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8685730011794439 | validation: 0.7177800234981035]
	TIME [epoch: 5.66 sec]
EPOCH 213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667840935676719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8667840935676719 | validation: 0.736745391091044]
	TIME [epoch: 5.66 sec]
EPOCH 214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8606870714762588		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8606870714762588 | validation: 0.7034951527351445]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_214.pth
	Model improved!!!
EPOCH 215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8680368615423092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8680368615423092 | validation: 0.7334743020202904]
	TIME [epoch: 5.67 sec]
EPOCH 216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9065618082388253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9065618082388253 | validation: 0.7276267757828334]
	TIME [epoch: 5.7 sec]
EPOCH 217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550600702793827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8550600702793827 | validation: 0.7189887827914916]
	TIME [epoch: 5.67 sec]
EPOCH 218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8714361303230951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8714361303230951 | validation: 0.7023914654473248]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_218.pth
	Model improved!!!
EPOCH 219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8637676311974002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8637676311974002 | validation: 0.7207037009868842]
	TIME [epoch: 5.7 sec]
EPOCH 220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8505603560634868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8505603560634868 | validation: 0.7078093058300539]
	TIME [epoch: 5.71 sec]
EPOCH 221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8506998180419092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8506998180419092 | validation: 0.70100997107637]
	TIME [epoch: 5.7 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_221.pth
	Model improved!!!
EPOCH 222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.850231021789956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.850231021789956 | validation: 0.7510231566552591]
	TIME [epoch: 5.69 sec]
EPOCH 223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8902679317372638		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8902679317372638 | validation: 0.722314259686555]
	TIME [epoch: 5.71 sec]
EPOCH 224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502370310173808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502370310173808 | validation: 0.7011817188391652]
	TIME [epoch: 5.69 sec]
EPOCH 225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8550216738675465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8550216738675465 | validation: 0.70790091780409]
	TIME [epoch: 5.71 sec]
EPOCH 226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8617121457524235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8617121457524235 | validation: 0.7199452530879086]
	TIME [epoch: 5.68 sec]
EPOCH 227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8627290688800912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8627290688800912 | validation: 0.7480910631725705]
	TIME [epoch: 5.68 sec]
EPOCH 228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8609919498555414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8609919498555414 | validation: 0.7175300606069827]
	TIME [epoch: 5.7 sec]
EPOCH 229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.868643441791538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.868643441791538 | validation: 0.749363078632095]
	TIME [epoch: 5.7 sec]
EPOCH 230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8925412124140268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8925412124140268 | validation: 0.7991107240227099]
	TIME [epoch: 5.7 sec]
EPOCH 231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9817566595981554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9817566595981554 | validation: 0.7084488443465734]
	TIME [epoch: 5.7 sec]
EPOCH 232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8504958559925073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8504958559925073 | validation: 0.7173761878683922]
	TIME [epoch: 5.71 sec]
EPOCH 233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8661851963995261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8661851963995261 | validation: 0.7347208540278137]
	TIME [epoch: 5.71 sec]
EPOCH 234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9159982288795152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9159982288795152 | validation: 0.741566616276299]
	TIME [epoch: 5.71 sec]
EPOCH 235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8428275107380929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8428275107380929 | validation: 0.6952039168640498]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_235.pth
	Model improved!!!
EPOCH 236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8378750380479534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8378750380479534 | validation: 0.676311866866635]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_236.pth
	Model improved!!!
EPOCH 237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8229474130626064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8229474130626064 | validation: 0.7034749933305475]
	TIME [epoch: 5.67 sec]
EPOCH 238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8319338567901781		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8319338567901781 | validation: 0.6891874349623595]
	TIME [epoch: 5.65 sec]
EPOCH 239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.811118497692913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.811118497692913 | validation: 0.7204103733625487]
	TIME [epoch: 5.64 sec]
EPOCH 240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8326997207006855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8326997207006855 | validation: 0.7243204196354919]
	TIME [epoch: 5.61 sec]
EPOCH 241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8119620990906304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8119620990906304 | validation: 1.37342090496925]
	TIME [epoch: 5.64 sec]
EPOCH 242/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7519259485104044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7519259485104044 | validation: 4.036672334194525]
	TIME [epoch: 5.65 sec]
EPOCH 243/2000:
	Training over batches...
		[batch 1/1] avg loss: 4.186982364393085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 4.186982364393085 | validation: 3.4296700661558095]
	TIME [epoch: 5.65 sec]
EPOCH 244/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.5908855521496235		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.5908855521496235 | validation: 1.862910773252059]
	TIME [epoch: 5.61 sec]
EPOCH 245/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.9707795218368733		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.9707795218368733 | validation: 1.5034148408225148]
	TIME [epoch: 5.63 sec]
EPOCH 246/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6358924511537312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6358924511537312 | validation: 1.1735089344847949]
	TIME [epoch: 5.62 sec]
EPOCH 247/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3099017471223267		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3099017471223267 | validation: 0.7567587554102087]
	TIME [epoch: 5.63 sec]
EPOCH 248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9157303113685612		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9157303113685612 | validation: 0.8396360324236778]
	TIME [epoch: 5.65 sec]
EPOCH 249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9546085286715525		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9546085286715525 | validation: 0.7811345070739406]
	TIME [epoch: 5.65 sec]
EPOCH 250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9043769215736305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9043769215736305 | validation: 0.7143848760557469]
	TIME [epoch: 5.64 sec]
EPOCH 251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8805032918932097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8805032918932097 | validation: 0.7234298696540403]
	TIME [epoch: 5.66 sec]
EPOCH 252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8915407469831004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8915407469831004 | validation: 0.7180329744399127]
	TIME [epoch: 5.64 sec]
EPOCH 253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8669971219488579		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8669971219488579 | validation: 0.726512368663583]
	TIME [epoch: 5.66 sec]
EPOCH 254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8693294905123407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8693294905123407 | validation: 0.7074337600825715]
	TIME [epoch: 5.66 sec]
EPOCH 255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8676320802489322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8676320802489322 | validation: 0.7226840581091899]
	TIME [epoch: 5.65 sec]
EPOCH 256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8642427812737458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8642427812737458 | validation: 0.7229106904007578]
	TIME [epoch: 5.64 sec]
EPOCH 257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.872429365830721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.872429365830721 | validation: 0.7137963412674857]
	TIME [epoch: 5.66 sec]
EPOCH 258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8615758478556884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8615758478556884 | validation: 0.7157409641005201]
	TIME [epoch: 5.67 sec]
EPOCH 259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8582115374694999		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8582115374694999 | validation: 0.6992618740182596]
	TIME [epoch: 5.67 sec]
EPOCH 260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.86446764301088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.86446764301088 | validation: 0.7063458709667286]
	TIME [epoch: 5.65 sec]
EPOCH 261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8573438754035623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8573438754035623 | validation: 0.7150769183330077]
	TIME [epoch: 5.67 sec]
EPOCH 262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8638924992524494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8638924992524494 | validation: 0.7160245697764431]
	TIME [epoch: 5.62 sec]
EPOCH 263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8543799077695178		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8543799077695178 | validation: 0.7146796169932307]
	TIME [epoch: 5.65 sec]
EPOCH 264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.857150581164405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.857150581164405 | validation: 0.6994525117331256]
	TIME [epoch: 5.65 sec]
EPOCH 265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8542689718349558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8542689718349558 | validation: 0.7020632775027282]
	TIME [epoch: 5.66 sec]
EPOCH 266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8605614328185734		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8605614328185734 | validation: 0.7112191636392806]
	TIME [epoch: 5.64 sec]
EPOCH 267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8649684231985546		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8649684231985546 | validation: 0.7024997289473994]
	TIME [epoch: 5.66 sec]
EPOCH 268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8512473982862071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8512473982862071 | validation: 0.7055701628597966]
	TIME [epoch: 5.65 sec]
EPOCH 269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8586363316900725		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8586363316900725 | validation: 0.7152508164025528]
	TIME [epoch: 5.65 sec]
EPOCH 270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8578766354776352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8578766354776352 | validation: 0.7080619237203417]
	TIME [epoch: 5.61 sec]
EPOCH 271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.860340837808445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.860340837808445 | validation: 0.7624753077766317]
	TIME [epoch: 5.67 sec]
EPOCH 272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8689759209085145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8689759209085145 | validation: 0.7311461340308274]
	TIME [epoch: 5.65 sec]
EPOCH 273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.913797987525573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.913797987525573 | validation: 0.7440167188065149]
	TIME [epoch: 5.67 sec]
EPOCH 274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.892142657493112		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.892142657493112 | validation: 0.69634791486146]
	TIME [epoch: 5.64 sec]
EPOCH 275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8582266280733862		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8582266280733862 | validation: 0.7075472006086949]
	TIME [epoch: 5.66 sec]
EPOCH 276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432502649620475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8432502649620475 | validation: 0.6954077200205769]
	TIME [epoch: 5.65 sec]
EPOCH 277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8369898591194931		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8369898591194931 | validation: 0.6873083447136048]
	TIME [epoch: 5.65 sec]
EPOCH 278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8288949921252635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8288949921252635 | validation: 0.6856862883910143]
	TIME [epoch: 5.65 sec]
EPOCH 279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8158509457304512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8158509457304512 | validation: 0.6865873548312105]
	TIME [epoch: 5.66 sec]
EPOCH 280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8299566485599793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8299566485599793 | validation: 1.0363663776681966]
	TIME [epoch: 5.64 sec]
EPOCH 281/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1619793793753785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1619793793753785 | validation: 0.7350850006159355]
	TIME [epoch: 5.66 sec]
EPOCH 282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8728209283893176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8728209283893176 | validation: 0.6987424152053022]
	TIME [epoch: 5.65 sec]
EPOCH 283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8539170439109434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8539170439109434 | validation: 0.7665248506897724]
	TIME [epoch: 5.65 sec]
EPOCH 284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8966168394999503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8966168394999503 | validation: 0.6988333589904832]
	TIME [epoch: 5.65 sec]
EPOCH 285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8464298123743794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8464298123743794 | validation: 0.6976359461144348]
	TIME [epoch: 5.64 sec]
EPOCH 286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82552481947174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82552481947174 | validation: 0.6852288497191705]
	TIME [epoch: 5.64 sec]
EPOCH 287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8081434757861241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8081434757861241 | validation: 0.6507491864092109]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_287.pth
	Model improved!!!
EPOCH 288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7863048226934152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7863048226934152 | validation: 4.6424884183002595]
	TIME [epoch: 5.64 sec]
EPOCH 289/2000:
	Training over batches...
		[batch 1/1] avg loss: 5.164422304845801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 5.164422304845801 | validation: 1.369706791336973]
	TIME [epoch: 5.66 sec]
EPOCH 290/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.54497471370827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.54497471370827 | validation: 1.1002851558409545]
	TIME [epoch: 5.61 sec]
EPOCH 291/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3516205945840363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3516205945840363 | validation: 1.0174598653310358]
	TIME [epoch: 5.66 sec]
EPOCH 292/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1102038772906124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1102038772906124 | validation: 1.0180146875955083]
	TIME [epoch: 5.6 sec]
EPOCH 293/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0973633285157691		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0973633285157691 | validation: 0.7962827276976397]
	TIME [epoch: 5.65 sec]
EPOCH 294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9020636800932951		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9020636800932951 | validation: 0.757790338831846]
	TIME [epoch: 5.65 sec]
EPOCH 295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8665690927189956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8665690927189956 | validation: 0.712704462568598]
	TIME [epoch: 5.66 sec]
EPOCH 296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8425320812524509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8425320812524509 | validation: 0.7011280510443281]
	TIME [epoch: 5.64 sec]
EPOCH 297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.82588405771049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.82588405771049 | validation: 0.6797469609819724]
	TIME [epoch: 5.67 sec]
EPOCH 298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8041703584557911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8041703584557911 | validation: 0.6770495511125735]
	TIME [epoch: 5.66 sec]
EPOCH 299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7801214000805334		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7801214000805334 | validation: 0.6499633628051795]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_299.pth
	Model improved!!!
EPOCH 300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292662554530527		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292662554530527 | validation: 0.7148609498567696]
	TIME [epoch: 5.65 sec]
EPOCH 301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7642592306704367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7642592306704367 | validation: 1.0782095151178228]
	TIME [epoch: 5.68 sec]
EPOCH 302/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.213246922329661		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.213246922329661 | validation: 0.6920574038107716]
	TIME [epoch: 5.65 sec]
EPOCH 303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8090447365528624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8090447365528624 | validation: 0.752787868091083]
	TIME [epoch: 5.66 sec]
EPOCH 304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8796616806376332		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8796616806376332 | validation: 0.668313555345949]
	TIME [epoch: 5.67 sec]
EPOCH 305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7998063013212664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998063013212664 | validation: 0.7073597344397604]
	TIME [epoch: 5.67 sec]
EPOCH 306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8161250625319562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8161250625319562 | validation: 0.6573756207743925]
	TIME [epoch: 5.68 sec]
EPOCH 307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.778820284906712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.778820284906712 | validation: 0.6295791694675881]
	TIME [epoch: 5.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_307.pth
	Model improved!!!
EPOCH 308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7165223865233676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7165223865233676 | validation: 0.7236206511805101]
	TIME [epoch: 5.71 sec]
EPOCH 309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7462419364090213		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7462419364090213 | validation: 1.1997859139126983]
	TIME [epoch: 5.65 sec]
EPOCH 310/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.375370106140512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.375370106140512 | validation: 0.7258191009321007]
	TIME [epoch: 5.65 sec]
EPOCH 311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8352195773577451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8352195773577451 | validation: 0.8281756304075736]
	TIME [epoch: 5.68 sec]
EPOCH 312/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0015211418301444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0015211418301444 | validation: 0.710169674086827]
	TIME [epoch: 5.66 sec]
EPOCH 313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8502864176173165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8502864176173165 | validation: 0.767896886656404]
	TIME [epoch: 5.67 sec]
EPOCH 314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8892848602415603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8892848602415603 | validation: 0.6711537001801025]
	TIME [epoch: 5.67 sec]
EPOCH 315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8051531685755009		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8051531685755009 | validation: 0.6695051340683118]
	TIME [epoch: 5.67 sec]
EPOCH 316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8133953902712415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8133953902712415 | validation: 0.6833186518203895]
	TIME [epoch: 5.64 sec]
EPOCH 317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7932852674913461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7932852674913461 | validation: 0.6473826218442063]
	TIME [epoch: 5.67 sec]
EPOCH 318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7798466055865427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7798466055865427 | validation: 0.6461072664168376]
	TIME [epoch: 5.67 sec]
EPOCH 319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7602018451915822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7602018451915822 | validation: 0.6229470623118405]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_319.pth
	Model improved!!!
EPOCH 320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7286603955979769		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7286603955979769 | validation: 0.5746387684000559]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_320.pth
	Model improved!!!
EPOCH 321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6482574370459279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6482574370459279 | validation: 0.8072548039398555]
	TIME [epoch: 5.69 sec]
EPOCH 322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8390378163668325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8390378163668325 | validation: 3.11444910901516]
	TIME [epoch: 5.68 sec]
EPOCH 323/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.2386615446227576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.2386615446227576 | validation: 0.746453820758358]
	TIME [epoch: 5.66 sec]
EPOCH 324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8491297999311089		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8491297999311089 | validation: 0.8296195998961555]
	TIME [epoch: 5.66 sec]
EPOCH 325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9769870982014197		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9769870982014197 | validation: 0.8416436260809527]
	TIME [epoch: 5.66 sec]
EPOCH 326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9532678385444314		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9532678385444314 | validation: 0.7610756816782467]
	TIME [epoch: 5.66 sec]
EPOCH 327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8893362917604622		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8893362917604622 | validation: 0.7415195983886189]
	TIME [epoch: 5.66 sec]
EPOCH 328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.864471792061936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.864471792061936 | validation: 0.6929922724287769]
	TIME [epoch: 5.67 sec]
EPOCH 329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8490866262302729		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8490866262302729 | validation: 0.6854181500446148]
	TIME [epoch: 5.67 sec]
EPOCH 330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082281131077449		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082281131077449 | validation: 0.7064715618614317]
	TIME [epoch: 5.67 sec]
EPOCH 331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8084695487964854		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8084695487964854 | validation: 0.660117925897792]
	TIME [epoch: 5.67 sec]
EPOCH 332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.816030610421305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.816030610421305 | validation: 0.6734522647398444]
	TIME [epoch: 5.67 sec]
EPOCH 333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8015964791335053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8015964791335053 | validation: 0.6621521828629723]
	TIME [epoch: 5.65 sec]
EPOCH 334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7870544578363436		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7870544578363436 | validation: 0.6465929918213238]
	TIME [epoch: 5.66 sec]
EPOCH 335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7878767320585209		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7878767320585209 | validation: 0.7009304667344834]
	TIME [epoch: 5.65 sec]
EPOCH 336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8076518770019047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8076518770019047 | validation: 0.6807618723833073]
	TIME [epoch: 5.65 sec]
EPOCH 337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8109617450820104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8109617450820104 | validation: 0.7062266170464444]
	TIME [epoch: 5.65 sec]
EPOCH 338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8132689358931119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8132689358931119 | validation: 0.6532830925913254]
	TIME [epoch: 5.66 sec]
EPOCH 339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8089312109310097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8089312109310097 | validation: 0.6269780695226768]
	TIME [epoch: 5.66 sec]
EPOCH 340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7433672861290495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7433672861290495 | validation: 0.6188013889352758]
	TIME [epoch: 5.66 sec]
EPOCH 341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7113298458845752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7113298458845752 | validation: 0.614014204807668]
	TIME [epoch: 5.66 sec]
EPOCH 342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6690349242178775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6690349242178775 | validation: 1.1685288258355342]
	TIME [epoch: 5.67 sec]
EPOCH 343/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2349041254431534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2349041254431534 | validation: 0.8513148639369826]
	TIME [epoch: 5.66 sec]
EPOCH 344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8365717430399826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8365717430399826 | validation: 0.8693067194113756]
	TIME [epoch: 5.68 sec]
EPOCH 345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9922598413653095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9922598413653095 | validation: 0.6969711349358576]
	TIME [epoch: 5.67 sec]
EPOCH 346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8215580825283562		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8215580825283562 | validation: 0.7687412491139831]
	TIME [epoch: 5.65 sec]
EPOCH 347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9225141636313694		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9225141636313694 | validation: 0.687545719175243]
	TIME [epoch: 5.67 sec]
EPOCH 348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7677296132849604		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7677296132849604 | validation: 0.720014630739801]
	TIME [epoch: 5.68 sec]
EPOCH 349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8082464842681419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8082464842681419 | validation: 0.7078940100851538]
	TIME [epoch: 5.67 sec]
EPOCH 350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8033334250101971		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8033334250101971 | validation: 0.6108635624577275]
	TIME [epoch: 5.65 sec]
EPOCH 351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6714407949254573		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6714407949254573 | validation: 0.5445907041577774]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_351.pth
	Model improved!!!
EPOCH 352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5990392041571662		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5990392041571662 | validation: 0.9905413053971941]
	TIME [epoch: 5.66 sec]
EPOCH 353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9881507536874773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9881507536874773 | validation: 1.4068678754138042]
	TIME [epoch: 5.66 sec]
EPOCH 354/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.7063202336032368		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.7063202336032368 | validation: 1.0738406520643913]
	TIME [epoch: 5.66 sec]
EPOCH 355/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3273450617980151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3273450617980151 | validation: 0.8648863703204912]
	TIME [epoch: 5.67 sec]
EPOCH 356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9812810869187775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9812810869187775 | validation: 0.8086728124977608]
	TIME [epoch: 5.67 sec]
EPOCH 357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9738263669350553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9738263669350553 | validation: 0.7331027442885022]
	TIME [epoch: 5.67 sec]
EPOCH 358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.851197742852773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.851197742852773 | validation: 0.7148942418867543]
	TIME [epoch: 5.65 sec]
EPOCH 359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8434735089122174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8434735089122174 | validation: 0.6709667596460064]
	TIME [epoch: 5.66 sec]
EPOCH 360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8113422495062268		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8113422495062268 | validation: 0.6543447889445304]
	TIME [epoch: 5.65 sec]
EPOCH 361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7805665321953419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7805665321953419 | validation: 0.6619422371310821]
	TIME [epoch: 5.63 sec]
EPOCH 362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7853690078281642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7853690078281642 | validation: 0.6499808339129242]
	TIME [epoch: 5.67 sec]
EPOCH 363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7743504538330035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7743504538330035 | validation: 0.639409165656105]
	TIME [epoch: 5.68 sec]
EPOCH 364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7667134277632173		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7667134277632173 | validation: 0.6597630202421905]
	TIME [epoch: 5.67 sec]
EPOCH 365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7618577197294016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7618577197294016 | validation: 0.6333813340357788]
	TIME [epoch: 5.66 sec]
EPOCH 366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7787498479867303		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7787498479867303 | validation: 0.6774956849018656]
	TIME [epoch: 5.69 sec]
EPOCH 367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7522860969090134		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7522860969090134 | validation: 0.6470195904955907]
	TIME [epoch: 5.67 sec]
EPOCH 368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7259909781361625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7259909781361625 | validation: 0.5745761209605978]
	TIME [epoch: 5.68 sec]
EPOCH 369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6196817347533868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6196817347533868 | validation: 1.02977227659418]
	TIME [epoch: 5.68 sec]
EPOCH 370/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0158473354001845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0158473354001845 | validation: 1.6038072302923299]
	TIME [epoch: 5.69 sec]
EPOCH 371/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8380727591980688		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8380727591980688 | validation: 0.9480607835115734]
	TIME [epoch: 5.68 sec]
EPOCH 372/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0978957727231096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0978957727231096 | validation: 0.9546236907073]
	TIME [epoch: 5.7 sec]
EPOCH 373/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1260574669847818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1260574669847818 | validation: 0.858665745184601]
	TIME [epoch: 5.66 sec]
EPOCH 374/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0158803107948904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0158803107948904 | validation: 0.6928438067197925]
	TIME [epoch: 5.7 sec]
EPOCH 375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7998481288369138		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7998481288369138 | validation: 0.7469234212023089]
	TIME [epoch: 5.66 sec]
EPOCH 376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8354865357232685		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8354865357232685 | validation: 0.6452893062942849]
	TIME [epoch: 5.69 sec]
EPOCH 377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773237106408522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773237106408522 | validation: 0.6284602956351185]
	TIME [epoch: 5.64 sec]
EPOCH 378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7517001481574246		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7517001481574246 | validation: 0.6104956928896812]
	TIME [epoch: 5.69 sec]
EPOCH 379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7350862922542322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7350862922542322 | validation: 0.6007250222056519]
	TIME [epoch: 5.64 sec]
EPOCH 380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6967508664329866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6967508664329866 | validation: 0.5742242563742234]
	TIME [epoch: 5.69 sec]
EPOCH 381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6342607514943831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6342607514943831 | validation: 0.5593956350575409]
	TIME [epoch: 5.63 sec]
EPOCH 382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5976835564342283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5976835564342283 | validation: 0.5621220681465168]
	TIME [epoch: 5.68 sec]
EPOCH 383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.590098288981492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.590098288981492 | validation: 0.6676341561360674]
	TIME [epoch: 5.69 sec]
EPOCH 384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7173248754926956		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7173248754926956 | validation: 1.1066091319378608]
	TIME [epoch: 5.68 sec]
EPOCH 385/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1380414384454218		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1380414384454218 | validation: 0.8629916025809842]
	TIME [epoch: 5.68 sec]
EPOCH 386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.963202138016927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.963202138016927 | validation: 0.6581516554800737]
	TIME [epoch: 5.66 sec]
EPOCH 387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7832643956417392		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7832643956417392 | validation: 0.763129178852345]
	TIME [epoch: 5.67 sec]
EPOCH 388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8889908995546287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8889908995546287 | validation: 0.6082780834991542]
	TIME [epoch: 5.67 sec]
EPOCH 389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7216544343098048		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7216544343098048 | validation: 0.6549456714130723]
	TIME [epoch: 5.64 sec]
EPOCH 390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7503677229739236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7503677229739236 | validation: 0.5564912388042901]
	TIME [epoch: 5.68 sec]
EPOCH 391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6439673808710283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6439673808710283 | validation: 0.5387020439388334]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_391.pth
	Model improved!!!
EPOCH 392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5837972622080561		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5837972622080561 | validation: 0.7982914025634749]
	TIME [epoch: 5.67 sec]
EPOCH 393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8273022178164285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8273022178164285 | validation: 1.4971992244436865]
	TIME [epoch: 5.62 sec]
EPOCH 394/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5399334495200914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5399334495200914 | validation: 0.8205814800262501]
	TIME [epoch: 5.68 sec]
EPOCH 395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8724378101680285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8724378101680285 | validation: 0.6818611659060957]
	TIME [epoch: 5.66 sec]
EPOCH 396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7773539825215715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7773539825215715 | validation: 0.6799951449055563]
	TIME [epoch: 5.68 sec]
EPOCH 397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8058957692356603		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8058957692356603 | validation: 0.6610337721353109]
	TIME [epoch: 5.66 sec]
EPOCH 398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7443534864908417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7443534864908417 | validation: 0.6202989455933854]
	TIME [epoch: 5.69 sec]
EPOCH 399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7166058311169705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7166058311169705 | validation: 0.5744808090674381]
	TIME [epoch: 5.68 sec]
EPOCH 400/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6911348619092916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6911348619092916 | validation: 0.590872708788149]
	TIME [epoch: 5.68 sec]
EPOCH 401/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6448545639077287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6448545639077287 | validation: 0.5364371635640504]
	TIME [epoch: 5.63 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_401.pth
	Model improved!!!
EPOCH 402/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5972772194634776		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5972772194634776 | validation: 0.593247307387716]
	TIME [epoch: 5.62 sec]
EPOCH 403/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6343667666612659		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6343667666612659 | validation: 1.2480410007544813]
	TIME [epoch: 5.65 sec]
EPOCH 404/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1739594268864628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1739594268864628 | validation: 0.9026014798410608]
	TIME [epoch: 5.65 sec]
EPOCH 405/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.030394079256249		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.030394079256249 | validation: 0.5499698540223663]
	TIME [epoch: 5.68 sec]
EPOCH 406/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6415063093447236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6415063093447236 | validation: 0.7199432482158579]
	TIME [epoch: 5.65 sec]
EPOCH 407/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8160379297342248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8160379297342248 | validation: 0.5374651136561168]
	TIME [epoch: 5.68 sec]
EPOCH 408/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.61695404656943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.61695404656943 | validation: 0.4971693229427461]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_408.pth
	Model improved!!!
EPOCH 409/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5365138251817946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5365138251817946 | validation: 0.60066163077955]
	TIME [epoch: 5.69 sec]
EPOCH 410/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6265841514217838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6265841514217838 | validation: 0.8552399894560136]
	TIME [epoch: 5.68 sec]
EPOCH 411/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8960645283978077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8960645283978077 | validation: 0.4660825366580104]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_411.pth
	Model improved!!!
EPOCH 412/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.508313661250778		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.508313661250778 | validation: 0.5180975404553324]
	TIME [epoch: 5.67 sec]
EPOCH 413/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5530312584926096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5530312584926096 | validation: 0.7240174648519382]
	TIME [epoch: 5.66 sec]
EPOCH 414/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.773803720270145		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.773803720270145 | validation: 0.48967133812311375]
	TIME [epoch: 5.66 sec]
EPOCH 415/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5055937536533309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5055937536533309 | validation: 0.46825658195413133]
	TIME [epoch: 5.66 sec]
EPOCH 416/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48893995302130405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48893995302130405 | validation: 0.7106507020513683]
	TIME [epoch: 5.62 sec]
EPOCH 417/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.749681616593958		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.749681616593958 | validation: 0.955771660265381]
	TIME [epoch: 5.64 sec]
EPOCH 418/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9532139439819903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9532139439819903 | validation: 0.7230323120195878]
	TIME [epoch: 5.63 sec]
EPOCH 419/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.839165448903263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.839165448903263 | validation: 0.5691975249081187]
	TIME [epoch: 5.67 sec]
EPOCH 420/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6567148696496963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6567148696496963 | validation: 0.5252324074251954]
	TIME [epoch: 5.64 sec]
EPOCH 421/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6286619090306586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6286619090306586 | validation: 0.4976995620299189]
	TIME [epoch: 5.66 sec]
EPOCH 422/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5405157122567824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5405157122567824 | validation: 0.7081749439521107]
	TIME [epoch: 5.63 sec]
EPOCH 423/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6993434717006113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6993434717006113 | validation: 0.9035348807554581]
	TIME [epoch: 5.68 sec]
EPOCH 424/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9862676935622208		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9862676935622208 | validation: 0.4871096064268173]
	TIME [epoch: 5.63 sec]
EPOCH 425/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5465860609192847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5465860609192847 | validation: 0.9203253659217117]
	TIME [epoch: 5.66 sec]
EPOCH 426/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9557891760490569		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9557891760490569 | validation: 0.5738372270243016]
	TIME [epoch: 5.63 sec]
EPOCH 427/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6436973870746351		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6436973870746351 | validation: 0.48319629410147624]
	TIME [epoch: 5.68 sec]
EPOCH 428/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.543220529448721		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.543220529448721 | validation: 0.5843946328563295]
	TIME [epoch: 5.62 sec]
EPOCH 429/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6258694351650698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6258694351650698 | validation: 0.6281458427118158]
	TIME [epoch: 5.67 sec]
EPOCH 430/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.676784615566645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.676784615566645 | validation: 0.5804300919458367]
	TIME [epoch: 5.64 sec]
EPOCH 431/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5750286648128736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5750286648128736 | validation: 0.5467676151554776]
	TIME [epoch: 5.67 sec]
EPOCH 432/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5829693598118015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5829693598118015 | validation: 0.44597136413199057]
	TIME [epoch: 5.65 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_432.pth
	Model improved!!!
EPOCH 433/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4614668117169063		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4614668117169063 | validation: 0.4303331119788477]
	TIME [epoch: 5.72 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_433.pth
	Model improved!!!
EPOCH 434/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4432760067719919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4432760067719919 | validation: 0.46799823256861295]
	TIME [epoch: 5.68 sec]
EPOCH 435/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5087138601726998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5087138601726998 | validation: 0.6951295693108848]
	TIME [epoch: 5.67 sec]
EPOCH 436/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.763729353822689		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.763729353822689 | validation: 0.41914386850919527]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_436.pth
	Model improved!!!
EPOCH 437/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46885941916024126		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46885941916024126 | validation: 0.3730146587779136]
	TIME [epoch: 5.67 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_437.pth
	Model improved!!!
EPOCH 438/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4173231408603387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4173231408603387 | validation: 0.670035218788714]
	TIME [epoch: 5.68 sec]
EPOCH 439/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7018656354563985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7018656354563985 | validation: 0.760812138849631]
	TIME [epoch: 5.7 sec]
EPOCH 440/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.713930042999955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.713930042999955 | validation: 0.6532184122565818]
	TIME [epoch: 5.68 sec]
EPOCH 441/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7590994964618508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7590994964618508 | validation: 0.497122079063154]
	TIME [epoch: 5.69 sec]
EPOCH 442/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5725071814394836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5725071814394836 | validation: 0.45146436201543294]
	TIME [epoch: 5.68 sec]
EPOCH 443/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5035404161086595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5035404161086595 | validation: 0.4719451012953532]
	TIME [epoch: 5.69 sec]
EPOCH 444/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4853218471251282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4853218471251282 | validation: 1.017654266669652]
	TIME [epoch: 5.68 sec]
EPOCH 445/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9167131765798248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9167131765798248 | validation: 0.8120979742057243]
	TIME [epoch: 5.67 sec]
EPOCH 446/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9074792675409465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9074792675409465 | validation: 0.44042844392534325]
	TIME [epoch: 5.65 sec]
EPOCH 447/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5229408174044633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5229408174044633 | validation: 0.5716536648433836]
	TIME [epoch: 5.69 sec]
EPOCH 448/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6212142850564388		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6212142850564388 | validation: 0.5943239059002574]
	TIME [epoch: 5.66 sec]
EPOCH 449/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6397035207828788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6397035207828788 | validation: 0.43873643662329914]
	TIME [epoch: 5.7 sec]
EPOCH 450/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44738636641323454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44738636641323454 | validation: 0.49634803530556]
	TIME [epoch: 5.67 sec]
EPOCH 451/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5133807430592512		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5133807430592512 | validation: 0.6252383666872915]
	TIME [epoch: 5.71 sec]
EPOCH 452/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6090823102363285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6090823102363285 | validation: 0.6362834339264029]
	TIME [epoch: 5.69 sec]
EPOCH 453/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6822108342344534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6822108342344534 | validation: 0.43061703327904244]
	TIME [epoch: 5.69 sec]
EPOCH 454/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4421533207832429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4421533207832429 | validation: 0.3587243000049113]
	TIME [epoch: 5.66 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_454.pth
	Model improved!!!
EPOCH 455/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38362671120290587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38362671120290587 | validation: 0.4133815118740951]
	TIME [epoch: 5.68 sec]
EPOCH 456/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44792639166852155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44792639166852155 | validation: 0.6297970241002363]
	TIME [epoch: 5.68 sec]
EPOCH 457/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7013878438187322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7013878438187322 | validation: 0.41847339209992235]
	TIME [epoch: 5.67 sec]
EPOCH 458/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4330433922281926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4330433922281926 | validation: 0.336342079756755]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_458.pth
	Model improved!!!
EPOCH 459/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38626850724894823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38626850724894823 | validation: 0.7014465826351424]
	TIME [epoch: 5.69 sec]
EPOCH 460/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6360410919876592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6360410919876592 | validation: 0.7794221506297273]
	TIME [epoch: 5.68 sec]
EPOCH 461/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8667209562799076		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8667209562799076 | validation: 0.4878591425478807]
	TIME [epoch: 5.68 sec]
EPOCH 462/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5410719534960495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5410719534960495 | validation: 0.6931837786574822]
	TIME [epoch: 5.69 sec]
EPOCH 463/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7292654591874502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7292654591874502 | validation: 0.3992625645484058]
	TIME [epoch: 5.67 sec]
EPOCH 464/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.45154330148739624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.45154330148739624 | validation: 0.44418075380482613]
	TIME [epoch: 5.69 sec]
EPOCH 465/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42820434797560325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42820434797560325 | validation: 0.5375424588391203]
	TIME [epoch: 5.67 sec]
EPOCH 466/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5635467909153518		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5635467909153518 | validation: 0.37959787714505633]
	TIME [epoch: 5.69 sec]
EPOCH 467/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38939663239876454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38939663239876454 | validation: 0.42725548663008905]
	TIME [epoch: 5.68 sec]
EPOCH 468/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46228899461433687		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46228899461433687 | validation: 0.5203466639501423]
	TIME [epoch: 5.7 sec]
EPOCH 469/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49526319349048187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49526319349048187 | validation: 0.48998685211137966]
	TIME [epoch: 5.69 sec]
EPOCH 470/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5421743139165077		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5421743139165077 | validation: 0.3734575527819226]
	TIME [epoch: 5.69 sec]
EPOCH 471/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3708984991593011		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3708984991593011 | validation: 0.38611707109037435]
	TIME [epoch: 5.69 sec]
EPOCH 472/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4082638278741347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4082638278741347 | validation: 0.6139249453201183]
	TIME [epoch: 5.69 sec]
EPOCH 473/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5371819746952083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5371819746952083 | validation: 0.5745972139532967]
	TIME [epoch: 5.69 sec]
EPOCH 474/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6439363633944221		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6439363633944221 | validation: 0.35679714205842417]
	TIME [epoch: 5.68 sec]
EPOCH 475/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40484945138605666		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40484945138605666 | validation: 0.5100752866734415]
	TIME [epoch: 5.67 sec]
EPOCH 476/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5060049401751171		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5060049401751171 | validation: 0.5348027519337503]
	TIME [epoch: 5.65 sec]
EPOCH 477/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5724477805587732		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5724477805587732 | validation: 0.4003118394560768]
	TIME [epoch: 5.66 sec]
EPOCH 478/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36369596110564073		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36369596110564073 | validation: 0.32594112023046473]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_478.pth
	Model improved!!!
EPOCH 479/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35877107071555614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35877107071555614 | validation: 0.4357964281834503]
	TIME [epoch: 5.7 sec]
EPOCH 480/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4573705797732508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4573705797732508 | validation: 0.4901650808504444]
	TIME [epoch: 5.68 sec]
EPOCH 481/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5504777595305052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5504777595305052 | validation: 0.26753560706029533]
	TIME [epoch: 5.69 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_481.pth
	Model improved!!!
EPOCH 482/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3031441443376022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3031441443376022 | validation: 0.31656569582869754]
	TIME [epoch: 5.67 sec]
EPOCH 483/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3095975579807232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3095975579807232 | validation: 0.5045247067030992]
	TIME [epoch: 5.7 sec]
EPOCH 484/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5504861776389712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5504861776389712 | validation: 0.5349447931227364]
	TIME [epoch: 5.66 sec]
EPOCH 485/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4880687910631963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4880687910631963 | validation: 0.5026414694734859]
	TIME [epoch: 5.7 sec]
EPOCH 486/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5834937358829865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5834937358829865 | validation: 0.3096616094167464]
	TIME [epoch: 5.68 sec]
EPOCH 487/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3344588680811214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3344588680811214 | validation: 0.3600734316190014]
	TIME [epoch: 5.68 sec]
EPOCH 488/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33841441035941683		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33841441035941683 | validation: 0.6779935636965903]
	TIME [epoch: 5.67 sec]
EPOCH 489/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7239251565101726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7239251565101726 | validation: 0.25766778613028757]
	TIME [epoch: 5.68 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_489.pth
	Model improved!!!
EPOCH 490/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29752272863774926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29752272863774926 | validation: 0.5375065954953399]
	TIME [epoch: 5.68 sec]
EPOCH 491/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4521705067833111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4521705067833111 | validation: 0.7612374136177421]
	TIME [epoch: 5.69 sec]
EPOCH 492/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8213798843523524		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8213798843523524 | validation: 0.2594015994706958]
	TIME [epoch: 5.66 sec]
EPOCH 493/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3057273589046528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3057273589046528 | validation: 1.0173477096256984]
	TIME [epoch: 5.68 sec]
EPOCH 494/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8640302780572258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8640302780572258 | validation: 0.5709044661656313]
	TIME [epoch: 5.63 sec]
EPOCH 495/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6251321336245294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6251321336245294 | validation: 0.5072079398246107]
	TIME [epoch: 5.64 sec]
EPOCH 496/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4274702670669957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4274702670669957 | validation: 0.300498777482328]
	TIME [epoch: 5.67 sec]
EPOCH 497/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3127225041051514		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3127225041051514 | validation: 0.42585682331414054]
	TIME [epoch: 5.65 sec]
EPOCH 498/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4752372910641997		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4752372910641997 | validation: 0.4744188774924659]
	TIME [epoch: 5.7 sec]
EPOCH 499/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4010057989412441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4010057989412441 | validation: 0.42589196471581725]
	TIME [epoch: 5.68 sec]
EPOCH 500/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.46214267754250904		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.46214267754250904 | validation: 0.2667972754137923]
	TIME [epoch: 5.67 sec]
EPOCH 501/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29116730115241335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29116730115241335 | validation: 0.23862652759611988]
	TIME [epoch: 199 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_501.pth
	Model improved!!!
EPOCH 502/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26253134865875494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26253134865875494 | validation: 0.36887790261441755]
	TIME [epoch: 12 sec]
EPOCH 503/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3207679257755515		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3207679257755515 | validation: 0.4561795755637424]
	TIME [epoch: 12.1 sec]
EPOCH 504/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5106430722329577		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5106430722329577 | validation: 0.33593210802651696]
	TIME [epoch: 11.9 sec]
EPOCH 505/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3155433181273941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3155433181273941 | validation: 0.22475425229202545]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_505.pth
	Model improved!!!
EPOCH 506/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26604660603643426		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26604660603643426 | validation: 0.4878473261126203]
	TIME [epoch: 12 sec]
EPOCH 507/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40119789230910974		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40119789230910974 | validation: 0.5446727293260727]
	TIME [epoch: 12 sec]
EPOCH 508/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6478232793427295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6478232793427295 | validation: 0.3750518199952393]
	TIME [epoch: 12 sec]
EPOCH 509/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49798760015543025		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49798760015543025 | validation: 0.37787766771219977]
	TIME [epoch: 12 sec]
EPOCH 510/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4328089393609846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4328089393609846 | validation: 0.24590714301648148]
	TIME [epoch: 12 sec]
EPOCH 511/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2743593511315746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2743593511315746 | validation: 0.4112717974408735]
	TIME [epoch: 12 sec]
EPOCH 512/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3592631018922937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3592631018922937 | validation: 0.4961541220456481]
	TIME [epoch: 12 sec]
EPOCH 513/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5711255894060816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5711255894060816 | validation: 0.40932064711677757]
	TIME [epoch: 12 sec]
EPOCH 514/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4250103179103826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4250103179103826 | validation: 0.46781582170613645]
	TIME [epoch: 12 sec]
EPOCH 515/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4259049400614642		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4259049400614642 | validation: 0.6473050424354909]
	TIME [epoch: 12 sec]
EPOCH 516/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7278391129537871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7278391129537871 | validation: 0.274887013981778]
	TIME [epoch: 12 sec]
EPOCH 517/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2832626752016481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2832626752016481 | validation: 0.9816419433026016]
	TIME [epoch: 12 sec]
EPOCH 518/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8080591952109123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8080591952109123 | validation: 0.4152854749947597]
	TIME [epoch: 12 sec]
EPOCH 519/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.48234185414998915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.48234185414998915 | validation: 0.3254879313868327]
	TIME [epoch: 12 sec]
EPOCH 520/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3521107174005991		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3521107174005991 | validation: 0.7203174634479642]
	TIME [epoch: 12 sec]
EPOCH 521/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6008828263645988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6008828263645988 | validation: 0.2847571207583149]
	TIME [epoch: 12.1 sec]
EPOCH 522/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3077621476154092		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3077621476154092 | validation: 0.26750600350516124]
	TIME [epoch: 12 sec]
EPOCH 523/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3074480112174723		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3074480112174723 | validation: 0.2197678763210051]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_523.pth
	Model improved!!!
EPOCH 524/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26196768051293234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26196768051293234 | validation: 0.3290911540023737]
	TIME [epoch: 12.1 sec]
EPOCH 525/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2947014628598371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2947014628598371 | validation: 0.31431365588228144]
	TIME [epoch: 12 sec]
EPOCH 526/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35426782375144455		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35426782375144455 | validation: 0.32802282192862925]
	TIME [epoch: 12.1 sec]
EPOCH 527/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28692557951258296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28692557951258296 | validation: 0.3207886861532047]
	TIME [epoch: 12 sec]
EPOCH 528/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3633029313900943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3633029313900943 | validation: 0.2587301950535119]
	TIME [epoch: 12.1 sec]
EPOCH 529/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.254552871021347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.254552871021347 | validation: 0.20778806451830018]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_529.pth
	Model improved!!!
EPOCH 530/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24172300196538818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24172300196538818 | validation: 0.4833473458617049]
	TIME [epoch: 12.1 sec]
EPOCH 531/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3647296316826554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3647296316826554 | validation: 0.42002215788783537]
	TIME [epoch: 12.1 sec]
EPOCH 532/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.507236104732705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.507236104732705 | validation: 0.2217085351205281]
	TIME [epoch: 12.1 sec]
EPOCH 533/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28661222209763976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28661222209763976 | validation: 0.3034625386614594]
	TIME [epoch: 12 sec]
EPOCH 534/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2818625539647481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2818625539647481 | validation: 0.44327544956469983]
	TIME [epoch: 12.1 sec]
EPOCH 535/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4835694515036981		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4835694515036981 | validation: 0.25046181192604744]
	TIME [epoch: 12 sec]
EPOCH 536/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23165470440212255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23165470440212255 | validation: 0.1737577477909297]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_536.pth
	Model improved!!!
EPOCH 537/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2099703317968978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2099703317968978 | validation: 0.4806704589048443]
	TIME [epoch: 12 sec]
EPOCH 538/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36409837941642276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36409837941642276 | validation: 0.5155596978714819]
	TIME [epoch: 12 sec]
EPOCH 539/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6102219290706405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6102219290706405 | validation: 0.3224907844215893]
	TIME [epoch: 12 sec]
EPOCH 540/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38796892831698354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38796892831698354 | validation: 0.5727985665809586]
	TIME [epoch: 12 sec]
EPOCH 541/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5175975946248534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5175975946248534 | validation: 0.328606820311282]
	TIME [epoch: 12 sec]
EPOCH 542/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3560342421354071		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3560342421354071 | validation: 0.3146309748303361]
	TIME [epoch: 12 sec]
EPOCH 543/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27113951276312287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27113951276312287 | validation: 0.24632923445936045]
	TIME [epoch: 12 sec]
EPOCH 544/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2504396919216873		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2504396919216873 | validation: 0.26233658844475116]
	TIME [epoch: 12 sec]
EPOCH 545/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605817638198812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2605817638198812 | validation: 0.31403248029614617]
	TIME [epoch: 12 sec]
EPOCH 546/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3502867767749001		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3502867767749001 | validation: 0.7761107638583026]
	TIME [epoch: 12 sec]
EPOCH 547/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5931461382555072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5931461382555072 | validation: 0.37940608348791466]
	TIME [epoch: 12 sec]
EPOCH 548/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3912750754187113		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3912750754187113 | validation: 0.1753796269576264]
	TIME [epoch: 12 sec]
EPOCH 549/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20627111874672405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20627111874672405 | validation: 0.2263783445348001]
	TIME [epoch: 12 sec]
EPOCH 550/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22464461283447407		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22464461283447407 | validation: 0.28077104335243014]
	TIME [epoch: 12 sec]
EPOCH 551/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3096779363096941		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3096779363096941 | validation: 0.2720476948208237]
	TIME [epoch: 12 sec]
EPOCH 552/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23423336578995851		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23423336578995851 | validation: 0.21137316531886632]
	TIME [epoch: 12 sec]
EPOCH 553/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25140435218080304		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25140435218080304 | validation: 0.401332140809298]
	TIME [epoch: 12 sec]
EPOCH 554/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3044928463401353		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3044928463401353 | validation: 0.34845280816533775]
	TIME [epoch: 12 sec]
EPOCH 555/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4082638782293583		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4082638782293583 | validation: 0.18438026506784289]
	TIME [epoch: 12 sec]
EPOCH 556/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20272339631141745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20272339631141745 | validation: 0.2830046576110357]
	TIME [epoch: 12 sec]
EPOCH 557/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25568155317327945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25568155317327945 | validation: 0.3890921148539222]
	TIME [epoch: 12 sec]
EPOCH 558/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44291685637241446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44291685637241446 | validation: 0.20071391126200472]
	TIME [epoch: 12 sec]
EPOCH 559/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19671078976424938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19671078976424938 | validation: 0.15978887445523773]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_559.pth
	Model improved!!!
EPOCH 560/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17921136171044516		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17921136171044516 | validation: 0.3355394414989079]
	TIME [epoch: 12.1 sec]
EPOCH 561/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2670832063415859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2670832063415859 | validation: 0.4772760738480415]
	TIME [epoch: 12 sec]
EPOCH 562/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5470375658555277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5470375658555277 | validation: 1.3521882735507582]
	TIME [epoch: 12 sec]
EPOCH 563/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5899924582788034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5899924582788034 | validation: 1.4151138663129457]
	TIME [epoch: 12 sec]
EPOCH 564/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.681443124357858		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.681443124357858 | validation: 1.1901574548515939]
	TIME [epoch: 12 sec]
EPOCH 565/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4494164166064736		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4494164166064736 | validation: 1.1157225201268128]
	TIME [epoch: 12 sec]
EPOCH 566/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2724529888237823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2724529888237823 | validation: 1.1237084906267167]
	TIME [epoch: 12 sec]
EPOCH 567/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2073760864298664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2073760864298664 | validation: 1.1877832634861265]
	TIME [epoch: 12 sec]
EPOCH 568/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.1903977854439536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.1903977854439536 | validation: 0.7536367364575948]
	TIME [epoch: 12 sec]
EPOCH 569/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9005993394070717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9005993394070717 | validation: 0.6187303785894717]
	TIME [epoch: 11.9 sec]
EPOCH 570/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8432386310893952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8432386310893952 | validation: 0.8704692042178768]
	TIME [epoch: 12 sec]
EPOCH 571/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.079701065567552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.079701065567552 | validation: 0.45472390019330106]
	TIME [epoch: 12 sec]
EPOCH 572/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.44328111187281877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.44328111187281877 | validation: 2.651811855712948]
	TIME [epoch: 12.1 sec]
EPOCH 573/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.667441068167847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.667441068167847 | validation: 0.6383077524698066]
	TIME [epoch: 12 sec]
EPOCH 574/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.697287427686274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.697287427686274 | validation: 0.40006563082298197]
	TIME [epoch: 12.1 sec]
EPOCH 575/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.436859763442599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.436859763442599 | validation: 0.9208361396786491]
	TIME [epoch: 11.9 sec]
EPOCH 576/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7765441324234553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7765441324234553 | validation: 0.30281753469381284]
	TIME [epoch: 12 sec]
EPOCH 577/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3038015159959664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3038015159959664 | validation: 0.3175126643794204]
	TIME [epoch: 11.9 sec]
EPOCH 578/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3561088555935447		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3561088555935447 | validation: 0.48187759805390495]
	TIME [epoch: 12 sec]
EPOCH 579/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4346654843764489		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4346654843764489 | validation: 0.22636084603561013]
	TIME [epoch: 12 sec]
EPOCH 580/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25632640656436567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25632640656436567 | validation: 0.19858038605381031]
	TIME [epoch: 11.9 sec]
EPOCH 581/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21997843411794443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21997843411794443 | validation: 0.19908269300831327]
	TIME [epoch: 12 sec]
EPOCH 582/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21960608239479498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21960608239479498 | validation: 0.5162781677729019]
	TIME [epoch: 12 sec]
EPOCH 583/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.41182793676037294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.41182793676037294 | validation: 0.4318718727611569]
	TIME [epoch: 11.9 sec]
EPOCH 584/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.49311887847209884		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.49311887847209884 | validation: 0.2659951671443325]
	TIME [epoch: 12 sec]
EPOCH 585/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27743184406182464		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27743184406182464 | validation: 0.5570820567738954]
	TIME [epoch: 12 sec]
EPOCH 586/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.480987587590697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.480987587590697 | validation: 0.24585783720421658]
	TIME [epoch: 12 sec]
EPOCH 587/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28217982644245665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28217982644245665 | validation: 0.22809571196573808]
	TIME [epoch: 12 sec]
EPOCH 588/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2457117222379852		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2457117222379852 | validation: 0.20426170991325543]
	TIME [epoch: 12 sec]
EPOCH 589/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2222554493842711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2222554493842711 | validation: 0.25767216660413783]
	TIME [epoch: 12 sec]
EPOCH 590/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25919812652913427		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25919812652913427 | validation: 0.2905689574666431]
	TIME [epoch: 11.9 sec]
EPOCH 591/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3348821839749098		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3348821839749098 | validation: 0.2889295687560647]
	TIME [epoch: 12 sec]
EPOCH 592/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2639073711259056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2639073711259056 | validation: 0.27164630608116624]
	TIME [epoch: 12 sec]
EPOCH 593/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3027667317716701		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3027667317716701 | validation: 0.3311907067779775]
	TIME [epoch: 12 sec]
EPOCH 594/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2822179033617574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2822179033617574 | validation: 0.24189268904532454]
	TIME [epoch: 11.9 sec]
EPOCH 595/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28097314189501715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28097314189501715 | validation: 0.24083079711300784]
	TIME [epoch: 12 sec]
EPOCH 596/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23160354783432255		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23160354783432255 | validation: 0.21786704682536395]
	TIME [epoch: 11.9 sec]
EPOCH 597/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2501340540217017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2501340540217017 | validation: 0.3488638516798791]
	TIME [epoch: 12 sec]
EPOCH 598/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29081020493661386		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29081020493661386 | validation: 0.3077127030597455]
	TIME [epoch: 12 sec]
EPOCH 599/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34842772149785695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34842772149785695 | validation: 0.18139194760172553]
	TIME [epoch: 12 sec]
EPOCH 600/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1997614055466874		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1997614055466874 | validation: 0.16575402896517405]
	TIME [epoch: 11.9 sec]
EPOCH 601/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18003287199168633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18003287199168633 | validation: 0.2089389656802811]
	TIME [epoch: 12 sec]
EPOCH 602/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19216842424446923		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19216842424446923 | validation: 0.2555194047839769]
	TIME [epoch: 11.9 sec]
EPOCH 603/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2815717973592543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2815717973592543 | validation: 0.42204112677123296]
	TIME [epoch: 12 sec]
EPOCH 604/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32104886607100724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32104886607100724 | validation: 0.2989967122546038]
	TIME [epoch: 11.9 sec]
EPOCH 605/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38820405180862405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38820405180862405 | validation: 0.19186875936814418]
	TIME [epoch: 12.1 sec]
EPOCH 606/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20892169073226632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20892169073226632 | validation: 0.18863364900148058]
	TIME [epoch: 12 sec]
EPOCH 607/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21928330751217004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21928330751217004 | validation: 0.3354303410938772]
	TIME [epoch: 12 sec]
EPOCH 608/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2642812247335775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2642812247335775 | validation: 0.2692109716053337]
	TIME [epoch: 12 sec]
EPOCH 609/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32619294760637274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32619294760637274 | validation: 0.40765593849338555]
	TIME [epoch: 12 sec]
EPOCH 610/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.29195375720160366		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.29195375720160366 | validation: 0.2104386616988268]
	TIME [epoch: 12 sec]
EPOCH 611/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24075508879821286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24075508879821286 | validation: 0.22317676518454432]
	TIME [epoch: 12 sec]
EPOCH 612/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22324656708347185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22324656708347185 | validation: 0.19714513093706693]
	TIME [epoch: 12 sec]
EPOCH 613/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24113364040651478		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24113364040651478 | validation: 2.2562403778722664]
	TIME [epoch: 12 sec]
EPOCH 614/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8519043249389273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8519043249389273 | validation: 1.2479270320909686]
	TIME [epoch: 12 sec]
EPOCH 615/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9840834411902782		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9840834411902782 | validation: 0.6754460497634827]
	TIME [epoch: 12 sec]
EPOCH 616/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7050673487238927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7050673487238927 | validation: 0.3376814473102708]
	TIME [epoch: 12.1 sec]
EPOCH 617/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3514932986933465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3514932986933465 | validation: 0.6046411593327308]
	TIME [epoch: 12 sec]
EPOCH 618/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5757502882157189		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5757502882157189 | validation: 0.28382631163243194]
	TIME [epoch: 12 sec]
EPOCH 619/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3475130641840809		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3475130641840809 | validation: 0.21844602044298578]
	TIME [epoch: 12 sec]
EPOCH 620/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2449918614784432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2449918614784432 | validation: 0.3505265039012459]
	TIME [epoch: 12 sec]
EPOCH 621/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.284774055541277		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.284774055541277 | validation: 0.25412888285487806]
	TIME [epoch: 12 sec]
EPOCH 622/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27046312476390655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27046312476390655 | validation: 0.21713126631315932]
	TIME [epoch: 12 sec]
EPOCH 623/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20712298242320404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20712298242320404 | validation: 0.18262163595282446]
	TIME [epoch: 12 sec]
EPOCH 624/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23112949284948045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23112949284948045 | validation: 0.228672778623933]
	TIME [epoch: 12 sec]
EPOCH 625/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21775172743790294		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21775172743790294 | validation: 0.2098390577321666]
	TIME [epoch: 12 sec]
EPOCH 626/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25067412456039484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25067412456039484 | validation: 0.2656144760018235]
	TIME [epoch: 12 sec]
EPOCH 627/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26158056089739357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26158056089739357 | validation: 0.18868867682725088]
	TIME [epoch: 12 sec]
EPOCH 628/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23177617413714102		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23177617413714102 | validation: 0.25265467619165766]
	TIME [epoch: 12 sec]
EPOCH 629/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21666283414200152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21666283414200152 | validation: 0.1786132545910211]
	TIME [epoch: 12 sec]
EPOCH 630/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20186489566897062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20186489566897062 | validation: 0.23630274798354486]
	TIME [epoch: 12 sec]
EPOCH 631/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2019100255930446		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2019100255930446 | validation: 0.1711604406519423]
	TIME [epoch: 12 sec]
EPOCH 632/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20714127979376357		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20714127979376357 | validation: 0.2732329252028723]
	TIME [epoch: 12 sec]
EPOCH 633/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2030544837691401		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2030544837691401 | validation: 0.19101783106453607]
	TIME [epoch: 12 sec]
EPOCH 634/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23737272151590616		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23737272151590616 | validation: 0.22960041279565246]
	TIME [epoch: 12 sec]
EPOCH 635/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20451268952616744		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20451268952616744 | validation: 1.4833673488267565]
	TIME [epoch: 12 sec]
EPOCH 636/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.6042646990772091		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.6042646990772091 | validation: 0.5280182881952983]
	TIME [epoch: 12 sec]
EPOCH 637/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5247893881789876		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5247893881789876 | validation: 0.4337841739168498]
	TIME [epoch: 12 sec]
EPOCH 638/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5289934541867284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5289934541867284 | validation: 0.2725405699054358]
	TIME [epoch: 12 sec]
EPOCH 639/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3479217122677664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3479217122677664 | validation: 0.47906488741307707]
	TIME [epoch: 11.9 sec]
EPOCH 640/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.42630091978075385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.42630091978075385 | validation: 0.1628563580486199]
	TIME [epoch: 12 sec]
EPOCH 641/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19117288021656328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19117288021656328 | validation: 0.18132903940116787]
	TIME [epoch: 12 sec]
EPOCH 642/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18846865197364415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18846865197364415 | validation: 0.1913133878187445]
	TIME [epoch: 12 sec]
EPOCH 643/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19986692166967907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19986692166967907 | validation: 0.23421373111758326]
	TIME [epoch: 12 sec]
EPOCH 644/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20190732238465242		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20190732238465242 | validation: 0.1593832596948343]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_644.pth
	Model improved!!!
EPOCH 645/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20488181137368316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20488181137368316 | validation: 0.2935573545903315]
	TIME [epoch: 12.2 sec]
EPOCH 646/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24482679434251345		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24482679434251345 | validation: 0.2154009893862388]
	TIME [epoch: 12.2 sec]
EPOCH 647/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24515423312229903		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24515423312229903 | validation: 0.18103733706034086]
	TIME [epoch: 12 sec]
EPOCH 648/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17308151058728968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17308151058728968 | validation: 0.1421590696246015]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_648.pth
	Model improved!!!
EPOCH 649/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17587893558657855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17587893558657855 | validation: 0.17871010523674646]
	TIME [epoch: 12 sec]
EPOCH 650/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16872141585462955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16872141585462955 | validation: 0.16456293543013023]
	TIME [epoch: 12 sec]
EPOCH 651/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18289779618671595		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18289779618671595 | validation: 0.2646297360433797]
	TIME [epoch: 12 sec]
EPOCH 652/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23076014189701027		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23076014189701027 | validation: 0.2405100695104484]
	TIME [epoch: 12 sec]
EPOCH 653/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27036883096984793		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27036883096984793 | validation: 0.255757467818376]
	TIME [epoch: 12 sec]
EPOCH 654/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20109587678567742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20109587678567742 | validation: 0.1644730633023871]
	TIME [epoch: 12 sec]
EPOCH 655/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19325978663961965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19325978663961965 | validation: 0.18903996774429527]
	TIME [epoch: 12 sec]
EPOCH 656/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16012024493500193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16012024493500193 | validation: 0.17100494389312657]
	TIME [epoch: 12.1 sec]
EPOCH 657/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20160138909910305		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20160138909910305 | validation: 0.4267884455952687]
	TIME [epoch: 12 sec]
EPOCH 658/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3152581820650309		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3152581820650309 | validation: 0.21411974364505992]
	TIME [epoch: 12 sec]
EPOCH 659/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26300676945660867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26300676945660867 | validation: 0.14566219765854171]
	TIME [epoch: 12 sec]
EPOCH 660/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14615113476365132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14615113476365132 | validation: 0.13554392839909032]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_660.pth
	Model improved!!!
EPOCH 661/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14048450111005387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14048450111005387 | validation: 0.15446866092156997]
	TIME [epoch: 11.9 sec]
EPOCH 662/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16134410023080306		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16134410023080306 | validation: 0.31826281625125175]
	TIME [epoch: 12 sec]
EPOCH 663/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24270220265434742		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24270220265434742 | validation: 0.18851799082672008]
	TIME [epoch: 11.9 sec]
EPOCH 664/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15584436644049424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15584436644049424 | validation: 0.1380068706682862]
	TIME [epoch: 12 sec]
EPOCH 665/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15957453543570624		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15957453543570624 | validation: 0.15219999098751713]
	TIME [epoch: 12 sec]
EPOCH 666/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13877395962573086		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13877395962573086 | validation: 0.4219872601860286]
	TIME [epoch: 12 sec]
EPOCH 667/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2605430441916365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2605430441916365 | validation: 0.39307137228885813]
	TIME [epoch: 12 sec]
EPOCH 668/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5159533597613837		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5159533597613837 | validation: 0.3453635061555911]
	TIME [epoch: 12 sec]
EPOCH 669/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4696893826441322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4696893826441322 | validation: 0.16576570555805437]
	TIME [epoch: 11.9 sec]
EPOCH 670/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2196296791910578		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2196296791910578 | validation: 0.8834357815161559]
	TIME [epoch: 11.9 sec]
EPOCH 671/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6938365596700481		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6938365596700481 | validation: 0.34106886521608726]
	TIME [epoch: 12 sec]
EPOCH 672/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3478523350599261		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3478523350599261 | validation: 0.1558715603713251]
	TIME [epoch: 12 sec]
EPOCH 673/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1484368929694258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1484368929694258 | validation: 0.8268276537149243]
	TIME [epoch: 11.9 sec]
EPOCH 674/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6817161476078022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6817161476078022 | validation: 0.1968110220818728]
	TIME [epoch: 12 sec]
EPOCH 675/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25440666262270695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25440666262270695 | validation: 0.1445744689727734]
	TIME [epoch: 11.9 sec]
EPOCH 676/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1621441407134269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1621441407134269 | validation: 0.31263749321580137]
	TIME [epoch: 12 sec]
EPOCH 677/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2335673974824355		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2335673974824355 | validation: 0.18895170168479203]
	TIME [epoch: 11.9 sec]
EPOCH 678/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2404338066334693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2404338066334693 | validation: 0.19581789772741295]
	TIME [epoch: 12 sec]
EPOCH 679/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16480913612153844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16480913612153844 | validation: 0.15228431775456697]
	TIME [epoch: 11.9 sec]
EPOCH 680/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15425650723362921		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15425650723362921 | validation: 0.30904994530567853]
	TIME [epoch: 12 sec]
EPOCH 681/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19892043285537914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19892043285537914 | validation: 0.1891928230635713]
	TIME [epoch: 12 sec]
EPOCH 682/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21403856705097699		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21403856705097699 | validation: 0.16560922893893848]
	TIME [epoch: 12 sec]
EPOCH 683/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15032125904982582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15032125904982582 | validation: 0.11965579368553203]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_683.pth
	Model improved!!!
EPOCH 684/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13017378641082336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13017378641082336 | validation: 0.14965966219051105]
	TIME [epoch: 12.1 sec]
EPOCH 685/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13430630703422766		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13430630703422766 | validation: 0.15351693922522883]
	TIME [epoch: 12 sec]
EPOCH 686/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18656416524117414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18656416524117414 | validation: 0.2318428823928448]
	TIME [epoch: 12.1 sec]
EPOCH 687/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15607593874657844		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15607593874657844 | validation: 0.1197138892462839]
	TIME [epoch: 12.1 sec]
EPOCH 688/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14576363850026972		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14576363850026972 | validation: 0.1978587006045977]
	TIME [epoch: 12.1 sec]
EPOCH 689/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1536628606151918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1536628606151918 | validation: 0.21622361159078035]
	TIME [epoch: 12.1 sec]
EPOCH 690/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24009062905338988		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24009062905338988 | validation: 2.1990078191873823]
	TIME [epoch: 12 sec]
EPOCH 691/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3418841391262286		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3418841391262286 | validation: 2.5364455847700764]
	TIME [epoch: 12 sec]
EPOCH 692/2000:
	Training over batches...
		[batch 1/1] avg loss: 3.138517284028289		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 3.138517284028289 | validation: 2.333729376225089]
	TIME [epoch: 12.1 sec]
EPOCH 693/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.9502051313063125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.9502051313063125 | validation: 2.0714071976119537]
	TIME [epoch: 12.1 sec]
EPOCH 694/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.3924567224926325		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.3924567224926325 | validation: 1.8846398869450731]
	TIME [epoch: 12.1 sec]
EPOCH 695/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1113304964543276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1113304964543276 | validation: 2.001806995003569]
	TIME [epoch: 12.1 sec]
EPOCH 696/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2276983958875656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2276983958875656 | validation: 2.047976524051851]
	TIME [epoch: 12.1 sec]
EPOCH 697/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.244615084897033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.244615084897033 | validation: 1.9182959967116417]
	TIME [epoch: 12.1 sec]
EPOCH 698/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.042219007974788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.042219007974788 | validation: 1.9583458021583462]
	TIME [epoch: 12.1 sec]
EPOCH 699/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1367206612650866		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1367206612650866 | validation: 1.955225482706634]
	TIME [epoch: 12.1 sec]
EPOCH 700/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.2487529189412108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.2487529189412108 | validation: 1.9313105663542522]
	TIME [epoch: 12.1 sec]
EPOCH 701/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1428327290945894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1428327290945894 | validation: 1.940535906850306]
	TIME [epoch: 12 sec]
EPOCH 702/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1590561623114066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1590561623114066 | validation: 1.943236654964138]
	TIME [epoch: 11.9 sec]
EPOCH 703/2000:
	Training over batches...
		[batch 1/1] avg loss: 2.1689472950271473		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 2.1689472950271473 | validation: 1.6159306916751053]
	TIME [epoch: 11.9 sec]
EPOCH 704/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5754625864685596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5754625864685596 | validation: 1.6366132325636435]
	TIME [epoch: 12 sec]
EPOCH 705/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5815952854255877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5815952854255877 | validation: 1.5342544840466998]
	TIME [epoch: 11.9 sec]
EPOCH 706/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5108729138448491		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5108729138448491 | validation: 1.5145620313841057]
	TIME [epoch: 11.9 sec]
EPOCH 707/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4846013609793443		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4846013609793443 | validation: 1.4904103591398616]
	TIME [epoch: 11.9 sec]
EPOCH 708/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.4530351270159902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.4530351270159902 | validation: 1.5665937228554088]
	TIME [epoch: 12 sec]
EPOCH 709/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.5006729488310493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.5006729488310493 | validation: 1.287102219685389]
	TIME [epoch: 11.9 sec]
EPOCH 710/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.295740704588078		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.295740704588078 | validation: 1.1823264961479272]
	TIME [epoch: 11.9 sec]
EPOCH 711/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2114710292924429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2114710292924429 | validation: 1.2652939730815975]
	TIME [epoch: 11.9 sec]
EPOCH 712/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2276364835463935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2276364835463935 | validation: 1.10245844144701]
	TIME [epoch: 11.9 sec]
EPOCH 713/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.0632549897827643		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.0632549897827643 | validation: 0.9349611188030853]
	TIME [epoch: 11.9 sec]
EPOCH 714/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.928425045216316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.928425045216316 | validation: 0.8909826097644805]
	TIME [epoch: 11.9 sec]
EPOCH 715/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8962699660247456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8962699660247456 | validation: 1.072628202789662]
	TIME [epoch: 11.9 sec]
EPOCH 716/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.040756813099419		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.040756813099419 | validation: 0.7786043329661094]
	TIME [epoch: 11.9 sec]
EPOCH 717/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8305045260435887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8305045260435887 | validation: 0.8296947685021929]
	TIME [epoch: 11.9 sec]
EPOCH 718/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8145431729409959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8145431729409959 | validation: 0.6206176040346127]
	TIME [epoch: 11.9 sec]
EPOCH 719/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.665609794079487		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.665609794079487 | validation: 0.549648369284727]
	TIME [epoch: 11.9 sec]
EPOCH 720/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.592251360904598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.592251360904598 | validation: 0.4820538600012103]
	TIME [epoch: 11.9 sec]
EPOCH 721/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.518494692144902		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.518494692144902 | validation: 0.4356514441642787]
	TIME [epoch: 11.9 sec]
EPOCH 722/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39436627034056204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39436627034056204 | validation: 0.5029441457316732]
	TIME [epoch: 11.9 sec]
EPOCH 723/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5274856128052033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5274856128052033 | validation: 0.2079412133652698]
	TIME [epoch: 11.9 sec]
EPOCH 724/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24914348867209918		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24914348867209918 | validation: 0.1977287226418396]
	TIME [epoch: 11.9 sec]
EPOCH 725/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2173546918192919		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2173546918192919 | validation: 0.3353032490004011]
	TIME [epoch: 11.9 sec]
EPOCH 726/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.40280103985279553		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.40280103985279553 | validation: 0.3213910504710057]
	TIME [epoch: 11.9 sec]
EPOCH 727/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.28608724177551675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.28608724177551675 | validation: 0.32578225804147487]
	TIME [epoch: 11.9 sec]
EPOCH 728/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38638773617904976		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38638773617904976 | validation: 0.21881830683952536]
	TIME [epoch: 11.9 sec]
EPOCH 729/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24847089930783825		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24847089930783825 | validation: 0.22472783516896855]
	TIME [epoch: 12 sec]
EPOCH 730/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2576881587593127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2576881587593127 | validation: 0.41901310649320983]
	TIME [epoch: 11.9 sec]
EPOCH 731/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34098485517994315		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34098485517994315 | validation: 0.21616187598002107]
	TIME [epoch: 12 sec]
EPOCH 732/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23043353884793405		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23043353884793405 | validation: 0.21285778469550293]
	TIME [epoch: 11.9 sec]
EPOCH 733/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1946766939256313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1946766939256313 | validation: 0.20100620683735873]
	TIME [epoch: 12 sec]
EPOCH 734/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25252814537270185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25252814537270185 | validation: 0.19885633100004527]
	TIME [epoch: 11.9 sec]
EPOCH 735/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1964439908628099		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1964439908628099 | validation: 0.19069410853602106]
	TIME [epoch: 11.9 sec]
EPOCH 736/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2107188403655747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2107188403655747 | validation: 0.28469154936052155]
	TIME [epoch: 11.9 sec]
EPOCH 737/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27227559722114836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27227559722114836 | validation: 0.2768874177130668]
	TIME [epoch: 12 sec]
EPOCH 738/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30116256597632307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30116256597632307 | validation: 0.23517324142286677]
	TIME [epoch: 11.9 sec]
EPOCH 739/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21692465385411655		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21692465385411655 | validation: 0.14970923560182992]
	TIME [epoch: 12 sec]
EPOCH 740/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1689909866402684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1689909866402684 | validation: 0.2496012726717317]
	TIME [epoch: 11.9 sec]
EPOCH 741/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2342487549569654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2342487549569654 | validation: 0.21403486047607673]
	TIME [epoch: 12 sec]
EPOCH 742/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2662574267513772		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2662574267513772 | validation: 0.16152881272455957]
	TIME [epoch: 11.9 sec]
EPOCH 743/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16720879018403934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16720879018403934 | validation: 0.2068917904207943]
	TIME [epoch: 12 sec]
EPOCH 744/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23019601234363174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23019601234363174 | validation: 0.2204558060283001]
	TIME [epoch: 11.9 sec]
EPOCH 745/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20918471886963755		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20918471886963755 | validation: 0.20388775632805528]
	TIME [epoch: 11.9 sec]
EPOCH 746/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21349689742652395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21349689742652395 | validation: 0.20521138145194034]
	TIME [epoch: 11.9 sec]
EPOCH 747/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19069704568833346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19069704568833346 | validation: 0.19760528453091186]
	TIME [epoch: 12 sec]
EPOCH 748/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20183117921678861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20183117921678861 | validation: 0.20442841032140657]
	TIME [epoch: 11.9 sec]
EPOCH 749/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16944726219858133		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16944726219858133 | validation: 0.1855155366151923]
	TIME [epoch: 12 sec]
EPOCH 750/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20593653119294003		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20593653119294003 | validation: 0.18717855824328344]
	TIME [epoch: 11.9 sec]
EPOCH 751/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1617497360883		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1617497360883 | validation: 0.16978219834889277]
	TIME [epoch: 12 sec]
EPOCH 752/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17557659621734623		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17557659621734623 | validation: 0.17604861313248807]
	TIME [epoch: 11.9 sec]
EPOCH 753/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16252611985282944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16252611985282944 | validation: 0.14979941869441377]
	TIME [epoch: 12 sec]
EPOCH 754/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17611254385498132		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17611254385498132 | validation: 0.11957917879667926]
	TIME [epoch: 11.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_754.pth
	Model improved!!!
EPOCH 755/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12981915315365558		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12981915315365558 | validation: 0.14849775065891158]
	TIME [epoch: 12.1 sec]
EPOCH 756/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1370592671145847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1370592671145847 | validation: 0.18545272522612627]
	TIME [epoch: 12 sec]
EPOCH 757/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22569756874238672		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22569756874238672 | validation: 0.46772593976957566]
	TIME [epoch: 12.1 sec]
EPOCH 758/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.30318757231754395		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.30318757231754395 | validation: 0.196351417129775]
	TIME [epoch: 11.9 sec]
EPOCH 759/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22299610145359253		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22299610145359253 | validation: 1.4560417640946561]
	TIME [epoch: 12 sec]
EPOCH 760/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.501463100530354		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.501463100530354 | validation: 1.837724620327645]
	TIME [epoch: 11.9 sec]
EPOCH 761/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.8597296991102719		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.8597296991102719 | validation: 1.5637450065117187]
	TIME [epoch: 12 sec]
EPOCH 762/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.586579490208468		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.586579490208468 | validation: 1.2953209392050753]
	TIME [epoch: 12 sec]
EPOCH 763/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2960043577998044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2960043577998044 | validation: 1.252478278344009]
	TIME [epoch: 12 sec]
EPOCH 764/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2390435995295774		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2390435995295774 | validation: 1.1677242176727123]
	TIME [epoch: 12 sec]
EPOCH 765/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2000559759188587		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2000559759188587 | validation: 1.1893365934282307]
	TIME [epoch: 12 sec]
EPOCH 766/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.235826541848724		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.235826541848724 | validation: 1.267441794802656]
	TIME [epoch: 12 sec]
EPOCH 767/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2868608404038628		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2868608404038628 | validation: 1.2166384295175372]
	TIME [epoch: 12 sec]
EPOCH 768/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.262959114991052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.262959114991052 | validation: 1.3225442653135215]
	TIME [epoch: 11.9 sec]
EPOCH 769/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.3127484053207412		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.3127484053207412 | validation: 1.173990341174976]
	TIME [epoch: 12 sec]
EPOCH 770/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2181068636897479		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2181068636897479 | validation: 1.2741892602878817]
	TIME [epoch: 12 sec]
EPOCH 771/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.2467571368446237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.2467571368446237 | validation: 1.1430255005638095]
	TIME [epoch: 12 sec]
EPOCH 772/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.20623445385279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.20623445385279 | validation: 0.7823710911583255]
	TIME [epoch: 12 sec]
EPOCH 773/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8634354331144886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8634354331144886 | validation: 0.6520770918350263]
	TIME [epoch: 12 sec]
EPOCH 774/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7251978421921841		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7251978421921841 | validation: 0.523866138156672]
	TIME [epoch: 12 sec]
EPOCH 775/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5501178921468367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5501178921468367 | validation: 0.5753659189060402]
	TIME [epoch: 12 sec]
EPOCH 776/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5875022558572629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5875022558572629 | validation: 0.32558970744576704]
	TIME [epoch: 12 sec]
EPOCH 777/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38350231067426066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38350231067426066 | validation: 0.32932969337918766]
	TIME [epoch: 11.9 sec]
EPOCH 778/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3763001237016684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3763001237016684 | validation: 0.3193036235838388]
	TIME [epoch: 12 sec]
EPOCH 779/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.34787015136184785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.34787015136184785 | validation: 0.29360050325801973]
	TIME [epoch: 12.1 sec]
EPOCH 780/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.33477851502135125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.33477851502135125 | validation: 0.2829806272873177]
	TIME [epoch: 12.1 sec]
EPOCH 781/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3571604293517836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3571604293517836 | validation: 0.44522922580670427]
	TIME [epoch: 12.1 sec]
EPOCH 782/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4669289087719002		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4669289087719002 | validation: 0.25977180245239656]
	TIME [epoch: 12.1 sec]
EPOCH 783/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3081739964735718		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3081739964735718 | validation: 0.25885272006224075]
	TIME [epoch: 12.1 sec]
EPOCH 784/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2996967398769414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2996967398769414 | validation: 0.38401324001915776]
	TIME [epoch: 12.1 sec]
EPOCH 785/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.39213001295303906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.39213001295303906 | validation: 0.2777071206718056]
	TIME [epoch: 12.1 sec]
EPOCH 786/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32315693161232373		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32315693161232373 | validation: 0.2507370897912266]
	TIME [epoch: 12.1 sec]
EPOCH 787/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27298696897008945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27298696897008945 | validation: 0.20386781024144904]
	TIME [epoch: 12.1 sec]
EPOCH 788/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23260210453281194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23260210453281194 | validation: 0.7643212004516325]
	TIME [epoch: 12.1 sec]
EPOCH 789/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.8062886857347584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.8062886857347584 | validation: 0.6687120662195121]
	TIME [epoch: 12.1 sec]
EPOCH 790/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7060412865063614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7060412865063614 | validation: 0.32068833090162674]
	TIME [epoch: 12.1 sec]
EPOCH 791/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.38541484895505185		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.38541484895505185 | validation: 0.3027223334397458]
	TIME [epoch: 12.1 sec]
EPOCH 792/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3473914277964957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3473914277964957 | validation: 0.33693823589461]
	TIME [epoch: 12 sec]
EPOCH 793/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.35427049500698155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.35427049500698155 | validation: 0.2530946564052285]
	TIME [epoch: 12 sec]
EPOCH 794/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.27030837027363047		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.27030837027363047 | validation: 0.24104024473381702]
	TIME [epoch: 12 sec]
EPOCH 795/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25441663513918444		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25441663513918444 | validation: 0.2657564622499561]
	TIME [epoch: 12.1 sec]
EPOCH 796/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2491565551052977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2491565551052977 | validation: 0.24451271678448352]
	TIME [epoch: 12 sec]
EPOCH 797/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22774205555002625		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22774205555002625 | validation: 0.21501668849621403]
	TIME [epoch: 12 sec]
EPOCH 798/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2115226996881994		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2115226996881994 | validation: 0.6397073795677956]
	TIME [epoch: 12 sec]
EPOCH 799/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5523565038510053		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5523565038510053 | validation: 0.37468865803404755]
	TIME [epoch: 12 sec]
EPOCH 800/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3462723751991456		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3462723751991456 | validation: 0.2545438691552875]
	TIME [epoch: 12 sec]
EPOCH 801/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24830600886897594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24830600886897594 | validation: 0.21634117116827167]
	TIME [epoch: 12 sec]
EPOCH 802/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23318594856948538		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23318594856948538 | validation: 0.22935326881757254]
	TIME [epoch: 11.9 sec]
EPOCH 803/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22427098570606535		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22427098570606535 | validation: 0.585601160287183]
	TIME [epoch: 12 sec]
EPOCH 804/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.43698484188823095		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.43698484188823095 | validation: 0.18979408886881854]
	TIME [epoch: 11.9 sec]
EPOCH 805/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20852849155212957		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20852849155212957 | validation: 0.1578750526670085]
	TIME [epoch: 12 sec]
EPOCH 806/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18760707437233795		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18760707437233795 | validation: 0.14396511771726445]
	TIME [epoch: 12 sec]
EPOCH 807/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16152397667153762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16152397667153762 | validation: 0.1579955794696389]
	TIME [epoch: 12 sec]
EPOCH 808/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1589264023970004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1589264023970004 | validation: 0.15620329913718245]
	TIME [epoch: 12 sec]
EPOCH 809/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15260482184030114		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15260482184030114 | validation: 0.15965067715617645]
	TIME [epoch: 12 sec]
EPOCH 810/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1560008905261062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1560008905261062 | validation: 0.1669738613924309]
	TIME [epoch: 11.8 sec]
EPOCH 811/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15687006758691335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15687006758691335 | validation: 0.14458015749674602]
	TIME [epoch: 12 sec]
EPOCH 812/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13625452160762064		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13625452160762064 | validation: 0.13774386041615802]
	TIME [epoch: 12 sec]
EPOCH 813/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13025207928143276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13025207928143276 | validation: 0.16341539225952506]
	TIME [epoch: 12 sec]
EPOCH 814/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1385105973561615		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1385105973561615 | validation: 0.15244155858904504]
	TIME [epoch: 11.9 sec]
EPOCH 815/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1837101967801463		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1837101967801463 | validation: 0.18166202996588698]
	TIME [epoch: 12 sec]
EPOCH 816/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14352805715711262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14352805715711262 | validation: 0.13057425787498908]
	TIME [epoch: 11.9 sec]
EPOCH 817/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1381788421564805		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1381788421564805 | validation: 0.14705585278427888]
	TIME [epoch: 12 sec]
EPOCH 818/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12713137499045085		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12713137499045085 | validation: 0.17651577609443325]
	TIME [epoch: 11.9 sec]
EPOCH 819/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13936511972948437		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13936511972948437 | validation: 0.14087619810480198]
	TIME [epoch: 12 sec]
EPOCH 820/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14079411380872808		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14079411380872808 | validation: 0.17243506311196757]
	TIME [epoch: 12 sec]
EPOCH 821/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1399594166556509		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1399594166556509 | validation: 0.1308339889903859]
	TIME [epoch: 12 sec]
EPOCH 822/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13492816945513586		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13492816945513586 | validation: 0.19064703654398119]
	TIME [epoch: 12 sec]
EPOCH 823/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13608751664852817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13608751664852817 | validation: 0.1299721424231585]
	TIME [epoch: 12 sec]
EPOCH 824/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16255939382426945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16255939382426945 | validation: 0.23946223034467337]
	TIME [epoch: 12 sec]
EPOCH 825/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15957677617005123		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15957677617005123 | validation: 0.12625749097217248]
	TIME [epoch: 12 sec]
EPOCH 826/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18530355313237418		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18530355313237418 | validation: 0.19732170536274363]
	TIME [epoch: 11.9 sec]
EPOCH 827/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1899606024521147		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1899606024521147 | validation: 0.2189694633778373]
	TIME [epoch: 12 sec]
EPOCH 828/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2138119733983072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2138119733983072 | validation: 0.11327182563056903]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_828.pth
	Model improved!!!
EPOCH 829/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051866663161452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1051866663161452 | validation: 0.09656839660086423]
	TIME [epoch: 12.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_829.pth
	Model improved!!!
EPOCH 830/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11436482334682346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11436482334682346 | validation: 0.1568730469814411]
	TIME [epoch: 12 sec]
EPOCH 831/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10622498481609262		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10622498481609262 | validation: 0.11695802214374545]
	TIME [epoch: 12 sec]
EPOCH 832/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1487823835386359		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1487823835386359 | validation: 0.227534199212649]
	TIME [epoch: 12 sec]
EPOCH 833/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17182383008506627		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17182383008506627 | validation: 0.21499628893592437]
	TIME [epoch: 12 sec]
EPOCH 834/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23849349077876864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23849349077876864 | validation: 0.1407526982883982]
	TIME [epoch: 12 sec]
EPOCH 835/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12356510281279352		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12356510281279352 | validation: 0.1406532587191187]
	TIME [epoch: 12 sec]
EPOCH 836/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13508117901259273		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13508117901259273 | validation: 0.1785513762135508]
	TIME [epoch: 12 sec]
EPOCH 837/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1308578366848554		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1308578366848554 | validation: 0.11919994069295986]
	TIME [epoch: 12 sec]
EPOCH 838/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1486353933286708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1486353933286708 | validation: 0.18192482210633565]
	TIME [epoch: 12 sec]
EPOCH 839/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305425468846863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305425468846863 | validation: 0.12317566421734889]
	TIME [epoch: 12 sec]
EPOCH 840/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1439071274699946		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1439071274699946 | validation: 0.69044942420517]
	TIME [epoch: 12 sec]
EPOCH 841/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7380698331795911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7380698331795911 | validation: 0.5691724310694962]
	TIME [epoch: 12 sec]
EPOCH 842/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5838304835519899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5838304835519899 | validation: 0.30096826722088116]
	TIME [epoch: 12.1 sec]
EPOCH 843/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3451480426014664		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3451480426014664 | validation: 0.2349649464768315]
	TIME [epoch: 12 sec]
EPOCH 844/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26931083531918637		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26931083531918637 | validation: 0.19346674324807794]
	TIME [epoch: 12.1 sec]
EPOCH 845/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2066034664676968		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2066034664676968 | validation: 0.22292950867429517]
	TIME [epoch: 12 sec]
EPOCH 846/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18153947765224424		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18153947765224424 | validation: 0.14552101717068525]
	TIME [epoch: 12.1 sec]
EPOCH 847/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1614391032850871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1614391032850871 | validation: 0.20444795236254426]
	TIME [epoch: 12 sec]
EPOCH 848/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14944067192086669		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14944067192086669 | validation: 0.17506198052726687]
	TIME [epoch: 12 sec]
EPOCH 849/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1588881559316907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1588881559316907 | validation: 0.19245221110442132]
	TIME [epoch: 12 sec]
EPOCH 850/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16730069982360135		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16730069982360135 | validation: 0.18034053506811581]
	TIME [epoch: 12 sec]
EPOCH 851/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1512636502070119		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1512636502070119 | validation: 0.15691446065318596]
	TIME [epoch: 12 sec]
EPOCH 852/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13032195793480786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13032195793480786 | validation: 0.13913748844797338]
	TIME [epoch: 12.1 sec]
EPOCH 853/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14037850816893335		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14037850816893335 | validation: 0.18758725153466946]
	TIME [epoch: 12 sec]
EPOCH 854/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1393287037013499		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1393287037013499 | validation: 0.14531847728305475]
	TIME [epoch: 12.1 sec]
EPOCH 855/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13982085915673328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13982085915673328 | validation: 0.18694461772461013]
	TIME [epoch: 12 sec]
EPOCH 856/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12819222505741598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12819222505741598 | validation: 0.104531778620452]
	TIME [epoch: 12.1 sec]
EPOCH 857/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1305150071388474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1305150071388474 | validation: 0.19039511314906893]
	TIME [epoch: 12 sec]
EPOCH 858/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11835184849397176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11835184849397176 | validation: 0.10666831851679576]
	TIME [epoch: 12 sec]
EPOCH 859/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12070843747638398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12070843747638398 | validation: 0.1320329065183142]
	TIME [epoch: 12 sec]
EPOCH 860/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11730422290339462		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11730422290339462 | validation: 0.18566083824815577]
	TIME [epoch: 12 sec]
EPOCH 861/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1455034786993835		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1455034786993835 | validation: 0.1365557293033747]
	TIME [epoch: 12 sec]
EPOCH 862/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12002553605199869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12002553605199869 | validation: 0.11563748633715615]
	TIME [epoch: 12.1 sec]
EPOCH 863/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12388359408114065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12388359408114065 | validation: 0.22554132360949977]
	TIME [epoch: 12 sec]
EPOCH 864/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1542422128783681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1542422128783681 | validation: 0.12604844834134538]
	TIME [epoch: 12 sec]
EPOCH 865/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19741160557685697		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19741160557685697 | validation: 0.10735188403351639]
	TIME [epoch: 12 sec]
EPOCH 866/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11797108929251947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11797108929251947 | validation: 0.19611194653190714]
	TIME [epoch: 12 sec]
EPOCH 867/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13597915017333598		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13597915017333598 | validation: 0.08023914091240246]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_867.pth
	Model improved!!!
EPOCH 868/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0998137096426581		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0998137096426581 | validation: 0.09210507393522047]
	TIME [epoch: 12 sec]
EPOCH 869/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09530712688679333		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09530712688679333 | validation: 0.17800743563897287]
	TIME [epoch: 12.1 sec]
EPOCH 870/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13073397206708287		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13073397206708287 | validation: 0.1844901528132936]
	TIME [epoch: 12.1 sec]
EPOCH 871/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17899025833220264		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17899025833220264 | validation: 0.15284025652823283]
	TIME [epoch: 12.1 sec]
EPOCH 872/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13273684106962752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13273684106962752 | validation: 0.10323853086408437]
	TIME [epoch: 12 sec]
EPOCH 873/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1087883054274307		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1087883054274307 | validation: 0.10756100200945856]
	TIME [epoch: 12.1 sec]
EPOCH 874/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09283241237127383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09283241237127383 | validation: 0.1310864728333022]
	TIME [epoch: 12 sec]
EPOCH 875/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1497476444779886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1497476444779886 | validation: 0.5518553376535476]
	TIME [epoch: 12.1 sec]
EPOCH 876/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.36430638295797296		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.36430638295797296 | validation: 0.27903665932236293]
	TIME [epoch: 12 sec]
EPOCH 877/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2847362392002504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2847362392002504 | validation: 0.1810664265748867]
	TIME [epoch: 12.1 sec]
EPOCH 878/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18518604795260174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18518604795260174 | validation: 0.15823392640915404]
	TIME [epoch: 12.1 sec]
EPOCH 879/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1455181505317618		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1455181505317618 | validation: 0.17241207212260995]
	TIME [epoch: 12.1 sec]
EPOCH 880/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16520780566718785		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16520780566718785 | validation: 0.12178381114460307]
	TIME [epoch: 12.1 sec]
EPOCH 881/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11610980103202342		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11610980103202342 | validation: 0.10868496058872959]
	TIME [epoch: 12.1 sec]
EPOCH 882/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10719757925198911		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10719757925198911 | validation: 0.12412492107919491]
	TIME [epoch: 12 sec]
EPOCH 883/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10614522561377567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10614522561377567 | validation: 1.0485611447679095]
	TIME [epoch: 12 sec]
EPOCH 884/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9794013667331681		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9794013667331681 | validation: 1.2023210891932021]
	TIME [epoch: 12.1 sec]
EPOCH 885/2000:
	Training over batches...
		[batch 1/1] avg loss: 1.106768102657429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 1.106768102657429 | validation: 0.5854617641231114]
	TIME [epoch: 12.1 sec]
EPOCH 886/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.5991677074404066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.5991677074404066 | validation: 0.42494533813864876]
	TIME [epoch: 12 sec]
EPOCH 887/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4857159666177104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4857159666177104 | validation: 0.3193066386659891]
	TIME [epoch: 12.1 sec]
EPOCH 888/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4552870817933156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4552870817933156 | validation: 0.3644696248255741]
	TIME [epoch: 12 sec]
EPOCH 889/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4316103979205925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4316103979205925 | validation: 0.25789372524200377]
	TIME [epoch: 12.1 sec]
EPOCH 890/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3561809681744269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3561809681744269 | validation: 0.302786370279992]
	TIME [epoch: 12.1 sec]
EPOCH 891/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.3227847363690466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.3227847363690466 | validation: 0.2503126513969313]
	TIME [epoch: 12 sec]
EPOCH 892/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25412334939743614		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25412334939743614 | validation: 0.2424682956505495]
	TIME [epoch: 12 sec]
EPOCH 893/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21427060819479152		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21427060819479152 | validation: 0.1935268287703177]
	TIME [epoch: 12.1 sec]
EPOCH 894/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19730720962879156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19730720962879156 | validation: 0.2205854036345134]
	TIME [epoch: 12 sec]
EPOCH 895/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14546628313572943		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14546628313572943 | validation: 0.1318022350938688]
	TIME [epoch: 12.1 sec]
EPOCH 896/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11772896281139508		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11772896281139508 | validation: 0.10247681504305826]
	TIME [epoch: 12 sec]
EPOCH 897/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10193299839838556		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10193299839838556 | validation: 0.12941058474657705]
	TIME [epoch: 12.1 sec]
EPOCH 898/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10901197029555065		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10901197029555065 | validation: 0.14646795406520494]
	TIME [epoch: 12 sec]
EPOCH 899/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1068073057495606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1068073057495606 | validation: 0.1636392965304727]
	TIME [epoch: 12.1 sec]
EPOCH 900/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.2023336300406762		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.2023336300406762 | validation: 0.2530496584265564]
	TIME [epoch: 12 sec]
EPOCH 901/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16801097177723698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16801097177723698 | validation: 0.15766907160125887]
	TIME [epoch: 12.1 sec]
EPOCH 902/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1857088013651846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1857088013651846 | validation: 0.109338891358421]
	TIME [epoch: 12 sec]
EPOCH 903/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09836554365670416		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09836554365670416 | validation: 0.1684095080902957]
	TIME [epoch: 12 sec]
EPOCH 904/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11386294364747786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11386294364747786 | validation: 0.13329283335007708]
	TIME [epoch: 12 sec]
EPOCH 905/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17410008464453752		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17410008464453752 | validation: 0.15007009672161772]
	TIME [epoch: 12 sec]
EPOCH 906/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13765087645546425		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13765087645546425 | validation: 0.1426951574705384]
	TIME [epoch: 12 sec]
EPOCH 907/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12558195307455952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12558195307455952 | validation: 0.11983036171207889]
	TIME [epoch: 12 sec]
EPOCH 908/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09903924302354461		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09903924302354461 | validation: 0.11723441154192088]
	TIME [epoch: 12 sec]
EPOCH 909/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12434631918519276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12434631918519276 | validation: 0.2608246586648075]
	TIME [epoch: 12 sec]
EPOCH 910/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1543674464445431		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1543674464445431 | validation: 0.13026906149913783]
	TIME [epoch: 12 sec]
EPOCH 911/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14511259242415062		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14511259242415062 | validation: 0.13717354286838185]
	TIME [epoch: 12 sec]
EPOCH 912/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10927618548587215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10927618548587215 | validation: 0.13086264640206205]
	TIME [epoch: 12 sec]
EPOCH 913/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10816725909987912		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10816725909987912 | validation: 0.1579081852121382]
	TIME [epoch: 12 sec]
EPOCH 914/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1128903915496021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1128903915496021 | validation: 0.10016364707684655]
	TIME [epoch: 12 sec]
EPOCH 915/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.123118170508024		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.123118170508024 | validation: 0.14175392107687826]
	TIME [epoch: 12 sec]
EPOCH 916/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10744234157517124		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10744234157517124 | validation: 0.11156165011354316]
	TIME [epoch: 12 sec]
EPOCH 917/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12284780497946177		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12284780497946177 | validation: 0.25497489987175176]
	TIME [epoch: 12.1 sec]
EPOCH 918/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1578123618260111		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1578123618260111 | validation: 0.10939132985013073]
	TIME [epoch: 12 sec]
EPOCH 919/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1519386201656717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1519386201656717 | validation: 0.11538544871194177]
	TIME [epoch: 12 sec]
EPOCH 920/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09606391537819801		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09606391537819801 | validation: 0.11966095673182187]
	TIME [epoch: 12.1 sec]
EPOCH 921/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0993966400790245		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0993966400790245 | validation: 0.1045930648417639]
	TIME [epoch: 12.1 sec]
EPOCH 922/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10224182563136802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10224182563136802 | validation: 0.12188043534650835]
	TIME [epoch: 12 sec]
EPOCH 923/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1131468254778149		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1131468254778149 | validation: 0.18033770658639875]
	TIME [epoch: 12 sec]
EPOCH 924/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13018022135891896		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13018022135891896 | validation: 0.14845080627523802]
	TIME [epoch: 12 sec]
EPOCH 925/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20151580759317916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20151580759317916 | validation: 0.17722489561520172]
	TIME [epoch: 12.1 sec]
EPOCH 926/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12917304530019127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12917304530019127 | validation: 0.08275711807210097]
	TIME [epoch: 12 sec]
EPOCH 927/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10224831812321322		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10224831812321322 | validation: 0.12635015074677258]
	TIME [epoch: 12 sec]
EPOCH 928/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09695775365413947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09695775365413947 | validation: 0.12167514647354204]
	TIME [epoch: 12 sec]
EPOCH 929/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11818985450727151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11818985450727151 | validation: 0.16455067646566268]
	TIME [epoch: 12.1 sec]
EPOCH 930/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11143404883224836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11143404883224836 | validation: 0.14849401367579787]
	TIME [epoch: 12.1 sec]
EPOCH 931/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1417947265359497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1417947265359497 | validation: 0.17649369958347097]
	TIME [epoch: 12 sec]
EPOCH 932/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14579417603231998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14579417603231998 | validation: 0.17831415924016789]
	TIME [epoch: 12 sec]
EPOCH 933/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16657926864357714		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16657926864357714 | validation: 0.4262428212333054]
	TIME [epoch: 12 sec]
EPOCH 934/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25523359502540727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25523359502540727 | validation: 0.11333451526426762]
	TIME [epoch: 12 sec]
EPOCH 935/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14224864229213613		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14224864229213613 | validation: 0.10995480782848688]
	TIME [epoch: 12 sec]
EPOCH 936/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09484942370427422		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09484942370427422 | validation: 0.11766758532880467]
	TIME [epoch: 12.1 sec]
EPOCH 937/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11854191273431715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11854191273431715 | validation: 0.19221057420944032]
	TIME [epoch: 12 sec]
EPOCH 938/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12529739728540398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12529739728540398 | validation: 0.11103218101309778]
	TIME [epoch: 12.1 sec]
EPOCH 939/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1447957834753465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1447957834753465 | validation: 0.12429017699820609]
	TIME [epoch: 12 sec]
EPOCH 940/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10041365106863032		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10041365106863032 | validation: 0.15618717990666228]
	TIME [epoch: 12.1 sec]
EPOCH 941/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14359265695433845		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14359265695433845 | validation: 0.11920184671611174]
	TIME [epoch: 12 sec]
EPOCH 942/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11746458224028125		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11746458224028125 | validation: 0.09914153766903924]
	TIME [epoch: 12 sec]
EPOCH 943/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1040538027923817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1040538027923817 | validation: 0.15478741813178112]
	TIME [epoch: 12 sec]
EPOCH 944/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1030846310415949		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1030846310415949 | validation: 0.14525091989621258]
	TIME [epoch: 12.1 sec]
EPOCH 945/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1455445902062676		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1455445902062676 | validation: 0.21865606024978546]
	TIME [epoch: 12.1 sec]
EPOCH 946/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14244534392085237		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14244534392085237 | validation: 0.11516597568185581]
	TIME [epoch: 12 sec]
EPOCH 947/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11598003396297812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11598003396297812 | validation: 0.1416063994621279]
	TIME [epoch: 12.1 sec]
EPOCH 948/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11286923349512773		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11286923349512773 | validation: 0.11640794860165482]
	TIME [epoch: 12 sec]
EPOCH 949/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1315994168686016		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1315994168686016 | validation: 0.22206073656917688]
	TIME [epoch: 12 sec]
EPOCH 950/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16079894430774794		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16079894430774794 | validation: 0.13152255072101424]
	TIME [epoch: 12.1 sec]
EPOCH 951/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12465007291121445		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12465007291121445 | validation: 0.10183468424803746]
	TIME [epoch: 12.1 sec]
EPOCH 952/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11434216990708629		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11434216990708629 | validation: 0.14618364278187937]
	TIME [epoch: 12.1 sec]
EPOCH 953/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09401949609104343		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09401949609104343 | validation: 0.07842433421868202]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_953.pth
	Model improved!!!
EPOCH 954/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11864262535582143		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11864262535582143 | validation: 0.10773054218172991]
	TIME [epoch: 12.1 sec]
EPOCH 955/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08747745843123828		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08747745843123828 | validation: 0.1689545178272232]
	TIME [epoch: 12 sec]
EPOCH 956/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10334266739022503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10334266739022503 | validation: 0.15775506293094804]
	TIME [epoch: 12 sec]
EPOCH 957/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1791291627278543		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1791291627278543 | validation: 0.18181176634341087]
	TIME [epoch: 12 sec]
EPOCH 958/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13087698487081942		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13087698487081942 | validation: 0.11967469788087635]
	TIME [epoch: 12 sec]
EPOCH 959/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1249448184359908		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1249448184359908 | validation: 0.08649895615924058]
	TIME [epoch: 12 sec]
EPOCH 960/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11073496417942348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11073496417942348 | validation: 0.36306263826406004]
	TIME [epoch: 12.1 sec]
EPOCH 961/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.32139424629811364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.32139424629811364 | validation: 0.301447705476148]
	TIME [epoch: 12 sec]
EPOCH 962/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.21664452776372717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.21664452776372717 | validation: 0.22219283712952065]
	TIME [epoch: 12.1 sec]
EPOCH 963/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14745499839106116		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14745499839106116 | validation: 0.11437127814964068]
	TIME [epoch: 12 sec]
EPOCH 964/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14391935592083374		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14391935592083374 | validation: 0.11506299995545076]
	TIME [epoch: 12.1 sec]
EPOCH 965/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10257378081824961		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10257378081824961 | validation: 0.15942464075787022]
	TIME [epoch: 12 sec]
EPOCH 966/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11230556700728887		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11230556700728887 | validation: 0.11367715442292842]
	TIME [epoch: 12.1 sec]
EPOCH 967/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10840249631599194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10840249631599194 | validation: 0.10767760740190489]
	TIME [epoch: 12 sec]
EPOCH 968/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11321644624972792		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11321644624972792 | validation: 0.33615868188814413]
	TIME [epoch: 12.1 sec]
EPOCH 969/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17521142530976966		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17521142530976966 | validation: 0.17324913279112475]
	TIME [epoch: 12 sec]
EPOCH 970/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18476543573902965		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18476543573902965 | validation: 0.1184791026143327]
	TIME [epoch: 12 sec]
EPOCH 971/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10092938803640639		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10092938803640639 | validation: 0.09403283392453202]
	TIME [epoch: 12 sec]
EPOCH 972/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08769773701638243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08769773701638243 | validation: 0.09370420887828843]
	TIME [epoch: 12 sec]
EPOCH 973/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08616217574547083		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08616217574547083 | validation: 0.1147193342053114]
	TIME [epoch: 12 sec]
EPOCH 974/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09441704911786877		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09441704911786877 | validation: 0.1551570387957019]
	TIME [epoch: 12 sec]
EPOCH 975/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10069535056210831		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10069535056210831 | validation: 0.10884636851903112]
	TIME [epoch: 12 sec]
EPOCH 976/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1430638562108735		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1430638562108735 | validation: 0.2663749585759782]
	TIME [epoch: 12 sec]
EPOCH 977/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14133814945965947		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14133814945965947 | validation: 0.09542215080755458]
	TIME [epoch: 12 sec]
EPOCH 978/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10549376169043044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10549376169043044 | validation: 0.11130477741949676]
	TIME [epoch: 12 sec]
EPOCH 979/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09964810698607154		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09964810698607154 | validation: 0.13258236321533645]
	TIME [epoch: 12.1 sec]
EPOCH 980/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10188244070330214		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10188244070330214 | validation: 0.06807545406747592]
	TIME [epoch: 12 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_980.pth
	Model improved!!!
EPOCH 981/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09239671703743998		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09239671703743998 | validation: 0.10299037041387452]
	TIME [epoch: 12.1 sec]
EPOCH 982/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09211730505140263		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09211730505140263 | validation: 0.13218236759863608]
	TIME [epoch: 12.1 sec]
EPOCH 983/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09429702736775472		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09429702736775472 | validation: 0.1454309440373794]
	TIME [epoch: 12 sec]
EPOCH 984/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15137024251875872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15137024251875872 | validation: 0.2220663209968722]
	TIME [epoch: 12.1 sec]
EPOCH 985/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14061943870177238		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14061943870177238 | validation: 0.10690366733930237]
	TIME [epoch: 12.1 sec]
EPOCH 986/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12519981018001847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12519981018001847 | validation: 0.09179751732237969]
	TIME [epoch: 12 sec]
EPOCH 987/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0837816149178387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0837816149178387 | validation: 0.09073451351322023]
	TIME [epoch: 12 sec]
EPOCH 988/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08335719761035279		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08335719761035279 | validation: 0.09727086983516171]
	TIME [epoch: 12.1 sec]
EPOCH 989/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0854958111354879		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0854958111354879 | validation: 0.09909057535343452]
	TIME [epoch: 12.1 sec]
EPOCH 990/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10555972060732244		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10555972060732244 | validation: 0.16411749504047046]
	TIME [epoch: 12.1 sec]
EPOCH 991/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11603672705106818		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11603672705106818 | validation: 0.10994082292731396]
	TIME [epoch: 12.1 sec]
EPOCH 992/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1296916849725944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1296916849725944 | validation: 0.15441459485415987]
	TIME [epoch: 12.1 sec]
EPOCH 993/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10335894171058137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10335894171058137 | validation: 0.11256274018514488]
	TIME [epoch: 12.1 sec]
EPOCH 994/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10276814459608698		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10276814459608698 | validation: 0.11775140288527504]
	TIME [epoch: 12.1 sec]
EPOCH 995/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11094969036670074		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11094969036670074 | validation: 0.10559272726513202]
	TIME [epoch: 12.1 sec]
EPOCH 996/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10373523038846387		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10373523038846387 | validation: 0.12387532165446276]
	TIME [epoch: 12.1 sec]
EPOCH 997/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0829217778213674		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0829217778213674 | validation: 0.09020300513097029]
	TIME [epoch: 12.1 sec]
EPOCH 998/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09181193882292905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09181193882292905 | validation: 0.1821009904232203]
	TIME [epoch: 12.1 sec]
EPOCH 999/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10632971251386533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10632971251386533 | validation: 0.09303927937888971]
	TIME [epoch: 12 sec]
EPOCH 1000/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11841231180710049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11841231180710049 | validation: 0.19622578094358328]
	TIME [epoch: 12.1 sec]
EPOCH 1001/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1091165892731654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1091165892731654 | validation: 0.09366895288081697]
	TIME [epoch: 212 sec]
EPOCH 1002/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09933009902406925		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09933009902406925 | validation: 0.1526162978731804]
	TIME [epoch: 25 sec]
EPOCH 1003/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10789109997123005		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10789109997123005 | validation: 0.11640326351620503]
	TIME [epoch: 24.8 sec]
EPOCH 1004/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11176412365703929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11176412365703929 | validation: 0.08458067952880044]
	TIME [epoch: 25 sec]
EPOCH 1005/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08636135139389815		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08636135139389815 | validation: 0.11025716214885972]
	TIME [epoch: 24.9 sec]
EPOCH 1006/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07843596202853712		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07843596202853712 | validation: 0.07645312333017289]
	TIME [epoch: 24.9 sec]
EPOCH 1007/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07665161892417746		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07665161892417746 | validation: 0.12304399713730639]
	TIME [epoch: 25 sec]
EPOCH 1008/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08549153230289636		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08549153230289636 | validation: 0.07687567723357262]
	TIME [epoch: 25.1 sec]
EPOCH 1009/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09816336946636606		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09816336946636606 | validation: 0.2554763827622259]
	TIME [epoch: 25 sec]
EPOCH 1010/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13537474521792855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13537474521792855 | validation: 0.11781150194234354]
	TIME [epoch: 25.1 sec]
EPOCH 1011/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16081588436388192		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16081588436388192 | validation: 0.21472728094155893]
	TIME [epoch: 24.9 sec]
EPOCH 1012/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17843515866287313		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17843515866287313 | validation: 0.1604696503315739]
	TIME [epoch: 25 sec]
EPOCH 1013/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13604339493529705		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13604339493529705 | validation: 0.11298571506261329]
	TIME [epoch: 24.9 sec]
EPOCH 1014/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09103089609821667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09103089609821667 | validation: 0.09088704754693964]
	TIME [epoch: 24.9 sec]
EPOCH 1015/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07906959061184017		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07906959061184017 | validation: 0.10833109589777307]
	TIME [epoch: 25 sec]
EPOCH 1016/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09030915363383404		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09030915363383404 | validation: 0.12962131957114473]
	TIME [epoch: 25 sec]
EPOCH 1017/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10026769759840834		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10026769759840834 | validation: 0.08581614390556544]
	TIME [epoch: 25 sec]
EPOCH 1018/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12371466414551761		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12371466414551761 | validation: 0.24515075511416937]
	TIME [epoch: 25.1 sec]
EPOCH 1019/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13380807602097022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13380807602097022 | validation: 0.12887829033169199]
	TIME [epoch: 24.9 sec]
EPOCH 1020/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15276642291010867		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15276642291010867 | validation: 0.1316205876561488]
	TIME [epoch: 25 sec]
EPOCH 1021/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10446952602681248		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10446952602681248 | validation: 0.11262180864009348]
	TIME [epoch: 24.9 sec]
EPOCH 1022/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09632968943204288		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09632968943204288 | validation: 0.17484768764454273]
	TIME [epoch: 25.1 sec]
EPOCH 1023/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11112781359517448		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11112781359517448 | validation: 0.09875956558415927]
	TIME [epoch: 24.9 sec]
EPOCH 1024/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11256442933953928		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11256442933953928 | validation: 0.13094168087151462]
	TIME [epoch: 25 sec]
EPOCH 1025/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09832287878282206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09832287878282206 | validation: 0.07123762026542309]
	TIME [epoch: 24.9 sec]
EPOCH 1026/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07426882810314693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07426882810314693 | validation: 0.112979718966682]
	TIME [epoch: 25 sec]
EPOCH 1027/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08154815205397513		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08154815205397513 | validation: 0.08737769474464072]
	TIME [epoch: 25 sec]
EPOCH 1028/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08727863184006777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08727863184006777 | validation: 0.12212786983668222]
	TIME [epoch: 25 sec]
EPOCH 1029/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08816964608830531		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08816964608830531 | validation: 0.08869156682455581]
	TIME [epoch: 25 sec]
EPOCH 1030/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09297760111160995		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09297760111160995 | validation: 0.2150435051444263]
	TIME [epoch: 25 sec]
EPOCH 1031/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12914065009691394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12914065009691394 | validation: 0.13543107459390988]
	TIME [epoch: 24.9 sec]
EPOCH 1032/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1721316018842023		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1721316018842023 | validation: 0.1669532092206313]
	TIME [epoch: 25 sec]
EPOCH 1033/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10538041115297843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10538041115297843 | validation: 0.06811116059398595]
	TIME [epoch: 25 sec]
EPOCH 1034/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0705567125698663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0705567125698663 | validation: 0.08160152561660292]
	TIME [epoch: 24.9 sec]
EPOCH 1035/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08614311261790779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08614311261790779 | validation: 0.1417918765061783]
	TIME [epoch: 24.9 sec]
EPOCH 1036/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09304576964832144		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09304576964832144 | validation: 0.08736568787506105]
	TIME [epoch: 25 sec]
EPOCH 1037/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09302296578756183		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09302296578756183 | validation: 0.10899544628148686]
	TIME [epoch: 24.9 sec]
EPOCH 1038/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09392049056037312		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09392049056037312 | validation: 0.1674005350316879]
	TIME [epoch: 25 sec]
EPOCH 1039/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12082941027883497		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12082941027883497 | validation: 0.12809886307693796]
	TIME [epoch: 24.9 sec]
EPOCH 1040/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1438729270199339		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1438729270199339 | validation: 0.24134152221387697]
	TIME [epoch: 25 sec]
EPOCH 1041/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1307152175618434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1307152175618434 | validation: 0.0994983198759299]
	TIME [epoch: 25 sec]
EPOCH 1042/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10686244684372864		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10686244684372864 | validation: 0.13069421239619144]
	TIME [epoch: 24.9 sec]
EPOCH 1043/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08687555847832759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08687555847832759 | validation: 0.09789078397914748]
	TIME [epoch: 25 sec]
EPOCH 1044/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08872027666688283		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08872027666688283 | validation: 0.11343153687642316]
	TIME [epoch: 25 sec]
EPOCH 1045/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08359744566163521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08359744566163521 | validation: 0.1035256362931639]
	TIME [epoch: 25 sec]
EPOCH 1046/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10611850324849258		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10611850324849258 | validation: 0.18224301134789664]
	TIME [epoch: 25 sec]
EPOCH 1047/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10320398256654935		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10320398256654935 | validation: 0.07795889955955704]
	TIME [epoch: 25 sec]
EPOCH 1048/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10458032632896107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10458032632896107 | validation: 0.1536333732429497]
	TIME [epoch: 25 sec]
EPOCH 1049/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08877272782958802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08877272782958802 | validation: 0.11498065031387711]
	TIME [epoch: 25 sec]
EPOCH 1050/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10470746592210156		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10470746592210156 | validation: 0.12736699169632187]
	TIME [epoch: 25 sec]
EPOCH 1051/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12445393226828802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12445393226828802 | validation: 0.12093182834670752]
	TIME [epoch: 25 sec]
EPOCH 1052/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11024697398062863		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11024697398062863 | validation: 0.06802954723020876]
	TIME [epoch: 24.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1052.pth
	Model improved!!!
EPOCH 1053/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06871175496576552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06871175496576552 | validation: 0.06689930622884274]
	TIME [epoch: 25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1053.pth
	Model improved!!!
EPOCH 1054/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0671879370888945		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0671879370888945 | validation: 0.14590558349307509]
	TIME [epoch: 25 sec]
EPOCH 1055/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09623189711025827		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09623189711025827 | validation: 0.15356420156707773]
	TIME [epoch: 25.1 sec]
EPOCH 1056/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.19510114966020495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.19510114966020495 | validation: 0.17535893950807885]
	TIME [epoch: 25 sec]
EPOCH 1057/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10836047289254955		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10836047289254955 | validation: 0.10317300539173842]
	TIME [epoch: 25.1 sec]
EPOCH 1058/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08112387883405779		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08112387883405779 | validation: 0.11015144574894585]
	TIME [epoch: 25 sec]
EPOCH 1059/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09372603761925122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09372603761925122 | validation: 0.15815323644027032]
	TIME [epoch: 25 sec]
EPOCH 1060/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10189122123281502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10189122123281502 | validation: 0.10085784843302321]
	TIME [epoch: 24.8 sec]
EPOCH 1061/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09443998707524667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09443998707524667 | validation: 0.11103690365011143]
	TIME [epoch: 25 sec]
EPOCH 1062/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11251115576471371		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11251115576471371 | validation: 0.2869897468338471]
	TIME [epoch: 24.9 sec]
EPOCH 1063/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15633370699813717		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15633370699813717 | validation: 0.1387460796746617]
	TIME [epoch: 24.9 sec]
EPOCH 1064/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16432620812769855		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16432620812769855 | validation: 0.10241905176893636]
	TIME [epoch: 25 sec]
EPOCH 1065/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0970058926315529		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0970058926315529 | validation: 0.1232794091648237]
	TIME [epoch: 24.9 sec]
EPOCH 1066/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0961379173333788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0961379173333788 | validation: 0.15698528796653477]
	TIME [epoch: 25 sec]
EPOCH 1067/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10184948196638505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10184948196638505 | validation: 0.13772861376071832]
	TIME [epoch: 25 sec]
EPOCH 1068/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14289158922768347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14289158922768347 | validation: 0.20442809369595075]
	TIME [epoch: 25 sec]
EPOCH 1069/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11181036091882726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11181036091882726 | validation: 0.07233192687511078]
	TIME [epoch: 25 sec]
EPOCH 1070/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07637882112678528		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07637882112678528 | validation: 0.09136700684335974]
	TIME [epoch: 25 sec]
EPOCH 1071/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07649818155151182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07649818155151182 | validation: 0.09942685452066323]
	TIME [epoch: 25 sec]
EPOCH 1072/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508680314193767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09508680314193767 | validation: 0.12397400934377244]
	TIME [epoch: 25 sec]
EPOCH 1073/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1051941505818978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1051941505818978 | validation: 0.1254173316468967]
	TIME [epoch: 25.1 sec]
EPOCH 1074/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12626674111641417		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12626674111641417 | validation: 0.2232777751503087]
	TIME [epoch: 25.1 sec]
EPOCH 1075/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13077222803898522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13077222803898522 | validation: 0.11301143609905125]
	TIME [epoch: 25 sec]
EPOCH 1076/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.145560709651193		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.145560709651193 | validation: 0.1262802254088363]
	TIME [epoch: 24.9 sec]
EPOCH 1077/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08788643199197711		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08788643199197711 | validation: 0.10673002267911547]
	TIME [epoch: 25 sec]
EPOCH 1078/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08514559646284052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08514559646284052 | validation: 0.08786688127180907]
	TIME [epoch: 25 sec]
EPOCH 1079/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09006097699266526		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09006097699266526 | validation: 0.10748578264683582]
	TIME [epoch: 25.1 sec]
EPOCH 1080/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08694562061067909		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08694562061067909 | validation: 0.07391883464267444]
	TIME [epoch: 25.1 sec]
EPOCH 1081/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07741640366302101		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07741640366302101 | validation: 0.08872775288416176]
	TIME [epoch: 25.1 sec]
EPOCH 1082/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07504698373104453		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07504698373104453 | validation: 0.07640343990332128]
	TIME [epoch: 25 sec]
EPOCH 1083/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07905812608272021		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07905812608272021 | validation: 0.10915157213588063]
	TIME [epoch: 25.1 sec]
EPOCH 1084/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11246968881667285		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11246968881667285 | validation: 0.35051010862580967]
	TIME [epoch: 24.9 sec]
EPOCH 1085/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17460227579413318		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17460227579413318 | validation: 0.1937621405529116]
	TIME [epoch: 25.1 sec]
EPOCH 1086/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18422130157370983		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18422130157370983 | validation: 0.11217487235643342]
	TIME [epoch: 25 sec]
EPOCH 1087/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12836134951773745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12836134951773745 | validation: 0.20032243105406433]
	TIME [epoch: 25 sec]
EPOCH 1088/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1422035105043663		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1422035105043663 | validation: 0.15051846147434533]
	TIME [epoch: 24.9 sec]
EPOCH 1089/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1488145827292036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1488145827292036 | validation: 0.12446863799876738]
	TIME [epoch: 25.1 sec]
EPOCH 1090/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11086139220452826		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11086139220452826 | validation: 0.07186089068456052]
	TIME [epoch: 24.9 sec]
EPOCH 1091/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06990575444958241		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06990575444958241 | validation: 0.08953677963846002]
	TIME [epoch: 25 sec]
EPOCH 1092/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07445647912492324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07445647912492324 | validation: 0.07046902631408399]
	TIME [epoch: 25 sec]
EPOCH 1093/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06605081832640501		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06605081832640501 | validation: 0.07102734847827764]
	TIME [epoch: 25.1 sec]
EPOCH 1094/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06652209237684985		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06652209237684985 | validation: 0.09036320735038861]
	TIME [epoch: 25 sec]
EPOCH 1095/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07793008023341927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07793008023341927 | validation: 0.07419829585047621]
	TIME [epoch: 25 sec]
EPOCH 1096/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07447105816576181		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07447105816576181 | validation: 0.07945294560844095]
	TIME [epoch: 25.1 sec]
EPOCH 1097/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600890078700952		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600890078700952 | validation: 0.07807036903572909]
	TIME [epoch: 24.9 sec]
EPOCH 1098/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07810676226384015		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07810676226384015 | validation: 0.09304809025775991]
	TIME [epoch: 25 sec]
EPOCH 1099/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10012817703414467		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10012817703414467 | validation: 0.3238128814262897]
	TIME [epoch: 25 sec]
EPOCH 1100/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16203309446351938		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16203309446351938 | validation: 0.09309983096657]
	TIME [epoch: 25 sec]
EPOCH 1101/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11010340423566031		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11010340423566031 | validation: 0.10098824380394072]
	TIME [epoch: 25 sec]
EPOCH 1102/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09329509476048013		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09329509476048013 | validation: 0.09751539611797937]
	TIME [epoch: 25 sec]
EPOCH 1103/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09048782817674905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09048782817674905 | validation: 0.108598575815387]
	TIME [epoch: 25.1 sec]
EPOCH 1104/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07966198044790977		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07966198044790977 | validation: 0.09175444926279021]
	TIME [epoch: 25 sec]
EPOCH 1105/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09390852241729886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09390852241729886 | validation: 0.10527263607988387]
	TIME [epoch: 25 sec]
EPOCH 1106/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0798759830006865		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0798759830006865 | validation: 0.08187896778720909]
	TIME [epoch: 25 sec]
EPOCH 1107/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0788897003801429		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0788897003801429 | validation: 0.20607259326762675]
	TIME [epoch: 25 sec]
EPOCH 1108/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10141610062266168		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10141610062266168 | validation: 0.08097270614323737]
	TIME [epoch: 25 sec]
EPOCH 1109/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08966913022825324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08966913022825324 | validation: 0.12364113536802256]
	TIME [epoch: 24.9 sec]
EPOCH 1110/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07533884473351914		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07533884473351914 | validation: 0.07880790815956945]
	TIME [epoch: 25 sec]
EPOCH 1111/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07892644643854056		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07892644643854056 | validation: 0.09737007336914728]
	TIME [epoch: 25 sec]
EPOCH 1112/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09358966020126347		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09358966020126347 | validation: 0.11913976132518592]
	TIME [epoch: 25 sec]
EPOCH 1113/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11629757800065861		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11629757800065861 | validation: 0.1338193715011955]
	TIME [epoch: 25 sec]
EPOCH 1114/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10004543727318485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10004543727318485 | validation: 0.09145332926834376]
	TIME [epoch: 25 sec]
EPOCH 1115/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10853069588386713		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10853069588386713 | validation: 0.23171549606895891]
	TIME [epoch: 25 sec]
EPOCH 1116/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11494562082093601		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11494562082093601 | validation: 0.09287142988368444]
	TIME [epoch: 25.1 sec]
EPOCH 1117/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11913251772858564		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11913251772858564 | validation: 0.2042630280658731]
	TIME [epoch: 25 sec]
EPOCH 1118/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11276924903619336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11276924903619336 | validation: 0.1570161659126885]
	TIME [epoch: 24.9 sec]
EPOCH 1119/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12774675038620498		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12774675038620498 | validation: 0.08453549701888313]
	TIME [epoch: 24.9 sec]
EPOCH 1120/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10552412499034494		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10552412499034494 | validation: 0.05918640441509442]
	TIME [epoch: 25.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1120.pth
	Model improved!!!
EPOCH 1121/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0731468868372673		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0731468868372673 | validation: 0.08086668836063633]
	TIME [epoch: 24.9 sec]
EPOCH 1122/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06787917417986361		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06787917417986361 | validation: 0.10842441379432884]
	TIME [epoch: 25 sec]
EPOCH 1123/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10643804138135458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10643804138135458 | validation: 0.16041322319588724]
	TIME [epoch: 25 sec]
EPOCH 1124/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10656203125757381		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10656203125757381 | validation: 0.07841438940136554]
	TIME [epoch: 24.9 sec]
EPOCH 1125/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0756900479201167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0756900479201167 | validation: 0.08323331188571031]
	TIME [epoch: 24.9 sec]
EPOCH 1126/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09120456324988045		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09120456324988045 | validation: 0.11819125743090082]
	TIME [epoch: 25 sec]
EPOCH 1127/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09286979373573548		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09286979373573548 | validation: 0.06274161233183173]
	TIME [epoch: 24.9 sec]
EPOCH 1128/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06972374662693788		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06972374662693788 | validation: 0.06480939666760874]
	TIME [epoch: 25 sec]
EPOCH 1129/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05621330443490596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05621330443490596 | validation: 0.09766980801865627]
	TIME [epoch: 24.9 sec]
EPOCH 1130/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07309259314839137		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07309259314839137 | validation: 0.11431603735453005]
	TIME [epoch: 24.9 sec]
EPOCH 1131/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12017723840941846		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12017723840941846 | validation: 0.23921288086736123]
	TIME [epoch: 25 sec]
EPOCH 1132/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13073253168591836		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13073253168591836 | validation: 0.09330177860069981]
	TIME [epoch: 25 sec]
EPOCH 1133/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09048569029882536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09048569029882536 | validation: 0.08331471678020469]
	TIME [epoch: 24.9 sec]
EPOCH 1134/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07477459049707534		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07477459049707534 | validation: 0.10501685320260012]
	TIME [epoch: 24.9 sec]
EPOCH 1135/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08523355221283635		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08523355221283635 | validation: 0.1486551028449637]
	TIME [epoch: 24.9 sec]
EPOCH 1136/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09773807570331217		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09773807570331217 | validation: 0.12963814663411574]
	TIME [epoch: 25 sec]
EPOCH 1137/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1443519951319599		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1443519951319599 | validation: 0.2950788455243238]
	TIME [epoch: 25 sec]
EPOCH 1138/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13740781398341434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13740781398341434 | validation: 0.06901958666510748]
	TIME [epoch: 25 sec]
EPOCH 1139/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08387927060219992		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08387927060219992 | validation: 0.1056313319579479]
	TIME [epoch: 25 sec]
EPOCH 1140/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0838143122589236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0838143122589236 | validation: 0.0799599457672197]
	TIME [epoch: 24.9 sec]
EPOCH 1141/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08042704196119502		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08042704196119502 | validation: 0.08326762177144365]
	TIME [epoch: 25 sec]
EPOCH 1142/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07753586015613284		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07753586015613284 | validation: 0.0839155912195384]
	TIME [epoch: 24.9 sec]
EPOCH 1143/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09053403028330731		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09053403028330731 | validation: 0.17225911495217947]
	TIME [epoch: 25.1 sec]
EPOCH 1144/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09860753099273682		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09860753099273682 | validation: 0.07652881212501096]
	TIME [epoch: 25 sec]
EPOCH 1145/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08562536867052338		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08562536867052338 | validation: 0.09733131915757318]
	TIME [epoch: 25 sec]
EPOCH 1146/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08006365049628812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08006365049628812 | validation: 0.10251811353633605]
	TIME [epoch: 25 sec]
EPOCH 1147/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10255665686793929		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10255665686793929 | validation: 0.1128488784033978]
	TIME [epoch: 25.1 sec]
EPOCH 1148/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08939633949101328		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08939633949101328 | validation: 0.09838929682371712]
	TIME [epoch: 25 sec]
EPOCH 1149/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1005110386326269		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1005110386326269 | validation: 0.27720845323041327]
	TIME [epoch: 25 sec]
EPOCH 1150/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13481344834340367		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13481344834340367 | validation: 0.07417338995637068]
	TIME [epoch: 25 sec]
EPOCH 1151/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08501197396700363		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08501197396700363 | validation: 0.05359652904980301]
	TIME [epoch: 25 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1151.pth
	Model improved!!!
EPOCH 1152/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06147785553571394		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06147785553571394 | validation: 0.0674979371085198]
	TIME [epoch: 25 sec]
EPOCH 1153/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06519624440409916		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06519624440409916 | validation: 0.08645686847470968]
	TIME [epoch: 25.2 sec]
EPOCH 1154/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07978301707868454		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07978301707868454 | validation: 0.09610195971230323]
	TIME [epoch: 25 sec]
EPOCH 1155/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09129941111356775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09129941111356775 | validation: 0.10628627841929617]
	TIME [epoch: 25 sec]
EPOCH 1156/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0819661365223696		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0819661365223696 | validation: 0.09176576222693572]
	TIME [epoch: 24.9 sec]
EPOCH 1157/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0854424315133052		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0854424315133052 | validation: 0.12411801577964088]
	TIME [epoch: 25 sec]
EPOCH 1158/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07865402574454812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07865402574454812 | validation: 0.0796468169389954]
	TIME [epoch: 25.1 sec]
EPOCH 1159/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0716287359805433		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0716287359805433 | validation: 0.12747533049885731]
	TIME [epoch: 25.1 sec]
EPOCH 1160/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07612371456771204		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07612371456771204 | validation: 0.08892009636829151]
	TIME [epoch: 25 sec]
EPOCH 1161/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09279000354459667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09279000354459667 | validation: 0.1761797671113506]
	TIME [epoch: 25 sec]
EPOCH 1162/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09959452363336194		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09959452363336194 | validation: 0.11036197403282846]
	TIME [epoch: 25 sec]
EPOCH 1163/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09859521544062391		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09859521544062391 | validation: 0.10789582166827204]
	TIME [epoch: 25.1 sec]
EPOCH 1164/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12418468287432		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12418468287432 | validation: 0.15362258391480163]
	TIME [epoch: 25 sec]
EPOCH 1165/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10298989257233344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10298989257233344 | validation: 0.06105425576716394]
	TIME [epoch: 25.1 sec]
EPOCH 1166/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06309303950590796		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06309303950590796 | validation: 0.07320276859394119]
	TIME [epoch: 25.1 sec]
EPOCH 1167/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061263116081896225		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061263116081896225 | validation: 0.17478293260223787]
	TIME [epoch: 24.9 sec]
EPOCH 1168/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10458373656964451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10458373656964451 | validation: 0.17709903057539653]
	TIME [epoch: 25 sec]
EPOCH 1169/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17806831202863266		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17806831202863266 | validation: 0.27818700101094057]
	TIME [epoch: 25 sec]
EPOCH 1170/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13451409582565035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13451409582565035 | validation: 0.11505029877561786]
	TIME [epoch: 25.1 sec]
EPOCH 1171/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07954988268603504		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07954988268603504 | validation: 0.09991803202237647]
	TIME [epoch: 25 sec]
EPOCH 1172/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10158322404974786		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10158322404974786 | validation: 0.11252602406171414]
	TIME [epoch: 25 sec]
EPOCH 1173/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08828207598376597		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08828207598376597 | validation: 0.11459877873282741]
	TIME [epoch: 24.9 sec]
EPOCH 1174/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09886865913892105		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09886865913892105 | validation: 0.08604068991912009]
	TIME [epoch: 25 sec]
EPOCH 1175/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09137042204068824		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09137042204068824 | validation: 0.09305727106958424]
	TIME [epoch: 24.9 sec]
EPOCH 1176/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08672321345022439		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08672321345022439 | validation: 0.10418316551377589]
	TIME [epoch: 25 sec]
EPOCH 1177/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08001718421070103		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08001718421070103 | validation: 0.09805248309499853]
	TIME [epoch: 25 sec]
EPOCH 1178/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08448388522784915		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08448388522784915 | validation: 0.12170411952958969]
	TIME [epoch: 25 sec]
EPOCH 1179/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07532161284875484		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07532161284875484 | validation: 0.13582978463659698]
	TIME [epoch: 25 sec]
EPOCH 1180/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1399963864879842		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1399963864879842 | validation: 0.311261263831776]
	TIME [epoch: 24.9 sec]
EPOCH 1181/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15932617162121082		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15932617162121082 | validation: 1.2034417920085059]
	TIME [epoch: 24.8 sec]
EPOCH 1182/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.9535561077612004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.9535561077612004 | validation: 0.9510107945445313]
	TIME [epoch: 24.9 sec]
EPOCH 1183/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.7619427187715706		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.7619427187715706 | validation: 0.4972848961829103]
	TIME [epoch: 24.8 sec]
EPOCH 1184/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.4894266442761574		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.4894266442761574 | validation: 0.3424261648723853]
	TIME [epoch: 24.9 sec]
EPOCH 1185/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.310959921183521		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.310959921183521 | validation: 0.2923012405914246]
	TIME [epoch: 24.9 sec]
EPOCH 1186/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24912828933671421		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24912828933671421 | validation: 0.2534079386963942]
	TIME [epoch: 24.9 sec]
EPOCH 1187/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.20991369861281822		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.20991369861281822 | validation: 0.23239201581227997]
	TIME [epoch: 24.9 sec]
EPOCH 1188/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17525933063449348		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17525933063449348 | validation: 0.18432181193033514]
	TIME [epoch: 25 sec]
EPOCH 1189/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.16562976397954626		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.16562976397954626 | validation: 0.20460979671578433]
	TIME [epoch: 24.8 sec]
EPOCH 1190/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1479521817357399		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1479521817357399 | validation: 0.16382064453371326]
	TIME [epoch: 24.9 sec]
EPOCH 1191/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12670915177324715		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12670915177324715 | validation: 0.14269042483156463]
	TIME [epoch: 24.8 sec]
EPOCH 1192/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12211179140389533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12211179140389533 | validation: 0.18007902991630953]
	TIME [epoch: 25 sec]
EPOCH 1193/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10903808431009317		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10903808431009317 | validation: 0.12854141946626713]
	TIME [epoch: 24.9 sec]
EPOCH 1194/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0907004726770106		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0907004726770106 | validation: 0.10679006266844818]
	TIME [epoch: 25 sec]
EPOCH 1195/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0841379480258641		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0841379480258641 | validation: 0.12050990390948156]
	TIME [epoch: 24.9 sec]
EPOCH 1196/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07845765107247415		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07845765107247415 | validation: 0.10563305827308317]
	TIME [epoch: 25 sec]
EPOCH 1197/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07335518571942913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07335518571942913 | validation: 0.06956039323167944]
	TIME [epoch: 24.8 sec]
EPOCH 1198/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08066823689683389		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08066823689683389 | validation: 0.1340924440601032]
	TIME [epoch: 24.9 sec]
EPOCH 1199/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08151101881993675		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08151101881993675 | validation: 0.07020051921710976]
	TIME [epoch: 24.8 sec]
EPOCH 1200/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06425813835593176		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06425813835593176 | validation: 0.11335390111155828]
	TIME [epoch: 24.9 sec]
EPOCH 1201/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11556800684962486		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11556800684962486 | validation: 0.10633674658781396]
	TIME [epoch: 25 sec]
EPOCH 1202/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08132721980466745		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08132721980466745 | validation: 0.08263745888171788]
	TIME [epoch: 24.9 sec]
EPOCH 1203/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07829687389343187		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07829687389343187 | validation: 0.10030280921366944]
	TIME [epoch: 25 sec]
EPOCH 1204/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06632303319120282		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06632303319120282 | validation: 0.10221205981244795]
	TIME [epoch: 25 sec]
EPOCH 1205/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10393786000400777		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10393786000400777 | validation: 0.16049591337422553]
	TIME [epoch: 25.1 sec]
EPOCH 1206/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.12206001657729008		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.12206001657729008 | validation: 0.09347600366098266]
	TIME [epoch: 25 sec]
EPOCH 1207/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10155692874168906		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10155692874168906 | validation: 0.0765957712321898]
	TIME [epoch: 25 sec]
EPOCH 1208/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07299569916623547		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07299569916623547 | validation: 0.08431956990192213]
	TIME [epoch: 25.1 sec]
EPOCH 1209/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06359617762579582		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06359617762579582 | validation: 0.07609666847513198]
	TIME [epoch: 25.1 sec]
EPOCH 1210/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061557122217726364		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061557122217726364 | validation: 0.06422591121611658]
	TIME [epoch: 25.1 sec]
EPOCH 1211/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07258192459838295		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07258192459838295 | validation: 0.11579573877565941]
	TIME [epoch: 25 sec]
EPOCH 1212/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07868471728539161		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07868471728539161 | validation: 0.08953198128924084]
	TIME [epoch: 25.1 sec]
EPOCH 1213/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09251758915455838		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09251758915455838 | validation: 0.15450768902420356]
	TIME [epoch: 25 sec]
EPOCH 1214/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09539395976089576		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09539395976089576 | validation: 0.07558931168931973]
	TIME [epoch: 25 sec]
EPOCH 1215/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07904925977496964		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07904925977496964 | validation: 0.08137027355514694]
	TIME [epoch: 25.1 sec]
EPOCH 1216/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07186145885759093		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07186145885759093 | validation: 0.07825067495376586]
	TIME [epoch: 25.1 sec]
EPOCH 1217/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07118996822109726		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07118996822109726 | validation: 0.07312892395561107]
	TIME [epoch: 24.9 sec]
EPOCH 1218/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07199204914449503		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07199204914449503 | validation: 0.08716545335094812]
	TIME [epoch: 24.9 sec]
EPOCH 1219/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08022635427526567		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08022635427526567 | validation: 0.07156932786792707]
	TIME [epoch: 24.9 sec]
EPOCH 1220/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06610132178215096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06610132178215096 | validation: 0.07211983165953778]
	TIME [epoch: 24.9 sec]
EPOCH 1221/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07221171112320278		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07221171112320278 | validation: 0.07649046262435963]
	TIME [epoch: 24.9 sec]
EPOCH 1222/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06592950419096		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06592950419096 | validation: 0.05913081281007574]
	TIME [epoch: 25 sec]
EPOCH 1223/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06981348678843127		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06981348678843127 | validation: 0.12495039789678111]
	TIME [epoch: 24.9 sec]
EPOCH 1224/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08088757319572037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08088757319572037 | validation: 0.08914791451893675]
	TIME [epoch: 24.9 sec]
EPOCH 1225/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10813994145519072		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10813994145519072 | validation: 0.26523568845110923]
	TIME [epoch: 24.9 sec]
EPOCH 1226/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14788699588702872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14788699588702872 | validation: 0.07331937161661678]
	TIME [epoch: 25.1 sec]
EPOCH 1227/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09115226971363452		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09115226971363452 | validation: 0.05399334820335932]
	TIME [epoch: 24.9 sec]
EPOCH 1228/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06370190506974523		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06370190506974523 | validation: 0.08848110434881105]
	TIME [epoch: 24.9 sec]
EPOCH 1229/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08649992511560978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08649992511560978 | validation: 0.08981636024268205]
	TIME [epoch: 25 sec]
EPOCH 1230/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892409641534088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0892409641534088 | validation: 0.0873283523209715]
	TIME [epoch: 25.1 sec]
EPOCH 1231/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09879227778300816		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09879227778300816 | validation: 0.11346455079918681]
	TIME [epoch: 24.9 sec]
EPOCH 1232/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07787771818789327		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07787771818789327 | validation: 0.060091391980917945]
	TIME [epoch: 24.9 sec]
EPOCH 1233/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057718536007471344		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057718536007471344 | validation: 0.048822637300522304]
	TIME [epoch: 24.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1233.pth
	Model improved!!!
EPOCH 1234/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.053208164628432274		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.053208164628432274 | validation: 0.06288409234733615]
	TIME [epoch: 25 sec]
EPOCH 1235/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05351787889825271		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05351787889825271 | validation: 0.07238258691084565]
	TIME [epoch: 25 sec]
EPOCH 1236/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0658221019612022		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0658221019612022 | validation: 0.1289887396755062]
	TIME [epoch: 25 sec]
EPOCH 1237/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13329236307168596		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13329236307168596 | validation: 0.20990441070593016]
	TIME [epoch: 24.9 sec]
EPOCH 1238/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11628895199325413		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11628895199325413 | validation: 0.10720655488224103]
	TIME [epoch: 25 sec]
EPOCH 1239/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09314261950029451		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09314261950029451 | validation: 0.08667496738722154]
	TIME [epoch: 24.8 sec]
EPOCH 1240/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08892463654970933		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08892463654970933 | validation: 0.09600410726631219]
	TIME [epoch: 25 sec]
EPOCH 1241/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07980031819875937		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07980031819875937 | validation: 0.07349288891324421]
	TIME [epoch: 24.8 sec]
EPOCH 1242/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.077049253377645		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.077049253377645 | validation: 0.11201424786839027]
	TIME [epoch: 25 sec]
EPOCH 1243/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09955488167972959		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09955488167972959 | validation: 0.09623407305804144]
	TIME [epoch: 24.8 sec]
EPOCH 1244/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09031160904687802		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09031160904687802 | validation: 0.10301245858250288]
	TIME [epoch: 25 sec]
EPOCH 1245/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.102256012301748		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.102256012301748 | validation: 0.15508284208866632]
	TIME [epoch: 24.9 sec]
EPOCH 1246/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1166785576506505		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1166785576506505 | validation: 0.11811518412707478]
	TIME [epoch: 24.9 sec]
EPOCH 1247/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14093633114919182		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14093633114919182 | validation: 0.1794773319498953]
	TIME [epoch: 24.9 sec]
EPOCH 1248/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11991025135324847		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11991025135324847 | validation: 0.11867505257588383]
	TIME [epoch: 25 sec]
EPOCH 1249/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11140041937358533		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11140041937358533 | validation: 0.25484271834501454]
	TIME [epoch: 25 sec]
EPOCH 1250/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.26208618802790684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.26208618802790684 | validation: 0.24535449842375168]
	TIME [epoch: 25 sec]
EPOCH 1251/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.22981128277143423		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.22981128277143423 | validation: 0.18451687111940576]
	TIME [epoch: 25 sec]
EPOCH 1252/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.24201752287790654		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.24201752287790654 | validation: 0.1554198072997456]
	TIME [epoch: 25 sec]
EPOCH 1253/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1781715500764234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1781715500764234 | validation: 0.16884287110465554]
	TIME [epoch: 24.9 sec]
EPOCH 1254/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.18742656936213817		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.18742656936213817 | validation: 0.1771629821784955]
	TIME [epoch: 25 sec]
EPOCH 1255/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15652429937854492		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15652429937854492 | validation: 0.16373305855566034]
	TIME [epoch: 24.8 sec]
EPOCH 1256/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17985749410726043		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17985749410726043 | validation: 0.3470178369996278]
	TIME [epoch: 25 sec]
EPOCH 1257/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1800281174147049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1800281174147049 | validation: 0.21898585038242968]
	TIME [epoch: 24.8 sec]
EPOCH 1258/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.23330342989341765		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.23330342989341765 | validation: 0.14126246769444792]
	TIME [epoch: 25 sec]
EPOCH 1259/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14808147309685346		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14808147309685346 | validation: 0.10982778221051342]
	TIME [epoch: 24.9 sec]
EPOCH 1260/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1163741552249871		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1163741552249871 | validation: 0.12028208105245604]
	TIME [epoch: 25 sec]
EPOCH 1261/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11272854033267042		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11272854033267042 | validation: 0.1252259445129983]
	TIME [epoch: 25 sec]
EPOCH 1262/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14119764825202552		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14119764825202552 | validation: 0.3673054087605908]
	TIME [epoch: 25 sec]
EPOCH 1263/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.17709766895832893		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.17709766895832893 | validation: 0.114878374538063]
	TIME [epoch: 24.9 sec]
EPOCH 1264/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.13314457986126704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.13314457986126704 | validation: 0.07694418434333072]
	TIME [epoch: 24.9 sec]
EPOCH 1265/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08703993787602485		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08703993787602485 | validation: 0.0845613026396335]
	TIME [epoch: 24.8 sec]
EPOCH 1266/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08355364375681122		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08355364375681122 | validation: 0.07448816922462473]
	TIME [epoch: 24.8 sec]
EPOCH 1267/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0714620271092476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0714620271092476 | validation: 0.11109792581842842]
	TIME [epoch: 24.9 sec]
EPOCH 1268/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09994879751069148		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09994879751069148 | validation: 0.1687155174899585]
	TIME [epoch: 24.8 sec]
EPOCH 1269/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11343296027527044		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11343296027527044 | validation: 0.10883452698983934]
	TIME [epoch: 24.8 sec]
EPOCH 1270/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11038007394202913		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11038007394202913 | validation: 0.08148340862527748]
	TIME [epoch: 24.9 sec]
EPOCH 1271/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0752826446749797		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0752826446749797 | validation: 0.09090656486762072]
	TIME [epoch: 24.8 sec]
EPOCH 1272/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.078192746938398		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.078192746938398 | validation: 0.10219533882079866]
	TIME [epoch: 24.9 sec]
EPOCH 1273/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07600772095701476		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07600772095701476 | validation: 0.08486456926863108]
	TIME [epoch: 24.8 sec]
EPOCH 1274/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09491120372218495		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09491120372218495 | validation: 0.1549956525006094]
	TIME [epoch: 24.9 sec]
EPOCH 1275/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09399311448969727		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09399311448969727 | validation: 0.08128244166028653]
	TIME [epoch: 24.9 sec]
EPOCH 1276/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07848930427742594		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07848930427742594 | validation: 0.06535684756438663]
	TIME [epoch: 24.9 sec]
EPOCH 1277/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07157233876324108		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07157233876324108 | validation: 0.1656659265506446]
	TIME [epoch: 24.8 sec]
EPOCH 1278/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09707883840007316		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09707883840007316 | validation: 0.07117010452154653]
	TIME [epoch: 24.9 sec]
EPOCH 1279/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0780648114348012		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0780648114348012 | validation: 0.07505583428253548]
	TIME [epoch: 24.8 sec]
EPOCH 1280/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07449503044502215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07449503044502215 | validation: 0.12929065100767576]
	TIME [epoch: 24.9 sec]
EPOCH 1281/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09192256564935049		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09192256564935049 | validation: 0.10825131035072562]
	TIME [epoch: 24.8 sec]
EPOCH 1282/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1123423236354466		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1123423236354466 | validation: 0.09823135233494816]
	TIME [epoch: 24.9 sec]
EPOCH 1283/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07718312210753872		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07718312210753872 | validation: 0.056012492893420474]
	TIME [epoch: 24.8 sec]
EPOCH 1284/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05378515109818121		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05378515109818121 | validation: 0.050953877779468526]
	TIME [epoch: 24.9 sec]
EPOCH 1285/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05846177476682458		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05846177476682458 | validation: 0.0708384686053573]
	TIME [epoch: 24.9 sec]
EPOCH 1286/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06118789635729037		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06118789635729037 | validation: 0.10948924117180653]
	TIME [epoch: 25 sec]
EPOCH 1287/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07991106794432605		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07991106794432605 | validation: 0.131870145965406]
	TIME [epoch: 24.8 sec]
EPOCH 1288/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1236449101786684		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1236449101786684 | validation: 0.14389833915558495]
	TIME [epoch: 25 sec]
EPOCH 1289/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.10948022172948749		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.10948022172948749 | validation: 0.09759887945705331]
	TIME [epoch: 24.9 sec]
EPOCH 1290/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07011018290115488		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07011018290115488 | validation: 0.05328440037573018]
	TIME [epoch: 24.9 sec]
EPOCH 1291/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06561793172169665		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06561793172169665 | validation: 0.9208211048801925]
	TIME [epoch: 25 sec]
EPOCH 1292/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.6759696497353084		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.6759696497353084 | validation: 0.31937596923857764]
	TIME [epoch: 24.9 sec]
EPOCH 1293/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.25008498879181107		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.25008498879181107 | validation: 0.25597668546024044]
	TIME [epoch: 25 sec]
EPOCH 1294/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.14977530549064869		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.14977530549064869 | validation: 0.16968600127379987]
	TIME [epoch: 25 sec]
EPOCH 1295/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.15180387942462656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.15180387942462656 | validation: 0.0876922803155471]
	TIME [epoch: 25 sec]
EPOCH 1296/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094047417741814		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.094047417741814 | validation: 0.04803522359330703]
	TIME [epoch: 24.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1296.pth
	Model improved!!!
EPOCH 1297/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0576773132355215		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0576773132355215 | validation: 0.042222881759013564]
	TIME [epoch: 24.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1297.pth
	Model improved!!!
EPOCH 1298/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051530300977460475		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051530300977460475 | validation: 0.024102631575594315]
	TIME [epoch: 24.9 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1298.pth
	Model improved!!!
EPOCH 1299/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03444677170860457		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03444677170860457 | validation: 0.030593450525389334]
	TIME [epoch: 24.9 sec]
EPOCH 1300/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.037276939422045886		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.037276939422045886 | validation: 0.02776537311534152]
	TIME [epoch: 25 sec]
EPOCH 1301/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.034837422774083575		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.034837422774083575 | validation: 0.031201774347877465]
	TIME [epoch: 25 sec]
EPOCH 1302/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.03443805347589067		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.03443805347589067 | validation: 0.07070579708341614]
	TIME [epoch: 24.9 sec]
EPOCH 1303/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06497779346346692		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06497779346346692 | validation: 0.07888384360895145]
	TIME [epoch: 25.1 sec]
EPOCH 1304/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08115585580652292		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08115585580652292 | validation: 0.07862526310182812]
	TIME [epoch: 24.8 sec]
EPOCH 1305/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06803486842376034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06803486842376034 | validation: 0.05618372581065287]
	TIME [epoch: 25.1 sec]
EPOCH 1306/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05618401817988584		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05618401817988584 | validation: 0.060430706933975564]
	TIME [epoch: 24.9 sec]
EPOCH 1307/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05273648850051075		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05273648850051075 | validation: 0.05678857561781055]
	TIME [epoch: 25 sec]
EPOCH 1308/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05516243657836155		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05516243657836155 | validation: 0.1207684382813476]
	TIME [epoch: 25 sec]
EPOCH 1309/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07501355392410973		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07501355392410973 | validation: 0.06695082560665237]
	TIME [epoch: 24.9 sec]
EPOCH 1310/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06995429208157097		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06995429208157097 | validation: 0.07473571705565925]
	TIME [epoch: 24.8 sec]
EPOCH 1311/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0631232447034667		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0631232447034667 | validation: 0.06073433198169786]
	TIME [epoch: 24.8 sec]
EPOCH 1312/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061084006756854894		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061084006756854894 | validation: 0.13286793852569767]
	TIME [epoch: 24.9 sec]
EPOCH 1313/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08397317999018704		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08397317999018704 | validation: 0.0824522254868166]
	TIME [epoch: 24.9 sec]
EPOCH 1314/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07707366099217054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07707366099217054 | validation: 0.07914874844763223]
	TIME [epoch: 24.9 sec]
EPOCH 1315/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06674138925363243		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06674138925363243 | validation: 0.05146813895281548]
	TIME [epoch: 24.8 sec]
EPOCH 1316/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052971793702829936		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052971793702829936 | validation: 0.0474533582440648]
	TIME [epoch: 24.9 sec]
EPOCH 1317/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.051873441539220536		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.051873441539220536 | validation: 0.0743837468166858]
	TIME [epoch: 24.8 sec]
EPOCH 1318/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0723465727696129		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0723465727696129 | validation: 0.19964638262964718]
	TIME [epoch: 24.7 sec]
EPOCH 1319/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11178948971630823		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11178948971630823 | validation: 0.07745541812896242]
	TIME [epoch: 24.9 sec]
EPOCH 1320/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.094262578896483		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.094262578896483 | validation: 0.0689696818501031]
	TIME [epoch: 24.6 sec]
EPOCH 1321/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06660419680461747		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06660419680461747 | validation: 0.06966910783235013]
	TIME [epoch: 24.9 sec]
EPOCH 1322/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06181915679527414		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06181915679527414 | validation: 0.11001485476737718]
	TIME [epoch: 24.7 sec]
EPOCH 1323/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06781138871107975		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06781138871107975 | validation: 0.1215810836157266]
	TIME [epoch: 24.9 sec]
EPOCH 1324/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11109761837324868		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11109761837324868 | validation: 0.21027029305357298]
	TIME [epoch: 24.7 sec]
EPOCH 1325/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11656822546827474		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11656822546827474 | validation: 0.08149264960406974]
	TIME [epoch: 24.8 sec]
EPOCH 1326/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07607932655619522		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07607932655619522 | validation: 0.05518455695151545]
	TIME [epoch: 24.7 sec]
EPOCH 1327/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07243030767647633		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07243030767647633 | validation: 0.06388337108022833]
	TIME [epoch: 24.8 sec]
EPOCH 1328/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05308067973573004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05308067973573004 | validation: 0.15506238010537438]
	TIME [epoch: 24.7 sec]
EPOCH 1329/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08926984600191895		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08926984600191895 | validation: 0.09260832571868396]
	TIME [epoch: 24.8 sec]
EPOCH 1330/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09464579480268771		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09464579480268771 | validation: 0.06635084363078245]
	TIME [epoch: 24.9 sec]
EPOCH 1331/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05694697258700767		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05694697258700767 | validation: 0.06066744015029314]
	TIME [epoch: 25 sec]
EPOCH 1332/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.049972011716782276		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.049972011716782276 | validation: 0.04125353146875656]
	TIME [epoch: 24.7 sec]
EPOCH 1333/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05315820860658054		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05315820860658054 | validation: 0.07546204667494906]
	TIME [epoch: 24.9 sec]
EPOCH 1334/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.061326055932003375		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.061326055932003375 | validation: 0.11038784934553408]
	TIME [epoch: 24.7 sec]
EPOCH 1335/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0892473976511441		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0892473976511441 | validation: 0.08265357685241194]
	TIME [epoch: 24.9 sec]
EPOCH 1336/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0874494060420167		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0874494060420167 | validation: 0.11917404174848811]
	TIME [epoch: 24.8 sec]
EPOCH 1337/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07340480683466759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07340480683466759 | validation: 0.07412559350250625]
	TIME [epoch: 25 sec]
EPOCH 1338/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07149369655931383		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07149369655931383 | validation: 0.07996004724666785]
	TIME [epoch: 24.8 sec]
EPOCH 1339/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06250340138432066		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06250340138432066 | validation: 0.055429326011443084]
	TIME [epoch: 24.9 sec]
EPOCH 1340/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05140594097423324		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05140594097423324 | validation: 0.04468911959224801]
	TIME [epoch: 24.9 sec]
EPOCH 1341/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046289859113795545		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046289859113795545 | validation: 0.050981533062748656]
	TIME [epoch: 24.8 sec]
EPOCH 1342/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.052428203657326033		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.052428203657326033 | validation: 0.06455439278339345]
	TIME [epoch: 24.8 sec]
EPOCH 1343/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06866419251649034		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06866419251649034 | validation: 0.08642712929458936]
	TIME [epoch: 24.7 sec]
EPOCH 1344/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08147459105923759		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08147459105923759 | validation: 0.10853641502351914]
	TIME [epoch: 24.8 sec]
EPOCH 1345/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08073364585728592		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08073364585728592 | validation: 0.12297613991021544]
	TIME [epoch: 24.6 sec]
EPOCH 1346/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11246666536999385		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11246666536999385 | validation: 0.158516945914568]
	TIME [epoch: 24.7 sec]
EPOCH 1347/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11211631565104695		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11211631565104695 | validation: 0.09065205981925721]
	TIME [epoch: 24.8 sec]
EPOCH 1348/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0878086694951656		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0878086694951656 | validation: 0.04559680049295405]
	TIME [epoch: 24.9 sec]
EPOCH 1349/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05520095642884251		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05520095642884251 | validation: 0.07461838472841502]
	TIME [epoch: 24.8 sec]
EPOCH 1350/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06266148354610165		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06266148354610165 | validation: 0.06781725429282284]
	TIME [epoch: 24.8 sec]
EPOCH 1351/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06766827942483088		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06766827942483088 | validation: 0.06945797863960547]
	TIME [epoch: 24.9 sec]
EPOCH 1352/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06281272541429693		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06281272541429693 | validation: 0.05400904319702478]
	TIME [epoch: 24.7 sec]
EPOCH 1353/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047350414278865174		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047350414278865174 | validation: 0.08512278789985213]
	TIME [epoch: 24.8 sec]
EPOCH 1354/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08299092429344927		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08299092429344927 | validation: 0.16290132798805934]
	TIME [epoch: 24.8 sec]
EPOCH 1355/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08143659927256632		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08143659927256632 | validation: 0.04600230424599338]
	TIME [epoch: 24.8 sec]
EPOCH 1356/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.047476893213945104		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.047476893213945104 | validation: 0.04177324144923349]
	TIME [epoch: 24.8 sec]
EPOCH 1357/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04476411497926311		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04476411497926311 | validation: 0.06029265101206277]
	TIME [epoch: 24.9 sec]
EPOCH 1358/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.046740120640686236		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.046740120640686236 | validation: 0.08094753268629251]
	TIME [epoch: 24.8 sec]
EPOCH 1359/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07144516077853565		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07144516077853565 | validation: 0.10408344681464646]
	TIME [epoch: 24.7 sec]
EPOCH 1360/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.11002759335977899		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.11002759335977899 | validation: 0.1769792450638049]
	TIME [epoch: 24.8 sec]
EPOCH 1361/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08292575916672223		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08292575916672223 | validation: 0.06108925873979342]
	TIME [epoch: 24.8 sec]
EPOCH 1362/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05358288040096319		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05358288040096319 | validation: 0.05488025321242887]
	TIME [epoch: 24.7 sec]
EPOCH 1363/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06260294784541634		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06260294784541634 | validation: 0.0945142873129832]
	TIME [epoch: 24.7 sec]
EPOCH 1364/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07939794002396544		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07939794002396544 | validation: 0.1074666552168528]
	TIME [epoch: 24.7 sec]
EPOCH 1365/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08966085102437839		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08966085102437839 | validation: 0.08387510168745495]
	TIME [epoch: 24.8 sec]
EPOCH 1366/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08859770584038708		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08859770584038708 | validation: 0.06784069277982856]
	TIME [epoch: 24.7 sec]
EPOCH 1367/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06543172279752905		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06543172279752905 | validation: 0.03750502009971623]
	TIME [epoch: 24.8 sec]
EPOCH 1368/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04135529143574934		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04135529143574934 | validation: 0.040572674627591565]
	TIME [epoch: 24.8 sec]
EPOCH 1369/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04344863929997004		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04344863929997004 | validation: 0.06315212513370265]
	TIME [epoch: 24.9 sec]
EPOCH 1370/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05854606370604465		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05854606370604465 | validation: 0.08899202247641969]
	TIME [epoch: 24.8 sec]
EPOCH 1371/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07137104780320812		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07137104780320812 | validation: 0.07485647965094439]
	TIME [epoch: 24.8 sec]
EPOCH 1372/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07652599947699336		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07652599947699336 | validation: 0.15771848997115284]
	TIME [epoch: 25 sec]
EPOCH 1373/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07709144893628775		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07709144893628775 | validation: 0.05742052802166828]
	TIME [epoch: 24.9 sec]
EPOCH 1374/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05824210941462232		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05824210941462232 | validation: 0.044350072329297954]
	TIME [epoch: 24.8 sec]
EPOCH 1375/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05029383937485196		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05029383937485196 | validation: 0.0641176512317613]
	TIME [epoch: 24.9 sec]
EPOCH 1376/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05544702981514609		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05544702981514609 | validation: 0.055403934076001395]
	TIME [epoch: 24.9 sec]
EPOCH 1377/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0648079326522234		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0648079326522234 | validation: 0.07049742112655831]
	TIME [epoch: 25 sec]
EPOCH 1378/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.0642648586674151		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.0642648586674151 | validation: 0.06472889612096903]
	TIME [epoch: 24.8 sec]
EPOCH 1379/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05046168053302247		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05046168053302247 | validation: 0.06593920128325846]
	TIME [epoch: 24.9 sec]
EPOCH 1380/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07197671129161963		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07197671129161963 | validation: 0.1924841897151612]
	TIME [epoch: 24.7 sec]
EPOCH 1381/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09508114230156978		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09508114230156978 | validation: 0.04983072075836523]
	TIME [epoch: 24.9 sec]
EPOCH 1382/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.050674927408101186		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.050674927408101186 | validation: 0.04321738803122874]
	TIME [epoch: 24.7 sec]
EPOCH 1383/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.04830723119847944		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.04830723119847944 | validation: 0.05450871271756286]
	TIME [epoch: 25 sec]
EPOCH 1384/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05656970989681206		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05656970989681206 | validation: 0.07926746072170726]
	TIME [epoch: 24.7 sec]
EPOCH 1385/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08517165704414356		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08517165704414356 | validation: 0.07594712439310103]
	TIME [epoch: 24.9 sec]
EPOCH 1386/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07627566166362365		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07627566166362365 | validation: 0.1084985376566535]
	TIME [epoch: 24.7 sec]
EPOCH 1387/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08315118709075907		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08315118709075907 | validation: 0.0951046876057477]
	TIME [epoch: 25 sec]
EPOCH 1388/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.08487747654247763		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.08487747654247763 | validation: 0.09417158204193843]
	TIME [epoch: 24.9 sec]
EPOCH 1389/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.076695309049035		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.076695309049035 | validation: 0.08692400851165548]
	TIME [epoch: 24.9 sec]
EPOCH 1390/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.07674682621217228		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.07674682621217228 | validation: 0.32376495963604823]
	TIME [epoch: 24.8 sec]
EPOCH 1391/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.1402598841221069		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.1402598841221069 | validation: 0.04592378404518982]
	TIME [epoch: 24.9 sec]
EPOCH 1392/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.054757695626713496		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.054757695626713496 | validation: 0.04868258673697961]
	TIME [epoch: 24.7 sec]
EPOCH 1393/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.057305251029760036		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.057305251029760036 | validation: 0.06897881315661723]
	TIME [epoch: 24.9 sec]
EPOCH 1394/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05151188635877207		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05151188635877207 | validation: 0.08076611133209029]
	TIME [epoch: 24.9 sec]
EPOCH 1395/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.06314478510263434		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.06314478510263434 | validation: 0.11492663704325073]
	TIME [epoch: 24.9 sec]
EPOCH 1396/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.09232108141422493		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.09232108141422493 | validation: 0.08878314173039181]
	TIME [epoch: 24.8 sec]
EPOCH 1397/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.075041812882859		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.075041812882859 | validation: 0.06134466778063288]
	TIME [epoch: 24.9 sec]
EPOCH 1398/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.05464939914366843		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.05464939914366843 | validation: 0.03775011563075478]
	TIME [epoch: 25 sec]
EPOCH 1399/2000:
	Training over batches...
		[batch 1/1] avg loss: 0.042834205737147926		[learning rate: 0.01]
	Learning Rate: 0.01
	LOSS [training: 0.042834205737147926 | validation: 0.05669778365452466]
	TIME [epoch: 25.1 sec]
	Saving model to: out/model_training/model_phi1_4c_v_mmd2_20241125_161213/states/model_phi1_4c_v_mmd2_1399.pth
Halted early. No improvement in validation loss for 100 epochs.
Finished training in 19025.037 seconds.
